[
  {
    "app_filing_date": "1998-06-03",
    "appl_id": "09089931",
    "assignees": [
      {
        "city": "Los Gatos",
        "country": "N/A",
        "name": "Silicon Genesis Corporation",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      }
    ],
    "composite_id": "26732223!US-US-06103599",
    "cpc_additional": [
      {
        "cpc_class": "Y10S",
        "cpc_subclass": "438/977",
        "version": "2013-01-01"
      }
    ],
    "cpc_inventive": [
      {
        "cpc_class": "H01L",
        "cpc_subclass": "21/30604",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H01L",
        "cpc_subclass": "21/76254",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "The present invention provides a multilayered wafer 10 such as an SOI wafer having a novel implanted layer. This implanted layer is removable and provides a resulting wafer having a substantially uniform surface. The wafer includes a bulk substrate 11 and an insulating layer 13 formed overlying the bulk substrate 15. A film of semiconductor material is formed overlying the insulating layer. Surface non-uniformities are formed overlying and in the film of semiconductor material. The non-uniformities are implanted, and are bordered by a substantially uniform interface 17 at a selected depth underlying the surface non-uniformities. The substantially uniform interface provides a substantially uniform resulting surface for the SOI wafer.",
      "brief": "(1) BACKGROUND OF THE INVENTION\n\n(2) The present invention relates to the manufacture of integrated circuits. More particularly, the present invention provides a technique for planarizing substrates using a novel implanting technique. This invention is illustrated using, for example, silicon-on-insulator wafers, but can be applied to other types of substrates.\n\n(3) Integrated circuits are fabricated on chips of semiconductor material. These integrated circuits often contain thousands, or even millions, of transistors and other devices. In particular, it is desirable to put as many transistors as possible within a given area of semiconductor because more transistors typically provide greater functionality, and a smaller chip means more chips per wafer and lower costs.\n\n(4) Some integrated circuits are fabricated on a slice or wafer, of single-crystal (monocrystalline) silicon, commonly termed a \"bulk\" silicon wafer. Devices on such \"bulk\" silicon wafer typically are isolated from each other. A variety of techniques have been proposed or used to isolate these devices from each other on the bulk silicon wafer, such as a local oxidation of silicon (\"LOCOS\") process, trench isolation, and others. These techniques, however, are not free from limitations. For example, conventional isolation techniques consume a considerable amount of valuable wafer surface area on the chip, and often generate a non-planar surface as an artifact of the isolation process. Either or both of these considerations generally limit the degree of integration achievable in a given chip. Additionally, trench isolation often requires a process of reactive ion etching, which is extremely time consuming and can be difficult to achieve accurately.\n\n(5) An approach to achieving very-large scale integration (\"VLSI\") or ultra-large scale integration (\"ULSI\") is by using a semiconductor-on-insulator (\"SOI\") wafer. An SOI wafer typically has a layer of silicon on top of a layer of an insulator material. A variety of techniques have been proposed or used for fabricating the SOI wafer. These techniques include, among others, growing a thin layer of silicon on a sapphire substrate, bonding a layer of silicon to an insulating substrate, and forming an insulating layer beneath a silicon layer in a bulk silicon wafer. In an SOI integrated circuit, essentially complete device isolation is often achieved using conventional device processing methods by surrounding each device, including the bottom of the device, with an insulator. An advantage SOI wafers have over bulk silicon wafers is that the area required for isolation between devices on an SOI wafer is less than the area typically required for isolation on a bulk silicon wafer.\n\n(6) SOI offers other advantages over bulk silicon technologies as well. For example, SOI offers a simpler fabrication sequence compared to a bulk silicon wafer. Devices fabricated on an SOI wafer may also have better radiation resistance, less photo-induced current, and less cross-talk than devices fabricated on bulk silicon wafers. Many problems, however, that have already been solved regarding fabricating devices on bulk silicon wafers remain to be solved for fabricating devices on SOI wafers.\n\n(7) Numerous limitations still exist with the use of SOI wafers for the fabrication of integrated circuits. For example, devices within integrated circuits in SOI wafers are very sensitive to the presence of even minute concentrations of some impurities. For example, metals, such as copper, nickel, silver, gold, or iron, within the active region of a device typically degrade several device characteristics, including leakage current and breakdown voltage. These and other metals rapidly diffuse through silicon at temperatures typical of semiconductor device fabrication processes. These impurities often become trapped in the active region of the SOI wafer. That is, the SOI wafer includes a dielectric layer or insulating layer underlying the active region that tends to keep impurities in the active layer, rather than diffusing down into the bulk silicon. Accordingly, SOI wafers are prone to device and reliability problems caused by the presence of impurities that cannot diffuse out of the active region.\n\n(8) SOI wafers generally must also be polished to remove any surface irregularities from the film of silicon overlying the insulating layer. Polishing generally includes, among others, chemical mechanical polishing, commonly termed CMP. CMP is generally time consuming and expensive, and can be difficult to perform cost efficiently to remove surface non-uniformities. That is, a CMP machine is expensive and requires large quantities of slurry mixture, which is also expensive.\n\n(9) Furthermore, the film of silicon on the insulator layer is often polished by way of polishing techniques such as chemical mechanical polishing and others. Polishing is often required to clean the silicon surface and remove any non-uniformities therefrom. Polishing, however, is extremely time consuming and expensive. Polishing also introduces the wafer to a slurry mixture, which can be highly acidic or caustic. Accordingly, the slurry mixture can influence functionality and reliability of devices that are fabricated on the SOI wafer.\n\n(10) From the above, it is seen that an improved technique for manufacturing a substrate such as an SOI wafer is highly desirable.\n\n(11) SUMMARY OF THE INVENTION\n\n(12) According to the present invention, a technique including a method and device for removing surface non-uniformities and impurities from a substrate such as an SOI wafer is provided. In an exemplary embodiment, the technique uses a novel implanting step to provide an implanted layer with surface roughness that can be planarized. Planarization occurs by way of processing steps such as oxidation and selective removal of the non-uniform film.\n\n(13) In a specific embodiment, the present invention provides a method for fabricating a substrate such as a silicon-on-insulator wafer using a novel implanting step, which enhances film uniformity. The method uses a step of providing a semiconductor substrate, which includes a thickness of material having a surface. The surface of the semiconductor substrate includes non-uniformities or \"roughness\" formed from, for example, a detachment process, that is, the non-uniformities can be made by way of a process such as a controlled cleaving process, or a process called Smart Cut.TM., or any others. The thickness of material has a volume defined by the surface and a selected depth, which has a substantially uniform surface at the selected depth. The thickness of material is also characterized by implant damage (e.g., structural, change in material property) which extends from the surface to the selected depth. The thickness of material is converted into an insulating material such as silicon dioxide at least up to the selected depth. The insulating material is selectively removed (e.g., wet or dry etching) from the semiconductor substrate to provide a substantially uniform surface overlying the semiconductor substrate.\n\n(14) In an alternative specific embodiment, the present invention provides a substrate such as an SOI wafer having a novel implanted layer to provide a uniform surface on the wafer. The wafer includes a bulk substrate and an insulating layer formed overlying the bulk substrate. A film of semiconductor material is formed overlying the insulating layer. Surface non-uniformities (e.g., roughness) are formed overlying a thickness of material in the film. The thickness of material has a volume defined by a selected depth, which has a substantially uniform planar surface at that selected depth. The thickness of material is characterized by implant damage, which extends from the surface to the selected depth. This implant\n\n(15)  damaged thickness of material can be removed by way of oxidation and selective etching techniques, thereby leaving a substantially uniform surface.\n\n(16) In still a further embodiment, the present invention generally provides a method and resulting (and intermediary) structures for planarizing a substrate or film overlying the substrate (e.g., silicon wafer). The substrate or film includes a thickness of material having surface non-uniformities or roughness. Particles are implanted into the thickness of material through the surface non-uniformities to a selected depth, which has a substantially planar surface. The implanted thickness of material includes damage therein, e.g., structural or change in composition. The thickness of material is converted into another material such as an oxide layer, which can be selectively removed by processing techniques such as etching or the like. By way of selective removal of the implanted thickness of material, a substantially planar surface remains on the film of material or the substrate.\n\n(17) Numerous benefits are achieved using the present invention over pre-existing techniques. For example, the present invention provides an efficient technique for forming a substantially uniform surface on an SOI wafer. Additionally, the substantially uniform surface is made by way of common oxidation and etching techniques. Furthermore, the present invention provides a novel non-uniform layer, which can act as a gettering layer for removing impurities from to be active regions of the SOI wafer. These and other benefits are described throughout the present specification and more particularly below.\n\n(18) These and other embodiments of the present invention, as well as other advantages and features are described in more detail in conjunction with the text below and attached Figs.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A method for fabricating a substrate, said method comprising steps of:\n\nproviding a substrate comprising a thickness of material having a non-uniform surface, said thickness of material being implant damaged and having a substantially planar interface region at a selected depth underying said non-uniform surface;\n\nconverting said implant damaged thickness of material that is at a lower density up to said substantially planar interface region into an insulating material; and\n\nselectively removing said insulating material from said substrate using at least an etching process to expose said substantially uniform interface region to provide a substantially uniform surface;\n\nwherein said separation process is provided by a Smart Cut.TM. process or controlled cleaving process.\n\n2. The method of claim 1 wherein said implant damaged thickness of material has a lower density than said substrate.\n\n3. The method of claim 1 wherein said insulating material is an oxide material.\n\n4. The method of claim 1 wherein said step of converting is an oxidation step.\n\n5. The method of claim 4 wherein said converting step converts said implant damaged thickness of material into an oxide material.\n\n6. The method of claim 1 wherein said implant damaged thickness of material is made using a process selected from beam ion implantation, PIII, or separation.\n\n7. The method of claim 1 wherein said implant damaged thickness of material is made by way of implantation.\n\n8. The method of claim 1 wherein said non-uniform surface is made by a cut process.\n\n9. The method of claim 1 wherein said etching process is a selective plasma etching process.\n\n10. The method of claim 1 wherein said etching process is a wet etching process.\n\n11. The method of claim 10 wherein said wet etching process is a hydrofluoric acid dip.\n\n12. The method of claim 1 wherein said substrate is a silicon wafer.\n\n13. The method of claim 12 wherein said converting step decomposes said implant damaged thickness of material into an oxide layer, said oxide layer having said substantially uniform interface region to single crystalline silicon.\n\n14. The method of claim 1 wherein said converting step also removes a possible impurity from said substrate, said impurity being trapped in said implant damaged thickness of material.\n\n15. A method for planarizing a surface of a silicon-on-insulator semiconductor substrate, said method comprising steps of:\n\nproviding a semconductor substrate, said semiconductor substrate comprising a semiconductor material over a substrate material with an insulating layer sandwiched in between;\n\nforming a thickness of material having implant damage that is at a lower density therein and surface non-uniformities thereon in said semiconductor material;\n\nconverting said thickness of material into an insulating material; and\n\nselectively removing said insulating material from said semiconductor material to provide a substantially uniform surface, said selective removing comprising an etching process;\n\nwherein said separation is provided by a Smart Cut.sup..TM.  process or a controlled cleaving process.\n\n16. The method of claim 15 wherein said thickness of material has a lower density relative to said semiconductor material.\n\n17. The method of claim 15 wherein said insulating material is an oxide material.\n\n18. The method of claim 15 wherein said step of converting is an oxidation step.\n\n19. The method of claim 18 wherein said converting step changes said thickness of material into an oxide layer.\n\n20. The method of claim 15 wherein said thickness of material is made using a process selected from ion implantation or separation of said semiconductor material.\n\n21. The method of claim 15 wherein said thickness of material is implant damaged.\n\n22. The method of claim 15 wherein said surface non-uniformities are surface roughness provided by a cut process.\n\n23. The method of claim 15 wherein said etching process is a selective plasma etching process.\n\n24. The method of claim 15 wherein said etching process is a wet etching process.\n\n25. The method of claim 24 wherein said wet etching process is a hydrofluoric acid dip.\n\n26. The method of claim 15 wherein said bulk substrate is a silicon wafer.\n\n27. The method of claim 26 wherein said converting step decomposes said thickness of material into an oxide layer, said oxide layer having a substantially uniform interface to single crystalline silicon in said semiconductor material.\n\n28. The method of claim 15 wherein said converting step also removes a possible impurity from said semiconductor material, said impurity being trapped in said thickness of material.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\nFIG. 1 is a simplified cross-sectional view diagram of an SOI wafer having a non-uniform surface; and\n\nFIGS. 2-7 are simplified cross-sectional view diagrams of the above SOI wafer according to the present invention.\n\n(1) DESCRIPTION OF SPECIFIC EMBODIMENTS\n\n(2) According to the present invention, a technique including a method and device for removing surface non-uniformities on a surface(s) of a substrate such as an SOI wafer is provided. In exemplary embodiments, the technique also removes impurities from the bulk material of the SOI wafer, as well as removing surface roughness and non-uniformities. More particularly, the invention uses an implanting technique, which causes implant damage to a thickness of material having surface \"roughness,\" where the thickness of material is later selectively removed, thereby leaving a substantially uniform surface on the SOI wafer. The SOI wafer is made by way of a variety of techniques including a \"blister\" process such as Smart Cut.sup..TM.  or preferably a controlled cleaving process.\n\n(3) FIG. 1 is a simplified cross-sectional view diagram of a partially completed SOI wafer 10 having an implanted material region according to the present invention. This diagram is merely an illustration and should not limit the scope of the claims herein. One of ordinary skill in the art would recognize other modifications, alternatives, and variations.\n\n(4) The SOI wafer 10 is a partially completed wafer, which has a novel implanted layer 21. Among other elements, the SOI wafer includes a bulk substrate 11, which can be made of a variety of materials such as silicon, glass, and others. Overlying the bulk substrate 11 is an insulating layer 13, which is often made of oxide, but can also be other materials such as silicon nitride, multi-layered materials, and the like. A film of semiconductor material 15 (e.g., single crystal silicon) overlies the insulating layer 13.\n\n(5) The semiconductor material 15 includes an implanted layer 21, which can have a lower material density than the bulk of the semiconductor material 15. As shown, the implanted layer includes an interface region defined by the dotted line and shown by reference numeral 17. This interface region is a substantially uniform plane relative to the bulk of semiconductor material. Additionally, average density of the region overlying the interface region shown by reference numeral 17 is less than average density of the material in the bulk region 15. By way of different properties of layer 21 and layer 15, layer 21 can be selectively removed by way of a variety of semiconductor processing techniques such as etching, and others. In a specific embodiment, the implanted layer 21 is oxidized into a silicon dioxide layer, but can be converted to other materials. By removing the implanted layer 21 having non-uniformities, a fully planarized and substantially uniform substrate surface 17 can be made according to the present invention.\n\n(6) In preferred embodiments, the implanted layer can also remove impurities by way of a gettering action from the bulk semiconductor material region 15, as well as other regions. These impurities include, among others, metals such as copper, nickel, silver, gold, or iron, and other mobile particles, which migrate within the semiconductor material region 15. With enhanced diffusion at elevated temperatures, impurities are trapped in the implanted layer by way of the rough or implanted structure. Alternatively, the type of material of the implanted layer attracts and holds impurities into the layer. The implanted layer therefore acts as a gettering layer for impurities, which become trapped in the implanted layer. These impurities can be eliminated by removing the implanted layer from the semiconductor material by way of selective etching and other fabrication techniques. Details of these fabrication techniques for this partially completed substrate, as well as completed fully planarized substrates are discussed below.\n\n(7) FIGS. 2-7 are simplified diagrams of a method for fabricating the above SOI wafer according to the present invention. These diagrams are merely illustrations and should not limit the scope of the claims herein. One of ordinary skill in the art would recognize other modifications, alternatives, and variations. The present invention provides a novel implanted layer, which can be removed, thereby leaving a substantially uniform SOI wafer, as well as eliminate impurities from active regions subsequently formed in the SOI wafer.\n\n(8) A variety of techniques can be used to form the implanted layer and rough surface on the SOI wafer, for example. FIG. 2 shows a simplified illustration of a cleaving or cut process, which effectively removes a portion 201 of a donor wafer from a thin film of detached material 15. Depending upon the type of cleaving or cut process, surface 205 can be relatively non-uniform and have imperfections thereon, as shown. For example, donor substrate portion 201 is removed by way of a blister technique commonly termed Smart Cut.TM.. An example of this blister technique for removing film material 15 from donor substrate portion 201 is described in U.S. Pat. No. 5,374,564, entitled, Process For The Production Of Thin Semiconductor Material Films, by Michel Bruel, issued Dec. 20, 1994 (the \"Bruel\" patent). The Bruel patent generally describes a process for globally raising the temperature of an implanted wafer to blister a film off of a wafer by way of expanding microbubbles. This technique can often form non-uniformities 205 or surface roughness and imperfections in the wafer surface, as shown. Additionally, the blister process produces a non-uniform layer, which has lateral and vertical roughness or non-uniformities. The non-uniformities must be removed before beginning the fabrication of active integrated circuit devices in the SOI wafer. The blister process forms implant damaged layer 203, which has a substantially uniform interface region 17. This uniform interface region is formed by way of the vertical straggle of the ion implant process used for a Smart Cut.TM..\n\n(9) In an alternative specific embodiment, an implanted layer 303 including the surface roughness can be made by way of a controlled cleaving process. This process allows an initiation of a cleaving process on a substrate using a single or multiple cleave region(s) through the use of controlled energy (e.g., spatial distribution) and selected conditions to allow an initiation of a cleave front(s) and to allow it to propagate through the substrate to remove a thin film of material 15 from the substrate. The process is described in Henley et al, entitled A CONTROLLED CLEAVAGE PROCESS AND RESULTING DEVICE, filed May 12, 1997 (\"Henley\") (Attorney Docket No. 18419-000100) and hereby incorporated by reference for all purposes. The controlled cleaving process provides a more uniform surface on the film as compared to the blister method described by Bruel, noted above. Additionally, the controlled cleaving process produces a uniform layer, which has limited lateral roughness that can be removed using the present technique. The controlled cleaving process also forms implant damaged layer 203, which has a substantially uniform interface region 17. This uniform interface region is formed by way of the straggle of the ion implant or plasma immersion implant process used for the controlled cleaving process.\n\n(10) In still a further alternative embodiment, the implanted layer having the non-uniform surface is made by lower energy implantation of a non-uniform film, as shown by FIG. 3. As shown, a substrate 300, which includes the film of material 15, includes surface roughness or non-uniformities 305. Energetic impurities or particles 301 can be introduced into the film of material 15 using one of numerous techniques. These techniques include beam line ion implantation, plasma immersion ion implantation, and others. The energetic particles cause implant damage to the film of material 303 having the surface non-uniformities 305. As merely an example, the energetic particles include, among other materials, charged (positive or negative) and/or neutral atoms or molecules, or electrons, or the like. In a specific embodiment, the particles can be neutral and/or charged particles including ions such as H+ ions, rare gas ions such as helium and its isotopes, and neon, and deuterium. The particles can also be derived from compounds such as gases, e.g., hydrogen gas, water vapor, methane, hydrides, and hydrogen compounds, and other atomic mass particles. Alternatively, the particles can be any combination of the above particles, and/or ions and/or molecular species and/or atomic species. The particles generally have sufficient kinetic energy to enter into material 15 to create the implanted layer 303 having a substantially uniform interface 17. The substantially uniform interface is enhanced when the lateral straggle of the implant is greater than the characteristic size of the surface roughness.\n\n(11) FIG. 3A is a more detailed diagram of FIG. 3 (or a detailed diagram similar to FIG. 2). This diagram is merely an illustration and should not limit the scope of the claims herein. One of ordinary skill in the art would recognize other variations, alternatives, and modifications. As shown, the implanted layer 303 includes the interface 17 at (z.sub.i), which is substantially uniform from the vertical straggle and the lateral straggle of the implant. The surface roughness 305 has an outer region defined at (z.sub.n) and an inner region defined by (z.sub.l). In most embodiments, the distance (defined by reference letter A) between the outer region and the inner region is less than the distance (defined by reference letter B) between the outer region and the interface. In a specific embodiment, the distance B is at least 1.0 or 1.5 times the distance A. Alternatively, the distance B is at least 2 times the distance A. Alternatively, the distance B is at least 3 times the distance of A. Of course, the particular relative distances will depend upon the application. An explanation of implant straggle is explained in, for example, C. J. John Peng and N. W. Cheung, \"Two Dimensional Implantation Profile Simulator--RETRO,\" Nuclear Instrum. Methods, Vol. B74, pp. 222-225 (1993), which is hereby incorporated by reference.\n\n(12) After forming a film (e.g., 303) having implanted surface non-uniformities such as anyone of the above SOI substrates, the material comprising the implanted surface non-uniformities is converted into another material, as illustrated by FIG. 4, for example. That is, the implanted surface non-uniformities made by silicon can be converted into silicon dioxide by way of an oxidation process. Oxidation occurs in a thermal annealing furnace using either oxygen or steam. Thermal annealing converts the implanted surface non-uniformities made of silicon into silicon dioxide. As shown, the surface includes non-uniformities 19 (or reference number 305 or 205) in the layer of silicon dioxide 21. A substantially uniform interface 17 is defined between the silicon dioxide 21 and silicon substrate material 15. The substantially uniform interface generally has a uniformity less than about 0.5% or more preferably less than about 0.3%.\n\n(13) In order to better understand this technique of forming this substantially uniform interface 17, it may be helpful to describe the oxidation of silicon process in more detail using FIGS. 5 and 6. FIG. 5 is a more detailed diagram of FIG. 4, and FIG. 6 is a diagram illustrating a density of the non-uniform implanted layer in silicon before oxidation. Referring to FIG. 5, for example, the SOI wafer includes, among other elements, the film of material 15, the interface 17, and the silicon dioxide layer 21, which includes the surface non-uniformities 19. The interface layer is defined along a horizontal plane at (z.sub.i) and the surface has an upper height at (z.sub.m). The lower height is defined by (z.sub.l). As shown, the silicon dioxide layer, which is previously silicon, converts into silicon dioxide, leaving a substantially uniform interface 17 in the silicon dioxide 21 layer, which is defined as the film of silicon 15.\n\n(14) Before converting the silicon into silicon dioxide, the surface non-uniformities are implanted. By way of implanting, which causes implant damage, the density of the region having the non-uniformities is less than the density of the bulk silicon material. To illustrate the difference in densities between the implanted layer and the bulk silicon material, FIG. 6 is provided. As shown, the vertical axis represents the density of the materials, where .rho..sub.s represents the density of the bulk substrate and .rho..sub.t represents the density of the implanted layer. As shown, the density of the bulk substrate is greater than the density of the implanted layer. At the interface region (z.sub.i), the density of the substrate structure decreases to the density of the implanted layer until a lateral distance (z.sub.m), which is defined as the outer most point of the surface non-uniformities. This difference in densities allows oxygen molecules or ions to diffuse through and react with the implanted layer at a much faster rate than the diffusion of oxygen into the bulk silicon material. Since the diffusion or mass transfer rate of the oxygen is faster through the implanted layer, the oxygen converts substantially all of the silicon in the implanted layer into silicon dioxide before beginning to convert any of the silicon in the bulk substrate, which has a higher density. By way of limitations in mass transfer rates in the bulk silicon layer, oxidation of the bulk silicon layer essentially stops or slows down to a point where the interface 17 region forms in a substantially uniform manner.\n\n(15) In a specific embodiment, the oxidized (or implanted) non-uniform layer also works as a gettering layer. For example, the non-uniform layer includes a surface roughness and an interface region, which can attract and trap impurities, that migrate in the subsequently formed active regions of the SOI wafer. Accordingly, the non-uniform layer can accumulate impurities such as metal contaminates from the active region. These impurities can be eliminated as the non-uniform layer is removed. This removal process is described in more detail below.\n\n(16) Subsequent to oxidation of the non-uniform implanted layer, a selective removal process can be used to remove the non-uniform implanted layer from the bulk substrate 15, as shown in FIG. 7. For example, since the non-uniform implanted layer is made of oxide and the bulk substrate is made of silicon, a selective etchant can be used to selectively remove the oxide from the bulk silicon substrate. As merely an example, the selective etchant can include a variety of solutions such as hydrofluoric acid or the like, if a wet process is desired. Alternatively, the oxide can be selectively removed by way of a plasma etching or reactive ion etching or a plasma immersion process. Using a plasma process, an etchant includes, among other materials, a fluorine bearing compound(s), e.g., CF.sub.4, SF.sub.6. Of course, the type of etchant used will depend upon the\n\n(17)  application.\n\n(18) The etching process selectively removes the non-uniformities from the material region to leave a substantially uniform surface 701, for example. This substantially uniform surface has a surface uniformity less than about 0.5%, or less than about 0.3% and lower, if needed. Preferably, the selective removal process is used with the controlled cleaving process, which generally provides a detached surface that is more uniform than those made by way of the blister technique such as Smart Cut.TM. and others.\n\n(19) In an alternative embodiment, a final polishing step can be performed before using the wafer for integrated circuit processing. For example, the detached surface of the film of silicon material can be slightly rough and may need finishing, which occurs using a combination of grinding and/or polishing techniques. In some embodiments, the detached surface undergoes a step of grinding using, for examples, techniques such as rotating an abrasive material overlying the detached surface to remove any imperfections or surface roughness therefrom. A machine such as a \"back grinder\" made by a company called Disco may provide this technique.\n\n(20) Alternatively, chemical mechanical polishing or planarization techniques finish the detached surface of the film. In CMP, a slurry mixture is applied directly to a polishing surface which is attached to a rotating platen. This slurry mixture can be transferred to the polishing surface by way of a metering pump, which is coupled to a slurry source. The slurry is often a solution containing an abrasive and an oxidizer, e.g., H.sub.2 O.sub.2, KIO.sub.3, ferric nitrate. The abrasive is often a borosilicate glass, titanium dioxide, titanium nitride, aluminum oxide, aluminum trioxide, iron nitrate, cerium oxide, silicon dioxide (colloidal silica), silicon nitride, silicon carbide, graphite, diamond, and any mixtures thereof. This abrasive is mixed in a solution of deionized water and oxidizer or the like. Preferably, the solution is acidic. Of course, the technique used to completely finish the surface of the wafer depends upon the application.\n\n(21) While the above is a full description of the specific embodiments, various modifications, alternative constructions and equivalents may be used. Therefore, the above description and illustrations should not be taken as limiting the scope of the present invention which is defined by the appended claims."
    },
    "document_structure": {
      "abstract_end": 2,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 12,
      "cert_correction_start": 12,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 11,
      "claims_start": 10,
      "description_end": 10,
      "description_start": 7,
      "drawings_end": 6,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 28,
      "number_of_drawing_sheets": 4,
      "number_of_figures": 7,
      "page_count": 12,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 10,
      "specification_start": 7,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 26732223,
    "field_of_search_us": [
      "438/528",
      "438/459",
      "438/974",
      "438/977",
      "438/766",
      "438/770",
      "438/697",
      "438/745",
      "438/756",
      "438/723",
      "438/513",
      "438/455",
      "438/458",
      "438/406",
      "438/4",
      "438/694"
    ],
    "foreign_references": [
      {
        "cited_by_examiner": false,
        "country_code": "EP",
        "patent_number": "0 379 828",
        "pub_month": "1989-12-01"
      },
      {
        "cited_by_examiner": false,
        "country_code": "EP",
        "patent_number": "0 459 177",
        "pub_month": "1991-05-01"
      },
      {
        "cited_by_examiner": false,
        "country_code": "JP",
        "patent_number": "0193904",
        "pub_month": "1984-11-01"
      },
      {
        "cited_by_examiner": false,
        "country_code": "GB",
        "patent_number": "2231197",
        "pub_month": "1990-03-01"
      }
    ],
    "group_art_unit": "282",
    "guid": "US-6103599-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/06/103/599",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H01L",
        "intl_subclass": "21/70",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01L",
        "intl_subclass": "21/762",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01L",
        "intl_subclass": "21/02",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01L",
        "intl_subclass": "21/306",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "H01L21/70"
    ],
    "inventors": [
      {
        "city": "Los Gatos",
        "country": "N/A",
        "name": "Henley; Francois J.",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Albany",
        "country": "N/A",
        "name": "Cheung; Nathan",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Henley; Francois J. et al.",
    "legal_firm_name": [
      "Townsend and Townsend and Crew LLP"
    ],
    "npl_references": [
      {},
      {},
      {},
      {},
      {}
    ],
    "patent_title": "Planarizing technique for multilayered substrates",
    "primary_examiner": "Mulpuri; Savitri",
    "publication_date": "2000-08-15",
    "publication_number": "6103599",
    "type": "USPAT",
    "us_class_current": [
      "438/770",
      "438/977",
      "438/694",
      "257/E21.568",
      "438/458",
      "257/E21.219"
    ],
    "us_class_issued": [
      "438/770",
      "438/459",
      "438/694",
      "438/977",
      "438/458"
    ],
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "Gorinas",
        "pub_month": "1977-02-01",
        "publication_number": "4006340"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Fournier",
        "pub_month": "1986-01-01",
        "publication_number": "4566403"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dolins et al.",
        "pub_month": "1989-07-01",
        "publication_number": "4846928"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Barna et al.",
        "pub_month": "1989-07-01",
        "publication_number": "4847792"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Boulose et al.",
        "pub_month": "1989-08-01",
        "publication_number": "4853250"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Rough et al.",
        "pub_month": "1989-12-01",
        "publication_number": "4887005"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Ogle",
        "pub_month": "1990-08-01",
        "publication_number": "4948458"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Popov",
        "pub_month": "1990-08-01",
        "publication_number": "4952273"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Suzuki et al.",
        "pub_month": "1990-10-01",
        "publication_number": "4960073"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Moslehi et al.",
        "pub_month": "1991-02-01",
        "publication_number": "4996077"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Henley et al.",
        "pub_month": "1991-04-01",
        "publication_number": "5010579"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Henley et al.",
        "pub_month": "1991-05-01",
        "publication_number": "5013563"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Hubler et al.",
        "pub_month": "1991-05-01",
        "publication_number": "5015353"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dandl",
        "pub_month": "1992-07-01",
        "publication_number": "5133826"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Houchin et al.",
        "pub_month": "1993-04-01",
        "publication_number": "5202095"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dandl",
        "pub_month": "1993-04-01",
        "publication_number": "5203960"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Johnson",
        "pub_month": "1993-08-01",
        "publication_number": "5234529"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Otto",
        "pub_month": "1993-10-01",
        "publication_number": "5250328"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Moslehi",
        "pub_month": "1993-10-01",
        "publication_number": "5252178"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Thomas, III et al.",
        "pub_month": "1993-12-01",
        "publication_number": "5273610"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Imahashi et al.",
        "pub_month": "1994-08-01",
        "publication_number": "5342472"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Chen et al.",
        "pub_month": "1994-11-01",
        "publication_number": "5368710"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dandl",
        "pub_month": "1994-12-01",
        "publication_number": "5370765"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bruel",
        "pub_month": "1994-12-01",
        "publication_number": "5374564"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Ohkuni et al.",
        "pub_month": "1995-04-01",
        "publication_number": "5404079"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Benzing et al.",
        "pub_month": "1995-04-01",
        "publication_number": "5405480"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Ovshinsky et al.",
        "pub_month": "1995-05-01",
        "publication_number": "5411592"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Minato et al.",
        "pub_month": "1995-07-01",
        "publication_number": "5435880"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Horiike et al.",
        "pub_month": "1996-01-01",
        "publication_number": "5487785"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bonser",
        "pub_month": "1996-04-01",
        "publication_number": "5504328"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Lee et al.",
        "pub_month": "1997-01-01",
        "publication_number": "5593622"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Srikrishnan",
        "pub_month": "1999-03-01",
        "publication_number": "5822987"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Hanson et al.",
        "pub_month": "1999-07-01",
        "publication_number": "5920764"
      }
    ]
  },
  {
    "app_filing_date": "2022-01-05",
    "appl_id": "17647036",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Singapore",
        "country": "SG",
        "name": "Marvell Asia Pte Ltd",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Singapore",
        "country": "SG",
        "name": "Marvell Asia Pte Ltd",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "composite_id": "1000006126434!US-US-11557320",
    "cpc_inventive": [
      {
        "cpc_class": "G11B",
        "cpc_subclass": "5/59666",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G11B",
        "cpc_subclass": "5/59627",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A method for writing repeatable run-out (RRO) data, to surfaces of a rotating magnetic storage medium in a storage device having two read channels, includes detecting, with a first head, using a first read channel, a servo sync mark (SSM) on a first track on a first surface, establishing a recurring servo-gating signal at a successive fixed interval from the SSM, detecting, with the first head, servo signals from the first track on occurrence of the recurring servo-gating signal, processing the servo signals from the first track, to generate first positioning signals for positioning the first head relative to the first track, following a similar procedure with a second read channel having a second head to generate second positioning signals for the second read head, and writing first and second RRO data to servo wedges of the first and second tracks according to the respective positioning signals.",
      "background": "CROSS REFERENCE TO RELATED APPLICATIONS\n(1) This disclosure claims the benefit of, commonly-assigned U.S. Provisional Patent Application No. 63/141,441, filed Jan. 25, 2021, which is hereby incorporated by reference herein in its entirety.",
      "brief": "FIELD OF USE\n\n(1) This disclosure relates to write operations for a repeatable run-out (RRO) field in storage devices such as disk drives. More particularly, this disclosure relates to the timing of RRO-write operations on two surfaces of a multi-surface storage device.\n\nBACKGROUND\n\n(2) The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the inventors hereof, to the extent that that work is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted to be prior art against the subject matter of the present disclosure.\n\n(3) In magnetic recording, as one example, reading and writing are performed by one or more heads that move relative to the surface of a storage medium. Magnetic disk drives, for example, include a one or more individual disks, or \u201cplatters,\u201d which may be two-sided\u2014i.e., each platter can store data on each of its two sides. Therefore, such a disk drive would have at least two heads for each platter. Indeed, for each platter, there is normally at least one write head and at least one separate read head, so that such a disk drive normally has at least four heads per platter.\n\n(4) In a common configuration, all of the heads in a given disk drive may be mounted on arms attached to a common actuator that controls the radial position of the heads (an angular, tangential or circumferential component of motion is provided by the rotation of the platters relative to the heads). This is true whether there is one or many platters, and one or multiple heads per platter. In alternative configurations, there may be multiple (e.g., two) actuators, each of which bears multiple arms, of which all normally carry respective read and write heads.\n\n(5) In order to control the radial position selected by the actuator, each surface of each platter has distributed upon it positional information referred to as \u201cservo\u201d data. The servo data are commonly distributed in spaced-apart servo \u201cwedges\u201d (generally spaced equiangularly) on the platter surface. By reading the servo data as each servo wedge passes under the read head, the disk drive controller can determine the precise radial (and angular) position of the head and can feed back that determination to control the position of the read head or the write head, depending on the required operation. One component of servo data is a \u201crepeatable run-out,\u201d or RRO, field, which characterizes repeatable contributions to error, so that it can be factored into position determinations.\n\n(6) Servo wedge information typically is recorded on a storage medium at the time of manufacture. One technique for recording servo wedge information is \u201cself-servo write\u201d (\u201cSSW\u201d), in which the storage device's own read/write mechanisms, including data channel controllers and read/write heads, are used to write the servo wedge information, possibly under control of an external processor.\n\n(7) Self-servo-write operations typically begin with writing of initial or \u201ccoarse\u201d reference spirals. The coarse reference spirals are sets of time data written in highly ramped spirals between the outer and inner diameter of the disk by sweeping the disk read/write head radially at a relatively high rate as the disk rotates. The coarse reference spirals may not be regularly or evenly spaced. After coarse reference spirals have been written, the read/write mechanism reads and \u201clearns\u201d the locations of the coarse reference spirals. The coarse reference spiral locations are then referenced to allow the writing of more refined location data as \u201cintermediate\u201d reference spirals. The intermediate reference spirals are less ramped than the coarse spirals and contain more precise time data. After the intermediate spirals have been written, the read/write mechanism reads and \u201clearns\u201d the locations of the intermediate spirals. From this finer location data, the read/write mechanism writes final servo control signal patterns as \u201cservo wedges\u201d along concentric arcs at varying radii (i.e., in tracks).\n\n(8) The quality of the spiral data and the servo wedge data is affected by the frequency at which the spiral data and the servo wedge data are processed. In typical implementations, both the spiral data and the servo wedge data are processed at frequencies derived from a common reference frequency, but using separate time base generators.\n\n(9) After the self-servo write operations, the aforementioned RRO data also must be written to each servo wedge.\n\n(10) As described, self-servo-write operations and RRO-write operations each utilize both the read-data channel (RDC) and the write-data channel (WDC) of a disk drive, and both the RDC and the WDC are typically limited to operating on only one disk surface at any one time. In a typical disk drive having multiple platters, each with hundreds of thousands of tracks, but only one RDC and one WDC, self-servo-write and RRO-write operations may require days or weeks to complete, adding to the time and cost of manufacturing the disk drive. Indeed, up to 40% of the time required to manufacture a high-performance disk drive is consumed by the self-servo-write and RRO-write processes.\n\nSUMMARY\n\n(11) According to implementations of the subject matter of this disclosure, a method for writing repeatable run-out data, representing a recurring contribution to position error, to multiple surfaces of a rotating magnetic storage medium in a storage device having two read channels, includes detecting, with a first read head of the storage device, using a first read channel, a servo sync mark on a first track on a first storage medium surface of the storage device, establishing a first recurring servo-gating signal for the first read channel at a successive fixed interval from the servo sync mark, detecting, with the first read head, servo signals from the first track on occurrence of the first recurring servo-gating signal, processing the servo signals from the first track, to generate first positioning signals for positioning the first read head relative to the first track for writing first repeatable run-out data, establishing a second recurring servo-gating signal for a second read channel at an offset from the first recurring servo-gating signal, detecting, with a second read head, servo signals from a second track on occurrence of the second recurring servo-gating signal, processing the servo signals from the second track, to generate second positioning signals for positioning the second read head relative to the second track for writing second repeatable run-out data, and writing first repeatable run-out data to servo wedges of the first track according to the first positioning signals, and second repeatable run-out data to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device.\n\n(12) In a first implementation of such a method, processing the servo signals from the first track may include processing the servo signals from the first track in a storage device controller separate from the storage device, and processing the servo signals from the second track may include processing the servo signals from the second track in servo writing circuitry of the storage device.\n\n(13) According to a first aspect of that first implementation, establishing the first recurring servo-gating signal for the first read channel may be performed using the storage device controller separate from the storage device, and establishing the second recurring servo-gating signal for the second read channel may be performed using the servo writing circuitry of the storage device.\n\n(14) A first instance of that first aspect of the first implementation may further include storing the first positioning signals and the second positioning signals, prior to the writing, in memory in the servo writing circuitry of the storage device.\n\n(15) A first variant of that first instance of the first aspect of the first implementation may claim 4 further include issuing respective first and second fetch commands from the servo writing circuitry to respectively fetch the first positioning signals and the second positioning signals from the memory.\n\n(16) In a second implementation of such a method, writing the first repeatable run-out signals to servo wedges of the first track according to the first positioning signals, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device, may include temporally interleaving writing the first repeatable run-out signals to servo wedges of the first track with writing the second repeatable run-out signals to servo wedges of the second track.\n\n(17) In a third implementation of such a method, establishing the first recurring servo-gating signal for the first read channel at the successive fixed interval from the initial servo sync mark may include setting the fixed interval based on rotation speed of the rotating magnetic storage medium.\n\n(18) According to a first aspect of that third implementation, the method may further include locking the fixed interval to the rotation speed of the rotating magnetic storage medium.\n\n(19) In a first instance of that first aspect of the third implementation, locking the fixed interval to the rotation speed of the rotating magnetic storage medium may include setting a counter as a timer of the fixed interval, marking a time difference between a selected position of the counter and the servo sync mark, periodically checking that the time difference between the selected position of the counter and the servo sync mark has not changed, and when the time difference between the selected position of the counter and the servo sync mark has changed, adjusting a clock source of the rotating magnetic storage medium.\n\n(20) In a first variant of that first instance of the first aspect of the third implementation, periodically checking may include checking once during each servo wedge.\n\n(21) A data storage system according to implementations of the subject matter of this disclosure includes a storage device, and a storage controller separate from the storage device, wherein the storage device includes a storage medium, having multiple storage medium surfaces on which servo data is written, the servo data on each storage medium surface including a repeatable run-out field representing a recurring contribution to position error, and circuitry for writing the repeatable run-out field. The circuitry includes a first read channel, a second read channel, a first detector on a first read head of the storage device, the first detector being configured to detect data, including a servo sync mark, on a first track on a first storage medium surface of the storage device, and a second detector on a second read head of the storage device, the second detector being configured to detect data on a second track on a second storage medium surface of the storage device. The system is configured to establish a first recurring servo-gating signal for the first read channel at a successive fixed interval from the servo sync mark, the detector on the first read head being configured to detect servo signals from the first track on occurrence of the first recurring servo-gating signal, process the servo signals from the first track, to generate first positioning signals for positioning the first read head relative to the first track for writing first repeatable run-out data, establish a second recurring servo-gating signal for the second read channel at an offset from the first recurring servo-gating signal, detect, with the second detector on the second read head, servo signals from the second track on occurrence of the second recurring servo-gating signal, process the servo signals from the second track, to generate second positioning signals for positioning the second read head relative to the second track for writing second repeatable run-out data, and write the first repeatable run-out data to servo wedges of the first track according to the first positioning signals, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device.\n\n(22) In a first implementation of such a data storage system, the system may be configured to process the servo signals from the first track by processing the servo signals from the first track in a storage device controller separate from the storage device, and process the servo signals from the second track by processing the servo signals from the second track in servo writing circuitry of the storage device.\n\n(23) According to a first aspect of that first implementation, the storage device controller separate from the storage device may be configured to establish the first recurring servo-gating signal for the first read channel, and the servo writing circuitry of the storage device may be configured to establish the second recurring servo-gating signal for the second read channel.\n\n(24) A first instance of that first aspect of the first implementation may further include memory in the servo writing circuitry of the storage device, the memory being configured to store the first positioning signals and the second positioning signals, prior to writing the first positioning signals to servo wedges of the first track and the second positioning signals to servo wedges of the second track.\n\n(25) In a first variant of that first instance of the first aspect of the first implementation, the servo writing circuitry may be configured to issue respective first and second fetch commands from the servo writing circuitry to respectively fetch the first positioning signals and the second positioning signals from the memory.\n\n(26) In a second implementation of such a data storage system, the system may be configured to write the first repeatable run-out signals to servo wedges of the first track according to the first positioning data, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning data, using a single write data channel of the storage device, by temporally interleaving writing the first repeatable run-out signals to servo wedges of the first track with writing the second repeatable run-out signals to servo wedges of the second track.\n\n(27) In a third implementation of such a data storage system the system may be configured to establish the first recurring servo-gating signal for the first read channel at the successive fixed interval from the initial servo sync mark by setting the fixed interval based on rotation speed of the rotating magnetic storage medium.\n\n(28) According to a first aspect of that third implementation, the data storage system may be configured to lock the fixed interval to the rotation speed of the rotating magnetic storage medium.\n\n(29) In a first instance of that first aspect of the third implementation, the system may be configured to lock the fixed interval to the rotation speed of the rotating magnetic storage medium by setting a counter as a timer of the fixed interval, marking a time difference between a selected position of the counter and the servo sync mark, periodically checking that the time difference between the selected position of the counter and the servo sync mark has not changed, and when the time difference between the selected position of the counter and the servo sync mark has changed, adjusting a clock source of the rotating magnetic storage medium.\n\n(30) In a first variant of that first instance of the first aspect of the third implementation, the system may be configured to periodically check that the time difference between the selected position of the counter and the servo sync mark has not changed by checking once during each servo wedge.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A method for writing repeatable run-out data, representing a recurring contribution to position error, to multiple surfaces of a rotating magnetic storage medium in a storage device having two read channels, the method comprising: detecting, with a first read head of the storage device, using a first read channel, a servo sync mark on a first track on a first storage medium surface of the storage device; establishing a first recurring servo-gating signal for the first read channel at a successive fixed interval from the servo sync mark; detecting, with the first read head, servo signals from the first track on occurrence of the first recurring servo-gating signal; processing the servo signals from the first track, to generate first positioning signals for positioning the first read head relative to the first track for writing first repeatable run-out data; establishing a second recurring servo-gating signal for a second read channel at an offset from the first recurring servo-gating signal; detecting, with a second read head, servo signals from a second track on occurrence of the second recurring servo-gating signal; processing the servo signals from the second track, to generate second positioning signals for positioning the second read head relative to the second track for writing second repeatable run-out data; and writing first repeatable run-out data to servo wedges of the first track according to the first positioning signals, and second repeatable run-out data to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device.  \n\n2. The method of claim 1 wherein: processing the servo signals from the first track comprises processing the servo signals from the first track in a storage device controller separate from the storage device; and processing the servo signals from the second track comprises processing the servo signals from the second track in servo writing circuitry of the storage device.  \n\n3. The method of claim 2 wherein: establishing the first recurring servo-gating signal for the first read channel is performed using the storage device controller separate from the storage device; and establishing the second recurring servo-gating signal for the second read channel is performed using the servo writing circuitry of the storage device.  \n\n4. The method of claim 3 further comprising storing the first positioning signals and the second positioning signals, prior to the writing, in memory in the servo writing circuitry of the storage device. \n\n5. The method of claim 4 further comprising issuing respective first and second fetch commands from the servo writing circuitry to respectively fetch the first positioning signals and the second positioning signals from the memory. \n\n6. The method of claim 1 wherein writing the first repeatable run-out signals to servo wedges of the first track according to the first positioning signals, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device, comprises temporally interleaving writing the first repeatable run-out signals to servo wedges of the first track with writing the second repeatable run-out signals to servo wedges of the second track. \n\n7. The method of claim 1 wherein establishing the first recurring servo-gating signal for the first read channel at the successive fixed interval from the initial servo sync mark comprises setting the fixed interval based on rotation speed of the rotating magnetic storage medium. \n\n8. The method of claim 7 further comprising locking the fixed interval to the rotation speed of the rotating magnetic storage medium. \n\n9. The method of claim 8 wherein locking the fixed interval to the rotation speed of the rotating magnetic storage medium comprises: setting a counter as a timer of the fixed interval; marking a time difference between a selected position of the counter and the servo sync mark; periodically checking that the time difference between the selected position of the counter and the servo sync mark has not changed; and when the time difference between the selected position of the counter and the servo sync mark has changed, adjusting a clock source of the rotating magnetic storage medium.  \n\n10. The method of claim 9 wherein periodically checking comprises checking once during each servo wedge. \n\n11. A data storage system comprising: a storage device; and a storage controller separate from the storage device; wherein: the storage device comprises: a storage medium, having multiple storage medium surfaces on which servo data is written, the servo data on each storage medium surface including a repeatable run-out field representing a recurring contribution to position error; and circuitry for writing the repeatable run-out field, the circuitry comprising: a first read channel, a second read channel, a first detector on a first read head of the storage device, the first detector being configured to detect data, including a servo sync mark, on a first track on a first storage medium surface of the storage device, and a second detector on a second read head of the storage device, the second detector being configured to detect data on a second track on a second storage medium surface of the storage device; and  the system is configured to: establish a first recurring servo-gating signal for the first read channel at a successive fixed interval from the servo sync mark, the detector on the first read head being configured to detect servo signals from the first track on occurrence of the first recurring servo-gating signal, process the servo signals from the first track, to generate first positioning signals for positioning the first read head relative to the first track for writing first repeatable run-out data, establish a second recurring servo-gating signal for the second read channel at an offset from the first recurring servo-gating signal, detect, with the second detector on the second read head, servo signals from the second track on occurrence of the second recurring servo-gating signal, process the servo signals from the second track, to generate second positioning signals for positioning the second read head relative to the second track for writing second repeatable run-out data, and write the first repeatable run-out data to servo wedges of the first track according to the first positioning signals, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning signals, using a single write data channel of the storage device.   \n\n12. The data storage system of claim 11 wherein the system is configured to: process the servo signals from the first track by processing the servo signals from the first track in a storage device controller separate from the storage device; and process the servo signals from the second track by processing the servo signals from the second track in servo writing circuitry of the storage device.  \n\n13. The data storage system of claim 12 wherein: the storage device controller separate from the storage device is configured to establish the first recurring servo-gating signal for the first read channel; and the servo writing circuitry of the storage device is configured to establish the second recurring servo-gating signal for the second read channel.  \n\n14. The data storage system of claim 13 further comprising memory in the servo writing circuitry of the storage device, the memory being configured to store the first positioning signals and the second positioning signals, prior to writing the first positioning signals to servo wedges of the first track and the second positioning signals to servo wedges of the second track. \n\n15. The data storage system of claim 14 wherein the servo writing circuitry is configured to issue respective first and second fetch commands from the servo writing circuitry to respectively fetch the first positioning signals and the second positioning signals from the memory. \n\n16. The data storage system of claim 11 wherein the system is configured to write the first repeatable run-out signals to servo wedges of the first track according to the first positioning data, and the second repeatable run-out signals to servo wedges of the second track according to the second positioning data, using a single write data channel of the storage device, by temporally interleaving writing the first repeatable run-out signals to servo wedges of the first track with writing the second repeatable run-out signals to servo wedges of the second track. \n\n17. The data storage system of claim 11 wherein the system is configured to establish the first recurring servo-gating signal for the first read channel at the successive fixed interval from the initial servo sync mark by setting the fixed interval based on rotation speed of the rotating magnetic storage medium. \n\n18. The data storage system of claim 17 wherein the system is configured to lock the fixed interval to the rotation speed of the rotating magnetic storage medium. \n\n19. The data storage system of claim 18 wherein the system is configured to lock the fixed interval to the rotation speed of the rotating magnetic storage medium by: setting a counter as a timer of the fixed interval; marking a time difference between a selected position of the counter and the servo sync mark; periodically checking that the time difference between the selected position of the counter and the servo sync mark has not changed; and when the time difference between the selected position of the counter and the servo sync mark has changed, adjusting a clock source of the rotating magnetic storage medium.  \n\n20. The data storage system of claim 19 wherein the system is configured to periodically check that the time difference between the selected position of the counter and the servo sync mark has not changed by checking once during each servo wedge.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) Further features of the disclosure, its nature and various advantages, will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:\n\n(2) FIGS. 1 and 2 show a disk drive as an example of a storage device which may incorporate the subject matter of this disclosure;\n\n(3) FIG. 3 is a view similar to FIG. 1 showing a disk drive having arms with two micro-actuators;\n\n(4) FIG. 4 is a schematic representation of a hard-drive controller which may incorporate the subject matter of this disclosure;\n\n(5) FIG. 5 is a diagram of a storage device control system according to implementations of the subject matter of this disclosure;\n\n(6) FIG. 6 is a table comparing a frequency control technique, according to implementations of the subject of this disclosure, to a typical frequency technique;\n\n(7) FIG. 7 is a timing diagram illustrating a technique, according to implementations of the subject matter of this disclosure, for monitoring frequency control;\n\n(8) FIG. 8 is a timing diagram showing the reading of servo data to generate RRO data in accordance with implementations of the subject matter of this disclosure;\n\n(9) FIG. 9 is a timing diagram showing the timing of write signals in accordance with implementations of the subject matter of this disclosure; and\n\n(10) FIG. 10 is a flow diagram illustrating a method according to implementations of the subject matter of this disclosure.\n\nDETAILED DESCRIPTION\n\n(11) As noted above, self-servo-write and RRO-write operations utilize both the read-data channel (RDC) and the write-data channel (WDC) of a disk drive, and both the RDC and the WDC are typically limited to operating on only one disk surface at any one time. A typical disk drive has multiple two-sided platters, each with hundreds of thousands of tracks (e.g., 200,000-400,000 tracks), but only one RDC and one WDC. In such disk drives, self-servo-write and RRO-write operations may require days or weeks to complete, adding to the time and cost of manufacturing the disk drive.\n\n(12) The time required to complete self-servo-write and RRO-write operations can be reduced by increasing the number of RDCs and WDCs in the disk drive. However, adding an RDC or a WDC is expensive. Nevertheless, recently disk drives have been developed that use two-dimensional magnetic recording (TDMR), and those disk drives include a second RDC (but only one WDC).\n\n(13) Therefore, in accordance with implementations of the subject matter of this disclosure, the time required to complete self-servo-write and RRO-write operations can be nearly halved for disk drives equipped for TDMR, by using both RDCs so that self-servo write operations, including RRO write operations, can be carried out concurrently on two different disk surfaces (which can be two sides of the same disk platter, or surfaces on two different disk platters).\n\n(14) Although a plurality of the read/write arms carrying the read/write heads of a disk drive assembly may be mounted on a single actuator and therefore theoretically move together to the same location over every disk platter surface, in practice vibrations and inertial forces may cause different ones of the read/write arms borne by a single actuator to move to slightly different locations. This may give rise to at least two difficulties in writing the RRO field to the two different surfaces.\n\n(15) First, the position of the actuator is determined by servo data read by one of the actuators from one surface (which may be considered the \u201cprimary\u201d surface), while the head position relative to a second surface (which may be considered the \u201csecondary\u201d surface) may be slightly different. This difficulty may be mitigated by providing, in disk drives with which the subject matter of this disclosure is used, not only two RDCs, but also micro-actuators at the end of each read/write arm. That is, in addition to each read/write arm pivoting about a main pivot point defined by the aforementioned actuator, each read/write arm should be articulated near its tip, so that the tip can be rotated about a second pivot point to fine-tune the location of the heads.\n\n(16) In implementations of the subject matter of this disclosure, the read head on one arm is locked to the servo signal (which will have been written earlier using, e.g., self-servo-write operations) of a track on its corresponding surface, which may be considered to be the \u201cprimary\u201d surface for the RRO-write operation. On a second surface on which the RRO-write operation is to be carried out concurrently, the micro-actuator of the arm for that surface is used to move the head, if necessary, radially so that it too may be locked to the servo signal of its respective track. With the heads on both surfaces aligned to their respective tracks, the write channel can be used to write the RRO pattern to each respective track on each respective surface.\n\n(17) However, because the micro-actuator motion is rotational, it includes not only a radial or cross-track component of motion for positioning the head properly over the track, but also a circumferential or angular component that moves the head in a down-track direction (i.e., along the track). Therefore, the two heads may be in slightly different down-track positions and a phase differential may arise between the RRO patterns being written on the two separate surfaces. Accordingly, in some implementations, the amount of rotation of the respective micro-actuator on each of the two arms involved in RRO writing of a particular pair of surfaces may be kept track of, so that the amount of down-track differential can be computed and appropriate compensation can be applied.\n\n(18) Although the micro-actuators on both arms involved in RRO writing of a particular pair of surfaces may be adjusted, in a simplification in some implementations of the subject matter of this disclosure, one arm may be kept straight (i.e., the centerline of the micro-actuator is maintained in alignment with the centerline of the main arm), with the micro-actuator on only the other arm being used to adjust the cross-track position of the corresponding head. This may simplify the calculation of the down-track differential.\n\n(19) The down-track phase differential may be only one component of a timing difference between the two surfaces. Normally, each servo channel has its own clock, and therefore the time bases for RRO writing to the two surfaces are different. In accordance with implementations of the subject matter of this disclosure, the writing of servo data, including RRO data, to the two different surfaces may be accomplished by toggling the WDC between writing to a first surface and writing to a second surface. Because each writing operation can be accomplished in substantially less time than the time needed for processing the servo data in the read channel to determine where to write the RRO data (normally this would be expected to be accomplished well within one wedge-to-wedge interval), the toggling of the write operation between two different heads does not slow down the operation of the storage device. However, adjustments are made to accommodate the different time bases for the different surfaces, because the single WDC operates according to only one of the time bases.\n\n(20) The subject matter of this disclosure may be better understood by reference to FIGS. 1-10.\n\n(21) FIGS. 1 and 2 show a disk drive 100 as an example of a storage device with which the subject matter of the present disclosure may be used. In this particular example, disk drive 100 has three platters 101, 102, 103, although any number of platters may be included in a disk drive with which the subject matter of the present disclosure may be used. As shown, each platter 101, 102, 103 has, on each of its upper and lower surfaces 111, 112, a coating 110 made from a material in which data can be stored, e.g., magnetically. The present disclosure also is relevant to a disk drive in which one or more platters includes coating 110 on only one of its surfaces, but such a disk drive would store less data in the same volume than a disk drive with two-sided platters. The platters 101-103 are mounted on a rotatable spindle 104. Motor 105 rotates spindle 104 to rotate platters 101-103 in the direction of arrow A (FIG. 2). Although motor 105 is shown connected directly to spindle 104, in some cases motor 105 may be located off-axis of spindle 104 and would be connected to spindle 104 through belts or gears (not shown).\n\n(22) Read/write head assembly 120 includes at least one actuator 121 that bears arms 122-125, one of which is disposed adjacent to each surface 111, 112 of a platter 101, 102, 103 that has a memory storage coating 110. In this example, with heads on both surfaces of each of arms 123, 124, that amounts to four arms 122-125, but in the single-sided platter example discussed above, there would be only three arms. In other examples, the number of arms would increase or decrease along with the number of platters, although in some examples the arms may be distributed among more than one actuator.\n\n(23) Each arm 122-125 bears, at or near its end furthest from actuator 121, and on both its upper and lower surfaces in the case of arms 123, 124, a plurality of read heads/sensors and write heads. In this case, two sensors 131, 132 are shown, and may represent, respectively, read heads/sensors and write heads, although it in some applications each arm 123, 124 may bear more than one read head/sensor and more than one write head (not shown). In the configuration shown in FIGS. 1 and 2, arms 122-125 are aligned along a radius of platters 101-103, bringing heads 131, 132 as close as they can get to spindle 104. It is noted that FIGS. 1 and 2 are schematic only and not to scale. Normally, the spindle diameter would be larger relative to the disk diameter. Moreover, arms 122-125 normally cannot point directly at the center of the disk.\n\n(24) A motor 126, commonly referred to as a \u201cvoice-coil motor,\u201d rotates actuator 121 back and forth along the directions of arrow B (FIG. 2) to move the heads 131, 132 along the path indicated by dashed arrow 201. The motion of actuator 121 thus changes both the radial and circumferential positions of heads 131, 132, but the circumferential positional change is relatively unimportant insofar as the platters are rotating. The motion of actuator 121 thus is used to control the radial position of heads 131, 132.\n\n(25) The location on surface 111 of platter 101 (the other surfaces are similar) of the aforementioned wedges is shown in FIG. 2. Each servo wedge 200 includes data identifying it by wedge index, track index, or sector number (to give an angular, tangential or circumferential position) and by data representing, at each point along a radius of the platter, the distance from spindle 104.\n\n(26) The tip of each arm 122-125 may include a micro-actuator 202 (FIG. 2). That is, in addition to the pivot point of actuator 121, a second pivot point 203 may allow the tip 204 of arm 122-125 to be rotated relative to arm 122-125 so that the centerline 214 of tip 204 is no longer collinear with the centerline 141 of the arm 122-125. A motor (not shown) similar to voice-coil motor 126 may control the micro-actuator rotation. The size of micro-actuator 202 is exaggeration in FIG. 2. Normally, micro-actuator 202 will be proportionally much smaller than the main body of arm 122-125.\n\n(27) If a particular one of arms 122-125 is between two disk platters and has heads on its upper and lower surface for reading from and writing to the respective lower and upper surfaces of two different platters, there may be two separate micro actuators 301, 302 as shown in FIG. 3 (disk drive controller 400 is omitted from FIG. 3 to avoid cluttering the drawing). Again, the sizes of micro-actuators 301, 302 are exaggerated relative to the main bodies of the respective arms 122-125.\n\n(28) Each of read heads 131, 132 is connected to one of a pair of read channels 401 of a hard drive controller 400 (there is a corresponding write channel 402) (FIG. 4). Hard drive controller 400 also includes a processor 410 and memory 411, as well as a connection 412 to a host processor (not shown). During normal disk operation, memory 411 may be used to store position error sensor (PES) data that indicates track position offsets. A servo control loop in hard drive controller 400 uses the PES data and the servo wedge data to keep the heads 131, 132 on track.\n\n(29) As part of the drive manufacturing process, or later but before first use of the drive, servo data may be written to servo wedges 200 on each surface of each platter using, e.g., the self-servo-write process described in commonly-assigned U.S. Pat. No. 10,971,187, which is hereby incorporated by reference herein in its entirety. As noted above, the availability of two RDCs in a drive equipped for TDMR allows the self-servo-write process to be performed on two disk surfaces at the same time, by allowing reading of servo spiral data from both surfaces at the same time. The single WDC can operate according to only one of the time bases, and implementations of the subject matter of this disclosure may be used to lock the timing of the time base of the WDC to the rotation of disk 101/102/103.\n\n(30) Self-servo-write operations may be followed by RRO-write operations, in which RRO data is written based on timing derived from concentric servo data written during the self-servo-write operations, as determined by the hard drive controller. For example, each cycle may begin with a servo-gating (SGATE) signal that is based on detection of a servo-sync-mark-found (SSMF) signal. However, in a dual-surface operation, after detection of SSMF and generation of an SGATE signal for a first (i.e., primary) surface, detection of SSMF for a secondary surface may occur too quickly for processing by the hard drive controller to generate an SGATE signal for the secondary surface.\n\n(31) The RRO data to be written is derived based on phase/timing/amplitude discrepancies relative to the head radial position based on the concentric servo data (e.g., the PES field) that was written during the self-servo-write operation. However, in a dual-surface implementation, the hard drive controller may be unable to demodulate the servo data to derive positioning data for the RRO fields from two surfaces within one cycle.\n\n(32) Therefore, according to implementations of the subject matter of this disclosure, an SGATE signal for the secondary surface may be generated internally to the self-servo-write circuitry, rather than in the hard drive controller. And the demodulated runout positioning data is stored in the servo memory SMEM, rather than in memory in (or controlled by) the hard drive controller. Accordingly, the subject matter of this disclosure is compatible with existing hard drive controllers.\n\n(33) In accordance with implementations of the subject matter of this disclosure, in a dual-surface RRO writing system the two surfaces are treated differently. One surface may be designated the primary surface. RRO writing on the primary surface is performed normally, reading the concentric servo data through one of read channels 401 to determine the runout data and to find SSMF to trigger SGATE. For the secondary surface, SGATE is generated by the self-servo-write system in the drive (rather than in the hard drive controller)\u2014e.g., a certain amount of time after SGATE for the primary surface\u2014and similarly the demodulated servo data that is read from the secondary surface through another of the read channels 401 is stored in SMEM 568 and is fetched from SMEM 568 upon completion of demodulation of the servo data from the primary surface. For writing of the RRO data to the primary and secondary surfaces, the single write data channel may be toggled between the write head for the primary surface and the write head for the secondary surface.\n\n(34) FIG. 5 is a diagram of a control system 500 according to implementations of the subject matter of this disclosure. Control system 500 reads servo data from two disk surfaces, keeps track of read and write head positions, and controls toggling of the write data channel to write to each of the two disk surfaces.\n\n(35) FIG. 5 depicts control system 500 as being coupled to two sets 501, 502 of read and write heads, each including a respective read head 503, 513 and a respective write head 504, 514. As depicted, the two sets 501, 502 of read/write heads serve opposing surfaces 515, 525 of a single disk platter 505. However, that is only an example, and control system 500 can be used to process any two surfaces in a multi-platter disk drive, even on different disk platters.\n\n(36) In addition to processing read and write signals in the manner described below, control system 500 also controls the movement of the read/write heads 501, 502. Specifically, control system 500 controls voice coil motor 126 (shown here schematically) which moves all heads at once as described above, as well as individual micro-actuator control motors 506 (shown here schematically), each respective one of which fine tunes the head positions on a respective single arm by moving a respective individual one of micro-actuators 202 (not shown in FIG. 5) on the respective arm.\n\n(37) As shown in FIG. 5, control system 500 includes a respective preamplifier 507, 517 interfaced with each respective set 501, 502 of read/write heads. Control system 500 may include as many preamplifiers (not shown) as there are sets of read/write heads (i.e., as many preamplifiers as there are disk surfaces) in the disk drive being controlled. Alternatively, because there are only two RDCs in the disk drive, there may be only two preamplifiers 507, 517, shared by all of the sets of read/write heads in the disk drive by using, e.g., multiplexers (not shown) to selectively couple the read/write heads to the preamplifiers.\n\n(38) Each respective preamplifier 507, 517 is capable of directing signals from its respective read head 503, 513 to either one of Read Data Channel 0 (R0) 518 or Read Data Channel 1 (R1) 528 in read/write control unit 508. The disk surface corresponding to one of read heads 503, 513 is designated the \u201cprimary\u201d surface. The selection of which of the two disk surfaces is \u201cprimary\u201d may be arbitrary. One of the two read channels 518, 528, to which the surface designated as \u201cprimary\u201d is coupled, is used to demodulate the servo signal from the primary surface. The output of the demodulation of the primary surface servo signal is used to lock the counters (not shown), which are used for timing in the self-servo-write system, to the disk rotation by adjusting a frequency generator in read/write control unit 508 to match the disk rotation frequency. For example, as shown, a phase-locked loop (PLL) 538 may be used as the frequency generator.\n\n(39) Disk lock control 519, which may be implemented, for example, in firmware 509, may use the primary surface servo signal as described below in connection with FIG. 6, to control the frequency of PLL 538, which can be used to adjust rotational velocity of storage media of the storage device. Disk lock control 519 may also use the primary surface servo signal to command main voice-coil motor control 520 to keep the radial position of the read head (503 or 513), that is used for the primary surface, aligned with the data track being processed.\n\n(40) The other one of read heads 503, 513\u2014i.e., the read head that is not being used for the primary surface\u2014is used for the secondary surface, and the secondary surface servo signal from that other one of read head 503, 513 is directed by the respective one of preamplifiers 507, 517 to the other one of read channels 518, 528\u2014i.e., the one that is not coupled to the primary surface servo signal. In the implementation seen in FIG. 5, the primary surface is coupled by preamplifier 507 to read channel 518, while the secondary surface is coupled by preamplifier 517 to read channel 528.\n\n(41) The output of the demodulation of the secondary surface servo signal\u2014e.g., the output of read channel 528 in the implementation shown in FIG. 5\u2014is used by \u201cdelta control\u201d 529 to command micro-actuator-2 control 522 to keep the secondary surface read head aligned with the track being read. The outputs of both read channels 518, 528 may be used by delta control 529 to determine any phase differential in the primary surface signal between successive passes, and in the secondary surface signal between successive passes.\n\n(42) With regard to head positioning, normally it may be sufficient to keep the micro-actuator of the arm carrying the primary read head (that micro-actuator is controlled by micro-actuator-1 control 521) in a neutral position (i.e., a position aligned with the main body of that arm). However, in some situations it may be necessary to adjust the micro-actuators of both heads, using micro-actuator-1 control 521 and micro-actuator-2 control 522, to maintain both heads over their respective tracks.\n\n(43) As noted above, the output of delta control 529 also is used to determine any phase differential in the primary surface signal between successive passes, and in the secondary surface signal between successive passes. As described in the preceding paragraph, normally the micro-actuator for the head on the primary surface is kept in a neutral position and the micro-actuator is moved only for the head on the secondary surface, meaning that normally a phase differential will occur between passes only for the secondary surface. However, if, as noted in the preceding paragraph, both micro-actuators are moved to keep both heads over their respective tracks, respective phase differentials may occur between passes for both the primary surface and the secondary surface. In any case, the result 539 of the determination of a phase differential by delta control 529 for either surface may be used by self-servo-write pattern generator 548 (in this implementation, part of self-servo-write finite state machine 558) to adjust the self-servo-write pattern for the that surface to account for that phase differential, as described, for example, in above-incorporated commonly-assigned U.S. Pat. No. 10,971,187.\n\n(44) Because of the importance of timing in these second implementations, control of the disk rotational frequency (disk synchronous write, or DSW, control) also is important. Typical DSW control measures the intervals between successive servo sync marks and uses feedback to control, e.g., a phased-lock loop to maintain constant disk rotational frequency. Because such a typical DSW approach looks only at the interval between the current servo sync mark and the immediately preceding servo sync mark, and only makes a correction when the interval deviates from the expected interval by more than a predetermined threshold, small errors\u2014each too small by itself to meet the threshold for correction\u2014can accumulate, but will never be detected.\n\n(45) Therefore, in accordance with implementations of the subject matter of this disclosure, an improved DSW control technique, which may be used in, e.g., disk lock control 519 (see above) compares the time of detection of the current servo sync mark not to the previous servo sync mark, but to the expected time of occurrence of the current servo sync mark, based on the time of occurrence of the initial servo sync mark when the system is initialized, the expected interval between successive servo sync marks, and the number of intervals that have passed since the initial servo sync mark. According to such a technique, once one servo sync mark is detected at a time other than the expected time, each subsequent servo sync mark\u2014even if each is detected at precisely the expected interval from the previous sync mark\u2014will deviate from its expected time by that same amount. And if multiple servo sync marks deviate from the expected interval by a small amount (below the detection threshold of the typical technique described above), then eventually the accumulated error will be detected, and corrective action\u2014e.g., adjusting the time base generator such as a phase-locked loop, or adjusting the disk angular velocity, or both\u2014can be taken.\n\n(46) WCNTR is a wedge timer or counter that, at the beginning of each servo wedge, resets to \u20180\u2019 and counts up until a next servo wedge is detected (or, optionally, until an adjustable limit is reached). An additional counter, NWCNTR, counts the number of wedges (i.e., increments by \u20181\u2019 each time a servo wedge is detected), optionally resetting when an adjustable limit, such as the maximum number of servo wedges on the disk, is reached. For n servo wedges, NWCNTR would count from 0 to n\u22121.\n\n(47) The difference between a typical DSW technique and the DSW technique described above (\u201cNew DSW\u201d) may be seen in the table in FIG. 6, which compares the two techniques over 17 servo wedges (NWCNTR=0, . . . , 16), and assumes that the target wedge-to-wedge (i.e., SSM-to-SSM) interval is 1000 time units. Row 601 shows the actual timestamp of the SSM detection events (SSMF), with the first wedge being detected at t=1000.\n\n(48) Row 604 shows the target SSMF timestamps based on that initial SSMF at t=1000. With a target interval of 1000 time units, the target timestamps for the second through seventeenth wedges are 2000, 3000, 4000, . . . , 17000. As seen on row 601, the actual timestamps for the second through seventeenth wedges are 2000.5, 3000.5, 4001, 5001, 6001, 7001.5, 8001.5, 9001.5, 10001.5, 11002, 12002, 13002, 14002, 15002, 16002 and 17002.\n\n(49) Rows 602 and 603 show how these discrepancies would be treated using a typical DSW technique. As shown on row 602, SSMF for the third, fifth, sixth, eighth, ninth, tenth and twelfth through seventeenth occurred at the expected interval of 1000 time units from the respective previous SSMF. Only for the second, fourth, seventh and eleventh wedges did each SSMF at an interval other than 1000 time units from the respective previous SSMF; in this example, SSMF for each of the second, fourth, seventh and eleventh wedges occurred at 1000.5 time units from the respective previous SSMF, which is a deviation for each of those wedges of 0.5 time units. According to a typical DSW technique, those deviations might be below a threshold of detection so that no corrective action would be taken, allowing errors on subsequent wedges to accumulate to 2 time units from their expected times (not detected at 603 but seen in the New DSW technique at 605).\n\n(50) In order to check actual occurrences of SSMF against the WCNTR timer for the technique described in connection with FIG. 6 (or for other DSW techniques), a location on the WCNTR timer \u201cramp\u201d can be selected at an initial time. As shown in FIG. 7, that location 701 on the timer ramp 702 may be at or near WCNTR=0, but any consistent location can be chosen. The duration of the interval 703 between SSMF 704 and the selected location 701 at the initial time is noted, and then periodically\u2014e.g., on each occurrence of a new servo wedge\u2014the actual duration between SSMF and the selected location is measured using time-stamp counter (TS_CNTR) 705, which is linked to the time base generator. If a change is detected in the actual duration between SSMF and the selected location, the relationship between the time base generator and the disk rotation can be adjusted.\n\n(51) In implementations of the subject matter of this disclosure, the time base generator (e.g., PLL 538; see FIG. 5 above) for the primary surface is locked to the disk rotation as discussed above and then the self-servo-write system is enabled. The time base counters WCNTR and NWCNTR are reset at the first SSMF signal, but then are set to continually run, resetting to zero at each passage of the expected wedge-to-wedge duration, as seen at 801 in timing diagram 800 of FIG. 8. Resets 801 correspond in time to SSMF signals 802 but in many implementations are not caused by SSMF signals 802. Rather, both the expected wedge-to-wedge duration, and the occurrences of SSMF, are based on same physical phenomenon\u2014i.e., the rotational speed of the disk.\n\n(52) As seen in timing diagram 800, the hard disk controller issues a read command at time T2, corresponding to SSMF 802 and WCNTR reset 801, to initiate a concentric servo read operation on the primary surface, by asserting SGATE signal 803. Each occurrence of SGATE 803 straddles the occurrence of SSMF 802, beginning at time T1 before occurrence of reset 801/SSMF 802. This is possible because timing diagram 800 represents a steady state after an initial period in which a first SGATE 803 may have occurred after reset 801/SSMF 802. With the expected wedge-to-wedge duration being known, SGATE 803 can begin before reset 801/SSMF 802. At time T3, approximately halfway through the wedge-to-wedge duration, the self-servo-write firmware issues internal SGATE signal 804 to initiate a servo read operation on the secondary surface. Internal SGATE signal 804 need not be precisely halfway between SGATE signals 803. However, halfway is convenient to avoid collisions.\n\n(53) At time T4, before beginning the next cycle of primary surface servo reading, demodulation of the secondary surface read operation initiated at time T3 is completed, followed at time T5 by demodulation of the primary surface read operation initiated at time T1, and the demodulated data is used to calculate positioning/timing data for storing the RRO data for each surface. That positioning/timing data may be stored as part of a write vector in servo memory (SMEM) 568. SMEM 568 may be configured as a circular buffer for storing the write vector.\n\n(54) As previously mentioned, there is only one write data channel. The RRO data to be written is output as signal W0 to both preamplifiers 507, 517. As seen in the timing diagram of FIG. 9, during each cycle 901 of WCNTR 911, as explained above, SGATE 912 is asserted once, and internal SGATE (INT SGATE) 913 also is asserted once\u2014about halfway between assertions of SGATE 912. To write the RRO data, WGATE signal 903 is asserted once for each surface\u2014i.e., twice for each cycle of WCNTR 911. The position in the servo wedge to which the RRO data is to be written is known, and WCNTR/NWCNTR are monitored so that WGATE signal 903 can be triggered when WCNTR reaches a value that corresponds to that position as demodulated from the servo data as described above.\n\n(55) The assertions of WGATE 903 occur just before the end of the respective SGATE 912 or INT SGATE 913 to which it applies. Whether assertion of WGATE 903 causes writing to the primary surface or the secondary surface is determined by WRITE SELECT (WS) signal 902. When WS 902 is asserted, WGATE 903 causes writing to the secondary surface, while when WS 902 is de-asserted, WGATE 903 causes writing to the primary surface, thereby alternating or interleaving the writing of RRO data to the primary and secondary surfaces with a single write channel.\n\n(56) Typically, the assertion of WGATE 903 would be handled by the servo interrupt service routine (servo ISR). However, in order to accommodate two assertions of WGATE 903 in each WCNTR cycle, the frequency of servo ISR would have to double, which could be a challenge for the hard drive controller. Accordingly, in implementations of the subject matter of this controller, the fetching, from SMEM 568, of the positioning/timing data for writing the RRO data, is controlled by a hardware SERVO VECTOR FETCH command.\n\n(57) As seen in the timing diagram 900 in FIG. 9, at time T1 901, when WS 902 selects the secondary surface, the SERVO VECTOR FETCH command loads the positioning/timing data for writing the RRO data that is to be subsequently written when WGATE 903 is asserted at time T3 904. At time T2 905, INT SGATE 913 for the secondary surface is asserted by the SERVO VECTOR FETCH routine. At time T3 904, RRO write for the secondary surface is started, writing the RRO data based on the positioning/timing data that was loaded at time T1 901. At time T4 906, when WS 902 selects the primary surface, the SERVO VECTOR FETCH command loads the positioning/timing data for writing the RRO data that is to be subsequently written when WGATE 903 is asserted at time T6 907. At time T5 908, SGATE 912 for the primary surface is asserted by the hard drive controller. At time T6 907, RRO write for the primary surface is started, writing the RRO data that based on the positioning/timing data was loaded at time T4 906. At time T7 909, triggered by an occurrence of SGATE 912 following completion of an RRO writing cycle, the servo ISR routine starts to process new servo demodulation results to keep the heads on track for the next cycle. The cycle begins again at time T1 910.\n\n(58) FIG. 10 is a flow diagram illustrating a method 1000 according to implementations of this disclosure for performing RRO write operations on two surfaces of a multi-surface storage device.\n\n(59) At 1001, a servo sync mark on a first track on a first storage medium surface of the storage device is detected with a first read head of the storage device, using a first read channel. At 1002, a first recurring servo-gating signal for the first read channel is established at a successive fixed interval from the servo sync mark. At 1003, servo signals from the first track are detected with the first read head on occurrence of the first recurring servo-gating signal. At 1004, the servo signals from the first track are processed to generate first signals for positioning the first read head relative to the first track for writing first repeatable run-out data. At 1005, a second recurring servo-gating signal for a second read channel is established at an offset from the first recurring servo-gating signal. At 1006, servo signals from a second track are detected with the second read head on occurrence of the second recurring servo-gating signal. At 1007, the servo signals from the second track are processed to generate signals for positioning the second read head relative to the second track for writing second repeatable run-out data. At 1008, the first repeatable run-out data is written to servo wedges of the first track, and the second repeatable run-out data is written to servo wedges of the second track, using a single write data channel of the storage device, and method 1000 ends.\n\n(60) Thus it is seen that a method of performing RRO write operations on two surfaces of a multi-surface storage device, and a multi-surface storage device configured for such a method, have been provided.\n\n(61) As used herein and in the claims which follow, the construction \u201cone of A and B\u201d shall mean \u201cA or B.\u201d\n\n(62) It is noted that the foregoing is only illustrative of the principles of the invention, and that the invention can be practiced by other than the described embodiments, which are presented for purposes of illustration and not of limitation, and the present invention is limited only by the claims which follow."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 19,
      "claims_start": 17,
      "description_end": 17,
      "description_start": 11,
      "drawings_end": 10,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "number_of_claims": 20,
      "number_of_drawing_sheets": 9,
      "number_of_figures": 10,
      "page_count": 19,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 17,
      "specification_start": 11,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006126434,
    "field_of_search_us": [
      "Non/e"
    ],
    "group_art_unit": "2688",
    "guid": "US-11557320-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G20/573/115",
    "intl_class_current_primary": [
      {
        "intl_class": "G11B",
        "intl_subclass": "5/596",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G11B5/596"
    ],
    "inventors": [
      {
        "city": "San Jose",
        "country": "US",
        "name": "Katchmart; Supaket",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Katchmart; Supaket",
    "patent_title": "Dual-surface RRO write in a storage device servo system",
    "primary_examiner": "Agustin; Peter Vincent",
    "publication_date": "2023-01-17",
    "publication_number": "11557320",
    "related_apps": [
      {
        "filing_date": "2021-01-25",
        "number": "63141441"
      }
    ],
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": true,
        "patentee_name": "Billings",
        "pub_month": "2001-06-01",
        "publication_number": "6249393"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Everett",
        "pub_month": "2014-04-01",
        "publication_number": "8711504"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Coker",
        "pub_month": "2014-07-01",
        "publication_number": "8767341"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Guo",
        "pub_month": "2015-01-01",
        "publication_number": "8934186"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "French, Jr.",
        "pub_month": "2015-06-01",
        "publication_number": "9053728"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Guo",
        "pub_month": "2015-09-01",
        "publication_number": "9129630"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Katchmart",
        "pub_month": "2021-04-01",
        "publication_number": "10971187"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Kim",
        "pub_month": "2005-08-01",
        "publication_number": "20050174672"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Kawabe",
        "pub_month": "2007-10-01",
        "publication_number": "20070242387"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Takeda",
        "pub_month": "2008-10-01",
        "publication_number": "20080239554"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Mizukoshi",
        "pub_month": "2009-07-01",
        "publication_number": "20090168218"
      }
    ]
  },
  {
    "app_filing_date": "2021-03-10",
    "appl_id": "17197933",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Tokyo",
        "country": "JP",
        "name": "Yahoo Japan Corporation",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Tokyo",
        "country": "JP",
        "name": "Yahoo Japan Corporation",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "attorney_name": [
      "Procopio, Cory, Hargreaves & Savitch LLP"
    ],
    "composite_id": "1000006811750!US-US-11556547",
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/24578",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/248",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/24573",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/24575",
        "version": "2019-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A determination device according to the present application has an acquisition unit, a categorization unit, and a determination unit. The acquisition unit acquires the search queries, which have been input by a plurality of input customers who have input a reference query. The categorization unit categorizes the search queries, which have been input in a predetermined period among search queries, into a plurality of categories. The determination unit determines whether a categorization result by the categorization unit satisfies a predetermined determination condition or not.",
      "brief": "CROSS-REFERENCE TO RELATED APPLICATIONS\n\n(1) The present application claims priority to and incorporates by reference the entire contents of Japanese Patent Application No. 2020-050222 filed in Japan on Mar. 19, 2020\n\nBACKGROUND OF THE INVENTION\n\n1. Field of the Invention\n\n(2) The present disclosure relates to a determination device, a determination method, and a determination program.\n\n2. Description of the Related Art\n\n(3) As the Internet has become common, various information analysis techniques have been proposed. For example, various query log analysis techniques have been proposed. The query log analysis techniques can be applied to search advertising. For example, in order to present search keyword options of advertisement, analyzing the search queries input by customers has been proposed. However, in the above described technique, there are cases in which it cannot be said that the search queries input by customers are appropriately analyzed.\n\n(4) For example, in the above described technique, the search queries are merely analyzed in accordance with the frequency by which the search queries are input. Therefore, with the above described technique, facts that how customers became to be related to various targets of various events, companies, etc. may not be found out.\n\n(5) The present application has been accomplished in view of the foregoing, and it is an object to more appropriately analyze the relations between customers and a target indicated by a predetermined search query.\n\nSUMMARY OF THE INVENTION\n\n(6) It is an object of the present invention to at least partially solve the problems in the conventional technology.\n\n(7) According to one aspect of the subject matter described in this disclosure, a determination device includes (i) an acquisition unit configured to acquire a search query(ies) input by a plurality of input customers who have input a reference query, (ii) a categorization unit configured to categorize the search queries input in a predetermined period among the search queries into a plurality of categories, and (iii) a determination unit configured to determine whether a categorization result by the categorization unit satisfies a predetermined determination condition or not.\n\n(8) The above and other objects, features, advantages and technical and industrial significance of this invention will be better understood by reading the following detailed description of presently preferred embodiments of the invention, when considered in connection with the accompanying drawings.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A determination device comprising: a processor configured to: acquire a search query(ies) input by a plurality of input customers who have input a reference query; categorize the search queries input in a predetermined period among the search queries into a plurality of categories; determine whether a categorization result of the categorization satisfies a predetermined determination condition or not; extract a search query categorized into the category that satisfies a predetermined categorization condition among the plurality of categories if it is determined that the categorization result satisfies the predetermined determination condition; and provide a list of the extracted search queries.   \n\n2. The determination device according to claim 1, wherein the processor is further configured to: determine whether a number of the categories into which the search queries have been categorized satisfies a predetermined condition or not.  \n\n3. The determination device according to claim 2, wherein the processor is further configured to: determine whether the number of the categories into which the search query of predetermined percentage or higher among the search queries are categorized is equal to or less than a threshold value.  \n\n4. The determination device according to claim 2, wherein the processor is further configured to: specify the category into which the search query of predetermined percentage or higher among the search queries is categorized and determine whether the number of the customer who have input the search query categorized into the specified category is equal to or higher than a predetermined threshold value.  \n\n5. The determination device according to claim 1, wherein the processor is further configured to: acquire the search query input in a period before reference time and date, at which the input customer has input a reference query, by predetermined time and date as the predetermined period.  \n\n6. The determination device according to claim 5, wherein the processor is further configured to: acquire the search query(ies) input in a mutually different plurality of periods based on the reference time and date; categorize the search queries into the plurality of categories for each of the period; and determine whether the categorization result satisfies the predetermined determination condition or not for each period.  \n\n7. The determination device according to claim 1, wherein the processor is further configured to: determine whether a high-level category into which the category into which the search query is categorized satisfies a predetermined condition or not.  \n\n8. The determination device according to claim 1, wherein the processor is further configured to: output information that the reference query is appropriate if it is determined that the categorization result satisfies the predetermined determination condition.  \n\n9. The determination device according to claim 1, wherein the processor is further configured to: output information that the plurality of categories into which the search queries are categorized are appropriate if it is determined that the categorization result satisfies the predetermined determination condition.  \n\n10. The determination device according to claim 1, wherein the processor is further configured to: specify the category into which the search query of predetermined percentage or higher is categorized among the search queries and extracts the search query categorized into the specified category.  \n\n11. The determination device according to claim 1, wherein the processor is further configured to: extract the search query having high relativity with the reference query among the search queries.  \n\n12. The determination device according to claim 1, wherein the processor is further configured to: specify, as a keyword used in targeting based on a first search query, a second search query input before the first search query is input and related to the first search query among the search queries.  \n\n13. A determination method executed by a determination device, the method including: acquiring a search query(ies) input by a plurality of input customers who have input a reference query; categorizing the search queries input in a predetermined period among the search queries into a plurality of categories; determining whether a categorization result of the categorization satisfies a predetermined determination condition or not; extracting a search query categorized into the category that satisfies a predetermined categorization condition among the plurality of categories if it is determined that the categorization result satisfies the predetermined determination condition; and providing a list of the extracted search queries.  \n\n14. A non-transitory computer readable storage medium having a determination program thereon, the determination program causes a computer to perform: acquiring a search query(ies) input by a plurality of input customers who have input a reference query; categorizing the search queries input in a predetermined period among the search queries into a plurality of categories; determining whether a categorization result of the categorization satisfies a predetermined determination condition or not; extracting a search query categorized into the category that satisfies a predetermined categorization condition among the plurality of categories if it is determined that the categorization result satisfies the predetermined determination condition; and providing a list of the extracted search queries.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) FIG. 1 is an explanatory diagram illustrating an example of a time-series data providing process which provides time-series data of search queries according to an exemplary embodiment of the present disclosure;\n\n(2) FIG. 2 is an explanatory diagram illustrating an example of a grouping evaluation process of evaluating grouping of time-series search queries according to the exemplary embodiment of the present disclosure;\n\n(3) FIG. 3 is an explanatory diagram illustrating an example of a time-interval modifying process of modifying the time interval of the search-query time series according to the exemplary embodiment of the present disclosure;\n\n(4) FIG. 4 is an explanatory diagram illustrating an example of a search-query predicting process of predicting a search query of a customer based on time-series data of search queries according to an exemplary embodiment of the present disclosure;\n\n(5) FIG. 5 is a diagram illustrating an example of an information providing system according to the embodiment;\n\n(6) FIG. 6 is a diagram illustrating an example of a search-query database according to the embodiment;\n\n(7) FIG. 7 is a diagram illustrating an example of a configuration of a first determination processing unit according to the embodiment;\n\n(8) FIG. 8 is a diagram illustrating an example of a configuration of a second determination processing unit according to the embodiment;\n\n(9) FIG. 9 is a diagram illustrating an example of a configuration of a learning processing unit according to the embodiment;\n\n(10) FIG. 10 is a flow chart illustrating an example of a process executed by an information providing device according to the embodiment for determining whether the categorization of keywords or search queries for categorizing customers is appropriate or not;\n\n(11) FIG. 11 is a flow chart illustrating an example of a process executed by the information providing device according to the embodiment for determining whether the period in the time series of search queries is appropriate or not;\n\n(12) FIG. 12 is a flow chart illustrating an example of a process executed by the information providing device according to the embodiment for learning a model for predicting a transition of categories of search queries input by customers; and\n\n(13) FIG. 13 is a diagram illustrating an example of a hardware configuration.\n\nDETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS\n\n(14) Hereinafter, embodiments of the present disclosure will be described in detail with reference to drawings. Note that the present invention is not limited by the embodiments. Details of one or a plurality of the embodiments are described in the following description and drawings. Also, a plurality of the embodiments can be appropriately combined within a range that does not cause a conflict in processing contents. Also, in following one or a plurality of the embodiments, the same parts are denoted by the same reference signs, and redundant description will be omitted.\n\n1. Exemplary Embodiment\n\n(15) First, with reference to FIG. 1, FIG. 2, FIG. 3, and FIG. 4, an exemplary embodiment of the present disclosure will be described in detail. An information providing device according to the exemplary embodiment executes an information providing process described below to find out a user who has particular needs from time-series search queries. Hereinafter, with reference to FIG. 1, FIG. 2, FIG. 3, and FIG. 4, the information providing process according to the exemplary embodiment will be described.\n\n(16) 1-1. Time-Series Data Providing Process\n\n(17) FIG. 1 is an explanatory diagram illustrating an example of a time-series data providing process which provides time-series data of search queries according to the exemplary embodiment of the present disclosure. In the exemplary embodiment, the information providing process is carried out by an information providing device 100 illustrated in FIG. 1. The information providing device 100 is an example of a determination device. Although it is not illustrated in FIG. 1, a network such as the Internet (for example, a network N described later with reference to FIG. 5) connects the information providing device 100, user devices 500.sub.1 to 500.sub.n (n is an arbitrary natural number), a log server 600, and an operator device 700 illustrated in FIG. 1.\n\n(18) In the example of FIG. 1, the information providing device 100 is illustrated as a server. In this example, the information providing device 100 provides a history list of search queries to the operator device 700 so that an operator (Operator) OP can specify needs of users (also referred to as \u201ccustomers\u201d) from the history of search queries. The history list of the search queries include, for example, search queries input in each of periods by a target user(s) (also referred to as \u201ctarget customer(s)\u201d). The target user is a user who has input a predetermined search query at certain time and date. In the present specification, the predetermined search query input at certain time and date may be referred to as \u201creference query\u201d.\n\n(19) In the example of FIG. 1, the user devices 500.sub.1 to 500.sub.n are illustrated as smartphones. For example, the user device 500.sub.1 is used by a user U1, and the user 500.sub.2 is used by the user U1. In the present specification, if there is no need to distinguish the user devices 500.sub.1 to 500.sub.n, the user devices 500.sub.1 to 500.sub.n are collectively referred to as \u201cuser devices 500\u201d. In this example, the user U1 and the user U2 are those who use a search service. The user device 500 requests the log server 600 to carry out searches about various information. For example, the user device 500 transmits a search request, which includes a search query, to the log server 600.\n\n(20) In the example of FIG. 1, the log server 600 is illustrated as a server. In this example, the log server 600 provides a search service(s) such as a portal application, a portal site, and/or the like to the users. In response to the search request from the user device 500, the log server 600 provides search results about various information to the user device 500. The log server 600 can accumulate search queries, which are included in search requests, as search logs. The log server 600 can provide the search logs to the information providing device 100.\n\n(21) In the example of FIG. 1, the operator device 700 is illustrated as a laptop personal computer (PC). The operator device 700 is used by the operator OP. The operator OP is, for example, a person of a particular Internet company. In this example, the operator OP provides above described \u201creference query\u201d, which is a predetermined search query, to the information providing device 100 in order to specify needs of the users from the history of search queries. For example, if the target users are the users who have input a search query \u201cnewborn baby\u201d at certain time and date, the operator device 700 provides the search query \u201cnewborn baby\u201d as a reference query to the information providing device 100. This means that the operator OP wants to specify the needs of such users from the history of the search queries of the users who have input the search query \u201cnewborn baby\u201d at certain time and date. For example, it is conceivable that the operator OP wants to know what search query(ies) has been input around (for example, 30 to 60 days after) reference time and date by the user who has input the search query \u201cnewborn baby\u201d at the reference time and date (for example, 2020/03/19).\n\n(22) As illustrated in FIG. 1, first, the user device 500 transmits a search query to the log server 600 (step S1).\n\n(23) Then, the log server 600 provides a search result to the user device 500 (step S2).\n\n(24) Then, the information providing device 100 acquires a search log from the log server 600 (step S3). For example, the information providing device 100 collects histories of search queries from the log server 600.\n\n(25) Then, the information providing device 100 acquires a reference query from the operator device 700 (step S4). As described above, the reference query is a predetermined search query input at certain time and date. In the example of FIG. 1, the reference query is a query \u201cnewborn baby\u201d input at certain time and date (for example, certain reference time and date).\n\n(26) Then, the information providing device 100 specifies an input customer(s) who have input the acquired reference query (step S5). For example, the information providing device 100 specifies the input customers, who have input the reference query, from the acquired search logs. In the example of FIG. 1, the information providing device 100 specifies users, who have input the search query \u201cnewborn baby\u201d at certain time and date, as the input customers. In the present specification, the input customers may be referred to as \u201cinput users\u201d.\n\n(27) Then, the information providing device 100 specifies search queries input by the customers in periods based on the reference time and date (step S6). The periods based on the reference time and date are the periods around the reference time and date. For example, the interval of the periods may be one month. For example, in a case in which the reference time and date is \u201c2020/03/19\u201d, the periods around the reference time and date are the periods after the reference time and date such as \u201c2020/03/19 to 2020/04/19\u201d and \u201c2020/04/19 to 2020/05/19\u201d or the periods before the reference time and date such as \u201c2020/02/19 to 2020/03/19\u201d and \u201c2020/01/19 to 2020/02/19\u201d. In the present specification, the period after the reference time and date and the period before the reference time and date may be referred to as a \u201cpositive period\u201d and a \u201cnegative period\u201d, respectively. In this manner, the information providing device 100 extracts the search queries of the users, who have input the reference query, from the acquired search logs in each period based on the reference time and date.\n\n(28) Then, the information providing device 100 generates a list of the search queries, which have high relativity with the reference query, for each period (step S7). The relativity between the reference query and other search queries can be determined based on the relevance degrees between the search queries. For example, the relevance degrees between the search queries can indicate how often a single user inputs these search queries at the same time. For example, if many users input a search query \u201cnewborn baby\u201d and a search query \u201cShichi-go-san (seven-five-three festival)\u201d at the same time, the relevance degree between the search query \u201cnewborn baby\u201d and the search query \u201cShichi-go-san\u201d may be high. In the example of FIG. 1, the information providing device 100 specifies, from the acquired search logs, for example, search queries such as \u201chospital stay\u201d, \u201critual visit\u201d, and \u201cweight\u201d as the search queries having high relativity with the reference query \u201cnewborn baby\u201d one month before the reference time and date. Then, the information providing device 100 generates a list including the specified search queries.\n\n(29) In some implementation modes, the information providing device 100 may determine the relativity between the search queries by using word embedding. The information providing device 100 may obtain embedding vectors by training a language model by using training data including word strings. For example, keywords corresponding to the search queries included in the search logs may be used as the training data including the word strings. The relativity between the search queries may be cosine similarity between the embedding vectors corresponding to the search queries.\n\n(30) Then, the information providing device 100 provides the generated list to the operator device 700 (step S8). In the example of FIG. 1, the generated list illustrates the search queries which have high relativity with the reference query \u201cnewborn baby\u201d for each period. As illustrated in FIG. 1, each period is the period around the reference time and date. For example, a period \u201c\u22121 to 0 month\u201d is a one-month period before the reference time and date. Also, for example, a period \u201c0 to 1 month\u201d is a one-month period after the reference time and date. The generated list illustrates the search queries which have been input by the users, who have input the reference query at the reference time and date, and have high relativity with the reference query in the periods around the reference time and date. In the example of FIG. 1, for example, the search queries which have been input by the users, who have input the reference query \u201cnewborn baby\u201d at the reference time and date (for example, \u201c2020/03/19\u201d), and have high relativity with the reference query \u201cnewborn baby\u201d in the period \u201c\u22121 to 0 month\u201d (for example, the period \u201c2020/02/19 to 2020/03/19\u201d) are search queries such as \u201chospital stay\u201d, \u201critual visit\u201d, and \u201cweight\u201d.\n\n(31) Note that the vertical-direction order of the list may illustrate the degree of the relativity between the reference query and the other search queries. For example, the relevance degree between the reference query \u201cnewborn baby\u201d and the search query \u201chospital stay\u201d may be higher than the relevance degree between the reference query \u201cnewborn baby\u201d and the search query \u201critual visit\u201d. The information providing device 100 may generate a table in which other search queries input in each period are arranged in the order of relevance degrees and provide the generated table to the operator device 700 as a list.\n\n(32) 1-2. Grouping Evaluation Process\n\n(33) If the list of the search queries is generated simply based on the relativity between the reference query and other search queries, the context (also referred to as \u201crelation\u201d) of the reference query may be dispersed. The term \u201ccontext of the reference query\u201d represents the context of input of the reference query, the background of input of the reference query, the circumstances in which the user who has input the reference query is in, the behavior pattern, interest, or concern of the user who has input the reference query, etc.\n\n(34) For example, in a case in which the reference query is a company C1 (exemplary company name), users who have input this reference query include, for example, users who like the company president of the company C1, the users who like a mascot of the company C1, the users who want to change a model of a mobile phone of the company C1, users who want to go to an amusement park run by the company C1, and users who use a comics (manga) browsing service provided by the company C1. In such a case, the context of the reference query is different depending on the user. Therefore, the search queries included in the generated list may vary. For example, the generated list may include search queries such as \u201ccompany president P1 (exemplary name of a person)\u201d, \u201cthat dog (exemplary mascot name)\u201d, \u201csmartphone SP1 (exemplary smartphone name)\u201d, \u201ccountry of dreams and magic (exemplary name of facilities)\u201d, and \u201cthat pirate (exemplary comics name)\u201d. It is sometimes difficult for the operator OP to extract useful information (for example, typical needs of users) from the generated list if the context of the reference query is dispersed. In addition, the operator OP may not be able to appropriately find out the transition of the needs of the users from the time series of the search queries included in the generated list.\n\n(35) Therefore, the information providing device 100 groups the search queries, which have been input in each period by target users, in order to enable the operator OP to appropriately evaluate the list of the search queries. The information providing device 100 can group the search queries, which have been input in each period, based on the context of the reference query. For example, the information providing device 100 acquires the search logs of the users, who have input the reference query, for each time series (for example, period around the reference time and date). Then, the information providing device 100 groups the search queries for each time series. For example, if the number of groups or the number of the users who have input the search queries related to groups satisfies a threshold value, the information providing device 100 extracts the search queries related to the groups as the search queries included in the list. As a result, the grouped search queries included in the list can have coherence, thereby enabling the operator OP to appropriately evaluate the list of the search queries.\n\n(36) FIG. 2 is an explanatory diagram illustrating an example of a grouping evaluation process of evaluating grouping of time-series search queries according to the exemplary embodiment of the present disclosure.\n\n(37) As illustrated in FIG. 2, first, the information providing device 100 acquires a reference query and designated categories from the operator device 700 (step S11). As described above, the reference query is a predetermined search query (for example, a search query \u201ccompany C1\u201d) input at certain time and date. On the other hand, the designated categories are the categories for grouping the time-series search queries. The information providing device 100 groups the search queries for each time series (for example, the period around the reference time and date) in accordance with the designated categories. The designated categories are designated, for example, in advance by the operator OP. The designated categories are, for example, categories such as company presidents, mascots, model change, amusement parks, comics, and the like. In this manner, the information providing device 100 receives designated categories from the operator OP.\n\n(38) Then, the information providing device 100 specifies input customers and specifies the search queries input by each of the input customers (step S12). As described above, the input customers are the users who have input the reference queries at the reference time and date. The information providing device 100 can specify the input customers from the search logs. Also, the information providing device 100 can specify the search queries, which have been input in the periods around the reference time and date by the input customers, from the search logs. In the example of FIG. 2, search queries input by an input customer IU1 include a search query #1-1 and a search query #1-2. Similarly, search queries input by an input customer IU2 include a search query #2-1 and a search query #2-2, and search queries input by an input customer IU3 include a search query #3-1 and a search query #3-2. For example, the search query #1-1 may be the company president P1. Also, for example, the search query #1-2 may be that dog (exemplary mascot name).\n\n(39) Then, the information providing device 100 specifies the search queries input in each period (step S13). As described above, each period is a period around the reference time and date. In the example of FIG. 2, the periods around the reference time and date include a first period (positive period), a first period (negative period), a second period (negative period), and so on. For example, the first period (positive period) may be the above described period \u201c0 to 1 month\u201d, which is a one-month period from the reference time and date. Also, for example, the first period (negative period) may be the above described period \u201c\u22121 to 0 month\u201d, which is a one-month period before the reference time and date. Also, for example, the second period (negative period) may be the above described period \u201c\u22122 to \u22121 month\u201d, which is a period 2 months before the reference time and date.\n\n(40) Then, the information providing device 100 specifies an element group of the number of queries or the number of customers of each designated category in each period (step S14). The term \u201celement group\u201d may include an array of the values corresponding to the grouped search queries. For example, the element group of time-series search queries may be an array of the numbers of grouped search queries (in other words, the search queries belonging to particular designated categories) in particular time series (for example, periods). Alternatively, the element group of time-series search queries may be an array of the numbers of the users who have input the search queries belonging to each designated category in particular time series. In other words, the element group may be an array of the values of indexes generated by decomposing the search queries by categories. The term \u201celement group\u201d includes, for example, a data element group such as an array of data elements, a distribution of data elements, etc. This data element may include, for example, the number of the search queries belonging to a particular category or the number of the users who have input the search queries belonging to a particular category.\n\n(41) Regarding a display mode of the above described element group, the array of the values of the indexes (for example, the array of the numbers of the grouped search queries in particular time series) can be presented by using a bar graph. For example, the heights in the bar graph represents the numbers of queries or the numbers of users of each designated category in a certain period. The height in the bar graph representing a designated category #1 may be 250, the height in the bar graph representing a designated category #2 may be 200, and the height in the bar graph representing a designated category #3 may be 150.\n\n(42) In the example of FIG. 2, for example, the information providing device 100 specifies the element group of the number of the search queries or the number of the users of each designated category in the second period (negative period). For example, the number of the search queries input in the second period and belonging to the designated category #1 is \u201c250\u201d. Alternatively, the number of the users who have input the search queries input in the second period and belonging to the designated category #1 may be \u201c250\u201d. The designated category #1 may be a category \u201ccompany president\u201d, or the designated category 21 may be a category \u201cmascot\u201d. In this case, the search query \u201ccompany president P1 (exemplary name of a person)\u201d belongs to the category \u201ccompany president\u201d. On the other hand, the search query \u201cthat dog (exemplary mascot name)\u201d belongs to the category \u201cmascot\u201d. In this manner, the information providing device 100 can specify the element group of the search queries by counting the number of the search queries, which have been categorized into the designated categories, and the number of the customers who have input the search queries.\n\n(43) Note that, in this exemplary embodiment, the information providing device 100 specifies the element group of the number of queries or the number of customers of each designated category in each period, but is not limited thereto. The information providing device 100 may categorize the search queries into a plurality of categories for each period. The plurality of categories may include a designated category. In other words, the information providing device 100 may group the search queries for each time series in accordance with the categories other than the designated categories. If categories are not fixed in advance (for example, if the information providing device 100 has not acquired designated categories), the information providing device 100 may categorize the search queries in the search logs into a plurality of categories (for example, categories other than designated categories) based on the reference query and the search logs. If the reference query is fixed in advance, the information providing device 100 can search for a category appropriate for this reference query. In other words, the information providing device 100 may determine whether the reference query is appropriate or not based on the category designated in advance or may search for an appropriate category for this reference query based on the reference query designated in advance.\n\n(44) Then, the information providing device 100 determines whether the element group satisfies a predetermined condition or not and evaluates the reference query or the designated category from the determination result (step S15). The predetermined condition is, for example, a condition that \u201cthe number of the groups of search queries converges to a particular value (for example, a natural number n)\u201d. The groups are, for example, a plurality of categories including a designated category or a category other than the designated category. For example, if the percentage of the search queries belonging to n designated categories with respect to all queries in each period (in other words, all search logs in each period) satisfies a threshold value (for example, 80%) (for example, n is \u201c3\u201d), the information providing device 100 can use the n designated categories as the groups for the list of the search queries. For simplicity, in the example of FIG. 2, it is assumed that the groups are does not include a category other than the designated category. For example, if the number of designated categories is \u201c3\u201d and if these three designated categories include 80% of search queries or customers, the information providing device 100 determines that these three designated categories satisfy the predetermined condition.\n\n(45) Regarding the number of the groups, if the number of the groups is large, the information providing device 100 can determine that the original reference query is not an appropriate query. If the number of groups in a certain period is equal to the number of groups in another period, the information providing device 100 can determine that the original reference query is an appropriate query. The information providing device 100 can determine whether the list of the search queries is appropriate or not based on whether the element group of the number of queries or the number of users is appropriately expressed or not. If the category is fixed in advance (for example, if the category is a designated category), the information providing device 100 can evaluate the element group of the number of queries or the number of users based on the categories and the search logs and, therefore, can specify an appropriate reference query.\n\n(46) In the example of FIG. 2, the predetermined condition is, for example, a condition that \u201cin any of the periods, the queries or the customers included in the designated category is 30% or less\u201d. If the element group satisfies this condition, the information providing device 100 determines that the designated category is not appropriate or the reference query is not appropriate. In addition to or alternatively, the predetermined condition may be, for example, a condition that \u201cvariation in the percentage of the search queries or customers included in the designated category in each period exceeds a threshold value\u201d. If the element group satisfies this condition, the information providing device 100 determines that the designated category is not appropriate or the reference query is not appropriate. In this manner, the information providing device 100 can evaluate the reference query and the designated category by determining whether the element group of the search queries satisfies the predetermined condition or not. If the element group converges, the information providing device 100 can determine that the list of the search queries is appropriate.\n\n(47) As described above, the determination result is used for determining whether the reference query is appropriate or not. For example, if the reference query satisfies the predetermined condition in each period, the information providing device 100 determines that the reference query is appropriate. The information providing device 100 may determine that the reference query is appropriate if the reference query satisfies the condition in the majority of periods. If the reference query does not satisfy the predetermined condition in the periods, the reference query may not be appropriately expressing a desired target of the operator OP.\n\n(48) Then, the information providing device 100 provides the evaluation result to the operator device 700 (step S16). For example, the information providing device 100 provides the information corresponding to the determination result to the operator OP. The information providing device 100 may automatically optimize categories (for example, designated category, the categories other than the designated category) based on the determination result. The information providing device 100 may display, by the operator device 700, a message indicating that the designated category is not appropriate or the reference query is not appropriate. Alternatively, the information providing device 100 may display, by the operator device 700, a message indicating that the search behavior of users is appropriately expressed by decomposing the time-series search queries into the element group of the category. The information providing device 100 can provide a list that satisfies the conditions about the element groups to the operator device 700. The number (for example, \u201c3\u201d) of arrays in a certain period may be the same number of arrays in another period. In this case, the operator OP can read transition modes of designated categories from the list. Such designated categories in each period can be paraphrased and applied to analysis of time-series search behavior.\n\n(49) As described above, the information providing device 100 can group the search queries in each period so that the contents of the search queries in each period are not varied. Therefore, the information providing device 100 can provide, to the operator OP, a list of useful search queries which can be used for analysis of particular needs for company presidents, mascots, model change, etc.\n\n(50) Note that the above described designated categories and the categories other than the designated categories may belong to a higher-level category(ies) than these categories. The high-level categories are transition categories described later with reference to FIG. 4. The transition category shows a path to a target shown by a reference query. For example, if, in the order of a category \u201cmascot\u201d, a category \u201ccomics\u201d, and a category \u201camusement park\u201d, search queries respectively corresponding to these categories are input, the transition mode of the categories of this user can be categorized into a transition category \u201ca path from mascot to amusement park via comics\u201d. The transition categories will be described below in detail with reference to FIG. 4. The designated category is a lower-level category corresponding to the transition category. In this exemplary embodiment, the information providing device 100 can determine whether the designated category is appropriate or not. Therefore, the information providing device 100 enables the operator OP to appropriately capture the search behavior of the users.\n\n(51) 1-3. Time-Interval Modifying Process\n\n(52) The operator OP sometimes does not know how the length of each period based on the above described reference time and date should be set. As described above, the periods based on the reference time and date are the periods around the reference time and date. For example, an appropriate length of the period for finding out search behavior may be 1 month. Alternatively, the length of the appropriate period may be one week. The operator OP may want to set an appropriate period for analyzing search behavior as each period in order to extract useful information (for example, relativity between search behavior) from the list of search queries. If the length of each period is not appropriate (for example, the length of each period is one year), the operator OP may not be able to appropriately find out the transition mode of categories of users. If the length of the period changes, the ranking of the relevance degrees between the reference query and the search queries also changes. For example, if the length of each period is too short, the list of search queries may include many buzzwords (in other words, words which have been popular topics in a particular period). In such a case, the operator OP may not be able to find out the change of the search queries related to the reference query from the list of search queries.\n\n(53) Therefore, the information providing device 100 specifies an appropriate length of the period by adjusting the length of the period. The information providing device 100 adjusts the density of search-query time series by using the above described element groups of the search queries.\n\n(54) FIG. 3 is an explanatory diagram illustrating an example of a time-interval modifying process of modifying the time interval of the search-query time series according to the exemplary embodiment of the present disclosure.\n\n(55) As illustrated in FIG. 4, first, as well as the above described case of step S11, the information providing device 100 acquires a reference query and designated categories from the operator device 700 (step S21). Then, as well as the above described case of step S12, the information providing device 100 specifies input customers and specifies the search queries input by each of the input customers (step S22). Then, as well as the above described case of step S13, the information providing device 100 specifies the search queries input in each period (step S23). Then, as well as the above described case of step S14, the information providing device 100 specifies an element group of the number of queries or the number of customers of each designated category in each period (step S24). Herein, redundant description is omitted.\n\n(56) Then, the information providing device 100 determines whether the element group satisfies a predetermined condition or not and determines whether the setting of each period is appropriate or not from the determination result (step S25). In the example of FIG. 3, the designated categories are fixed to particular categories. The particular category may be, for example, one of categories such as marriage, pregnancy, relocation, etc. Alternatively, the particular category may be one of the categories described above with reference to FIG. 2 such as company presidents, mascots, model change, amusement parks, comics, etc. The designated categories may be optimized by the above described grouping evaluation process. For example, in a case in which the reference query is a search query \u201cnewborn baby\u201d, the particular category may be a category such as marriage, pregnancy, or relocation. For example, a designated category #1 may be a category \u201cmarriage\u201d, a designated category #2 may be a category \u201cpregnancy\u201d, and a designated category #3 may be a category \u201crelocation\u201d.\n\n(57) In the example of FIG. 2, if variation in the element group in each period is equal to or less than a predetermined threshold value, the information providing device 100 determines that the length of each period is appropriate. For example, if the element groups are common among the periods, the information providing device 100 determines that the length of each period is appropriate. If the element group is shifted in a certain period, the information providing device 100 modifies the length of this period. If dispersion of the element group in a predetermined period is equal to or higher than a predetermined threshold value, the information providing device 100 determines that the length of the predetermined period is too long. In this manner, the information providing device 100 modifies periods so that the element groups are common among the periods.\n\n(58) If the type of the users who have input the search queries in each period satisfies a predetermined condition, the information providing device 100 may determine that the length of each period is appropriate. The type of the users is, for example, an attribute of the users such as a demographics attribute, a psychographics attribute, or the like. For example, the information providing device 100 may determine whether the length of each period is appropriate or not based on the male-to-female ratio of the input users who have input the reference query \u201cnewborn baby\u201d. If the length of a certain period changes, the male-to-female ratio of the input users in this period also changes. For example, the information providing device 100 may determine the change of the male-to-female ratio of the input users by changing the length of the period. For example, if the male-to-female ratio of the input users is one to one, the information providing device 100 may determine that the length of the period is appropriate. Note that the predetermined condition may be different in each period. Therefore, a plurality of periods (for example, first period, second period) may have different lengths.\n\n(59) Then, the information providing device 100 modifies the length of each period based on the determination result and generates a list again (step S26). For example, the information providing device 100 modifies the length of each period so that the type of the users who have input search queries in each period satisfies the predetermined condition. The information providing device 100 may provide various information to the operator OP based on the density of adjusted periods. For example, if the length of the adjusted period is short, the information providing device 100 may display, by the operator device 700, a message indicating that this period is important or changes in the behavior of users are intense in this period. For example, if the reference query is a search query \u201cnewborn baby\u201d, the length of the first period (negative period) (for example, period \u201c\u22121 to 0 month\u201d) may be one week. This is for a reason that circumstances of users may largely change at the timing of childbirth. On the other hand, the length of a tenth period (negative period) (for example, period \u201c\u221210 to \u22129 month\u201d) may be one month. This is for a reason that the circumstances of users do not change at the timing of pregnancy in some cases. In this manner, the length of each period can be determined based on whether the target shown by search queries are connected to important changes of the circumstances of the input users. A short period can be present at a hot point of search-query time series. The operator OP can find out important needs of users from the search queries at such a hot point.\n\n(60) As described above, the information providing device 100 can adjust an increment/decrement length of each period based on the above described reference time and date. The information providing device 100 may collect the conditions (for example, above described predetermined condition) for adjusting the increment/decrement length of each period from workers of crowdsourcing. The workers of crowdsourcing may be the input users who have input the reference query. For example, the predetermined condition collected from the workers of crowdsourcing may be a condition that \u201cthe length of each period is one month\u201d. As another example, the collected predetermined condition may be a condition that \u201cthe male-to-female ratio of input users is one to one\u201d.\n\n(61) 1-4. Search-Query Predicting Process\n\n(62) In a case in which future search queries can be estimated from the history (for example, search logs) of search queries of users, buzzwords (in other words, words which have been popular topics in a particular period) may generate noise. Also, the search queries which have been input by users indicate specific targets such as a title of a comics on a certain magazine, an event name of an amusement park, etc. However, the targets desired to be grasped by the operator OP may be more abstract contexts. In some cases, the operator OP wants to find out behavior of users by more abstract context regardless of details of comics titles and event names.\n\n(63) For example, it is assumed that a user has input search queries, i.e., a search query \u201cthat devil (exemplary comics name)\u201d, a search query \u201cthat pirate (exemplary comics name)\u201d, and a search query \u201ccountry of dreams and magic (exemplary name of facilities\u201d) in this order. Furthermore, it is assumed that another user has input search queries, i.e., a search query \u201cthat adventure (exemplary comics name)\u201d, a search query \u201cthat pirate (exemplary comics name)\u201d, and a search query \u201ccountry of dreams and magic (exemplary name of facilities\u201d) in this order. In this example, that devil (exemplary comics name), that adventure (exemplary comics name), and that pirate (exemplary comics name) are on the same magazine. The users who are readers of this magazine may have a tendency to input the search query \u201ccountry of dreams and magic (exemplary name of facilities)\u201d. For example, the user may have purchased this magazine to read that devil (exemplary comics name) and then got to like that pirate (exemplary comics name) on the magazine. This user may have been to the country of dreams and magic (exemplary name of facilities) where an event related to that pirate (exemplary comics name) is held. In such a case, a context from a comics on a magazine to amusement park via another comics on the magazine can be found. For example, the operator OP can find out that users who have input the search queries corresponding to one of a plurality of comics on a magazine tend to go to the country of dreams and magic (exemplary name of facilities).\n\n(64) Therefore, the information providing device 100 predicts a future search query(ies) by carrying out machine learning using search queries. More specifically, the information providing device 100 causes a machine learning model to learn characteristics of the above described path to the reference query. For example, the information providing device 100 causes the machine learning model to learn a mode of change of categories (for example, designated categories) corresponding to the search queries. The information providing device 100 inputs the search queries of a user to a learned model to estimate a search query(ies) to be input by the user. For example, the learned model can output a vector representing a mode of change of categories. Alternatively, the learned model can output a vector representing a reference query. The information providing device 100 can estimate a search query, which is input by the user, based on such a vector.\n\n(65) FIG. 4 is an explanatory diagram illustrating an example of a search-query predicting process of predicting a search query of a customer based on time-series data of search queries according to an exemplary embodiment of the present disclosure. Note that categories of search queries (for example, designated categories) may be optimized by the above described grouping evaluation process. Furthermore, each period based on the above described reference time and date may be optimized by the above described time-interval modifying process.\n\n(66) As illustrated in FIG. 4, first, the information providing device 100 specifies categories to which search queries input by input customers in each period belong (step S31). In the example of FIG. 4, the input customer IU1 inputs, for example, search queries such as a search query #1-1, a search query #1-2, and a search query #1-3. The search queries belong to the above described designated categories. If the search query #1-1, the search query #1-2, and the search query #1-3 correspond to a designated category #1-1, a designated category #1-2, and a designated category #1-3, respectively, the information providing device 100 determines that the input customer IU1 has input the search queries in the order of the designated category #1-1, the designated category #1-2, and the designated category #1-3. In this manner, the information providing device 100 categorizes the search queries, which have been input by the input customer in each period, into categories. Furthermore, the information providing device 100 specifies transition modes of categories (for example, designated categories).\n\n(67) Then, the information providing device 100 categorizes the transition modes of the categories of input customers into a plurality of categories (step S32). In the example of FIG. 4, the information providing device 100, for example, categorizes the transition mode of the plurality of designated categories respectively corresponding to the plurality of search queries input by the input customer IU1 into categories. For example, if the transition of the categories occur in the order of the designated category #1-1, the designated category #1-2, and the designated category #1-3 in this order, the transition of these categories is categorized into a transition category #1. More specifically, the information providing device 100 categorizes the transition mode of the categories of the input customer into a transition category corresponding to a path to the reference query.\n\n(68) Then, the information providing device 100 learns a model so that, if a history of search queries of a customer having a similar transition category is input, a similar vector is generated and, if a history of search queries of customers having a dissimilar transition categories is input, a dissimilar vector is generated (step S33). For example, the information providing device 100 groups users based on the search queries input in the past by the users who have input the reference query. In the example of FIG. 4, for example, groups are groups of transition categories such as the transition category #1, the transition category #2, and the transition category #3. Then, the information providing device 100 causes the model to learn characteristics of the change of the input search queries for each group. More specifically, the information providing device 100 causes the model to learn the characteristics of the change of the categories to which the search queries belong. In other words, the information providing device 100 groups users by the change categories of the categories and causes the model to learn the change of the search queries of the users.\n\n(69) In the example of FIG. 4, the input customer IU1 is categorized into a transition category #1. Also, the input customer IU2 is categorized into a transition category #2. The information providing device 100 can specify another transition category similar to a certain transition category by referencing a predetermined dictionary. For example, if the transition category #1 is related to the transition category #2 in the dictionary, the information providing device 100 can specify the transition category #2 as a category similar to the transition category #1.\n\n(70) In the example of FIG. 4, a query history #1 is a history of the search queries input by the input customers who have categorized into the transition category #1. Similarly, a query history #2 is a history of the search queries input by the input customers who have categorized into the transition category #2.\n\n(71) In the example of FIG. 4, a vector #1 is a vector representing a transition mode of categories. For example, the vector #1 represents the transition mode of the transition category #1. In other words, the vector #1 represents characteristics of the changes of the categories. For example, it is assumed that the transition category #1 corresponds to a transition mode in which the transition of categories occur in the order of a designated category \u201cthat magazine (exemplary magazine name), a designated category \u201cthat magazine (exemplary magazine name)\u201d, and a designated category \u201ccountry of dreams and magic (exemplary name of facilities)\u201d. In this example, each element of the vector corresponds to the designated category. For example, first, second, third, fourth, and fifth elements respectively correspond to designated categories such as that magazine (exemplary magazine name), the country of dreams and magic (exemplary name of facilities), marriage, pregnancy, and relocation. In this case, the vector #1 may be (1,1,0,0,0). The first element \u201c1\u201d represents a fact that the transition of the category includes the designated category \u201cthat magazine (exemplary magazine name)\u201d. For example, the first element \u201c1\u201d represents a fact that the user has input a query such as that devil (exemplary comics name), that adventure (exemplary comics name), or that pirate (exemplary comics name). On the other hand, the fourth element \u201c0\u201d represents a fact that the transition of the category does not include a designated category \u201cpregnancy\u201d. For example, the third element \u201c0\u201d represents a fact that the user has not input a query such as newborn baby.\n\n(72) The information providing device 100 carries out machine learning so that, even in a case in which a plurality of users inputs the same reference query, the model outputs different vectors if changes of categories are different. Therefore, the information providing device 100 can cause the model to accurately learn whether the user is the user who reaches the reference query or the characteristics of future search queries input by a certain user by using the search queries of the user or the categories of the search queries. As described above, the information providing device 100 specifies the categories to which the search queries input by the user in the past belong and carries out learning of the model for each type of change in the specified categories. For example, if the user inputs a search query \u201ccountry of dreams and magic (exemplary name of facilities), the information providing device 100 can cause the model to learn the way how the user reaches this search query. In this manner, the information providing device 100 can cause the model to learn high-level concepts (for example, transition modes) of the categories of search queries.\n\n(73) Then, the information providing device 100 predicts a search query, which is input by a target user in the future, from the history of the search queries of the target user by using the model (step S34). In the example of FIG. 4, the information providing device 100 generates a vector #4 by inputting the history of the search queries (for example, a search query #4-1, a search query #4-2) of the target user TU1 to the learned model. The vector #4 represents the transition mode of the categories of the target user TU1. For example, the information providing device 100 can specify another target user corresponding to a vector similar to the generated vector #4 and carry out emphatic filtering based on the user information of the specified other target user. In this manner, the information providing device 100 can enable prediction of needs of the target user TU1 and/or targeting on the target user TU1. The above described group (for example, designated category) may correspond to particular marketing (for example, model change).\n\n(74) Hereinafter, the information providing device 100, which carries out such an information providing process, will be described in detail.\n\n2. Information Providing System\n\n(75) Next, a configuration example of a system including the information providing device 100 will be described with reference to FIG. 5.\n\n(76) 2-1. Constituent Elements of Information Providing System\n\n(77) FIG. 5 is a diagram illustrating an example of an information providing system 1 according to the embodiment. As illustrated in FIG. 5, the information providing system 1 includes constituent elements such as the information providing device 100, the user device 500, the log server 600, and the operator device 700. Although not illustrated in FIG. 1, the information providing system 1 may include a plurality of information providing devices 100, a plurality of user devices 500, a plurality of log servers 600, and/or a plurality of operator devices 700. Also, the information providing system 1 may include other constituent elements such as devices of entities (for example, business operators, end users) related to the information providing device 100.\n\n(78) In the information providing system 1, each of the information providing device 100, the user device 500, the log server 600, and the operator device 700 is connected to a network N by wire or wirelessly. The network N is, for example, a network such as the Internet, a WAN (Wide Area Network), or a LAN (Local Area Network). The constituent elements of the information providing system 1 can communicate with each other via the network N.\n\n(79) The information providing device 100 (corresponding to one example of the determination device) is an information processing device, which executes processing for evaluating time-series data of search queries. The information providing device 100 can find out users who have particular needs from the time-series data of search queries. Also, the information providing device 100 can predict needs of users from the time-series data of search queries. The information providing device 100 may be an information processing device of an arbitrary type including a server. The plurality of information providing devices 100 may provide functions of various servers such as a web server, an application server, and a database server, respectively. A configuration example of the information providing device 100 will be described in detail in a following section.\n\n(80) The user device 500 is an information processing device used by a user. The user device 500 can transmit search queries via various services (for example, portal site, portal application) on the Internet. Also, the user device 500 can receive search results via these various services. The user device 500 may be an information processing device of an arbitrary type including a client device such as a smartphone, a desk-top PC, a laptop PC, or a tablet PC.\n\n(81) The log server 600 is an information processing device, which provides various services (for example, portal site, portal application) on the Internet. The log server 600 can receive search queries from the user device 500 via these various services. Also, the log server 600 can accumulate the received search queries as search logs. The log server 600 may be an information processing device of an arbitrary type including a server.\n\n(82) The operator device 700 is an information processing device used by an operator. The operator is, for example, a person who is related to a particular Internet company related to the information providing device 100 or the log server 600. The operator device 700 enables the operator to input information to the information providing device 100. For example, if the operator wants to analyze time-series data, the operator can set a keyword or a category of an analysis target with respect to the information providing device 100. As well as the case of the user device 500, the operator device 700 may be an information processing device of an arbitrary type including a client device.\n\n(83) 2-2. Configuration of Information Providing Device\n\n(84) As illustrated in FIG. 5, the information providing device 100 has a communication unit 200, a storage unit 300, and a control unit 400. Note that the information providing device 100 may have an input unit (for example, keyboard, mouse, or the like), which receives various operations from an administrator or the like who uses the information providing device 100 and a display unit (liquid crystal display or the like) for displaying various information.\n\n(85) Communication Unit 200\n\n(86) The communication unit 200 is realized, for example, by a Network Interface Card (NIC) or the like. The communication unit 200 is connected to a network by wire or wirelessly. The communication unit 200 may be communicably connected to the user device 500, the log server 600, and the operator device 700 via the network N. The communication unit 200 can carry out transmission/reception of information to/from the user device 500, the log server 600, and the operator device 700 via the network.\n\n(87) Storage Unit 300\n\n(88) The storage unit 300 is realized by, for example, a semiconductor memory element such as a Random Access Memory (RAM) or a flash memory (Flash Memory) or a storage device such as a hard disk or an optical disk. As illustrated in FIG. 5, the storage unit 300 has a search-query database 310.\n\n(89) Search-Query Database 310\n\n(90) FIG. 6 is a diagram illustrating an example of a search-query database 310 according to the embodiment. The search-query database 310 stores search queries input by users.\n\n(91) In the example of FIG. 6, the search-query database 310 has items such as \u201ccustomer ID\u201d, \u201csearch query\u201d, and \u201csearch time and date\u201d. The search-query database 310 may be implemented as structured data sets such as search histories (for example, search logs).\n\n(92) The \u201ccustomer ID\u201d represents an identifier for identifying a user (also referred to as \u201ccustomer\u201d). The \u201csearch query\u201d represents a search query input by the user. The \u201csearch time and date\u201d represents time and date at which the search query is input.\n\n(93) For example, FIG. 6 illustrates a fact that a user identified by a customer ID \u201cUID #1\u201d has input a search query \u201csearch query #1-1\u201d. The customer ID \u201cUID #1\u201d is, for example, a predetermined character string. The search query \u201csearch query #1-1\u201d is a character string such as \u201chospital stay\u201d, \u201critual visit\u201d, and \u201cweight\u201d.\n\n(94) Also, for example, FIG. 6 illustrates a fact that the search query \u201csearch query #1-1\u201d has been input at time and date \u201csearch time and date #1\u201d. The time and date \u201csearch time and date #1\u201d is, for example, a time stamp of \u201c2020/03/19 12:00:00\u201d.\n\n(95) Control Unit 400\n\n(96) The control unit 400 is a controller and can be realized, for example, when a processor such as a Central Processing Unit (CPU) or a Micro Processing Unit (MPU) executes a various program(s) (corresponding to an example of a determination program), which is stored in a storage device in the information providing device 100, by using a RAM or the like a s a work area. Also, the control unit 400 is a controller and may be realized, for example, by an integrated circuit such as an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a General Purpose Graphic Processing Unit (GPGPU), or the like.\n\n(97) As illustrated in FIG. 5, the control unit 400 has a first determination processing unit 410, a second determination processing unit 420, and a learning processing unit 430 and realizes or executes functions or working of information processing described below. Also, the control unit 400 can realize the information providing process described above with reference to FIG. 1, FIG. 2, FIG. 3, and FIG. 4. One or a plurality of processors of the information providing device 100 can realize functions of control units in the control unit 400 by executing commands stored in one or a plurality of memories of the information providing device 100. Note that the internal configuration of the control unit 400 is not limited to the configuration illustrated in FIG. 5, and another configuration may be used as long as the configuration carries out later-described information processing. For example, the first determination processing unit 410 may carry out all or part of the information processing which is described later about the units other than the first determination processing unit 410.\n\n(98) First Determination Processing Unit 410\n\n(99) FIG. 7 is a diagram illustrating an example of a configuration of the first determination processing unit 410 according to the embodiment. The first determination processing unit 410 can determine whether the categorization of the time-series search queries satisfies a predetermined condition or not. The categorization of the time-series search queries can be used for evaluating the time-series data of search queries.\n\n(100) As illustrated in FIG. 7, the first determination processing unit 410 has an acquisition unit 411, a categorization unit 412, a determination unit 413, an output unit 414, an extraction unit 415, and a provision unit 416. Note that the internal configuration of the first determination processing unit 410 is not limited to the configuration illustrated in FIG. 7, but may be another configuration as long as the configuration carries out the later-described information processing. For example, the determination unit 413 may carry out all or part of the information processing described later about the units other than the determination unit 413.\n\n(101) For example, the first determination processing unit 410 groups the search queries, which have been input in each period by target users, in order to enable the operator to appropriately evaluate the list of the search queries. The first determination processing unit 410 can group the search queries, which have been input in each period, based on the context of the reference query. For example, the first determination processing unit 410 acquires the search logs of the users, who have input the reference query, for each time series (for example, period around the reference time and date). Then, the first determination processing unit 410 groups the search queries for each time series. For example, if the number of groups or the number of the users who have input the search queries related to groups satisfies a threshold value, the first determination processing unit 410 extracts the search queries related to the groups as the search queries included in the list. As a result, the grouped search queries included in the list can have coherence, thereby enabling the operator to appropriately evaluate the list of the search queries.\n\n(102) Acquisition Unit 411\n\n(103) The acquisition unit 411 can acquire various information used for evaluating the time-series data of search queries. The acquisition unit 411 can receive such various information from a predetermined information processing device (for example, a device of an entity related to the information providing device 100 (for example, a particular Internet company)). Also, for example, the acquisition unit 411 can receive such information from an administrator using the information providing device 100 via a user interface. The acquisition unit 411 may store received various information in the storage unit 300. For example, the acquisition unit 411 may store the received search queries in the search-query database 310. The acquisition unit 411 can acquire various information from the storage unit 300. For example, the acquisition unit 411 can acquire search queries (for example, search logs of search histories or the like) from the search-query database 310.\n\n(104) At least in one embodiment, the acquisition unit 411 acquires the search queries, which have been input by a plurality of input customers who have input the reference query.\n\n(105) At least in one embodiment, the acquisition unit 411 acquires the search queries which have been input in a period before the reference time and date, at which the input customer has input the reference query, by predetermined time and date as a predetermined period. For example, the acquisition unit 411 acquires the search queries, which have been input respectively in a plurality of different periods based on the reference time and date.\n\n(106) For example, the acquisition unit 411 acquires search logs from the log server 600. For example, the acquisition unit 411 collects histories of search queries from the log server 600.\n\n(107) For example, the acquisition unit 411 acquires a reference query from the operator device 700. As described above, the reference query is a predetermined search query input at certain time and date. As described above with reference to FIG. 1, the reference query is, for example, a query \u201cnewborn baby\u201d input at certain time and date (for example, certain reference time and date).\n\n(108) For example, the acquisition unit 411 acquires a reference query and designated categories from the operator device 700. As described above, the reference query is a predetermined search query (for example, a search query \u201ccompany C1\u201d) input at certain time and date. On the other hand, the designated categories are the categories for grouping the time-series search queries. The acquisition unit 411 groups the search queries for each time series (for example, the period around the reference time and date) in accordance with the designated categories. The designated categories are designated, for example, in advance by the operator. The designated categories are, for example, categories such as company presidents, mascots, model change, amusement parks, comics, and the like. In this manner, the acquisition unit 411 receives designated categories from the operator.\n\n(109) Categorization Unit 412\n\n(110) At least in one embodiment, the categorization unit 412 categorizes the search queries, which have been input in a predetermined period among search queries, into a plurality of categories.\n\n(111) At least in one embodiment, the categorization unit 412 categorizes the search queries into a plurality of categories for each period.\n\n(112) For example, the categorization unit 412 specifies the input customers who have input the reference query acquired by the acquisition unit 411. For example, the categorization unit 412 specifies the input customers, who have input the reference query, from the search logs acquired by the acquisition unit 411. As described above with reference to FIG. 1, the categorization unit 412 specifies, for example, a user who has input a search query \u201cnewborn baby\u201d at certain time and date, as the input customer.\n\n(113) Then, the categorization unit 412 specifies the search queries, which have been input by the customers in each period based on the reference time and date. The periods based on the reference time and date are the periods around the reference time and date. For example, the interval of the periods may be one month. For example, in a case in which the reference time and date is \u201c2020/03/19\u201d, the periods around the reference time and date are the periods after the reference time and date such as \u201c2020/03/19 to 2020/04/19\u201d and \u201c2020/04/19 to 2020/05/19\u201d or the periods before the reference time and date such as \u201c2020/02/19 to 2020/03/19\u201d and \u201c2020/01/19 to 2020/02/19\u201d. In this manner, the categorization unit 412 extracts the search queries of the users, who have input the reference query, from the search logs acquired by the acquisition unit 411 in each period based on the reference time and date.\n\n(114) For example, the categorization unit 412 specifies input customers and specifies the search queries input by each input customer. As described above, the input customers are the users who have input the reference queries at the reference time and date. The categorization unit 412 can specify the input customers from search logs. Also, the categorization unit 412 can specify the search queries, which have been input in the periods around the reference time and date by the input customers, from the search logs. Then, the categorization unit 412 specifies the search queries input in each period. As described above, each period is a period around the reference time and date. Then, the categorization unit 412 specifies the element group of the number of queries or the number of customers of each designated category in each period. As described above with reference to FIG. 2, for example, the categorization unit 412 specifies the element group of the number of the search queries or the number of the users of each designated category in the second period (negative period). The categorization unit 412 can specify the element group of the search queries by counting the number of the search queries, which have been categorized into the designated categories, and the number of the customers who have input the search queries.\n\n(115) The categorization unit 412 may categorize the search queries into a plurality of categories for each period. The plurality of categories may include a designated category. More specifically, the categorization unit 412 may group the search queries for each time series in accordance with the categories other than the designated categories. If categories are not fixed in advance (for example, if the acquisition unit 411 has not acquired designated categories), the categorization unit 412 may categorize the search queries in the search logs into a plurality of categories (for example, categories other than designated categories) based on the reference query and the search logs. If the reference query is fixed in advance, the categorization unit 412 can search for a category appropriate for this reference query.\n\n(116) For example, the categorization unit 412 may automatically optimize categories (for example, designated category, the categories other than the designated category) based on the determination result of the determination unit 413.\n\n(117) Determination Unit 413\n\n(118) At least in one embodiment, the determination unit 413 determines whether the categorization result of the categorization unit 412 satisfies a predetermined determination condition or not.\n\n(119) At least in one embodiment, the determination unit 413 determines whether the number of the categories to which the search queries have been categorized satisfies a predetermined condition or not. For example, the determination unit 413 determines whether the number of the categories to which the search queries of predetermined percentage or higher among the search queries are categorized is equal to or less than a predetermined threshold value. Also, for example, the determination unit 413 specifies the category to which the search queries of predetermined percentage or higher among the search queries are categorized and determines whether the number of the customers who have input the search queries categorized to the specified category is equal to or higher than a predetermined threshold value or not.\n\n(120) At least in one embodiment, whether the categorization result satisfies the predetermined determination condition or not is determined for each period.\n\n(121) At least in one embodiment, the determination unit 413 determines whether the high-level categories to which the search queries are categorized satisfies a predetermined condition or not.\n\n(122) For example, the determination unit 413 determines whether the element group of search queries satisfies a predetermined condition or not and evaluates the reference query or the designated category from the determination result. The predetermined condition is, for example, a condition that \u201cthe number of the groups of search queries converges to a particular value (for example, a natural number n)\u201d. The groups are, for example, a plurality of categories including a designated category or a category other than the designated category. For example, if the percentage of the search queries belonging to n designated categories with respect to all queries in each period (in other words, all search logs in each period) satisfies a threshold value (for example, 80%) (for example, n is \u201c3\u201d), the determination unit 413 can use the n designated categories as the groups for the list of the search queries. For example, if the number of designated categories is \u201c3\u201d and if these three designated categories include 80% of search queries or customers, the determination unit 413 determines that these three designated categories satisfy the predetermined condition.\n\n(123) Regarding the number of the groups, if the number of the groups is large, the determination unit 413 can determine that the original reference query is not an appropriate query. If the number of groups in a certain period is equal to the number of groups in another period, the determination unit 413 can determine that the original reference query is an appropriate query. The determination unit 413 can determine whether the list of the search queries is appropriate or not based on whether the element group of the number of queries or the number of users is appropriately expressed or not. If the category is fixed in advance (for example, if the category is a designated category), the determination unit 413 can evaluate the element group of the number of queries or the number of users based on the categories and the search logs and, therefore, can specify an appropriate reference query.\n\n(124) As described above with reference to FIG. 2, the predetermined condition is, for example, a condition that \u201cin any of the periods, the queries or the customers included in the designated category is 30% or less\u201d. If the element group of the search queries satisfies this condition, the determination unit 413 determines that the designated category is not appropriate or the reference query is not appropriate. In addition to or alternatively, the predetermined condition may be, for example, a condition that \u201cvariation in the percentage of the search queries or customers included in the designated category in each period exceeds a threshold value\u201d. If the element group satisfies this condition, the determination unit 413 determines that the designated category is not appropriate or the reference query is not appropriate. In this manner, the determination unit 413 can evaluate the reference query and the designated category by determining whether the element group of the search queries satisfies the predetermined condition or not. If the element group converges, the determination unit 413 can determine that the list of the search queries is appropriate.\n\n(125) As described above, the determination result is used for determining whether the reference query is appropriate or not. For example, if the reference query satisfies the predetermined condition in each period, the determination unit 413 determines that the reference query is appropriate. The determination unit 413 may determine that the reference query is appropriate if the reference query satisfies the condition in the majority of periods. If the reference query does not satisfy the predetermined condition in the periods, the reference query may not be appropriately expressing a desired target of the operator.\n\n(126) Output Unit 414\n\n(127) At least in one embodiment, if it is determined by the determination unit 413 that the categorization result satisfies a predetermined determination condition, the output unit 414 (for example, the output unit 414 implemented as a first output unit) outputs the information that the reference query is appropriate.\n\n(128) At least in one embodiment, if it is determined by the determination unit 413 that the categorization result satisfies a predetermined determination condition, the output unit 414 (for example, the output unit 414 implemented as a second output unit) outputs the information that the plurality of categories that categorizes the search queries is appropriate.\n\n(129) For example, the output unit 414 provides an evaluation result to the operator device 700. For example, the output unit 414 provides the information corresponding to the determination result to the operator. The output unit 414 may display, by the operator device 700, a message indicating that the designated category is not appropriate or the reference query is not appropriate. Alternatively, the output unit 414 may display, by the operator device 700, a message indicating that the search behavior of users is appropriately expressed by decomposing the time-series search queries into the element group of the category.\n\n(130) Extraction Unit 415\n\n(131) At least in one embodiment, if it is determined by the determination unit 413 that the categorization result satisfies a predetermined determination condition, the extraction unit 415 extracts the search queries categorized into a category, which satisfies a predetermined categorization condition among a plurality of categories. For example, the extraction unit 415 specifies the category to which the search queries of predetermined percentage or higher are categorized among search queries and extracts the search queries categorized to the specified category. Also, for example, the extraction unit 415 extracts the search queries having high relativity with the reference query among search queries.\n\n(132) For example, the extraction unit 415 acquires a search query, which satisfies a predetermined condition in each period and similar to the reference query, from the search-query database 310. The predetermined condition is, for example, a condition about the element group of search queries.\n\n(133) Provision Unit 416\n\n(134) At least in one embodiment, the provision unit 416 provides a list of search queries extracted by the extraction unit 415.\n\n(135) For example, the provision unit 416 generates a list of search queries having high relativity with the reference query for each period. As described above with reference to FIG. 1, the provision unit 416 specifies, from the search logs acquired by the acquisition unit 411, for example, search queries such as \u201chospital stay\u201d, \u201critual visit\u201d, and \u201cweight\u201d as the search queries having high relativity with the reference query \u201cnewborn baby\u201d one month before the reference time and date. Then, the provision unit 416 generates a list including the specified search queries.\n\n(136) Then, the provision unit 416 provides the generated list to the operator device 700. As described above with reference to FIG. 1, the generated list illustrates, for example, the search queries which have high relativity with the reference query \u201cnewborn baby\u201d for each period. The provision unit 416 may generate a table in which other search queries input in each period are arranged in the order of relevance degrees and provide the generated table to the operator device 700 as a list.\n\n(137) For example, the provision unit 416 can provide a list that satisfies the conditions about the element groups of search queries to the operator device 700. The number (for example, \u201c3\u201d) of arrays in a certain period may be the same number of arrays in another period. In this case, the operator can read transition modes of designated categories from the list. Such designated categories in each period can be paraphrased and applied to analysis of time-series search behavior.\n\n(138) Second Determination Processing Unit 420\n\n(139) FIG. 8 is a diagram illustrating an example of a configuration of the second determination processing unit 420 according to the embodiment. The second determination processing unit 420 can determine whether the period in the time series of search queries are appropriate or not. The period in the time series of search queries can be used for evaluating the time-series data of search queries.\n\n(140) As illustrated in FIG. 8, the second determination processing unit 420 has an acquisition unit 421, a categorization unit 422, a determination unit 423, and an output unit 424. Note that the internal configuration of the second determination processing unit 420 is not limited to the configuration illustrated in FIG. 8, but may be another configuration as long as the configuration carries out the later-described information processing. For example, the determination unit 423 may carry out all or part of the information processing described later about the units other than the determination unit 423.\n\n(141) For example, the second determination processing unit 420 specifies an appropriate length of the period by adjusting the length of the period. The second determination processing unit 420 adjusts the density of search-query time series by using the above described element groups of the search queries.\n\n(142) Acquisition Unit 421\n\n(143) The acquisition unit 421 can acquire various information used for evaluating the time-series data of search queries. The acquisition unit 421 can receive such various information from a predetermined information processing device (for example, a device of an entity related to the information providing device 100 (for example, a particular Internet company)). Also, for example, the acquisition unit 421 can receive such information from an administrator using the information providing device 100 via a user interface. The acquisition unit 421 may store received various information in the storage unit 300. For example, the acquisition unit 421 may store the received search queries in the search-query database 310. The acquisition unit 421 can acquire various information from the storage unit 300. For example, the acquisition unit 421 can acquire search queries (for example, search logs of search histories or the like) from the search-query database 310.\n\n(144) At least in one embodiment, the acquisition unit 421 acquires the search queries, which are the search queries input by a plurality of input customers who have input the reference query and input within a predetermined period.\n\n(145) At least in one embodiment, the acquisition unit 421 acquires search queries, which have been input by input customers in a plurality of different periods respectively based on the reference time and date at which reference query has been input.\n\n(146) For example, the acquisition unit 421 acquires search logs from the log server 600. For example, the acquisition unit 421 collects histories of search queries from the log server 600.\n\n(147) For example, the acquisition unit 421 acquires a reference query from the operator device 700. As described above, the reference query is a predetermined search query input at certain time and date. As described above with reference to FIG. 1, the reference query is, for example, a query \u201cnewborn baby\u201d input at certain time and date (for example, certain reference time and date).\n\n(148) For example, the acquisition unit 421 acquires a reference query and designated categories from the operator device 700. As described above, the reference query is a predetermined search query (for example, a search query \u201ccompany C1\u201d) input at certain time and date. On the other hand, the designated categories are the categories for grouping the time-series search queries. The acquisition unit 421 groups the search queries for each time series (for example, the period around the reference time and date) in accordance with the designated categories. The designated categories are designated, for example, in advance by the operator. The designated categories are, for example, categories such as company presidents, mascots, model change, amusement parks, comics, and the like. In this manner, the acquisition unit 421 receives designated categories from the operator.\n\n(149) For example, the acquisition unit 421 may collect the conditions (for example, above described predetermined condition) for adjusting the increment/decrement length of each period from workers of crowdsourcing. The workers of crowdsourcing may be the input users who have input the reference query. For example, the predetermined condition collected from the workers of crowdsourcing may be a condition that \u201cthe length of each period is one month\u201d. As another example, the collected predetermined condition may be a condition that \u201cthe male-to-female ratio of input users is one to one\u201d.\n\n(150) Categorization Unit 422\n\n(151) At least in one embodiment, the categorization unit 422 (for example, the categorization unit 422 implemented as a first categorization unit) categorizes the input customers, who have input search queries in a predetermined period, into a plurality of categories in accordance with the attributes of the input customers.\n\n(152) At least in one embodiment, the categorization unit 422 (for example, the categorization unit 422 implemented as a second categorization unit) categorizes the search queries, which have been input in a predetermined period, into a plurality of categories.\n\n(153) For example, the categorization unit 422 specifies the input customers who have input the reference query acquired by the acquisition unit 421. For example, the categorization unit 422 specifies the input customers, who have input the reference query, from the search logs acquired by the acquisition unit 421. As described above with reference to FIG. 1, the categorization unit 422 specifies, for example, a user who has input a search query \u201cnewborn baby\u201d at certain time and date, as the input customer.\n\n(154) Then, the categorization unit 422 specifies the search queries, which have been input by the customers in each period based on the reference time and date. The periods based on the reference time and date are the periods around the reference time and date. For example, the interval of the periods may be one month. For example, in a case in which the reference time and date is \u201c2020/03/19\u201d, the periods around the reference time and date are the periods after the reference time and date such as \u201c2020/03/19 to 2020/04/19\u201d and \u201c2020/04/19 to 2020/05/19\u201d or the periods before the reference time and date such as \u201c2020/02/19 to 2020/03/19\u201d and \u201c2020/01/19 to 2020/02/19\u201d. In this manner, the categorization unit 422 extracts the search queries of the users, who have input the reference query, from the search logs acquired by the acquisition unit 421 in each period based on the reference time and date.\n\n(155) For example, the categorization unit 422 specifies input customers and specifies the search queries input by each input customer. As described above, the input customers are the users who have input the reference queries at the reference time and date. The categorization unit 422 can specify the input customers from search logs. Also, the categorization unit 422 can specify the search queries, which have been input in the periods around the reference time and date by the input customers, from the search logs. Then, the categorization unit 422 specifies the search queries input in each period. As described above, each period is a period around the reference time and date. Then, the categorization unit 422 specifies the element group of the number of queries or the number of customers of each designated category in each period. As described above with reference to FIG. 3, for example, the categorization unit 422 specifies the element group of the number of the search queries or the number of the users of each designated category in the second period (negative period). The categorization unit 422 can specify the element group of the search queries by counting the number of the search queries, which have been categorized into the designated categories, and the number of the customers who have input the search queries.\n\n(156) The categorization unit 422 may categorize the search queries into a plurality of categories for each period. The plurality of categories may include a designated category. More specifically, the categorization unit 422 may group the search queries for each time series in accordance with the categories other than the designated categories. If categories are not fixed in advance (for example, if the acquisition unit 421 has not acquired designated categories), the categorization unit 422 may categorize the search queries in the search logs into a plurality of categories (for example, categories other than designated categories) based on the reference query and the search logs. If the reference query is fixed in advance, the categorization unit 422 can search for a category appropriate for this reference query.\n\n(157) Determination Unit 423\n\n(158) At least in one embodiment, the determination unit 423 determines whether a predetermined period is appropriate or not based on the attributes of the input customers who have input search queries or based on whether these search queries satisfy predetermined conditions or not.\n\n(159) At least in one embodiment, the determination unit 423 determines whether the length of the predetermined period is appropriate or not.\n\n(160) At least in one embodiment, the determination unit 423 determines whether the predetermined period is appropriate or not based on whether the categorization result by the categorization unit 422 (for example, the categorization unit 422 implemented as a first categorization unit) satisfies a predetermined condition or not. For example, if the number of the categories to which the input customers are categorized is equal to or less than a predetermined threshold value, the determination unit 423 determines that the predetermined period is appropriate. Also, for example, if the percentage of the customers categorized into each category by the categorization unit 422 (for example, the categorization unit 422 implemented as the first categorization unit) satisfies a predetermined condition, the determination unit 423 determines that the predetermined period is appropriate.\n\n(161) At least in one embodiment, the determination unit 423 determines whether the predetermined period is appropriate or not based on whether the categorization result by the categorization unit 422 (for example, the categorization unit 422 implemented as a second categorization unit) satisfies a predetermined condition or not. For example, the determination unit 423 determines whether the number of the categories to which the search queries have been categorized satisfies a predetermined condition or not. For example, the determination unit 423 determines whether the number of the categories to which the search queries of predetermined percentage or higher among the search queries are categorized is equal to or less than a predetermined threshold value. For example, the determination unit 423 specifies the category to which the search queries of predetermined percentage or higher among the search queries are categorized and determines whether the number of the customers who have input the search queries categorized to the specified category is equal to or higher than a predetermined threshold value or not.\n\n(162) At least in one embodiment, whether this period is appropriate or not is determined for each period.\n\n(163) For example, the determination unit 423 determines whether the element group of search queries satisfies a predetermined condition or not and determines whether the setting of each period is appropriate or not from the determination result.\n\n(164) As described above with reference to FIG. 3, for example, if variation in the element group in each period is equal to or less than a predetermined threshold value, the determination unit 423 determines that the length of each period is appropriate. For example, if the element groups are common among the periods, the determination unit 423 determines that the length of each period is appropriate. If the element group is shifted in a certain period, the determination unit 423 modifies the length of this period. If dispersion of the element group in a predetermined period is equal to or higher than a predetermined threshold value, the determination unit 423 determines that the length of the predetermined period is too long. The determination unit 423 modifies periods so that the element groups are common among the periods.\n\n(165) If the type of the users who have input the search queries in each period satisfies a predetermined condition, the determination unit 423 may determine that the length of each period is appropriate. The type of the users is, for example, an attribute of the users such as a demographics attribute, a psychographics attribute, or the like. For example, the determination unit 423 may determine whether the length of each period is appropriate or not based on the male-to-female ratio of the input users who have input the reference query \u201cnewborn baby\u201d. If the length of a certain period changes, the male-to-female ratio of the input users in this period also changes. For example, the determination unit 423 may determine the change of the male-to-female ratio of the input users by changing the length of the period. For example, if the male-to-female ratio of the input users is one to one, the determination unit 423 may determine that the length of the period is appropriate. Note that the predetermined condition may be different in each period. Therefore, a plurality of periods (for example, first period, second period) may have different lengths.\n\n(166) Output Unit 424\n\n(167) If the determination unit 423 determines that the element group of search queries satisfies a predetermined condition, the output unit 424 can output the information that the setting of the period is appropriate.\n\n(168) For example, the output unit 424 modifies the length of each period based on the determination result and generates a list again. For example, the output unit 424 modifies the length of each period so that the type of the users who have input search queries in each period satisfies the predetermined condition. The output unit 424 may provide various information to the operator based on the density of adjusted periods. For example, if the length of the adjusted period is short, the output unit 424 may display, by the operator device 700, a message indicating that this period is important or changes in the behavior of users are intense in this period. For example, if the reference query is a search query \u201cnewborn baby\u201d, the length of the first period (negative period) (for example, period \u201c\u22121 to 0 month\u201d) may be one week. This is for a reason that circumstances of users may largely change at the timing of childbirth. On the other hand, the length of a tenth period (negative period) (for example, period \u201c\u221210 to \u22129 month\u201d) may be one month. This is for a reason that the circumstances of users do not change at the timing of pregnancy in some cases. In this manner, the length of each period can be determined based on whether the target shown by search queries are connected to important changes of the circumstances of the input users. A short period can be present at a hot point of search-query time series. The operator can find out important needs of users from the search queries at such a hot point.\n\n(169) Learning Processing Unit 430\n\n(170) FIG. 9 is a diagram illustrating an example of a configuration of the learning processing unit 430 according to the embodiment. The learning processing unit 430 can learn the transition of the categories of search queries input by users from time-series data of the search queries. The learning from time-series data includes generation of a model from the time-series data. The process of generating a model from data may be referred to as training or learning. The model can be generated by training a machine learning algorithm by using training data.\n\n(171) The above described categories of the search queries may be the categorization of time-series search queries based on the determination by the above described first determination processing unit 410. For example, the time-series search queries may be categorized based on the determination executed by the first determination processing unit 410 whether the categorization of the time-series search queries satisfies a predetermined condition or not. The time-series search queries may be categorized so that the categorization of the time-series search queries satisfies a predetermined condition. As described above, such determination can be used for improving the categorization of time-series search queries.\n\n(172) The time interval of the above described transition may be the length of a period based on the determination by the above described second determination processing unit 420. For example, the length of the period in the time series of search queries may be determined based on the determination of whether the period in the time series of search queries is appropriate or not executed by the second determination processing unit 420. As described above, such determination can be used for improving the length of the period in the time series of search queries.\n\n(173) The above described transition of categories can be used for evaluating time-series data of search queries. As illustrated in FIG. 9, the learning processing unit 430 has an acquisition unit 431, a specifying unit 432, a learning unit 433, and an estimation unit 434. Note that the internal configuration of the learning processing unit 430 is not limited to the configuration illustrated in FIG. 9, but may be another configuration as long as the configuration carries out the later-described information processing. For example, the learning unit 433 may carry out all or part of the information processing described later about the units other than the learning unit 433.\n\n(174) For example, the learning processing unit 430 predicts a future search query(ies) by carrying out machine learning using search queries. More specifically, the learning processing unit 430 causes a machine learning model to learn characteristics of the above described path to the reference query. For example, the learning processing unit 430 causes the machine learning model to learn a mode of change of categories (for example, designated categories) corresponding to the search queries. The learning processing unit 430 inputs the search queries of a user to a learned model to estimate a search query(ies) to be input by the user. For example, the learned model can output a vector representing a mode of change of categories. Alternatively, the learned model can output a vector representing a reference query. The learning processing unit 430 can estimate a search query, which is input by the user, based on such a vector.\n\n(175) Acquisition Unit 431\n\n(176) The acquisition unit 431 can acquire various information used for evaluating the time-series data of search queries. The acquisition unit 431 can receive such various information from a predetermined information processing device (for example, a device of an entity related to the information providing device 100 (for example, a particular Internet company)). Also, for example, the acquisition unit 431 can receive such information from an administrator using the information providing device 100 via a user interface. The acquisition unit 431 may store received various information in the storage unit 300. For example, the acquisition unit 431 may store the received search queries in the search-query database 310. The acquisition unit 431 can acquire various information from the storage unit 300. For example, the acquisition unit 431 can acquire search queries (for example, search logs of search histories or the like) from the search-query database 310.\n\n(177) At least in one embodiment, the acquisition unit 431 acquires the search queries, which are the search queries input by a plurality of input customers who have input the reference query and input in mutually different periods.\n\n(178) At least in one embodiment, the acquisition unit 431 acquires a search query, which is input after the input customer inputs the reference query, as an objective query. For example, the acquisition unit 431 acquires a plurality of search queries, which are input after the input customer inputs the reference query, as objective queries.\n\n(179) For example, the acquisition unit 431 acquires search logs from the log server 600. For example, the acquisition unit 431 collects histories of search queries from the log server 600.\n\n(180) For example, the acquisition unit 431 acquires a reference query from the operator device 700. As described above, the reference query is a predetermined search query input at certain time and date. As described above with reference to FIG. 1, the reference query is, for example, a query \u201cnewborn baby\u201d input at certain time and date (for example, certain reference time and date).\n\n(181) Specifying Unit 432\n\n(182) At least in one embodiment, the specifying unit 432 specifies the categories to which the search queries input in each period for each input customer.\n\n(183) For example, the specifying unit 432 specifies the input customers who have input the reference query acquired by the acquisition unit 431. For example, the specifying unit 432 specifies the input customers, who have input the reference query, from the search logs acquired by the acquisition unit 431. As described above with reference to FIG. 1, the specifying unit 432 specifies, for example, a user who has input a search query \u201cnewborn baby\u201d at certain time and date, as the input customer.\n\n(184) Then, the specifying unit 432 specifies the search queries, which have been input by the customers in each period based on the reference time and date. The periods based on the reference time and date are the periods around the reference time and date. For example, the interval of the periods may be one month. For example, in a case in which the reference time and date is \u201c2020/03/19\u201d, the periods around the reference time and date are the periods after the reference time and date such as \u201c2020/03/19 to 2020/04/19\u201d and \u201c2020/04/19 to 2020/05/19\u201d or the periods before the reference time and date such as \u201c2020/02/19 to 2020/03/19\u201d and \u201c2020/01/19 to 2020/02/19\u201d. In this manner, the specifying unit 432 extracts the search queries of the users, who have input the reference query, from the search logs acquired by the acquisition unit 431 in each period based on the reference time and date.\n\n(185) For example, the specifying unit 432 specifies the categories to which the search queries input by the input customer in each period belong. As described above with reference to FIG. 4, the input customer IU1 inputs, for example, search queries such as a search query #1-1, a search query #1-2, and a search query #1-3. The search queries belong to the above described designated categories. If the search query #1-1, the search query #1-2, and the search query #1-3 correspond to a designated category #1-1, a designated category #1-2, and a designated category #1-3, respectively, the specifying unit 432 determines that the input customer IU1 has input the search queries in the order of the designated category #1-1, the designated category #1-2, and the designated category #1-3. In this manner, the specifying unit 432 categorizes the search queries, which have been input by the input customer in each period, into categories. Furthermore, the specifying unit 432 specifies transition modes of categories (for example, designated categories).\n\n(186) For example, the specifying unit 432 categorizes the transition modes of the categories of each input customer into a plurality of categories. As described above with reference to FIG. 4, the specifying unit 432, for example, categorizes the transition mode of the plurality of designated categories respectively corresponding to the plurality of search queries input by the input customer IU1 into categories. For example, if the transition of the categories occur in the order of the designated category #1-1, the designated category #1-2, and the designated category #1-3 in this order, the transition of these categories is categorized into a transition category #1. More specifically, the specifying unit 432 categorizes the transition mode of the categories of the input customer into a transition category corresponding to a path to the reference query.\n\n(187) Learning Unit 433\n\n(188) At least in one embodiment, the learning unit 433 causes a model to learn characteristics of changes in the categories specified by the specifying unit 432.\n\n(189) At least in one embodiment, the learning unit 433 carries out learning of a model so that objective queries are output when changes in the categories to which the search queries input by input customers are input. For example, the learning unit 433 carries out learning of a model so that objective queries are output in the input order when changes in the categories to which the search queries input by input customers are input.\n\n(190) At least in one embodiment, the learning unit 433 carries out learning of a model for each change of categories. For example, the learning unit 433 carries out learning of this model so that a search query input by an input customer after a reference query is output when search queries input by this input customer are input in the input order with respect to a model corresponding to changes in the categories to which the search queries input by this input customer belong.\n\n(191) At least in one embodiment, the learning unit 433 carries out learning of a model so that, a similar vector is output if a search query corresponding to a similar change in categories is input, and a dissimilar vector is output if a search query corresponding to a dissimilar change in categories is input.\n\n(192) At least in one embodiment, the learning unit 433 causes a model to learn the characteristics of changes in the categories of search queries input by an input customer on the way to a target corresponding to the reference query.\n\n(193) For example, the learning unit 433 learns a model so that, if a history of search queries of a customer having a similar transition category is input, a similar vector is generated and, if a history of search queries of customers having a dissimilar transition categories is input, a dissimilar vector is generated. For example, the learning unit 433 groups users based on the search queries input in the past by the users who have input the reference query. As described above with reference to FIG. 4, for example, groups are groups of transition categories such as the transition category #1, the transition category #2, and the transition category #3. Then, the learning unit 433 causes the model to learn characteristics of the change of the input search queries for each group. More specifically, the learning unit 433 causes the model to learn the characteristics of the change of the categories to which the search queries belong. In other words, the learning unit 433 groups users by the change categories of the categories and causes the model to learn the change of the search queries of the users.\n\n(194) As described above with reference to FIG. 4, for example, the input customer IU1 is categorized into the transition category #1. Also, the input customer IU2 is categorized into a transition category #2. The learning unit 433 can specify another transition category similar to a certain transition category by referencing a predetermined dictionary. For example, if the transition category #1 is related to the transition category #2 in the dictionary, the learning unit 433 can specify the transition category #2 as a category similar to the transition category #1.\n\n(195) The learning unit 433 carries out machine learning so that, even in a case in which a plurality of users inputs the same reference query, the model outputs different vectors if changes of categories are different. Therefore, the learning unit 433 can cause the model to accurately learn whether the user is the user who reaches the reference query or the characteristics of future search queries input by a certain user by using the search queries of the user or the categories of the search queries. As described above, the learning unit 433 specifies the categories to which the search queries input by the user in the past belong and carries out learning of the model for each type of change in the specified categories. For example, if the user inputs a search query \u201ccountry of dreams and magic (exemplary name of facilities), the learning unit 433 can cause the model to learn the way how the user reaches this search query. In this manner, the learning unit 433 can cause the model to learn high-level concepts (for example, transition modes) of the categories of search queries.\n\n(196) Estimation Unit 434\n\n(197) At least in one embodiment, the estimation unit 434 estimates a search query, which is to be input in the future by a customer, by using a model learned by the learning unit 433 from changes in the categories to which the search queries input by this customer belong.\n\n(198) For example, the estimation unit 434 predicts a search query, which is to be input by a target user in the future, from the history of the search queries of the target user by using a model. As described above with reference to FIG. 4, the estimation unit 434 generates a vector #4 by inputting the history of the search queries (for example, a search query #4-1, a search query #4-2) of the target user TU1 to the learned model. The vector #4 represents the transition mode of the categories of the target user TU1. For example, the estimation unit 434 can specify another target user corresponding to a vector similar to the generated vector #4 and carry out emphatic filtering based on the user information of the specified other target user. In this manner, the estimation unit 434 can enable prediction of needs of the target user TU1 and/or targeting on the target user TU1. The above described group (for example, designated category) may correspond to particular marketing (for example, model change).\n\n3. Flow of Category Determination Process\n\n(199) Then, with reference to FIG. 10, a procedure of a category determination process by the information providing device 100 according to the embodiment will be described.\n\n(200) FIG. 10 is a flow chart illustrating an example of a process executed by the information providing device 100 according to the embodiment for determining whether the categorization of keywords or search queries for categorizing customers is appropriate or not.\n\n(201) As illustrated in FIG. 10, first, the first determination processing unit 410 (for example, acquisition unit 411) of the information providing device 100 acquires a search history from the search-query database 310 of the information providing device 100 (step S101).\n\n(202) Then, the first determination processing unit 410 (for example, acquisition unit 411) specifies the input customers who have input the reference query from the acquired search history (step S102).\n\n(203) Then, the first determination processing unit 410 (for example, the categorization unit 412) categorizes the search queries input by the specified input customer (for example, the input customer specified by the acquisition unit 411) into categories for each period (step S103).\n\n(204) Then, the first determination processing unit 410 (for example, the determination unit 413) determines whether the element group of the categorization result in each period (for example, the categorization result by the categorization unit 412 in each period) satisfies a predetermined condition or not (step S104).\n\n(205) Then, the first determination processing unit 410 (for example, the determination unit 413) determines whether the reference query or the category is appropriate or not in accordance with the determination result (step S105).\n\n4. Flow of Period Determination Process\n\n(206) Then, with reference to FIG. 11, a procedure of a period determination process by the information providing device 100 according to the embodiment will be described.\n\n(207) FIG. 11 is a flow chart illustrating an example of a process executed by the information providing device 100 according to the embodiment for determining whether the period in the time series of search queries is appropriate or not.\n\n(208) As illustrated in FIG. 11, first, the second determination processing unit 420 (for example, acquisition unit 421) of the information providing device 100 acquires a search history from the search-query database 310 of the information providing device 100 (step S201).\n\n(209) Then, the second determination processing unit 420 (for example, acquisition unit 421) specifies the input customers who have input the reference query from the acquired search history (step S202).\n\n(210) Then, the second determination processing unit 420 (for example, the categorization unit 422) categorizes the search queries input by the specified input customer (for example, the input customer specified by the acquisition unit 421) into categories for each period (step S203).\n\n(211) Then, the second determination processing unit 420 (for example, the determination unit 423) determines whether the element group of the categorization result in each period (for example, the categorization result by the categorization unit 422 in each period) satisfies a predetermined condition or not (step S204).\n\n(212) Then, the second determination processing unit 420 (for example, the determination unit 423) determines whether the period is appropriate or not in accordance with the determination result (step S205).\n\n5. Flow of Model Learning Process\n\n(213) Then, with reference to FIG. 12, a procedure of a model learning process by the information providing device 100 according to the embodiment will be described.\n\n(214) FIG. 12 is a flow chart illustrating an example of a process executed by the information providing device 100 according to the embodiment for learning a model for predicting a transition of categories of search queries input by customers.\n\n(215) As illustrated in FIG. 12, first, the learning processing unit 430 (for example, the acquisition unit 431) of the information providing device 100 acquires the search queries input in each period by input customers (step S301).\n\n(216) Then, the learning processing unit 430 (for example, the specifying unit 432) categorizes the acquired search queries (for example, the search queries acquired by the acquisition unit 431) into categories (step S302).\n\n(217) Then, the learning processing unit 430 (for example, the specifying unit 432) categorizes each input customer by each transition mode of categories (step S303). Each input customer is categorized into each transition category representing a transition mode of categories.\n\n(218) Then, the learning processing unit 430 (for example, the learning unit 433) learns a model so that a similar vector is output if a history of search queries of a similar transition category is input and that a dissimilar vector is output if a history of search queries of a dissimilar transition category is input (step S304).\n\n6. Other Embodiments\n\n(219) The information providing device 100 according to the above described embodiment may be implemented by various other modes other than the above described embodiments. Therefore, hereinafter, other embodiments of the above described information providing device 100 will be described.\n\n(220) 6-1. Expression of Designated Categories\n\n(221) The above described designated categories may be the categories indicating a path to a target represented by the reference query. For example, it is assumed that a user inputs the title of first comics on a first magazine as a search query in a period \u201c\u22123 to \u22122 month\u201d, inputs the title of second comics on a second magazine as a search query in a period \u201c\u22122 to \u22121 month\u201d, and inputs the title of third comics on a third magazine as a search query in a period \u201c\u22121 to 0 month\u201d. Furthermore, it is assumed that another user inputs a first event name of a first amusement park as a search query in a period \u201c\u22123 to \u22122 month\u201d, inputs a second event name of a second amusement park as a search query in a period \u201c\u22122 to \u22121 month\u201d, and inputs a third event name of a third amusement park as a search query in a period \u201c\u22121 to 0 month\u201d. In this example, the search query \u201cthe title of the first comics\u201d, the search query \u201cthe title of the second comics\u201d, and the search query \u201cthe title of the third comics\u201d belong to a designated category \u201ccomics\u201d. The search query \u201cthe title of the first comics\u201d does not have to belong to the designated category \u201cfirst magazine\u201d, the search query \u201cthe title of the second comics\u201d does not have to belong to a designated category \u201csecond magazine\u201d, and the search query \u201cthe title of the third comics\u201d does not have to belong to a designated category \u201cthird magazine\u201d. More specifically, the designated categories may be higher-level categories (for example, comics, amusement parks) of normal categories (for example, the first magazine, the second magazine, the third magazine, the first amusement park, the second amusement park, and the third amusement park) for categorizing search queries.\n\n(222) The above described designated category \u201ccomics\u201d can subject the path from the search query \u201cthe title of the first comics\u201d to \u201cthe title of the third comics\u201d via the search query \u201cthe title of the second comics\u201d to coarse graining. Similarly, the above described designated category \u201camusement park\u201d can subject the path from the search query \u201cthe first event name\u201d to the \u201cthe third event name\u201d via the search query \u201cthe second event name\u201d to coarse graining. As described above, the first determination processing unit 410 (for example, the categorization unit 412) of the information providing device 100 can categorize search queries into any of a plurality of categories respectively corresponding to a plurality of paths to a target represented by the reference query for each period. Then, the first determination processing unit 410 (for example, the categorization unit 412) can determine whether the reference query or category (for example, designated category) is appropriate or not based on the categorization result.\n\n(223) 6-2. Dispersion of Element Group of Search Queries\n\n(224) In the above described embodiments, if the element group of search queries converges, the first determination processing unit 410 (for example, the determination unit 413) of the information providing device 100 determines that the list of the search queries is appropriate, but the embodiment is not limited thereto. If the dispersion of the element group is high (for example, the element group is formed in a dispersed manner), it may be determined that the list of the search queries is appropriate. If dispersion of the element group is high, the completeness of the list of the search queries may be high. Such a list can be a list having a wide range of targets. For example, the list can include a keyword (for example, search query) in which many users are interested. On the other hand, if the dispersion of the element group is low, the list can include a keyword (for example, search query) in which particular target users are interested.\n\n(225) 6-3. Prediction of Search Behavior by Machine Learning Model\n\n(226) In the above described embodiment, the learning processing unit 430 (for example, the learning unit 433) of the information providing device 100 carries out learning of a model so that the model outputs a vector representing a transition mode of categories, but the embodiment is not limited thereto. The learning processing unit 430 (for example, the learning unit 433) may predict a future search query from a particular time-series search history of a certain user by using a machine learning model, which has learned search histories of particular time series. Generally, a past behavior is a trigger of a future behavior. A future behavior may be caused by a plurality of past behaviors. In such machine learning, a search query input in a period before the reference time and date corresponds to an explanatory variable. On the other hand, a search query input in a period after the reference time and date corresponds to an objective variable. If a particular time-series search history of a user is input to a learned model, the learned model can estimate a future search query.\n\n(227) A plurality of search queries along time series may be converted into vectors in advance. As described above, for example, an embedding vector corresponding to a search query can be obtained by training various language expression models.\n\n(228) In some implementation modes, the learning processing unit 430 (for example, the learning unit 433) of the information providing device 100 may train a model architecture such as a series transformation model (Sequence To Sequence Model) by using training data including search logs. For example, the learning unit 433 can train a series transformation model by minimizing a negative logarithmic likelihood corresponding to the series transformation model by using training data including search logs. Examples of the series transformation model include a model having an attention mechanism such as a Transformer Model and a Recurrent Neural Network (RNN) (for example, gate-equipped RNN such as Long Short Term Memory (LSTM)). Instances included in the training data are, for example, time-series search queries and time-series categories (in other words, transitions of categories). Labels related to the instances are, for example, search queries (for example, reference query) and categories (for example, designated categories). The learning unit 433 can predict a future search query or a future category by inputting time-series search queries to a trained series transformation model.\n\n(229) 6-4. Targeting based on Time-Series Search Queries\n\n(230) In some embodiments, the first determination processing unit 410 or the second determination processing unit 420 of the information providing device 100 may have a specifying unit (not illustrated) which specifies a second search query, which is input before a first search query is input and is related to the first search query, as a keyword used in targeting based on the first search query among search queries. Regarding the learning processing unit 430 of the information providing device 100, the above described specifying unit 432 may be implemented as a first specifying unit. In this case, the above described specifying unit may be implemented as a second specifying unit in the learning processing unit 430.\n\n(231) The specifying unit specifies the second search query, which has relativity with the first search query input at certain time and date and has been input before the time and date, from time-series search queries. As described later, the specifying unit can use the specified second query for targeting based on the first search query.\n\n(232) The time-series search queries are, for example, search queries, which have been acquired by the acquisition unit 411 or the acquisition unit 421 and input by a plurality of input customers who have input the reference query. The first search query may be the reference query or a search query other than the reference query.\n\n(233) For example, the second search query having the relativity with the first search query may be specified based on the above described relevance degrees between the search queries with reference to FIG. 1. As described above, the relevance degree between the search queries may be, for example, cosine similarity between the embedding vectors corresponding to the search queries. For example, if the relevance degree between the first search query and another search query satisfies a threshold value, the specifying unit may specify the other search query as the second search query. For example, if the first search query is \u201cmodel change\u201d, the second search query may be a search query such as battery exchange, a billing plan, a packet fee, or the like. Alternatively, the second search query may be the search query described above with reference to FIG. 1 such as the company C1 (exemplary company name), the company president P1 (exemplary name of a person), that dog (exemplary mascot name), or the smartphone SP1 (exemplary smartphone name).\n\n(234) As another example, the second search query having relativity with the first search query may be specified based on the number of search queries or the number of users who have input search queries in a predetermined period before the first search query is input. For example, if the number of the search queries or the number of the users satisfies a threshold value, the specifying unit may specify these search queries as the second search query.\n\n(235) The targeting based on the first search query is the targeting of, for example, a trade target (for example, commercial product or service) corresponding to the first search query. The specifying unit can target the information about the trade target (for example, advertisement contents) on users. In other words, the targets of the advertisement contents are narrowed down to particular users. In some implementation modes, the storage unit 300 of the information providing device 100 may have a user database (not illustrated) which stores user information of the user of the user device 500. The specifying unit may process Structured Query Language (SQL) queries and specify the users who have input the second search query from the user database. Then, the specifying unit may provide the information about the trade target to the specified users.\n\n(236) As an example for explanation, it is assumed that the first search query is \u201cmodel change\u201d. Furthermore, it is assumed that the second search query having relativity with the first search query is \u201cbattery exchange\u201d. In this example, the specifying unit can target the information about the trade target corresponding to the search query \u201cmodel change\u201d on the users who have input the search query \u201cbattery exchange\u201d. The specifying unit can provide various information about the model change (for example, information about a model change campaign, advertisement contents) via electronic mail accounts of the users, push notifications to the users, personal pages of the users, etc.\n\n(237) The specifying unit may provide the specified second query to the entities related to the information providing device 100 as advertisement targeting data (for example, targeting keyword). Also, the specifying unit may specify the users who have input the specified second query from the user database and categorize the specified users into a targeting group. The targeting group includes, for example, the information (for example, user ID) about the users who have input the second query. The specifying unit may provide the targeting group, which is associated with the second query, to the entities related to the information providing device 100 as advertisement targeting data.\n\n7. Others\n\n(238) Also, among the processes described in the above described embodiments, some of the processes described to be automatically carried out can be also manually carried out. Alternatively, all or some of the processes described to be manually carried out may also be automatically carried out by a publicly known method. Other than that, the processing procedures illustrated in the above described document or drawings, specific names, and information including various data and parameters can be arbitrarily changed unless otherwise specifically stated. For example, the various information illustrated in the drawings is not limited to the illustrated information.\n\n(239) Also, each of the constituent elements of each illustrated device is a functional idea and is not necessarily required to be physically configured as the illustration. More specifically, the specific mode of the distribution/integration of each device is not limited to that of the illustration, and all or part thereof can be distributed/integrated functionally or physically in an arbitrary unit in accordance with various load, usage circumstances, etc.\n\n(240) For example, part or all of the storage unit 300 illustrated in FIG. 5 may be retained in a storage server or the like instead of being retained by the information providing device 100. In such a case, the information providing device 100 acquires various information such as search queries by accessing the storage server.\n\n8. Hardware Configuration\n\n(241) Also, the information providing device 100 according to the above described embodiment is realized, for example, by a computer 1000 having a configuration as illustrated in FIG. 13. FIG. 13 is a diagram illustrating an example of a hardware configuration. The computer 1000 is connected to an output device 1010 and an input device(s) 1020 and has a configuration in which a computation device 1030, a primary storage device 1040, a secondary storage device 1050, an output IF (Interface) 1060, an input IF 1070, and a network IF 1080 are connected by a bus 1090.\n\n(242) The computation device 1030 operates based on, for example, a program(s) stored in the primary storage device 1040 and/or the secondary storage device 1050 and/or a program(s) read from the input device 1020 and executes various processes. The primary storage device 1040 is a memory device such as a RAM, which temporarily stores data used by the computation device 1030 in various computation. Also, the secondary storage device 1050 is a storage device in which data used in various computation and various databases by the computation device 1030 are registered and is realized by a Read Only Memory (ROM), a Hard Disk Drive (HDD), a flash memory, or the like.\n\n(243) The output IF 1060 is an interface for transmitting information, which serves as an output target, to the output device 1010 such as a monitor or a printer, which outputs various information, and is realized, for example by a connector of standards such as a Universal Serial Bus (USB), a Digital Visual Interface (DVI), a High Definition Multimedia Interface (HDMI (registered tradename)). Also, the input IF 1070 is an interface for receiving information from the various input devices 1020 such as a mouse, a keyboard, and a scanner and is realized, for example, by a USB or the like.\n\n(244) Note that the input device 1020 may be a device which reads information from, for example, an optical recording medium such as a Compact Disc (CD), a Digital Versatile Disc (DVD), or a Phase change rewritable Disk (PD), a magnetooptical recording medium such as a Magneto-Optical disk (MO), a tape medium, a magnetic recording medium, or a semiconductor memory. Also, the input device 1020 may be an external storage medium such as a USB memory.\n\n(245) The network IF 1080 receives data from another equipment via a network N, transmits the data to the computation device 1030, and also transmits the data generated by the computation device 1030 to another equipment via the network N.\n\n(246) The computation device 1030 carries out control of the output device 1010 and the input device 1020 via the output IF 1060 and the input IF 1070. For example, the computation device 1030 loads the program from the input device 1020 or the secondary storage device 1050 to the primary storage device 1040 and executes the loaded program.\n\n(247) For example, if the computer 1000 functions as the information providing device 100, the computation device 1030 of the computer 1000 realizes the function of the control unit 400 by executing the program loaded to the primary storage device 1040.\n\n9. Effects\n\n(248) As described above, the first determination processing unit 410 of the information providing device 100 according to the embodiment has the acquisition unit 411, the categorization unit 412, and the determination unit 413.\n\n(249) In the information providing device 100 according to the embodiment, the acquisition unit 411 acquires the search queries, which have been input by a plurality of input customers who have input the reference query. Also, in the information providing device 100 according to the embodiment, the categorization unit 412 categorizes the search queries, which have been input in a predetermined period among the search queries, into a plurality of categories. Also, in the information providing device 100 according to the embodiment, the determination unit 413 determines whether the categorization result by the categorization unit 412 satisfies a predetermined determination condition or not.\n\n(250) Also, in the information providing device 100 according to the embodiment, the determination unit 413 determines whether the number of the categories to which the search queries have been categorized satisfies a predetermined condition or not.\n\n(251) Also, in the information providing device 100 according to the embodiment, the determination unit 413 determines whether the number of the categories to which the search queries of predetermined percentage or higher among the search queries are categorized is equal to or less than a predetermined threshold value.\n\n(252) Also, in the information providing device 100 according to the embodiment, the determination unit 413 specifies the category to which the search queries of predetermined percentage or higher among the search queries are categorized and determines whether the number of the customers who have input the search queries categorized to the specified category is equal to or higher than a predetermined threshold value or not.\n\n(253) Also, in the information providing device 100 according to the embodiment, the acquisition unit 411 acquires the search queries which have been input in a period before the reference time and date, at which the input customer has input the reference query, by predetermined time and date as a predetermined period.\n\n(254) Also, in the information providing device 100 according to the embodiment, the acquisition unit 411 acquires the search queries, which have been input respectively in a plurality of different periods based on the reference time and date. Also, in the information providing device 100 according to the embodiment, the categorization unit 412 categorizes the search queries into a plurality of categories for each period. Also, in the information providing device 100 according to the embodiment, the determination unit 413 determines whether the categorization result satisfies a predetermined determination condition or not for each period.\n\n(255) Also, in the information providing device 100 according to the embodiment, the determination unit 413 determines whether the high-level categories to which the categories to which the search queries have been categorized satisfies a predetermined condition or not.\n\n(256) Also, the first determination processing unit 410 of the information providing device 100 according to the embodiment has the output unit 414 (for example, the output unit 414 implemented as a first output unit), which outputs the information that the reference query is appropriate if it is determined by the determination unit 413 that the categorization result satisfies the predetermined condition.\n\n(257) Also, the first determination processing unit 410 of the information providing device 100 according to the embodiment has the output unit 414 (for example, the output unit 414 implemented as a second output unit), which outputs the information that the plurality of categories, which categorizes the search queries, is appropriate if it is determined by the determination unit 413 that the categorization result satisfies the predetermined determination condition.\n\n(258) Also, the first determination processing unit 410 of the information providing device 100 according to the embodiment has the extraction unit 415 which, if it is determined by the determination unit 413 that the categorization result satisfies a predetermined determination condition, extracts the search queries categorized into a category, which satisfies a predetermined categorization condition among a plurality of categories. Also, the first determination processing unit 410 of the information providing device 100 according to the embodiment has the provision unit 416, which provides a list of the search queries extracted by the extraction unit 415.\n\n(259) Also, in the information providing device 100 according to the embodiment, the extraction unit 415 specifies the category to which the search queries of predetermined percentage or higher are categorized among search queries and extracts the search queries categorized to the specified category.\n\n(260) Also, in the information providing device 100 according to the embodiment, the extraction unit 415 extracts the search queries having high relativity with the reference query among search queries.\n\n(261) The first determination processing unit 410 of the information providing device 100 according to the embodiment has the specifying unit which specifies a second search query, which is input before the first search query is input and is related to the first search query, as a keyword used in targeting based on the first search query among search queries.\n\n(262) By the above described processes, the information providing device 100 can more appropriately analyze the relation between a customer and a target indicated by a predetermined search query.\n\n(263) Hereinabove, some of the embodiments of the present application have been described in detail based on the drawings. However, these are examples, and, including the aspects described in the section of disclosure of the invention, the present invention can be carried out in other modes with various modifications and/or improvements based on the knowledge of the persons skilled in the art.\n\n(264) Also, the above described information providing device 100 may be realized by a plurality of server computers. Also, the configuration can be flexibly changed depending on the functions, for example, by invoking and realizing an external platform or the like by an Application Programming Interface (API), a network computing, or the like.\n\n(265) Also, the above described \u201cpart (section, module, unit)\u201d can be replaced by \u201cmeans\u201d, \u201ccircuit\u201d, or the like. For example, the acquisition unit can be replaced by an acquisition means or an acquisition circuit.\n\n(266) Although the invention has been described with respect to specific embodiments for a complete and clear disclosure, the appended claims are not to be thus limited but are to be construed as embodying all modifications and alternative constructions that may occur to one skilled in the art that fairly fall within the basic teaching herein set forth."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 33,
      "claims_start": 33,
      "description_end": 33,
      "description_start": 14,
      "drawings_end": 13,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 14,
      "number_of_drawing_sheets": 11,
      "number_of_figures": 13,
      "page_count": 33,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 33,
      "specification_start": 14,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006811750,
    "field_of_search_cpc": [
      "G06F 16/24575",
      "G06F 16/24573",
      "G06F 16/24578",
      "G06F 16/248",
      "G06F 16/2453",
      "G06Q 30/0251",
      "G06Q 30/0282"
    ],
    "foreign_priority": [
      {
        "app_filing_date": "2020-03-19",
        "app_number": "JP2020-050222",
        "country": "JP"
      }
    ],
    "foreign_references": [
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "JP",
        "patent_number": "2011118759",
        "pub_month": "2011-06-01"
      },
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "JP",
        "patent_number": "2014-006757",
        "pub_month": "2014-01-01"
      },
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "JP",
        "patent_number": "2019053519",
        "pub_month": "2019-04-01"
      }
    ],
    "group_art_unit": "2165",
    "guid": "US-11556547-B2",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G47/565/115",
    "intl_class_current_primary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "16/2457",
        "version": "2019-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "16/248",
        "version": "2019-01-01"
      }
    ],
    "intl_class_issued": [
      "G06F16/2457",
      "G06F16/248"
    ],
    "inventors": [
      {
        "city": "Tokyo",
        "country": "JP",
        "name": "Tsubouchi; Kota",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tokyo",
        "country": "JP",
        "name": "Yamaguchi; Shuji",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tokyo",
        "country": "JP",
        "name": "Taguchi; Hiroaki",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "Tsubouchi; Kota et al.",
    "legal_firm_name": [
      "Procopio, Cory, Hargreaves & Savitch LLP"
    ],
    "npl_references": [
      {
        "citation": "Office Action dated Aug. 31, 2021 from counterpart JP Patent Application No. 2020-050222, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Office Action dated Feb. 1, 2022 from counterpart JP Patent Application No. 2020-050222, 4 pages.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "Determination apparatus, determination method, and non-transitory computer readable storage medium",
    "primary_examiner": "Casanova; Jorge A",
    "publication_date": "2023-01-17",
    "publication_number": "11556547",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": true,
        "patentee_name": "Inohara",
        "pub_month": "2004-06-01",
        "publication_number": "6757670"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "He",
        "pub_month": "2011-11-01",
        "publication_number": "8055655"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "He",
        "pub_month": "2020-05-01",
        "publication_number": "10664512"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Ron",
        "pub_month": "2021-11-01",
        "publication_number": "11176495"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Zhang",
        "pub_month": "2022-02-01",
        "publication_number": "11243992"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Taira",
        "pub_month": "2005-03-01",
        "publication_number": "20050050045"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Xie",
        "pub_month": "2010-07-01",
        "publication_number": "20100179948"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Li",
        "pub_month": "2014-04-01",
        "publication_number": "20140101119"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Fey",
        "pub_month": "2015-06-01",
        "publication_number": "20150160806"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Higuchi",
        "pub_month": "2018-02-01",
        "publication_number": "20180032579"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Jarr",
        "pub_month": "2018-06-01",
        "publication_number": "20180181569"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Lal",
        "pub_month": "2020-06-01",
        "publication_number": "20200184018"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Ballard",
        "pub_month": "2021-07-01",
        "publication_number": "20210224337"
      }
    ]
  },
  {
    "app_filing_date": "2022-04-06",
    "appl_id": "17714585",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Los Altos",
        "country": "US",
        "name": "Toyota Research Institute, Inc.",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Los Altos",
        "country": "US",
        "name": "Toyota Research Institute, Inc.",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      },
      {
        "city": "Toyota",
        "country": "JP",
        "name": "Toyota Jidosha Kabushiki Kaisha",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "attorney_name": [
      "Darrow; Christopher G.",
      "Darrow Mustafa PC"
    ],
    "composite_id": "1000006314444!US-US-11554716",
    "cpc_inventive": [
      {
        "cpc_class": "B60Q",
        "cpc_subclass": "3/80",
        "version": "2017-02-01"
      },
      {
        "cpc_class": "B62D",
        "cpc_subclass": "1/06",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B60Q",
        "cpc_subclass": "3/283",
        "version": "2017-02-01"
      },
      {
        "cpc_class": "B62D",
        "cpc_subclass": "15/029",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "System, methods, and other embodiments described herein relate to providing visual feedback on a steering apparatus using a pscyhophysical model. In one embodiment, a method includes obtaining a difference between a current angle of a steering apparatus and a recommended angle of the steering apparatus. The method further includes determining an appearance parameter based upon the difference and a psychophysical model, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. The method further includes illuminating a region of the steering apparatus with light based upon the appearance parameter.",
      "brief": "TECHNICAL FIELD\n\n(1) The subject matter described herein relates, in general, to providing visual feedback on a steering apparatus, and, more particularly, to utilizing a psychophysical model to provide the visual feedback.\n\nBACKGROUND\n\n(2) A vehicle may be equipped with systems that provide an operator of the vehicle with driving instructions and/or recommendations. For instance, the driving instructions/recommendations may enable the vehicle to be operated by the operator such that the vehicle follows a route and/or avoids an obstacle as the vehicle navigates about an environment. Conventional systems for providing driving instructions/recommendations may present information to the operator in a manner that distracts the operator from looking at the road. Additionally, conventional systems tend to be deficient with respect to communicating precise driving instructions/recommendations to the operator of the vehicle. Furthermore, conventional systems tend not to account for factors that influence perception of the driving instructions/recommendations by the operator.\n\nSUMMARY\n\n(3) An example visual feedback system for communicating a recommended steering angle change through visual cues on a steering apparatus is described herein. In one embodiment, the visual feedback system obtains a difference between a current angle of a steering apparatus and a recommended angle of the steering apparatus. The visual feedback system determines an appearance parameter(s) (e.g., hue, lightness, brightness, chroma, colorfulness, or saturation) based upon the difference and a psychophysical model. The psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. The visual feedback system illuminates a region of the steering apparatus with light based upon the appearance parameter. In an example, the visual feedback system activates one or more lighting elements in the region such that the one or more lighting elements emit the light, where an operator of the vehicle perceives the light as having the appearance parameter(s) in order to communicate the difference to the operator. For instance, when the difference is relatively large, the one or more lighting elements may emit relatively bright light, whereas when the difference is relatively small, the one or more lighting elements may emit relatively dim light. In this manner, the visual feedback system is able to visually inform the operator of the difference in a precise manner that is based upon human perception.\n\n(4) In one embodiment, a computing system for providing visual feedback on a steering apparatus of a vehicle is disclosed. The computing system comprises a processor and memory communicably coupled to the processor. The memory stores instructions that, when executed by the processor, cause the processor to obtain a difference between a current angle of the steering apparatus and a recommended angle of the steering apparatus. The instructions further cause the processor to determine an appearance parameter based upon the difference and a psychophysical model stored in the memory, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. The instructions further cause the processor to illuminate a region of the steering apparatus with light based upon the appearance parameter.\n\n(5) In one embodiment, a non-transitory computer-readable medium for providing visual feedback on a steering apparatus of a vehicle and including instructions that, when executed by a processor, cause the processor to perform one or more functions is disclosed. The instructions cause the processor to obtain a difference between a current angle of the steering apparatus and a recommended angle of the steering apparatus. The instructions further cause the processor to determine an appearance parameter based upon the difference and a psychophysical model, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. The instructions further cause the processor to illuminate a region of the steering apparatus with light based upon the appearance parameter.\n\n(6) In one embodiment, a method is disclosed. The method includes obtain a difference between a current angle of a steering apparatus and a recommended angle of the steering apparatus. The method further includes determine an appearance parameter based upon the difference and a psychophysical model, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. The method further includes illuminate a region of the steering apparatus with light based upon the appearance parameter.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A computing system for providing visual feedback on a steering apparatus of a vehicle, the computing system comprising: a processor; and memory communicably coupled to the processor and storing instructions that, when executed by the processor, cause the processor to: obtain a difference between a current angle of the steering apparatus and a recommended angle of the steering apparatus; determine an appearance parameter based upon the difference and a psychophysical model stored in the memory, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light; and illuminate a region of the steering apparatus with light based upon the appearance parameter.   \n\n2. The computing system of claim 1, wherein the instructions for illuminate the region of the steering apparatus with the light based upon the appearance parameter comprise further instructions that cause the processor to activate at least one light emitting diode (LED) comprised by the steering apparatus, wherein the at least one LED emits the light. \n\n3. The computing system of claim 1, wherein the psychophysical model comprises a perceptually uniform color map. \n\n4. The computing system of claim 1, wherein the instructions further cause the processor to: obtain sensor data that is indicative of an ambient light level within a cabin of the vehicle, wherein illuminate the region of the steering apparatus with the light is additionally based upon the sensor data.  \n\n5. The computing system of claim 1, wherein the instructions further cause the processor to: subsequent to illuminate the region of the steering apparatus with the light based upon the appearance parameter, obtain a second difference between a second current angle of the steering apparatus and the recommended angle of the steering apparatus, wherein the second current angle is between the current angle and the recommended angle; determine a second appearance parameter based upon the second difference and the psychophysical model stored in the memory; and illuminate the region of the steering apparatus with second light based upon the second appearance parameter.  \n\n6. The computing system of claim 5, wherein the light is associated with a first set of points defined by a first tolerance contour within a color space, wherein the second light is associated with a second set of points defined by a second tolerance contour within the color space, wherein the first tolerance contour makes contact with the second tolerance contour within the color space. \n\n7. The computing system of claim 1, wherein the appearance parameter includes at least one of: hue; lightnesss; brightness; chroma; colorfulness; or saturation.  \n\n8. The computing system of claim 1, wherein the region is selected based upon a position of a hand of an operator of the vehicle on the steering apparatus. \n\n9. A non-transitory computer-readable medium for providing visual feedback on a steering apparatus of a vehicle and including instructions that, when executed by a processor, cause the processor to: obtain a difference between a current angle of the steering apparatus and a recommended angle of the steering apparatus; determine an appearance parameter based upon the difference and a psychophysical model, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light; and illuminate a region of the steering apparatus with light based upon the appearance parameter.  \n\n10. The non-transitory computer-readable medium of claim 9, wherein the appearance parameter is based upon a sign of the difference. \n\n11. The non-transitory computer-readable medium of claim 9, wherein the instructions further cause the processor to: prior to illuminate the region, select the region based upon a sign of the difference.  \n\n12. The non-transitory computer-readable medium of claim 9, wherein the instructions for determine the appearance parameter based upon the difference and the psychophysical model comprise further instructions that cause the processor to: determine a Red Green Blue (RGB) value based upon the difference; and determine the appearance parameter based upon the RGB value and the psychophysical model.  \n\n13. The non-transitory computer-readable medium of claim 9, wherein the instructions for illuminate the region of the steering apparatus with the light based upon the appearance parameter comprise further instructions that cause the processor to change a brightness of the region from a first brightness to a second brightness. \n\n14. A method comprising: obtaining a difference between a current angle of a steering apparatus and a recommended angle of the steering apparatus; determining an appearance parameter based upon the difference and a psychophysical model, wherein the psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light; and illuminating a region of the steering apparatus with light based upon the appearance parameter.  \n\n15. The method of claim 14, wherein a color of the light is based upon a sign of the difference. \n\n16. The method of claim 14, further comprising: prior to illuminating the region, determining a position of a hand on the steering apparatus based upon sensor data generated by a sensor comprised by the steering apparatus; determining a sign of the difference; and selecting the region based upon the position of the hand and the sign of the difference.  \n\n17. The method of claim 14, wherein the difference is obtained from an advanced driver assistance-system (ADAS) of a vehicle that includes the steering apparatus. \n\n18. The method of claim 14, wherein the difference is obtained from a navigation system of a vehicle that includes the steering apparatus. \n\n19. The method of claim 14, wherein the appearance parameter varies as the steering apparatus is rotated from the current angle to the recommended angle. \n\n20. The method of claim 14, further comprising: subsequent to illuminating the region, determining that the steering apparatus has been rotated such that the current angle is the recommended angle; and unilluminating the region based upon the current angle being the recommended angle.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) The accompanying drawings, which are incorporated in and constitute a part of the specification, illustrate various systems, methods, and other embodiments of the disclosure. It will be appreciated that the illustrated element boundaries (e.g., boxes, groups of boxes, or other shapes) in the figures represent one embodiment of the boundaries. In some embodiments, one element may be designed as multiple elements or multiple elements may be designed as one element. In some embodiments, an element shown as an internal component of another element may be implemented as an external component and vice versa. Furthermore, elements may not be drawn to scale.\n\n(2) FIG. 1 illustrates one embodiment of a vehicle within which systems and methods disclosed herein may be implemented.\n\n(3) FIG. 2 illustrates one embodiment of a visual feedback system that is associated with recommending a steering angle change using a psychophysical model.\n\n(4) FIG. 3 illustrates one embodiment of a steering apparatus.\n\n(5) FIG. 4 illustrates a table that details different variants of recommending a steering angle change using different brightnesses.\n\n(6) FIGS. 5A-D illustrate the different variants of recommending a steering angle change detailed in FIG. 4.\n\n(7) FIG. 6 illustrates a table that details different variants of recommending a steering angle change using different colors.\n\n(8) FIGS. 7A-D illustrate the different variants of recommending a steering angle change detailed in FIG. 6.\n\n(9) FIG. 8 illustrates a table that details different variants of recommending a steering angle change using different numbers of lighting elements.\n\n(10) FIGS. 9A-D illustrate the different variants of recommending a steering angle change detailed in FIG. 8.\n\n(11) FIG. 10 illustrates one embodiment of a method that is associated with recommending a steering angle change using a psychophysical model.\n\nDETAILED DESCRIPTION\n\n(12) Systems, methods, and other embodiments associated with improving a manner in which a recommended steering angle is communicated to an operator of a vehicle are disclosed herein. As noted above, conventional systems for providing driving instructions/recommendations to an operator of a vehicle tend to struggle between providing precise driving instructions/recommendations while at the same time presenting the driving instructions/recommendations to the operator in an easily understandable manner. Furthermore, conventional systems may present driving instructions/recommendations in a distracting manner. Additionally, human perception with respect to color and/or brightness is non-uniform with respect to perceptual contrasts. In an example, a system may cause a lighting element to emit first light of a first wavelength and/or first amplitude to convey a driving instruction/recommendation that involves rotating a steering apparatus from a first angle to a second angle. The operator perceives the first light as having a first brightness and/or a first color, but the operator does not perceive the first brightness/color as being different than a default brightness/color associated with default light emitted by the lighting element, even though the light and the default light have different wavelengths and amplitudes. Hence, the operator may not implement the driving instruction/recommendation as the operator does not perceive a change with respect to emitted light. In another example, a system may cause the lighting element to emit second light of a second wavelength and/or second amplitude to convey a driving instruction that involves rotating a steering apparatus from the first angle to the second angle. The operator perceives the second light as having a second brightness/color that is highly different than the default brightness/color, and hence the operator may \u201cover-implement\u201d the driving instruction/recommendation by rotating the steering apparatus from the first angle to a third angle, where the third angle is greater than the second angle. As such, a system that provides visual feedback without regard to human perception may inaccurately convey a driving instruction/recommendation to an operator of a vehicle.\n\n(13) To address these problems, an improved visual feedback system that utilizes a psychophysical model to provide visual feedback that is optimized for human perception is described herein. In one embodiment, the system obtains a current angle of a steering apparatus of a vehicle from sensor data generated by a steering angle sensor of the steering apparatus. The system also obtains a recommended steering angle of the steering apparatus. In an example, the visual feedback system obtains the recommended steering angle from a navigation system and/or an advanced driver assistance-system (ADAS) of the vehicle. As such, the recommended steering angle may be associated with a maneuver that would cause the vehicle to follow a route to a destination and/or avoid an obstacle in the vicinity of the vehicle. The system determines a difference between the current angle of the steering apparatus and the recommended angle of the steering apparatus.\n\n(14) The system determines at least one appearance parameter (e.g., hue, lightness, brightness, chroma, colorfulness, saturation) based upon the difference and a psychophysical model. The psychophysical model optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. In an example, the psychophysical model is a perceptually uniform color map. According to embodiments, the vehicle includes an ambient light sensor that generates sensor data that is indicative of an ambient light level in a cabin of the vehicle. According to embodiments, the system determines the appearance parameter additionally based upon the sensor data.\n\n(15) The system illuminates a region of the steering apparatus with light based upon the appearance parameter in order to communicate the difference, that is, in order to communicate a driving instruction/recommendation. For instance, the system may activate light emitting diodes (LEDs) comprised by the steering apparatus, where the LEDs emit the light. In an example, the operator perceives the light as having a color and/or brightness that communicates a magnitude and/or sign of the difference. In another example, a number of LEDs activated communicates the magnitude and/or sign of the difference. According to embodiments, the system obtains a position of a hand of the operator based upon sensor data output by a hand position sensor associated with the steering apparatus. According to the embodiments, the system selects the region based upon the position of the hand. According to embodiments, the system selects the region based upon a sign of the difference.\n\n(16) It is contemplated that the system reacts in real-time to changes in a steering angle of the steering apparatus. In an example, the steering apparatus may be angled at a first angle, a second angle, and a third angle, where the second angle is between the first angle and the third angle. In the example, the current angle of the steering apparatus is initially the first angle and the recommended angle of the steering apparatus is the third angle. The system determines a first difference between the first angle (i.e., the current angle) and the third angle (i.e., the recommended angle), determines a first appearance parameter based upon the first difference and the psychophysical model as described above, and illuminates the region with first light based upon the first appearance parameter. When the steering apparatus is rotated from the first angle to the second angle, the system determines a second difference between the second angle (i.e., the current angle) and the third angle (i.e., the recommended angle), determines a second appearance parameter based upon the second difference and the psychophysical model as described above, and illuminates the region with second light based upon the second appearance parameter. In an example, the operator of the vehicle perceives the first light as having a first brightness/color and the second light as having a second brightness/color that is perceivably different than the first brightness/color due to the use of the psychophysical model.\n\n(17) The above-described present various advantages over conventional technologies that provide driving instructions to an operator of a vehicle. First, as the visual feedback system provides visual cues on the steering apparatus that are observable in the peripheral vision of the operator, the visual feedback system is able to communicate maneuver-related information to the operator without requiring the operator to look away from the road. Second, through use of the psychophysical model described above, the visual feedback system helps to ensure that the operator of the vehicle precisely implements driving instructions/recommendations such that the vehicle is not over-steered or under-steered by accounting for human perception of light. Third, by utilizing sensor data that is indicative of an ambient light level of a cabin of the vehicle in conjunction with the psychophysical model, the above-described technologies help to ensure that the operator precisely implements driving instructions/recommendations even when lighting conditions in the vehicle change.\n\n(18) Referring to FIG. 1, an example of a vehicle 100 is illustrated. As used herein, a \u201cvehicle\u201d is any form of motorized transport. In one or more implementations, the vehicle 100 is an automobile. While arrangements will be described herein with respect to automobiles, it will be understood that embodiments are not limited to automobiles. In some implementations, the vehicle 100 may be any robotic device or form of motorized transport that, for example, includes sensors to perceive aspects of the surrounding environment, and thus benefits from the functionality discussed herein associated with improving driving instructions/recommendations provided to an operator of the vehicle 100. As a further note, this disclosure generally discusses the vehicle 100 as traveling on a roadway with surrounding vehicles, which are intended to be construed in a similar manner as the vehicle 100 itself. That is, the surrounding vehicles can include any vehicle that may be encountered on a roadway by the vehicle 100.\n\n(19) The vehicle 100 also includes various elements, such as a steering system 143, a navigation system 147, an advanced-driver assistance system (ADAS) 148, and a visual feedback system 170. It will be understood that in various embodiments it may not be necessary for the vehicle 100 to have all of the elements shown in FIG. 1. The vehicle 100 can have any combination of the various elements shown in FIG. 1. Further, the vehicle 100 can have additional elements to those shown in FIG. 1. In some arrangements, the vehicle 100 may be implemented without one or more of the elements shown in FIG. 1. While the various elements are shown as being located within the vehicle 100 in FIG. 1, it will be understood that one or more of these elements can be located external to the vehicle 100. Further, the elements shown may be physically separated by large distances. For example, as discussed, one or more components of the disclosed system can be implemented within a vehicle while further components of the system are implemented within a cloud-computing environment or other system that is remote from the vehicle 100.\n\n(20) Some of the possible elements of the vehicle 100 are shown in FIG. 1 and will be described along with subsequent figures. However, a description of many of the elements in FIG. 1 will be provided after the discussion of FIGS. 2-10 for purposes of brevity of this description. Additionally, it will be appreciated that for simplicity and clarity of illustration, where appropriate, reference numerals have been repeated among the different figures to indicate corresponding or analogous elements. In addition, the discussion outlines numerous specific details to provide a thorough understanding of the embodiments described herein. Those of skill in the art, however, will understand that the embodiments described herein may be practiced using various combinations of these elements. In either case, the vehicle 100 includes the visual feedback system 170 that is implemented to perform methods and other functions as disclosed herein relating to improving communication of a recommended steering angle by utilizing a psychophysical model to optimize a relationship between the recommended steering angle and human perception of the recommended steering angle. The vehicle 100 also includes the steering apparatus 180. As will be discussed in greater detail subsequently, the visual feedback system 170, in various embodiments, is implemented partially within the vehicle 100, and as a cloud-based service. For example, in one approach, functionality associated with at least one module of the visual feedback system 170 is implemented within the vehicle 100 while further functionality is implemented within a cloud-based computing system.\n\n(21) With reference to FIG. 2, one embodiment of the visual feedback system 170 of FIG. 1 is further illustrated. The visual feedback system 170 is shown as including a processor 110 from the vehicle 100 of FIG. 1. Accordingly, the processor 110 may be a part of the visual feedback system 170, the visual feedback system 170 may include a separate processor from the processor 110 of the vehicle 100, or the visual feedback system 170 may access the processor 110 through a data bus or another communication path. In one embodiment, the visual feedback system 170 includes a memory 210 that stores an angle module 220, an appearance parameter module 225, and a visual feedback module 230. The memory 210 is a random-access memory (RAM), read-only memory (ROM), a hard-disk drive, a flash memory, or other suitable memory for storing the angle module 220, the appearance parameter module 225, and the visual feedback module 230. The angle module 220, the appearance parameter module 225, and the visual feedback module 230 are, for example, computer-readable instructions that when executed by the processor 110 cause the processor 110 to perform the various functions disclosed herein.\n\n(22) The visual feedback system 170 as illustrated in FIG. 2 is generally an abstracted form of the visual feedback system 170 as may be implemented between the vehicle 100 and a cloud-computing environment. According to embodiments, the visual feedback system 170 is embodied at least in part within the cloud-computing environment.\n\n(23) With reference to FIG. 2, the angle module 220 generally includes instructions that function to control the processor 110 to receive data inputs from one or more sensors of the vehicle 100 and/or from one or more of the vehicle systems 140. As will be described in greater detail below, the angle module 220 is generally configured to obtain a current angle of the steering apparatus 180 and a recommended angle of the steering apparatus 180. The angle module 220 is also configured to determine a different between the current angle of the steering apparatus 180 and the recommended angle of the steering apparatus 180.\n\n(24) The appearance parameter module 225 generally includes instructions that function to control the processor 110 to receive data inputs from the angle module 220. As will be described in greater detail below, the appearance parameter module 225 is generally configured to determine an appearance parameter based upon the difference and a psychophysical model 260.\n\n(25) The visual feedback module 230 generally includes instructions that function to control the processor to receive data input from the appearance parameter module 225. As will be described in greater detail below, the visual feedback module 230 is generally configured to cause a region of the steering apparatus 180 to be illuminated with light based upon the appearance parameter received from the appearance parameter module 225.\n\n(26) According to embodiments, the visual feedback system 170 may be incorporated into the navigation system 147 and/or the ADAS 148. The visual feedback system 170 is in communication with a steering system 143 of the vehicle 100. The visual feedback system 170 may also be in communication with the navigation system 147 and/or the ADAS 148.\n\n(27) Moreover, in one embodiment, the visual feedback system 170 includes the database 240. The database 240 is, in one embodiment, an electronic data structure stored in the memory 210 or another data store and that is configured with routines that can be executed by the processor 110 for analyzing stored data, providing stored data, organizing stored data, and so on. Thus, in one embodiment, the database 240 stores data used by the visual feedback module 220, the appearance parameter module 225, and/or the visual feedback module 230. In one embodiment, the database 240 includes the sensor data 250 along with, for example, metadata that characterize various aspects of the sensor data 250. For example, the metadata can include location coordinates (e.g., longitude and latitude), relative map coordinates or tile identifiers, time/date stamps from when the separate sensor data 250 was generated, and so on.\n\n(28) In one embodiment, database 240 includes the psychophysical model 260. The psychophysical model 260 optimizes a relationship between perceived light and a difference between a current angle of the steering apparatus 180 and a recommended angle of the steering apparatus 180 such that a change in the difference produces a proportional change in the perceived light. According to embodiments, the psychophysical model 260 comprises a perceptually uniform color map, where a given numerical change corresponds to a similarly perceived change in color.\n\n(29) Referring now to FIG. 3, a functional block diagram of an example of the steering apparatus 180 is depicted. In general, the steering apparatus 180 is configured to be operated by an operator of the vehicle 100. When the steering apparatus 180 is rotated, the steering system 143 causes a trajectory of the vehicle 100 to be altered based upon a direction and magnitude of the rotation. According to embodiments, the steering apparatus 180 is a steering wheel.\n\n(30) The steering apparatus 180 includes a grip 310 that is gripped by one or more hands of the operator of the vehicle 100. According to embodiments, the grip 310 is a circular or substantially circular ring. The steering apparatus 180 may include spokes (e.g., one spoke, two spokes, three spokes, etc.) that connect different portions of the grip 310 to one another. Various controls may be located on the spokes.\n\n(31) The steering apparatus 180 further includes a steering angle sensor 320 that is configured to generate sensor data that is indicative of a current angle of the steering apparatus 180. In an example, when the vehicle 100 is travelling straight, the steering angle sensor 320 generates sensor data indicating that the current angle of the steering apparatus 180 is 0\u00b0.\n\n(32) According to embodiments, the steering apparatus 180 includes one or more lighting elements 330 (referred to herein as \u201cthe lighting elements 330\u201d for ease of reading) that are configured to emit light in one or more colors and/or at one or more intensity levels. In an example, the lighting elements 330 are configured to emit light in accordance with one or more appearance parameters such that the operator perceives the light as having a desired hue, lightness, brightness, chroma, colorfulness, and/or saturation. The lighting elements 330 may also be configured to emit light according to different patterns. According to some embodiments, the lighting elements 330 are disposed within the grip 310. According to embodiments, each of the lighting elements 330 represents an angle. According to some embodiments, the lighting elements 330 include thirty-six lighting elements that are arranged radially throughout the grip 310, where each lighting element represents a 10\u00b0 angle. According to some embodiments, the lighting elements 330 are light emitting diodes (LEDs). According to embodiments, some of the lighting elements 330 may be disposed within the spokes of the steering apparatus 180.\n\n(33) According to embodiments, the one or more lighting elements 330 are a single circular lighting element that is disposed within the grip 310. Different regions of the single circular lighting element may be selectively activated/deactivated.\n\n(34) Although the lighting elements 330 are depicted in FIG. 3 as being a part of the steering apparatus 180, other possibilities are contemplated. According to embodiments, the lighting elements 330 are mounted in an interior of the vehicle 100 (other than on the steering apparatus 180). According to the embodiments, the lighting elements 330 emit light onto a region of the grip 310 of the steering apparatus 180. The light reflects off of the region of the grip 310 and is then perceived by an operator of the vehicle 100.\n\n(35) The steering apparatus 180 may include one or more hand position sensors 340 (referred to herein as the \u201chand position sensors 340\u201d for ease of reading). The hand position sensors 340 are generally configured to generate sensor data that is indicative of a position of one or more hands of the operator of the vehicle 100 on the grip 310. The hand position sensors 340 may also be configured to generate sensor data that indicative of a size of the one or more hands of the operator of the vehicle 100 on the grip 310. The hand positions sensors 340 may further be configured to generate sensor data that is indicative of a number of hands (e.g., one or two) of the operator on the grip 310. The hand position sensors 340 may be disposed within the grip 310. According to embodiments, the hand position sensors 340 are capacitive touch sensors, force-based sensors, or optical sensors.\n\n(36) Although the hand position sensors 340 are depicted in FIG. 3 as being a part of the steering apparatus 180, other possibilities are contemplated. According to embodiments, the hand position sensors 340 include one or more cameras that are disposed in an interior of the vehicle 100. The one or more cameras are configured to capture an image of the one or more hands of the operator of the vehicle 100 on the grip 310. The visual feedback system 170 may determine a number, a size, and/or a position of the one or more hands of the operator on the grip 310 based upon the image.\n\n(37) Example operation of the visual feedback system 170 is now set forth. It is contemplated that the vehicle 100 is being operated by an operator (also referred to as a driver). The angle module 220 obtains a current angle of the steering apparatus 180 based upon sensor data generated by the steering angle sensor 320. The angle module 220 also obtains a recommended angle of the steering apparatus 180. The angle module 220 determines a difference between the current angle of the steering apparatus 180 and the recommended angle of the steering apparatus 180.\n\n(38) According to some embodiments, the angle module 220 obtains the recommended angle of the steering apparatus 180 from the navigation system 147. In an example, the recommended angle is part of a maneuver that enables the vehicle 100 to follow a route to a destination. According to some embodiments, the angle module 220 obtains the recommended angle of the steering apparatus 180 from the ADAS 148. In an example, the recommended angle is part of a maneuver that enables the vehicle 100 to avoid an obstacle on a road, such as a pedestrian, a cyclist, another vehicle, an animal, etc. In another example, the recommended angle is part of a parking maneuver, such as parallel parking. In yet another example, the recommended angle is part of a lane change maneuver. According to some embodiments, the angle module 220 determines the recommended angle based upon sensor data generated by the sensor systems 120 of the vehicle 100.\n\n(39) The appearance parameter module 225 obtains the difference from the angle module 220. The appearance parameter module 225 determines an appearance parameter based upon the difference and the psychophysical model 260. The psychophysical model 260 optimizes a relationship between perceived light and the difference such that a change in the difference (e.g., a 1\u00b0 change) produces a proportional change in the perceived light from the perspective of the operator of the vehicle 100. The appearance parameter includes one or more of hue, lightness, brightness, chroma, colorfulness, or saturation.\n\n(40) According to embodiments, the appearance parameter module 225 determines a Red Green Blue (RGB) value based upon the difference, such as a RGB hexadecimal value. According to the embodiments, the appearance parameter module 225 determines the appearance parameter based upon the RGB value and the psychophysical model 260.\n\n(41) The visual feedback module 230 obtains the appearance parameter from the appearance parameter module 225. The visual feedback module 230 illuminates a region of the steering apparatus 180 with light that is based upon the appearance parameter. For instance, the visual feedback module 230 may transmit data to the steering system 143. The steering system 143 transmits a signal based upon the data, where the signal causes the lighting elements 330 to emit the light, where the light is perceived by the operator as having the appearance parameter (e.g., hue, lightness, etc.). In an example in which the lighting elements 330 are LEDs, the signal causes the LEDs to emit the light.\n\n(42) In one example, a perceived color of the light is indicative of a sign and/or a magnitude of the difference. In another example, a perceived brightness of the light is indicative of the sign and/or magnitude of the difference. In yet another example, a perceived number of lighting elements that emit the light are indicative of the sign and/or magnitude of the difference.\n\n(43) In an example, subsequent to the region of the steering apparatus 180 being illuminated with the light, the angle module 220 determines that the steering apparatus 180 has been rotated such that the current angle is now the recommended angle. In the example, the visual feedback module 230 unilluminates the region based upon the current angle now being the recommended angle in order to inform the operator that the operator is now following a driving instruction/recommendation.\n\n(44) According to embodiments, the appearance parameter module 225 obtains sensor data that is indicative of an ambient light level within a cabin of the vehicle 100. According to the embodiments, the visual feedback module 230 illuminates the region of the steering apparatus 180 additionally based upon the sensor data that indicative of the ambient light level within the cabin of the vehicle 100.\n\n(45) It is contemplated that the visual feedback system 170 reacts in real-time to steering angle changes implemented by the operator of the vehicle 100 such that the appearance parameter varies as the steering apparatus 180 is rotated from the current angle to the recommended angle. In an example, the steering apparatus 180 is rotated from the current angle to a second current angle, where the second current angle is located between the current angle and the recommended angle. Using the processes described above, the appearance parameter module 225 obtains a second difference between the second current angle of the steering apparatus 180 and the recommended angle from the angle module 220. Using the processes described above, the appearance parameter module 225 determines a second appearance parameter based upon the second difference and the psychophysical model 260, where the second appearance parameter differs from the appearance parameter. The visual feedback module 230 illuminates the region of the steering apparatus 180 with second light based upon the second appearance parameter.\n\n(46) According to embodiments, the light is associated with a first set of points defined by a first tolerance contour within a color space, such as a Commission Internationale de l'Elcairage (CIE) color space. Each point in the first set of points is associated with first colors that are not perceivably different from one another. The second light is associated with a second set of points defined by a second tolerance contour within the color space. Each point in the second set of points is associated with second colors that are not perceivably different from one another. In an example, the first tolerance contour makes contact with the second tolerance contour within the color space such that no perceivably different color exists between the first colors and the second colors. In this manner, the visual feedback system 170 helps to precisely communicate driving instructions/recommendations to the operator.\n\n(47) According to embodiments, the visual feedback module 230 selects the region of the steering apparatus 180 based upon a sign (e.g., a positive sign or a negative sign) of the difference. According to embodiments, the visual feedback module 230 selects the region based upon a position of a hand (or hands) of the operator, where the visual feedback module 230 determines the position of the hand based (or hands) based upon sensor data generated by the hand position sensor 330.\n\n(48) FIGS. 4, 5A-D, 6, 7A-D, 8, and 9A-D detail various embodiments of the visual assistance system 170 and the steering apparatus 180.\n\n(49) With reference now to FIG. 4, a table 400 is illustrated which details various embodiments of the visual assistance system 170 and the steering apparatus 180 in which brightness of one or more regions of the steering apparatus 180 is changed to communicate a difference between a current steering angle and a recommended steering angle. It is to be understood that the psychophysical model 260 as described above is utilized to determine the brightness for each of the various embodiments.\n\n(50) In a first embodiment 402, the visual assistance system 170 utilizes an absolute value of a difference (referred to in FIG. 4 as \u201cdelta\u201d) between a current angle and a recommended angle of the steering apparatus 180, distribution of lighting on the steering apparatus 180 is independent of a sign of the difference, and brightness is changed in one direction. In a second embodiment 404, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is independent of the sign of the difference, and the brightness is changed in two directions. In a third embodiment 406, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and the brightness is changed in one direction. In a fourth embodiment 408, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and the brightness is changed in two directions.\n\n(51) Turning now to FIG. 5A, the steering apparatus 180 according to the first embodiment 402 is illustrated. In a first example at 502A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 turns off the lighting elements 330 of the steering apparatus 180. In the first example at 502B, when the current angle and the recommended angle of the steering apparatus 180 are not the same (i.e., the difference is non-zero) or are not within the threshold range of one another, the visual feedback module 230 causes the lighting elements 330 on the steering apparatus 180 to be illuminated to indicate the difference via a change in brightness. In a second example at 504A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within the threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated at a default brightness. In the second example at 504B, when the current angle and the recommended angle of the steering apparatus 180 are not the same (i.e., the difference is non-zero) or are not within the threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated at a brightness that is different than the default brightness.\n\n(52) Turning now to FIG. 5B, the steering apparatus 180 according to the second embodiment 404 is illustrated. At 506A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated at a default brightness. At 506B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes the lighting elements 330 to be illuminated at a first brightness, where the first brightness is greater than the default brightness. At 506C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 causes the lighting elements 330 to be illuminated at a second brightness, where the second brightness is less than the default brightness. Alternatively, the first brightness may be less than the default brightness and the second brightness may be greater than the first brightness.\n\n(53) Turning now to FIG. 5C, the steering apparatus 180 according to the third embodiment 406 is illustrated. At 508A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 turns off the lighting elements 330 of the steering apparatus 180. At 508B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes first lighting elements in a first region of the steering apparatus 180 to be illuminated at a first brightness. At 508C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 220 causes second lighting elements in a second region of the steering apparatus 180 to be illuminated at the first brightness. The visual feedback module 230 selects the first region and/or the second region based upon a sign of the difference. The first light elements and the second lighting elements are included in the light elements 330.\n\n(54) Turning now to FIG. 5D, the steering apparatus 180 according to the fourth embodiment 408 is illustrated. At 510A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated at a default brightness. At 510B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes first lighting elements in a first region of the steering apparatus 180 to be illuminated at a first brightness and second lighting elements in a second region of the steering apparatus 180 to be illuminated at a second brightness, where the first brightness is less than the default brightness and where the second brightness is greater than the default brightness. At 510C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 causes the first lighting elements in the first region of the steering apparatus 180 to be illuminated at the second brightness and the second lighting elements in the second region of the steering apparatus 180 to be illuminated at the first brightness. The visual feedback module 230 selects the first region and the second region based upon a sign of the difference. The first lighting elements and the second lighting elements are included in the lighting elements 330.\n\n(55) With reference now to FIG. 6, a table 600 is illustrated which details various embodiments of the visual assistance system 170 and the steering apparatus 180 in which color of one or more regions of the steering apparatus 180 is changed to communicate a difference between a current steering angle and a recommended steering angle. It is to be understood that the psychophysical model 260 as described above is utilized to determine the color for each of the various embodiments.\n\n(56) In a first embodiment 602, the visual assistance system 170 utilizes an absolute value of a difference (referred to in FIG. 6 as \u201cdelta\u201d) between a current angle and a recommended angle of the steering apparatus 180, distribution of lighting on the steering apparatus 180 is independent of a sign of the difference, and color is changed in one direction. In a second embodiment 604, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is independent of the sign of the difference, and the color is changed in two directions. In a third embodiment 606, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and color is changed in one direction. In a fourth embodiment 608, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and color is changed in two directions.\n\n(57) Referring now to FIG. 7A, the steering apparatus 180 according to the first embodiment 602 is illustrated. At 702A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated in a default color. At 702B, when the current angle and the recommended angle of the steering apparatus 180 are not the same (i.e., the difference is non-zero) or are not within the threshold range of one another, the visual feedback module 230 causes the lighting elements 330 to be illuminated in a first color, where the first color is different than the default color.\n\n(58) Referring now to FIG. 7B, the steering apparatus 180 according to the second embodiment 604 is illustrated. At 704A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes the lighting elements 330 of the steering apparatus 180 to be illuminated in a default color. At 704B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes the lighting elements 330 to be illuminated in a first color, where the first color is different than the default color. At 704C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 causes the lighting elements 330 to be illuminated in a second color, where the second color is different than the default color and different than the first color.\n\n(59) Referring now to FIG. 7C, the steering apparatus 180 according to the third embodiment 606 is illustrated. At 706A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes the lighting elements 330 of the steering apparatus 180 to be illuminated in a default color. At 706B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes first lighting elements in a first region of the steering apparatus 180 to be illuminated in a color that is different than the default color while second lighting elements in a second region of the steering apparatus 180 remain illuminated in the default color. The first lighting elements and the second lighting elements are included in the lighting elements 330. At 706C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 causes the second lighting elements in the second region to be illuminated in the color while the first lighting elements in the first region remain illuminated in the default color. The visual feedback module 230 selects the first region and the second region based upon the sign of the difference.\n\n(60) Referring now to FIG. 7D, the steering apparatus 180 according to the fourth embodiment 608 is illustrated. At 708A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes first lighting elements in a first region of the steering apparatus 180 and second lighting elements in a second region of the steering apparatus 180 to be illuminated in a default color. At 708B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 causes the first lighting elements in the first region to be illuminated in a first color and the second lighting elements in the second region to be illuminated in a second color, where the first color and the second color are different than the default color and where the first color and second color are different. At 708C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 causes the first lighting elements in the first region to be illuminated in the second color and the second lighting elements in the second region to be illuminated in the first color. In an example, the first color is associated with moving a hand of the operator upwards while the hand remains on the steering apparatus 180 and the second color is associated with moving the hand downwards while the hand remains on the steering apparatus 180. The visual feedback module 230 selects the first region and the second region based upon the sign of the difference. The first lighting elements and the second lighting elements are included in the lighting elements 330.\n\n(61) With reference now to FIG. 8, a table 800 is illustrated which details various embodiments of the visual assistance system 170 and the steering apparatus 180 in which a number of illuminated lighting elements in one or more regions of the steering apparatus 180 is changed to communicate a difference between a current steering angle and a recommended steering angle.\n\n(62) In a first embodiment 802, the visual assistance system 170 utilizes an absolute value of a difference (referred to in FIG. 8 as \u201cdelta\u201d) between a current angle and a recommended angle of the steering apparatus 180, distribution of lighting on the steering apparatus 180 is independent of a sign of the difference, and a number of lighting elements illuminated is changed in one direction. In a second embodiment 804, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is independent of the sign of the difference, and the number of lighting elements illuminated is changed in two directions. In a third embodiment 806, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and the number of lighting elements is changed in one direction. In a fourth embodiment 808, the visual assistance system 170 utilizes the difference (including the sign of the difference) between the current angle and the recommended angle of the steering apparatus 180, the distribution of lighting on the steering apparatus 180 is location-specific, and the number of lighting elements is changed in two directions.\n\n(63) Referring now to FIG. 9A, the steering apparatus 180 according to the first embodiment 802 is illustrated. At 902A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 does not cause any of the lighting elements 330 to be illuminated. Alternatively, at 902B, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0), the visual feedback module 230 causes a default number of the lighting elements 330 to be illuminated. At 902C, when the current angle and the recommended angle of the steering apparatus 180 are not the same (i.e., the difference is non-zero) or are not within the threshold range of one another, the visual feedback module 230 causes a number of the lighting elements 330 to be illuminated. In scenarios in which none of the lighting elements 330 are illuminated by default, such as in 902A, the number of lighting elements 330 is non-zero. In scenarios in which the default number of lighting elements 330 are illuminated, such as in 902B, the number of lighting elements 330 is greater than zero and different than the default number of the lighting elements 330.\n\n(64) Referring now to FIG. 9B, the steering apparatus 180 according to the second embodiment 804 is illustrated. At 904A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 causes a default number of the lighting elements 330 to be illuminated. At 904B, when the difference has a first sign (e.g., a negative sign), the visual feedback module 230 selects and causes a first number of the lighting elements 330 to be illuminated. In the example in 904B, the first number of the lighting elements 330 is less than the default number of the lighting elements 330 and illuminating the first number of the lighting elements 330 includes unilluminating a portion of the default number of the lighting elements 330. At 904C, when the difference has a second sign (e.g., a positive sign), the visual feedback module 230 selects and causes a second number of the lighting elements 330 to be illuminated. In the example in 904C, the second number of the lighting elements 330 is greater than the default number of the lighting elements 330 and illuminating the second number of the lighting elements 330 comprises illuminating a group of the lighting elements 330 in addition to the (already illuminated) default number of the lighting elements 330.\n\n(65) Referring now to FIG. 9C, the steering apparatus 180 according to the third embodiment 806 is illustrated. At 906A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 does not cause any of the lighting elements 330 to be illuminated. At 906B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 selects a first number of the lighting elements 330 in a first region of the steering apparatus 180 and causes the first number of the lighting elements 330 to be illuminated. The visual feedback module 230 selects the first region based upon the first sign and/or a position of a hand or hands of the operator on the steering apparatus 180. At 906C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 selects a second number of the lighting elements 330 in a second region of the steering apparatus 180 and causes the second number of the lighting elements 330 to be illuminated. The visual feedback module 230 selects the second region based upon the second sign and/or the position of the hand or hands of the operator on the steering apparatus 180.\n\n(66) Referring now to FIG. 9D, the steering apparatus 180 according to the fourth embodiment 808 is illustrated. At 908A, when the current angle and the recommended angle of the steering apparatus 180 are the same (i.e., the difference is 0\u00b0) or within a threshold range of one another, the visual feedback module 230 selects a first default number of the lighting elements 330 in a first region of the steering apparatus 180 and a second default number of the lighting elements 330 in a second region of the steering apparatus 180. The visual feedback module 230 causes the first default number of the lighting elements 330 and the second default number of the lighting elements 330 to be illuminated. At 908B, when the difference has a first sign (e.g., a positive sign), the visual feedback module 230 selects a first number of the lighting elements 330 in the first region for unillumination and a second number of the lighting elements 330 in the second region for illumination. In the example shown in 908B, the visual feedback module 230 increases a number of the lighting elements 330 illuminated in the second region from the second default number of the lighting elements 330 and decreases a number of the lighting elements 330 illuminated in the first region from the first default number of the lighting elements 330. At 908C, when the difference has a second sign (e.g., a negative sign), the visual feedback module 230 selects a first number of the lighting elements 330 in the first region for illumination and a second number of the lighting elements 330 in the second region for unillumination. In the example shown in 908C, the visual feedback module 230 increases a number of the lighting elements 330 illuminated in the first region from the first default number of the lighting elements 330 and decreases a number of the lighting elements 330 illuminated in the second region from the second default number of the lighting elements 330.\n\n(67) Additional aspects of the visual feedback system 170 will be discussed in relation to FIG. 10. FIG. 10 illustrates a flowchart of a method 1000 that is associated with recommending a steering angle using a psychophysical model. The method 1000 will be discussed from the perspective of the visual feedback system 170 of FIGS. 1, and 2. While method 1000 is discussed in combination with the visual feedback system 170, it should be appreciated that the method 1000 is not limited to being implemented within the visual feedback system 170 but is instead one example of a system that may implement the method 1000.\n\n(68) At 1010, the visual feedback system 170 obtains a difference between a current angle of the steering apparatus 180 and a recommended angle of the steering apparatus 180. In an example, the visual feedback system 170 obtains the recommended angle from the navigation system 147 or the ADAS 148 of the vehicle 100. According to some embodiments, the visual feedback system 170 determines a position of a hand of an operator of the vehicle 100 on the steering apparatus 180 based upon sensor data generated by the hand position sensor 330.\n\n(69) At 1020, the visual feedback system 170 determines an appearance parameter based upon the difference and the psychophysical model 260. The psychophysical model 260 optimizes a relationship between perceived light and the difference such that a change in the difference produces a proportional change in the perceived light. In an example, the appearance parameter includes at least one of hue, lightnesss, brightness, chroma, colorfulness, or saturation.\n\n(70) At 1030, the visual feedback system 170 illuminates a region of the steering apparatus 180 with light based upon the appearance parameter. According to embodiments, the visual feedback system 170 selects the region based upon the position of the hand of the operator on the steering apparatus 180 and/or a sign (e.g., a positive sign or a negative sign) of the difference. According to embodiments, as the operator rotates the steering apparatus 180 from the current angle to the recommended angle, the visual feedback system 170 varies the appearance parameter such that the light appears different to the operator to indicate a decrease in the difference. In an example, when the steering apparatus 180 has been rotated such that the current angle equals the recommended angle (or is within a threshold angle of the recommended angle), the visual feedback system 170 unilluminates the region, thereby indicating to the operator that the vehicle 100 is traveling in a recommended direction.\n\n(71) FIG. 1 will now be discussed in full detail as an example environment within which the system and methods disclosed herein may operate. In some instances, the vehicle 100 is configured to switch selectively between an autonomous mode, one or more semi-autonomous operational modes, and/or a manual mode. Such switching can be implemented in a suitable manner, now known or later developed. \u201cManual mode\u201d means that all of or a majority of the navigation and/or maneuvering of the vehicle 100 is performed according to inputs received from a user (e.g., human driver). In one or more arrangements, the vehicle 100 can be a conventional vehicle that is configured to operate in only a manual mode.\n\n(72) In one or more embodiments, the vehicle 100 is an autonomous vehicle. As used herein, \u201cautonomous vehicle\u201d refers to a vehicle that operates in an autonomous mode. \u201cAutonomous mode\u201d refers to navigating and/or maneuvering the vehicle 100 along a travel route using one or more computing systems to control the vehicle 100 with minimal or no input from a human driver. In one or more embodiments, the vehicle 100 is highly automated or completely automated. In one embodiment, the vehicle 100 is configured with one or more semi-autonomous operational modes in which one or more computing systems perform a portion of the navigation and/or maneuvering of the vehicle 100 along a travel route, and a vehicle operator (i.e., driver) provides inputs to the vehicle 100 to perform a portion of the navigation and/or maneuvering of the vehicle 100 along a travel route.\n\n(73) The vehicle 100 can include one or more processors 110. In one or more arrangements, the processor(s) 110 can be a main processor of the vehicle 100. For instance, the processor(s) 110 can be an electronic control unit (ECU). The vehicle 100 can include one or more data stores 115 for storing one or more types of data. The data store 115 can include volatile and/or non-volatile memory. Examples of suitable data stores 115 include RAM (Random Access Memory), flash memory, ROM (Read Only Memory), PROM (Programmable Read-Only Memory), EPROM (Erasable Programmable Read-Only Memory), EEPROM (Electrically Erasable Programmable Read-Only Memory), registers, magnetic disks, optical disks, hard drives, or any other suitable storage medium, or any combination thereof. The data store 115 can be a component of the processor(s) 110, or the data store 115 can be operatively connected to the processor(s) 110 for use thereby. The term \u201coperatively connected,\u201d as used throughout this description, can include direct or indirect connections, including connections without direct physical contact.\n\n(74) In one or more arrangements, the one or more data stores 115 can include map data 116. The map data 116 can include maps of one or more geographic areas. In some instances, the map data 116 can include information or data on roads, traffic control devices, road markings, structures, features, and/or landmarks in the one or more geographic areas. The map data 116 can be in any suitable form. In some instances, the map data 116 can include aerial views of an area. In some instances, the map data 116 can include ground views of an area, including 360-degree ground views. The map data 116 can include measurements, dimensions, distances, and/or information for one or more items included in the map data 116 and/or relative to other items included in the map data 116. The map data 116 can include a digital map with information about road geometry. The map data 116 can be high quality and/or highly detailed.\n\n(75) In one or more arrangements, the map data 116 can include one or more terrain maps 117. The terrain map(s) 117 can include information about the ground, terrain, roads, surfaces, and/or other features of one or more geographic areas. The terrain map(s) 117 can include elevation data in the one or more geographic areas. The map data 116 can be high quality and/or highly detailed. The terrain map(s) 117 can define one or more ground surfaces, which can include paved roads, unpaved roads, land, and other things that define a ground surface.\n\n(76) In one or more arrangements, the map data 116 can include one or more static obstacle maps 118. The static obstacle map(s) 118 can include information about one or more static obstacles located within one or more geographic areas. A \u201cstatic obstacle\u201d is a physical object whose position does not change or substantially change over a period of time and/or whose size does not change or substantially change over a period of time. Examples of static obstacles include trees, buildings, curbs, fences, railings, medians, utility poles, statues, monuments, signs, benches, furniture, mailboxes, large rocks, hills. The static obstacles can be objects that extend above ground level. The one or more static obstacles included in the static obstacle map(s) 118 can have location data, size data, dimension data, material data, and/or other data associated with it. The static obstacle map(s) 118 can include measurements, dimensions, distances, and/or information for one or more static obstacles. The static obstacle map(s) 118 can be high quality and/or highly detailed. The static obstacle map(s) 118 can be updated to reflect changes within a mapped area.\n\n(77) The one or more data stores 115 can include sensor data 119. In this context, \u201csensor data\u201d means any information about the sensors that the vehicle 100 is equipped with, including the capabilities and other information about such sensors. As will be explained below, the vehicle 100 can include the sensor system 120. The sensor data 119 can relate to one or more sensors of the sensor system 120. As an example, in one or more arrangements, the sensor data 119 can include information on one or more LIDAR sensors 124 of the sensor system 120.\n\n(78) In some instances, at least a portion of the map data 116 and/or the sensor data 119 can be located in one or more data stores 115 located onboard the vehicle 100. Alternatively, or in addition, at least a portion of the map data 116 and/or the sensor data 119 can be located in one or more data stores 115 that are located remotely from the vehicle 100.\n\n(79) As noted above, the vehicle 100 can include the sensor system 120. The sensor system 120 can include one or more sensors. \u201cSensor\u201d means any device, component and/or system that can detect, and/or sense something. The one or more sensors can be configured to detect, and/or sense in real-time. As used herein, the term \u201creal-time\u201d means a level of processing responsiveness that a user or system senses as sufficiently immediate for a particular process or determination to be made, or that enables the processor to keep up with some external process.\n\n(80) In arrangements in which the sensor system 120 includes a plurality of sensors, the sensors can work independently from each other. Alternatively, two or more of the sensors can work in combination with each other. In such case, the two or more sensors can form a sensor network. The sensor system 120 and/or the one or more sensors can be operatively connected to the processor(s) 110, the data store(s) 115, and/or another element of the vehicle 100 (including any of the elements shown in FIG. 1). The sensor system 120 can acquire data of at least a portion of the external environment of the vehicle 100 (e.g., nearby vehicles).\n\n(81) The sensor system 120 can include any suitable type of sensor. Various examples of different types of sensors will be described herein. However, it will be understood that the embodiments are not limited to the particular sensors described. The sensor system 120 can include one or more vehicle sensors 121. The vehicle sensor(s) 121 can detect, determine, and/or sense information about the vehicle 100 itself. In one or more arrangements, the vehicle sensor(s) 121 can be configured to detect, and/or sense position and orientation changes of the vehicle 100, such as, for example, based on inertial acceleration. In one or more arrangements, the vehicle sensor(s) 121 can include one or more accelerometers, one or more gyroscopes, an inertial measurement unit (IMU), a dead-reckoning system, a global navigation satellite system (GNSS), a global positioning system (GPS), a navigation system 147, and/or other suitable sensors. The vehicle sensor(s) 121 can be configured to detect, and/or sense one or more characteristics of the vehicle 100. In one or more arrangements, the vehicle sensor(s) 121 can include a speedometer to determine a current speed of the vehicle 100.\n\n(82) Alternatively, or in addition, the sensor system 120 can include one or more environment sensors 122 configured to acquire, and/or sense driving environment data. \u201cDriving environment data\u201d includes data or information about the external environment in which an autonomous vehicle is located or one or more portions thereof. For example, the one or more environment sensors 122 can be configured to detect, quantify and/or sense obstacles in at least a portion of the external environment of the vehicle 100 and/or information/data about such obstacles. Such obstacles may be stationary objects and/or dynamic objects. The one or more environment sensors 122 can be configured to detect, measure, quantify and/or sense other things in the external environment of the vehicle 100, such as, for example, lane markers, signs, traffic lights, traffic signs, lane lines, crosswalks, curbs proximate the vehicle 100, off-road objects, etc.\n\n(83) Various examples of sensors of the sensor system 120 will be described herein. The example sensors may be part of the one or more environment sensors 122 and/or the one or more vehicle sensors 121. However, it will be understood that the embodiments are not limited to the particular sensors described.\n\n(84) As an example, in one or more arrangements, the sensor system 120 can include one or more radar sensors 123, one or more LIDAR sensors 124, one or more sonar sensors 125, and/or one or more cameras 126. In one or more arrangements, the one or more cameras 126 can be high dynamic range (HDR) cameras or infrared (IR) cameras.\n\n(85) The vehicle 100 can include an input system 130. An \u201cinput system\u201d includes any device, component, system, element or arrangement or groups thereof that enable information/data to be entered into a machine. The input system 130 can receive an input from a vehicle passenger (e.g., a driver or a passenger). The vehicle 100 can include an output system 135. An \u201coutput system\u201d includes any device, component, or arrangement or groups thereof that enable information/data to be presented to a vehicle passenger (e.g., a person, a vehicle passenger, etc.).\n\n(86) The vehicle 100 can include one or more vehicle systems 140. Various examples of the one or more vehicle systems 140 are shown in FIG. 1. However, the vehicle 100 can include more, fewer, or different vehicle systems. It should be appreciated that although particular vehicle systems are separately defined, each or any of the systems or portions thereof may be otherwise combined or segregated via hardware and/or software within the vehicle 100. The vehicle 100 can include a propulsion system 141, a braking system 142, a steering system 143, throttle system 144, a transmission system 145, a signaling system 146, and/or a navigation system 147. Each of these systems can include one or more devices, components, and/or a combination thereof, now known or later developed.\n\n(87) The navigation system 147 can include one or more devices, applications, and/or combinations thereof, now known or later developed, configured to determine the geographic location of the vehicle 100 and/or to determine a travel route for the vehicle 100. The navigation system 147 can include one or more mapping applications to determine a travel route for the vehicle 100. The navigation system 147 can include a global positioning system, a local positioning system or a geolocation system.\n\n(88) According to embodiments, the vehicle systems 140 include an advanced driver-assistance system (ADAS) 148. The ADAS 148 includes one or more of adaptive cruise control (ACC), an anti-lock braking system (ABS), an automotive night vision system, a backup camera, a blind spot monitor, a collision avoidance system, a crosswind stabilization system, cruise control, an electronic stability control (ESC) system, a forward collision warning (FCW) system, an intersection assistant system, an intelligent speed adaptation system, a lane centering system, a lane departure warning system (LDW), a lane change assistance system, parking sensors, a pedestrian protection system, rain sensors, tire pressure monitoring, a traction control system, a traffic sign recognition system, and/or a wrong-way driving warning.\n\n(89) The processor(s) 110, the visual feedback system 170, and/or the autonomous driving module(s) 160 can be operatively connected to communicate with the various vehicle systems 140 and/or individual components thereof. For example, returning to FIG. 1, the processor(s) 110 and/or the autonomous driving module(s) 160 can be in communication to send and/or receive information from the various vehicle systems 140 to control the movement, speed, maneuvering, heading, direction, etc. of the vehicle 100. The processor(s) 110, the visual feedback system 170, and/or the autonomous driving module(s) 160 may control some or all of these vehicle systems 140 and, thus, may be partially or fully autonomous.\n\n(90) The processor(s) 110, the visual feedback system 170, and/or the autonomous driving module(s) 160 can be operatively connected to communicate with the various vehicle systems 140 and/or individual components thereof. For example, returning to FIG. 1, the processor(s) 110 and/or the autonomous driving module(s) 160 can be in communication to send and/or receive information from the various vehicle systems 140 to control the movement, speed, maneuvering, heading, direction, etc. of the vehicle 100. The processor(s) 110 and/or the autonomous driving module(s) 160 may control some or all of these vehicle systems 140.\n\n(91) The processor(s) 110 and/or the autonomous driving module(s) 160 may be operable to control the navigation and/or maneuvering of the vehicle 100 by controlling one or more of the vehicle systems 140 and/or components thereof. For instance, when operating in an autonomous mode, the processor(s) 110 and/or the autonomous driving module(s) 160 can control the direction and/or speed of the vehicle 100. The processor(s) 110 and/or the autonomous driving module(s) 160 can cause the vehicle 100 to accelerate (e.g., by increasing the supply of fuel provided to the engine), decelerate (e.g., by decreasing the supply of fuel to the engine and/or by applying brakes) and/or change direction (e.g., by turning the front two wheels). As used herein, \u201ccause\u201d or \u201ccausing\u201d means to make, force, compel, direct, command, instruct, and/or enable an event or action to occur or at least be in a state where such event or action may occur, either in a direct or indirect manner.\n\n(92) The vehicle 100 can include one or more actuators 150. The actuators 150 can be any element or combination of elements operable to modify, adjust and/or alter one or more of the vehicle systems 140 or components thereof to responsive to receiving signals or other inputs from the processor(s) 110 and/or the autonomous driving module(s) 160. Any suitable actuator can be used. For instance, the one or more actuators 150 can include motors, pneumatic actuators, hydraulic pistons, relays, solenoids, and/or piezoelectric actuators, just to name a few possibilities.\n\n(93) The vehicle 100 can include one or more modules, at least some of which are described herein. The modules can be implemented as computer-readable program code that, when executed by a processor 110, implement one or more of the various processes described herein. One or more of the modules can be a component of the processor(s) 110, or one or more of the modules can be executed on and/or distributed among other processing systems to which the processor(s) 110 is operatively connected. The modules can include instructions (e.g., program logic) executable by one or more processor(s) 110. Alternatively, or in addition, one or more data store 115 may contain such instructions.\n\n(94) In one or more arrangements, one or more of the modules described herein can include artificial or computational intelligence elements, e.g., neural network, fuzzy logic or other machine learning algorithms. Further, in one or more arrangements, one or more of the modules can be distributed among a plurality of the modules described herein. In one or more arrangements, two or more of the modules described herein can be combined into a single module.\n\n(95) The vehicle 100 can include one or more autonomous driving modules 160. The autonomous driving module(s) 160 can be configured to receive data from the sensor system 120 and/or any other type of system capable of capturing information relating to the vehicle 100 and/or the external environment of the vehicle 100. In one or more arrangements, the autonomous driving module(s) 160 can use such data to generate one or more driving scene models. The autonomous driving module(s) 160 can determine position and velocity of the vehicle 100. The autonomous driving module(s) 160 can determine the location of obstacles, obstacles, or other environmental features including traffic signs, trees, shrubs, neighboring vehicles, pedestrians, etc.\n\n(96) The autonomous driving module(s) 160 can be configured to receive, and/or determine location information for obstacles within the external environment of the vehicle 100 for use by the processor(s) 110, and/or one or more of the modules described herein to estimate position and orientation of the vehicle 100, vehicle position in global coordinates based on signals from a plurality of satellites, or any other data and/or signals that could be used to determine the current state of the vehicle 100 or determine the position of the vehicle 100 with respect to its environment for use in either creating a map or determining the position of the vehicle 100 in respect to map data.\n\n(97) The autonomous driving module(s) 160 can be configured to determine travel path(s), current autonomous driving maneuvers for the vehicle 100, future autonomous driving maneuvers and/or modifications to current autonomous driving maneuvers based on data acquired by the sensor system 120, driving scene models, and/or data from any other suitable source such as determinations from the sensor data 250. \u201cDriving maneuver\u201d means one or more actions that affect the movement of a vehicle. Examples of driving maneuvers include: accelerating, decelerating, braking, turning, moving in a lateral direction of the vehicle 100, changing travel lanes, merging into a travel lane, and/or reversing, just to name a few possibilities. The autonomous driving module(s) 160 can be configured can be configured to implement determined driving maneuvers. The autonomous driving module(s) 160 can cause, directly or indirectly, such autonomous driving maneuvers to be implemented. As used herein, \u201ccause\u201d or \u201ccausing\u201d means to make, command, instruct, and/or enable an event or action to occur or at least be in a state where such event or action may occur, either in a direct or indirect manner. The autonomous driving module(s) 160 can be configured to execute various vehicle functions and/or to transmit data to, receive data from, interact with, and/or control the vehicle 100 or one or more systems thereof (e.g., one or more of vehicle systems 140).\n\n(98) Detailed embodiments are disclosed herein. However, it is to be understood that the disclosed embodiments are intended only as examples. Therefore, specific structural and functional details disclosed herein are not to be interpreted as limiting, but merely as a basis for the claims and as a representative basis for teaching one skilled in the art to variously employ the aspects herein in virtually any appropriately detailed structure. Further, the terms and phrases used herein are not intended to be limiting but rather to provide an understandable description of possible implementations. Various embodiments are shown in FIGS. 1-10, but the embodiments are not limited to the illustrated structure or application.\n\n(99) The flowcharts and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments. In this regard, each block in the flowcharts or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved.\n\n(100) The systems, components and/or processes described above can be realized in hardware or a combination of hardware and software and can be realized in a centralized fashion in one processing system or in a distributed fashion where different elements are spread across several interconnected processing systems. Any kind of processing system or another apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software can be a processing system with computer-usable program code that, when being loaded and executed, controls the processing system such that it carries out the methods described herein. The systems, components and/or processes also can be embedded in a computer-readable storage, such as a computer program product or other data programs storage device, readable by a machine, tangibly embodying a program of instructions executable by the machine to perform methods and processes described herein. These elements also can be embedded in an application product which comprises all the features enabling the implementation of the methods described herein and, which when loaded in a processing system, is able to carry out these methods.\n\n(101) Furthermore, arrangements described herein may take the form of a computer program product embodied in one or more computer-readable media having computer-readable program code embodied, e.g., stored, thereon. Any combination of one or more computer-readable media may be utilized. The computer-readable medium may be a computer-readable signal medium or a computer-readable storage medium. The phrase \u201ccomputer-readable storage medium\u201d means a non-transitory storage medium. A computer-readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer-readable storage medium would include the following: a portable computer diskette, a hard disk drive (HDD), a solid-state drive (SSD), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a portable compact disc read-only memory (CD-ROM), a digital versatile disc (DVD), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer-readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.\n\n(102) Generally, modules as used herein include routines, programs, objects, components, data structures, and so on that perform particular tasks or implement particular data types. In further aspects, a memory generally stores the noted modules. The memory associated with a module may be a buffer or cache embedded within a processor, a RAM, a ROM, a flash memory, or another suitable electronic storage medium. In still further aspects, a module as envisioned by the present disclosure is implemented as an application-specific integrated circuit (ASIC), a hardware component of a system on a chip (SoC), as a programmable logic array (PLA), or as another suitable hardware component that is embedded with a defined configuration set (e.g., instructions) for performing the disclosed functions.\n\n(103) Program code embodied on a computer-readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber, cable, RF, etc., or any suitable combination of the foregoing. Computer program code for carrying out operations for aspects of the present arrangements may be written in any combination of one or more programming languages, including an object-oriented programming language such as Java\u2122 Smalltalk, C++ or the like and conventional procedural programming languages, such as the \u201cC\u201d programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n\n(104) The terms \u201ca\u201d and \u201can,\u201d as used herein, are defined as one or more than one. The term \u201cplurality,\u201d as used herein, is defined as two or more than two. The term \u201canother,\u201d as used herein, is defined as at least a second or more. The terms \u201cincluding\u201d and/or \u201chaving,\u201d as used herein, are defined as comprising (i.e., open language). The phrase \u201cat least one of . . . and . . . \u201d as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. As an example, the phrase \u201cat least one of A, B, and C\u201d includes A only, B only, C only, or any combination thereof (e.g., AB, AC, BC or ABC).\n\n(105) Aspects herein can be embodied in other forms without departing from the spirit or essential attributes thereof. Accordingly, reference should be made to the following claims, rather than to the foregoing specification, as indicating the scope hereof."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 29,
      "claims_start": 28,
      "description_end": 28,
      "description_start": 17,
      "drawings_end": 16,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 20,
      "number_of_drawing_sheets": 14,
      "number_of_figures": 19,
      "page_count": 29,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 28,
      "specification_start": 17,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006314444,
    "field_of_search_cpc": [
      "B60Q 3/283",
      "B60Q 3/80",
      "B62D 1/06",
      "B62D 15/029"
    ],
    "foreign_references": [
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "KR",
        "patent_number": "20000033374",
        "pub_month": "2000-06-01"
      }
    ],
    "group_art_unit": "2875",
    "guid": "US-11554716-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G16/547/115",
    "intl_class_current_primary": [
      {
        "intl_class": "B60Q",
        "intl_subclass": "3/283",
        "version": "2017-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "B62D",
        "intl_subclass": "15/02",
        "version": "2006-01-01"
      },
      {
        "intl_class": "B62D",
        "intl_subclass": "1/06",
        "version": "2006-01-01"
      },
      {
        "intl_class": "B60Q",
        "intl_subclass": "3/80",
        "version": "2017-01-01"
      }
    ],
    "intl_class_issued": [
      "B60Q3/283",
      "B62D15/02"
    ],
    "inventors": [
      {
        "city": "San Francisco",
        "country": "US",
        "name": "Yasuda; Hiroshi",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Mountain View",
        "country": "US",
        "name": "Kuehner; Manuel Ludwig",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Redwood City",
        "country": "US",
        "name": "Pita Gil; Guillermo",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Yasuda; Hiroshi et al.",
    "legal_firm_name": [
      "Darrow Mustafa PC"
    ],
    "npl_references": [
      {
        "citation": "Wikipedia, \u201cWeber-Fechner Law\u201d, Accessed on Apr. 4, 2022, Retrieved from https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law#Fechner's_law, 10 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Cadillac, \u201cSuper Cruise\u2014Hands Free Driving\u201d, Accessed on Apr. 4, 2022, Retrieved from https://www.cadillac.com/world-of-cadillac/innovation/super-cruise, 9 pages.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "Systems and methods for recommending a steering angle using visual feedback on a wheel",
    "primary_examiner": "Han; Jason M",
    "publication_date": "2023-01-17",
    "publication_number": "11554716",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "Mei\u00dfner",
        "pub_month": "2008-08-01",
        "publication_number": "7414520"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Beutnagel-Buchner et al.",
        "pub_month": "2010-03-01",
        "publication_number": "7679495"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Hatano et al.",
        "pub_month": "2012-01-01",
        "publication_number": "8103410"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Szczerba",
        "pub_month": "2013-07-01",
        "publication_number": "8482430"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Salter et al.",
        "pub_month": "2016-11-01",
        "publication_number": "9481297"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Spencer et al.",
        "pub_month": "2019-09-01",
        "publication_number": "10414439"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kato",
        "pub_month": "2020-04-01",
        "publication_number": "10632904"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Talamonti et al.",
        "pub_month": "2020-08-01",
        "publication_number": "10754029"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Lisseman et al.",
        "pub_month": "2021-12-01",
        "publication_number": "11208037"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Hatano et al.",
        "pub_month": "2009-04-01",
        "publication_number": "20090093930"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Strumolo et al.",
        "pub_month": "2011-08-01",
        "publication_number": "20110187518"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Sanma et al.",
        "pub_month": "2014-08-01",
        "publication_number": "20140244115"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Walford",
        "pub_month": "2018-11-01",
        "publication_number": "20180336329"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Ferrer et al.",
        "pub_month": "2021-02-01",
        "publication_number": "20210039715"
      }
    ]
  },
  {
    "app_filing_date": "2021-10-28",
    "appl_id": "17452708",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Boston",
        "country": "US",
        "name": "OpenExchange, Inc.",
        "state": "MA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Boston",
        "country": "US",
        "name": "OpenExchange, Inc.",
        "postal_code": "N/A",
        "state": "MA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Hamilton, Brook, Smith & Reynolds, P.C."
    ],
    "composite_id": "1000005960073!US-US-11558444",
    "cpc_inventive": [
      {
        "cpc_class": "H04L",
        "cpc_subclass": "65/70",
        "version": "2022-05-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "65/75",
        "version": "2022-05-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "Embodiments are directed to computer systems and methods that stream selected media content to a client device. Selection of particular content to stream to a device of a given user is performed based on parameters specified in advance by the given user. Selection is made from received content that is parsed, indexed, and stored in real-time in such a way as to allow for real-time monitoring and searching of the content according to the user-specified parameters. The user is alerted as to the discovery of the selected content and enabled to connect to a stream presenting the selected content. The selected content is presented within the stream beginning from a playback time corresponding to a moment that triggered the discovery of the selected content, even if the moment has passed, thus providing the user with a comprehensive presentation of the selected content.",
      "brief": "RELATED APPLICATIONS\n\n(1) This application is related to U.S. application Ser. No. 16/843,661, filed Apr. 8, 2020, and to U.S. application Ser. No. 17/223,634, filed Apr. 6, 2021, both of which are incorporated by reference in their entirety.\n\nBACKGROUND\n\n(2) Streaming video and audio services have recently come into more widespread use in increasingly diverse applications, including virtual event hosting, teleconferencing, and entertainment. A product of this proliferation of streaming applications has been an extensive expansion in video and audio content available for streaming. As users of various streaming services seek to identify and connect with content of interest from a potentially vast and ever-growing array of available content, streaming services have sought to provide interfaces that facilitate such connection.\n\nSUMMARY\n\n(3) Embodiments of the present invention provide an approach for managing video and audio content that is available for streaming, and that may be of interest to a user. The approach enables users to specify content parameters according to their interests, to receive alerts when such content is available, and to connect, via a client device, to a media stream carrying an element of such content in order to examine and consume the content.\n\n(4) Embodiments of the present invention are directed to computer systems, methods, and program products for proactively identifying content of interest to users, and for streaming selected media content to client devices of these users.\n\n(5) The computer system embodiments include a streaming media server. In some embodiments, the streaming media server is one of: Wowza Streaming Engine, Adobe Media Server, or a cloud-hosted SaaS/PaaS provider, including one of: Brightcove Live Streaming Service, Knovio Live, Microsoft Stream, Zencoder Live Transcoding, Encoding.com Live Cloud Encoding, AWS Elemental MediaLive, Wowza Streaming Cloud, or such. In some embodiments, the computer systems (e.g., streaming media server component), methods, and program products receive an input media stream from a media encoder. The media encoder captures and encodes input content from the source device into the input media stream. In some embodiments, the media encoder is implemented as one of a software-based media encoder, a hardware-based media encoder, or a cloud-based media encoder. In example embodiments, the media encoder is one of: Telstra Wirecast, Adobe Live Media Encoder, NewTek TriCaster, Zoom Video Webinar, Pexip Infinity, or such. In some embodiments, the input is at least one of video and audio, and the source devices are at least one of: camera, video player, or microphone. The media encoder encodes the captured input to a standard media format, such as MPEG-4, H.264, and the like. In some embodiments, the media encoder transmits the encoded input as a stream, using a real-time streaming protocol, to the streaming media server. The real-time streaming protocol may be one of: Real-Time Messaging Protocol (RTMP), Real-Time Streaming Protocol (RTSP), Web Real-Time Communications (WebRTC), and such.\n\n(6) In some embodiments, the encoded input content may be generated by a plurality of source devices, which plurality of source devices may be individually or otherwise distributively deployed in a plurality of separate physical locations. In such embodiments, the media encoder may generate a plurality of input media streams, which may individually correspond with respective source devices, and which may be received by the streaming media server simultaneously. In other embodiments, a source device or a plurality thereof may generate various input media streams at different times, to be gathered, e.g., recorded, and managed as a group by the systems (e.g., streaming media server), methods, and program products.\n\n(7) The computer systems (e.g., streaming media server component), methods, and program products perform operations to process the encoded input content of the input media stream. The processed encoded input content may include identifiers for respective individual content elements and time-stamps assigned to the respective individual content elements according to a playback time at which each individual content element manifests within the input media stream. The processed encoded input content, including the individual content elements and respective assigned time-stamps, is stored in a content element file. In some embodiments, the operations include processing operations performed locally, such as by components (e.g., streaming media server component) of the computer systems, modules employed by the methods, and elements executing instructions of the computer program products. It should be appreciated that \u201clocally\u201d may herein refer to distributed, embedded, or other possible processing architectures within the scope of the present disclosure. In some embodiments, the operations may include transmission of the content element file to a third-party processing service, such as a voice-to-text transcription service (e.g., Amazon Web Services (AWS) Transcribe or Google Cloud Speech-To-Text. In embodiments wherein the operations include transmission of the encoded input content to a third-party service as described hereinabove, the operations further include transmission of the content element file from the third-party service to the components (e.g., streaming media server), the modules employed by the methods, or the elements executing instructions of the computer program products, as the case may be. In any of the aforementioned embodiments, the content element file includes the individual content elements, and respective assigned time-stamps, to allow the computer systems (e.g., streaming media server), methods, and program products to search and/or monitor the content element file as described hereinbelow.\n\n(8) The computer systems (e.g., streaming media server component), methods, and program products continue by receiving an alert request from a client device of a user. The alert request specifies a potential content element in which the user may hold interest. The computer systems (e.g., streaming media server component), methods, and program products monitor the content element file for an instance of the potential content element by loading and executing comparison instructions representing a real-time search engine. The real-time search engine compares the potential content element with the stored individual content elements. The real-time search engine facilitates selection of a given content element of the stored individual content elements upon determining that the given content element substantially matches the potential content element according to the matching. The real-time search engine may be configured to make the selection automatically, or may enable the user to make the selection manually.\n\n(9) The computer systems (e.g., streaming media server component), methods, and program products generate and transmit, to the client device, an alert corresponding to the selected given content element. The alert includes a prompt enabling the client device to connect to the input media stream via the streaming media server. In some embodiments, the alert is one of: a text message, e-mail, mobile push notification, on-screen notification, or such.\n\n(10) The computer systems (e.g., streaming media server component), methods, and program products transcode the input media stream in a streaming format compatible with content delivery. The transcoded media stream is the output stream. In some embodiments, the media stream is transcoded by the streaming media server using a Hypertext Transfer Protocol (HTTP) protocol that is one of: HTTP Live Streaming (HLS), MPEG-DASH, or such.\n\n(11) The computer system embodiments also include a multimedia player coupled to the streaming media server and executing on the client device. In some embodiments, the multimedia player runs in one of: a web browser, a mobile application, or such on the client device. The computer systems (e.g., multimedia player component), methods, and program products load the output media stream from a location parsed from the content element file. The computer systems (e.g., multimedia player component), methods, and program products cue playback of the output media stream to a playback time based on the time-stamp assigned to the selected given content element. The computer systems (e.g., multimedia player component), methods, and program products start playback of the output media stream, presenting the output media stream to the user.\n\n(12) In some computer system embodiments, the individual content elements include individual spoken words, or groups thereof, received audibly within the encoded input content. In some embodiments, the individual content elements include aspects of individual spoken words, or groups thereof. In example embodiments, such aspects are at least one of topic, tone, sentiment, and volume. In some embodiments, the individual content elements include individual written words, or groups thereof, received visually within the encoded input content. Written words received may be presented within the individual content elements, for example, upon slides, such as lecture slides or presentation slides, included with an input media stream. Such slides may be included, for example, via video capture of an overhead-projected representation thereof, or via direct juxtaposition or overlay of a digital representation of such slides with other content of the input media stream. In some embodiments, the individual content elements include aspects of individual written words, or groups thereof, including topic. In some embodiments, the individual content elements include images received visually within the encoded input content. In some embodiments, the individual content elements include aspects of images, including types. In example embodiments, such types are at least one of photographs, technical data plots, and financial data plots. In other example embodiments, such types may include lecture slides or presentation slides, which may contain written words as described hereinabove.\n\n(13) In some computer system embodiments, the streaming media server includes an artificial intelligence (AI) module. The AI module may be configured to use machine learning to perform at least one of the operations to process the encoded input content and the searching and/or monitoring of the content element file. In some embodiments, the streaming media server includes an optical character recognition (OCR) module. The OCR module may be configured to identify individual content elements received visually. For example, the streaming media server may employ the OCR module to recognize written words presented upon lecture slides or presentation slides included in the input media stream.\n\n(14) In some embodiments, the output media stream is a live stream. The live stream may be buffered or unbuffered. In some embodiments, the output media stream is a stored media segment. In some embodiments, the output media stream is a video stream formatted as one of: MPEG-4, Windows Media, QuickTime, Audio Video Interleave (AVI), and the like. In these embodiments, the video stream may be at least one of transcoded and transmitted over the network using HTTP Live Streaming (HLS) protocol, MPEG-DASH protocol, or another streaming protocol.\n\n(15) In some computer system embodiments, the playback time is an initial playback time. In these embodiments, the computer systems (e.g., streaming media server component), methods, and program products computationally select a new playback time of the output media stream. In these embodiments, the new playback time is different from the initial playback time. The new playback time may be static or dynamic. In these embodiments, the computer systems (e.g., streaming media server component), methods, and program products adjust playback of the output media stream such that the present playback time beginning at the initial playback time approaches the new playback time. In these embodiments, the computer systems (e.g., streaming media server component), methods, and program products monitor the present playback time of the output media stream as adjusted, including by polling the content element file based on the present playback time as adjusted.\n\n(16) In some instances, the user may not wish to immediately connect the client device to a given input media stream. Furthermore, even if the user were to so connect immediately, the moment of interest may already have passed. As such, upon activation of the link in the alert by the user, embodiments may connect the client device to the input media stream of the given presentation track at a playback time that is earlier than the present time. Subsequently, embodiments may respond to user actuation of various playback controls to adjust playback of the output media stream generated by the streaming media server as described hereinbelow. Such actuation of playback controls may enable the user to consume contextual information surrounding a topic of interest presented within the given input media stream.\n\n(17) In some computer system embodiments, adjusting playback of the output media stream includes restarting the output media stream at a frame of the output media stream corresponding to the new playback time, such that the initial playback time is the same as the new playback time. In these embodiments, adjusting playback of the output media stream also includes presenting the media stream at the multimedia player beginning with the restarted frame corresponding to the new playback time. In some embodiments, adjusting playback of the output media stream includes automatically or manually rewinding the output media stream to an earlier playback time in an available timeline for the output media stream, or forwarding the output media stream to a later playback time in the available timeline. In some embodiments, adjusting playback of the output media stream includes controlling a playback rate parameter. In some embodiments, adjusting playback of the output media stream includes updating a displayed ancillary element associated with the output media stream to be displayed according to the monitored present playback time.\n\n(18) Some computer-implemented method embodiments stream selected media content to a client device of a user. In computer-implemented method embodiments, the method performs operations to implement any embodiments or combination of embodiments described herein.\n\n(19) Some computer program product embodiments include a non-transitory computer-readable medium having computer-readable program instructions stored thereon. In some computer program product embodiments, the instructions, when executed by a processor, cause the processor to implement any embodiments or combination of embodiments described herein.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A computer system for streaming selected media content to a client device of a user, the system comprising: a streaming media server configured to: receive an input media stream from a media encoder, the media encoder capturing and encoding input content from a source device into the input media stream; parse the encoded input content of the input media stream into time-stamped individual content elements, or representations thereof, stored within a content element file; receive an alert request from a client device of a user, the alert request specifying a potential content element; monitor the content element file for an instance of the potential content element by loading and executing comparison instructions representing a real-time search engine, the real-time search engine comparing the potential content element with the stored individual content elements, the real-time search engine facilitating selection of a given content element of the stored individual content elements upon determining that the given content element substantially matches the potential content element according to the comparing; generate and transmit to the client device an alert corresponding to the selected given content element, the alert including a prompt enabling the client device to connect to the input media stream via the streaming media server; transcode the input media stream in a streaming format compatible with content delivery, the transcoded media stream being the output media stream; and  a multimedia player coupled to the streaming media server, the multimedia player executing on the client device and configured to: load the output media stream from a location parsed from the content element file; cue playback of the output media stream to a playback time based on the time-stamp assigned to the selected given content element of the stored individual content elements; and start playback of the output media stream, thereby streaming the selected given content element to the client device.   \n\n2. The system of claim 1 wherein the individual content elements include at least one of: individual spoken words, or groups thereof, received audibly within the encoded input content; aspects of individual spoken words, or groups thereof, including at least one of topic, tone, sentiment, and volume; individual written words, or groups thereof, received visually within the encoded input content; aspects of individual written words, or groups thereof, including topic; images received visually within the encoded input content; and aspects of images including types, the types including at least one of lecture slides, photographs, technical data plots, and financial data plots.  \n\n3. The system of claim 1 wherein the streaming media server includes an artificial intelligence module configured to use machine learning to perform at least one of the parsing and the monitoring. \n\n4. The system of claim 1 wherein the streaming media server includes an optical character recognition module configured to identify the individual content elements. \n\n5. The system of claim 1 wherein the output media stream is a live stream, and wherein the live stream is buffered or unbuffered. \n\n6. The system of claim 1 wherein the output media stream is a stored media segment. \n\n7. The system of claim 1 wherein the playback time is an initial playback time and the multimedia player is further configured to: computationally select a new playback time of the output media stream, the new playback time being different from the initial playback time, the new playback time being static or dynamic; adjust playback of the output media stream such that a present playback time, beginning at the initial playback time, approaches the new playback time; and monitor the present playback time of the output media stream as adjusted, including by polling the content element file based on the present playback time as adjusted.  \n\n8. The system of claim 7 wherein adjusting playback of the output media stream comprises restarting the output media stream at a frame of the output media stream corresponding to the new playback time, such that the initial playback time is the same as the new playback time, and presenting the output media stream at the multimedia player beginning with the restarted frame corresponding to the new playback time. \n\n9. The system of claim 7 wherein adjusting playback of the output media stream comprises automatically rewinding the output media stream to an earlier playback time in an available timeline for the output media stream. \n\n10. The system of claim 7 wherein adjusting playback of the output media stream comprises controlling a playback rate parameter. \n\n11. The system of claim 7 wherein adjusting playback of the output media stream comprises updating a displayed ancillary element associated with the output media stream to be displayed according to the monitored present playback time. \n\n12. A computer-implemented method of streaming selected media content to a client device of a user, the method comprising: at a streaming media server: receiving an input media stream from a media encoder, the media encoder capturing and encoding input content from a source device into the input media stream; parsing the encoded input content of the input media stream into time-stamped individual content elements, or representations thereof, stored within a content element file; receiving an alert request from a client device of a user, the alert request specifying a potential content element; monitoring the content element file for an instance of the potential content element by loading and executing comparison instructions representing a real-time search engine, the real-time search engine comparing the potential content element with the stored individual content elements, the real-time search engine facilitating selection of a given content element of the stored individual content elements upon determining that the given content element substantially matches the potential content element according to the comparing; generating and transmitting to the client device an alert corresponding to the selected given content element, the alert including a prompt enabling the client device to connect to the input media stream via the streaming media server; transcoding the input media stream in a streaming format compatible with content delivery, the transcoded media stream being the output media stream; and  at a multimedia player coupled with the streaming media server, the multimedia player executing on the client device: loading the output media stream from a location parsed from the content element file; cueing playback of the output media stream to a playback time based on the time-stamp assigned to the selected given content element; and starting playback of the output media stream, thereby streaming the selected media content element to the client device.   \n\n13. The method of claim 12 wherein the individual content elements include at least one of: individual spoken words, or groups thereof, received audibly within the encoded input content; aspects of individual spoken words, or groups thereof, including at least one of topic, tone, sentiment, and volume; individual written words, or groups thereof, received visually within the encoded input content; aspects of individual written words, or groups thereof, including topic; images received visually within the encoded input content; and aspects of images including types, the types including at least one of lecture slides, photographs, technical data plots, and financial data plots.  \n\n14. The method of claim 12 wherein at least one of the parsing and the monitoring are performed by an artificial intelligence module using machine learning. \n\n15. The method of claim 12 wherein the streaming media server identifies the individual content elements by controlling an optical character recognition module associated with the streaming media server. \n\n16. The method of claim 12 wherein the output media stream is a live stream, and wherein the live stream is buffered or unbuffered. \n\n17. The method of claim 12 wherein the output media stream is a stored media segment. \n\n18. The method of claim 12 wherein the playback time is a initial playback time, the method further comprising, at the multimedia player: computationally selecting a new playback time of the output media stream, the new playback time being different from the initial playback time, the new playback time being static or dynamic; adjusting playback of the output media stream such that a present playback time, beginning at the initial playback time, approaches the new playback time; and monitoring the present playback time of the output media stream as adjusted, including by polling the content element file based on the present playback time as adjusted.  \n\n19. The method of claim 18 wherein adjusting playback of the output media stream comprises restarting the output media stream at a frame of the output media stream corresponding to the new playback time, such that the initial playback time is the same as the new playback time, and presenting the output media stream at the multimedia player beginning with the restarted frame corresponding to the new playback time. \n\n20. The method of claim 18 wherein adjusting playback of the output media stream comprises automatically rewinding the output media stream to an earlier playback time in an available timeline for the output media stream. \n\n21. The method of claim 18 wherein adjusting playback of the output media stream comprises controlling a playback rate parameter. \n\n22. The method of claim 18 wherein adjusting playback of the output media stream comprises updating a displayed ancillary element associated with the output media stream to be displayed according to the monitored present playback time.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) The foregoing will be apparent from the following more particular description of example embodiments, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating embodiments.\n\n(2) FIG. 1A is a schematic diagram of an example computer network environment in which embodiments of the present invention are deployed.\n\n(3) FIG. 1B is a block diagram of the computer nodes in the network of FIG. 1A.\n\n(4) FIGS. 2A-2C are block diagrams of a system for streaming selected media content to a client device of a user according to embodiments of the present invention.\n\n(5) FIG. 3A is a block diagram of computer components of a multimedia player in embodiments of the present invention.\n\n(6) FIGS. 3B and 3C are block diagrams of computer components of a streaming media server in embodiments of the present invention.\n\n(7) FIG. 4A is a flowchart of an example method of streaming selected media content to a client device of a user according to embodiments of the present invention.\n\n(8) FIG. 4B is a flowchart of an example method of monitoring a content element file to be used in an embodiment of a method of streaming selected media content to a client device of a user.\n\n(9) FIGS. 5A and 5B are flowcharts of example methods of adjusting playback of an output media stream to be used in an embodiment of a method of streaming selected media content to a client device of a user.\n\nDETAILED DESCRIPTION\n\n(10) A description of example embodiments follows.\n\n(11) The teachings of all patents, published applications and references cited herein are incorporated by reference in their entirety.\n\n(12) Digital Processing Environment\n\n(13) Example implementations of a multimedia system 100 for streaming selected media content to a client device 150 of a user may be implemented in a software, firmware, or hardware environment. FIG. 1A illustrates one such environment. One or more client devices 150 (e.g. a mobile phone) and a cloud 160 (or server computer or cluster thereof) provide processing, storage, and input/output devices executing application programs and the like. Client devices may herein be referred to interchangeably as client computers.\n\n(14) Client devices 150 are linked through communications network 170 to other computing devices, including other client devices/processes 150 and server computer(s) 160. Communications network 170 can be part of a remote access network, a global network (e.g., the Internet), an out-of-band network, a worldwide collection of computers, Local area or Wide area networks, cloud networks, and gateways that currently use respective protocols (TCP/IP, HTTP, Bluetooth, etc.) to communicate with one another. Other electronic device/computer network architectures are suitable.\n\n(15) Server computers 160 may be configured to implement a streaming media server (e.g., 215 of FIG. 2A) for provisioning, formatting, and storing selected media content (such as audio, video, text, and images/pictures) of a presentation, which are processed and played at client devices 150 (such as multimedia player 238 in FIG. 2A). The server computers 160 are communicatively coupled to client devices 150 that implement respective media encoders (e.g., 210 of FIG. 2A) for capturing, encoding, loading, or otherwise providing the selected media content that is transmitted to the server computers 160. In one example embodiment, one or more of the server computers 160 are Java application servers that are scalable such that if there are spikes in traffic, the servers can handle the load increase.\n\n(16) FIG. 1B is a diagram of the internal structure of a computer/computing node (e.g., client processor/device/mobile phone device/tablet 150 or server computers 160) in the processing environment of FIG. 1A, which may be used to facilitate displaying such audio, video, image, or data signal information. Each computer 150, 160 contains a system bus 110, where a bus is a set of actual or virtual hardware lines used for data transfer among the components of a computer or processing system. Bus 110 is essentially a shared conduit that connects different elements of a computer system (e.g., processor, disk storage, memory, input/output ports, etc.) that enables the transfer of data between the elements. Attached to system bus 110 is I/O device interface 82 for connecting various input and output devices (e.g., keyboard, mouse, touch screen interface, displays, printers, speakers, etc.) to the computer 150, 160. Network interface 86 allows the computer to connect to various other devices attached to a network (for example the network illustrated at 170 of FIG. 1A). Memory 114 provides volatile storage for computer software instructions 115 and data 116 used to implement a software implementation of the present invention (e.g. capturing/loading, provisioning, formatting, retrieving, downloading, and/or storing streams of selected media content and streams of user-initiated commands).\n\n(17) Disk storage 95 provides non-volatile storage for computer software instructions 92 (equivalently \u201cOS program\u201d) and data 94 used to implement embodiments of the multimedia system 100 of the present invention. Central processor unit 84 is also attached to system bus 110 and provides for the execution of computer instructions.\n\n(18) In one embodiment, the processor routines 92 and data 94 are a computer program product, including a computer readable medium capable of being stored on a storage device 95, which provides at least a portion of the software instructions for the multimedia system 100. Instances of the player 307, real-time search engine 314, publisher 317, optical character recognition module 322, artificial intelligence module 323 (of FIGS. 3A-3C), and other software embodiments of the multimedia system 100 may be implemented as a computer program product 92, and can be installed by any suitable software installation procedure, as is well known in the art. In another embodiment, at least a portion of the multimedia system 100 instructions may also be downloaded over a cable, communication and/or wireless connection. In other embodiments, the multimedia system 100 software components may be implemented as a computer program propagated signal product embodied on a propagated signal on a propagation medium (e.g., a radio wave, an infrared wave, a laser wave, a sound wave, or an electrical wave propagated over a global network such as the Internet, or other network(s)). Such carrier medium or signals provide at least a portion of the software instructions for the multimedia system 100 routines/program 92.\n\n(19) In alternate embodiments, the propagated signal is an analog carrier wave or digital signal carried on the propagated medium. For example, the propagated signal may be a digitized signal propagated over a global network (e.g., the Internet), a telecommunications network, an out-of-band network, or other network. In one embodiment, the propagated signal is transmitted over the propagation medium over a period of time, such as the instructions for a software application sent in packets over a network over a period of milliseconds, seconds, minutes, or longer. In another embodiment, the computer readable medium of computer program product 92 is a propagation medium that the computer system 150 may receive and read, such as by receiving the propagation medium and identifying a propagated signal embodied in the propagation medium, as described above for computer program propagated signal product.\n\n(20) The multimedia system 100 described herein may be configured using any known programming language, including any high-level, object-oriented programming language. A client computer/device 150 (e.g., multimedia player 238 of FIG. 2A) of the multimedia system 100 may be implemented via a software embodiment and may operate within a browser session. The multimedia system 100 may be developed using HTML, JavaScript, Flash, and such. The HTML code may be configured to embed the system into a web browsing session at a client 150. The Java Script can be configured to perform clickstream and session tracking at the client 150 (e.g., publisher 317 of FIG. 3B) and store the streaming media recordings and editing data in a cache. In another embodiment, the system may be implemented in HTML5 for client devices 150 that do not have Flash installed and use HTTP Live Streaming (HLS) or MPEG-DASH protocol. The system may be implemented to transmit media streams using a real-time streaming protocol, such as: Real-Time Messaging Protocol (RTMP), Real-Time Streaming Protocol (RTSP), Web Real-Time Communications (WebRTC), and the like. Components of the multimedia system 100 may be configured to create and load an XML, JSON, or CSV data file or other structured metadata file (such as a manifest file) with information about where and how components of the multimedia system 100 are stored, hosted, or formatted, such as timing information, size, footnote, attachments, interactive components, style sheets, etc.\n\n(21) In an example mobile implementation, the user interface framework for the components of the multimedia system 100 may be based on XHP, Javelin and WURFL. In another example mobile implementation for OS X and iOS operating systems and their respective APIs, Cocoa and Cocoa Touch may be used to implement the player 102 using Objective-C or any other high-level programming language that adds Smalltalk-style messaging to the C programming language.\n\n(22) System for Streaming\n\n(23) FIG. 2A is a block diagram of a system 200a for streaming selected media content to a client device of a user in an example embodiment of the present invention. The system 200a is an example implementation of the computer network environment 100 of FIG. 1A. In FIG. 2A, the system 200a includes components of a streaming media platform configured to divert attention of a user of the platform toward a media stream potentially of interest to the user. Live presentations, speeches, talks, panels, or other events may be live-streamed to or otherwise captured by a media encoder 210 of a video or audio streaming or recording system. Multiple simultaneous such events may be thus streamed or captured. A user of the streaming media platform may command the platform to issue an alert to the user, via a client device 150, as to coverage of a topic of interest, or as to any other occurrence in at least one of the events. For example, five, ten, twenty, or another number of simultaneous company presentations at an investor conference may be live-streamed to the media encoder 210, and a user may wish to be alerted as to a particular piece of content, e.g., spoken or visual content, being detected in a given media stream, so that the user may then connect the client device 150 to the given media stream for viewing or listening as the case may be. The user may thus connect to the stream live, or may connect to an on-demand stream of a previously recorded representation of the event at a later time or date. In either a live-streaming or on-demand configuration, a user may be enabled to view or listen to the given media stream beginning from the exact moment within the given media stream at which the platform discovered the particular piece of content and subsequently issued the alert, or at an earlier moment, such as, for example, 20 seconds before the appearance of the particular spoken content. The streaming media platform may identify the aforementioned exact moment of discovery of the particular piece of content based on a time-stamp corresponding to the exact moment or to a moment adjacent thereto. Such time-stamps may be assigned to the given media stream upon capturing or parsing same, and may be referenced upon encoding same for streaming so as to cue playback of the given media stream to a playback time based on a corresponding time-stamp.\n\n(24) The system 200a includes a media encoder 210 that captures, loads, or otherwise provides a live media stream (containing media content) representing an input media stream 212 to a streaming media server 215. In some embodiments, the media encoder 210 may be: Telstra Wirecast, Adobe Live Media Encoder, NewTek TriCaster, Zoom Video Webinar, Pexip Infinity, and the like. In some embodiments, the streaming media server 215 may be Wowza Streaming Engine, Adobe Media Server, or a cloud-hosted SaaS/PaaS provider, including one of: Brightcove Live Streaming Service, Zencoder Live Transcoding, Encoding.com Live Cloud Encoding, AWS Elemental MediaLive, Wowza Streaming Cloud, or such. The media content of the input media stream 212 may be audio and/or video, and the like. In example embodiments, the input media stream 212 may contain video content, which is being captured live (in real-time) from a source device such as a camera/recorder (e.g., webcam) configured on the media encoder 210, a camera/recorder communicatively coupled to the media encoder 210, or any other such live capture of video. In other embodiments, the video content of the input media steam 212 may be pre-recorded videos stored on the media encoder 210 or at a storage device communicatively coupled to the media encoder 210, a live video feed from a web link accessible from the media encoder 210, and such.\n\n(25) In some embodiments, the media content of the input media stream 212 may be generated by a plurality of source devices, which plurality of source devices may be individually or otherwise distributively deployed in a plurality of separate physical locations. In such embodiments, the media encoder 210 may generate a plurality of input media streams 212, which may individually correspond with respective source devices, and which may be received by the streaming media server 215 simultaneously. In other embodiments, a source device or a plurality thereof may generate various input media streams at different times, to be gathered, e.g., recorded, and managed as a group by the systems (e.g., streaming media server), methods, and program products. In an example embodiment, a streaming media server 215 is configured to enable a coordinator of an investor conference to transmit media representations of ten simultaneous presentation tracks, occurring in ten different rooms at the conference location, to client devices. However, users of the client devices may wish to view a subset of the simultaneous presentation tracks; or, users may prefer to remain disengaged until alerted as to an instance of a content element of interest. The computer systems, methods, and program products facilitate such media transmission by conference coordinators, and subsequent consumption of associated content by users of client devices, as described hereinbelow.\n\n(26) In example embodiments, the captured video content of the input video stream 212 may be formatted as MPEG-4, Windows Media, Quicktime, Audio Video Interleave (AVI), Flash, or any other video format without limitation. In some example embodiments, the input video stream 212 may also be transcoded (video encoded) or otherwise digitally converted into an output media stream 230 for transfer and use at the streaming media server 215 and multimedia player 238. The input video stream (or other media stream) 212 may be transferred to the streaming media server 215 and multimedia player 238 using Real-Time Messaging Protocol (RTMP) or HTTP Live Streaming (HLS) or other such streaming protocol.\n\n(27) The streaming media server 215 receives the (continuous) input media stream 212 from the media encoder 210 over network 340. The streaming media server 215 is configured to generate and maintain a metadata file or structure, to be referred to herein as a content element file 218, that maintains all references to older media segments while gaining new references, throughout the full duration of the stream (e.g., 2 hours). The content element file 218 may be stored on the streaming media server 215, on a device accessible via the network 340, or may be split or otherwise distributed among a combination thereof. The streaming media server 215 determines a dedicated memory location (e.g., directory or folder) for storing the input media stream 212. The streaming media server 215 provisions the received input media stream 212 for playback on multimedia player 238. The streaming media server 215 transcodes the input media stream 212 into segments or packets of a target time length, to be referred to herein as content elements 217, which are stored at the dedicated memory location. The content element file 218 may be formatted as, for example, a text, XML, or CSV file, with information on content elements 217 of the input media stream 212, including time-stamps indicating a date and time of original creation of each content element at a source device. The information stored in the content element file 218 may further include information on each content element such as where stored, where hosted, an identifier, date, time, size (time length), and the like. The stored content elements and content element file 218 may be structured according to the player configuration of the multimedia players 238, such as in HTML5-capable browser client.\n\n(28) The streaming media server 215 receives an alert request 213 from the user via the client device 150. The alert request may include, for example, Boolean search parameters, or parameters describing a decision to be made based on a fuzzy multiple-criteria decision-making technique. The streaming media server 215 monitors the content element file 218 for an instance of a potential content element based on the alert request 213. The streaming media server 215 may perform such monitoring by searching text associated with the potential content elements for keywords. Such text may be derived from transcription of spoken content of the content elements 217, or from visually recognized text within an image or video of the content elements 217, such as by optical character recognition (OCR). Upon discovery of a given content element 226 that substantially matches the potential content element specified in the alert request 213, the streaming media server 215 retrieves the given content element 226 from the content element file 218. The streaming media server 215 generates and transmits an alert 243 to the client device 150 corresponding to the selected given content element 226. The alert 243 includes a prompt enabling the client device 150 to connect to the input media stream 212 via the streaming media server 215. In response to the prompt, the user may issue a command to connect 247 to the input media stream 212 via the streaming media server 215. The alert 243 may include sufficient context to enable the user to evaluate the alert and decide whether or not to issue the command to connect 247 to the input media stream 212 via the streaming media server 215. Such context may include time and location of the subject event, a name of a participant in the event, and other information describing the event. The streaming media server 215 transcodes the input media stream 212 in a streaming format compatible with content delivery. The transcoded media stream is the output media stream 230.\n\n(29) Some embodiments include a user feedback system to rate aspects of the alerts 243 such as accuracy or relevance. Such a feedback system may enable tuning of future alerts for improved relevance. In some embodiments, a user may set a relevance threshold to determine whether or not a particular alert should be sent to the client device 150. Alerts thus held back may still be retained in memory for later viewing, or may be collected and sent to the client device 150 periodically, or upon reaching a specified level of accumulation, rather than immediately.\n\n(30) Based on the content element file 218, the multimedia player 238 retrieves the (continuous) output media stream 230 from the streaming media server 215, and may cache the output media stream 230 at the multimedia player 238. Permissions may be required for a user to access certain individual content elements 217, or to connect to certain input media streams 212. The multimedia player 238 displays the output media stream 230 at a client device 150 via respective media players (e.g., HTML5 Video Player or the like) configured on the multimedia players 238 via a web browser. The output media stream 230 may be displayed in a first window or panel of a multimedia player 238.\n\n(31) The multimedia player 238 may synchronize ancillary items including commands and on-screen events to an output media stream 230 even if the output media stream 230 is paused, \u201crewound\u201d, \u201cforwarded,\u201d or otherwise adjusted in time, such as watching a digital video recorder (DVR). That is, a user may adjust the output media stream 230 to a new time (a different time than the current playback time of the output media stream 230) or stop and later re-start the output media stream 230. For example, the output media stream 230 may have an available timeline ranging from (a) time of a first frame of the selected given content element 226 to (b) time of a current last frame of the selected given content element 226 stored at the streaming media server 215. The user may select a new time anywhere on the available timeline to rewind, forward, or otherwise adjust and restart playback of the output media stream 230. For another example, the user may interact with a visual element (on-screen event) displayed on the multimedia player 238, which causes the multimedia player 238 to re-cue/adjust the output media stream 230 to a new selected time that is associated with the visual element. The multimedia player 238 may store in memory (e.g., in a cookie) the current playback time prior to adjustment to the new selected time, and later choose an option to re-adjust playback time back to the stored current playback time.\n\n(32) The multimedia player 238 may synchronize the on-screen events (visual elements) displayed on the interfaces of the multimedia player 238 to the adjusted new playback time of the output media stream 230. For example, if the output media stream 230 is paused/restarted after a delay or rewound to a point/moment in time earlier in the output media stream 230, the multimedia player 238 synchronizes the on-screen events (visual elements) to the earlier point/moment of the output media stream 230. A user may also select an on-screen event associated with an earlier point in the output media stream 230, and embodiments re-cue the current playback of the output media stream 230 to the time of the selected visual element and synchronize the other on-screen events to the adjusted playback time of the output media stream 230.\n\n(33) To synchronize to the adjusted (e.g., \u201crewound\u201d or paused/restarted) new time of the output media stream 230, the multimedia player 238 updates the current playback time of the output media stream 230 to the adjusted time, and restarts the output media stream 230 at the current playback time as adjusted. In some embodiments, multimedia player 238 restarts the output media stream 230 at a frame of a stored media segment at the streaming media server 215 corresponding to the new time, and presents the output media stream 230 at the multimedia player 238 beginning with the restarted frame corresponding to the new selected time. The multimedia player 238 then monitors the current playback time of the output media stream 230 as adjusted. As part of the monitoring, the multimedia player 238 polls the content element file 218 based on the current playback time as adjusted to determine corresponding one or more commands and executes the one or more commands to display on-screen events (visual elements) synchronized to the current playback time of the output media stream 230 as adjusted.\n\n(34) FIG. 2B is a block diagram 200b of examples of individual content elements 217 to be stored in the content element file 218 according to embodiments of the present invention. In some embodiments, individual content elements 217 include individual spoken words or groups thereof 240, received audibly within the encoded input content. The individual content elements 217 may include aspects of individual spoken words or groups thereof 245. Such aspects may include topic 246, tone 247, volume 248, and/or sentiment 249. For example, an alert might be set for whenever an argument breaks out with participants exhibiting raised voices or heated language. In some embodiments, individual content elements 217 may include individual written words or groups thereof 250, received visually within the encoded input content. The individual content elements 217 may include aspects of individual words or groups thereof 255. Such aspects may include topic 256. Such written words or groups thereof 250 may be received visually upon lecture slides or presentation slides 251 included in an associated input media stream 212. Such lecture slides or presentation slides 251 may, for example, present as a portion of recorded content of the associated input media stream 212, or may be otherwise digitally embedded within the associated input media stream 212. Such lecture slides or presentation slides 251 may be included, for example, via video capture of an overhead-projected representation thereof, or via direct juxtaposition or overlay of a digital representation of such slides with other content of the input media stream 212. In some embodiments, the individual content elements 217 may include images received visually 260 within the encoded input content. The individual content elements 217 may include aspects of images 265. Such aspects may include types 266. Such types may include photographs 267, technical data plots 268, and/or financial data plots 269. Such types may also include lecture slides or presentation slides 251 as described hereinabove. As such, it should be appreciated that individual content elements 217 may fall into more than one of the categories introduced herein, such as images of lecture slides containing written words.\n\n(35) FIG. 2C is a block diagram 200c of example output media streams 230 in embodiments of the present invention. In some embodiments, the output media stream 230 may be a buffered 232 or unbuffered 234 live stream. In some embodiments, the output media stream 230 may be a stored media segment 236.\n\n(36) System Components for Streaming\n\n(37) FIG. 3A is a block diagram 300a of example computer components of the multimedia player 238 of FIG. 2A in embodiments of the present invention. The multimedia player 238 includes an interface 301a configured to retrieve an output media stream 230 from a streaming media server 215. The multimedia player 238 includes storage 305a, which may retain or cache selected given content elements 226 of the output media stream 230 for later viewing, or to enable various playback controls such as pause and rewind. In some embodiments, such storage 305a may enable the user to control playback speed of the output media stream 230. For example, a user may wish to use a slower playback speed in order to examine a selected given content element 226 more closely, or a user may wish to use a faster playback speed in order to catch up to a live moment of a selected given content element 226 without missing any information imparted by the content element 226. Storage 305a may also retain a calculated playback time of the output media stream 230. The multimedia player 238 includes a player 307 configured to play the output media stream 230 (received via interface 301a) and a search engine 304 configured to locate a particular point in the output media stream 230. In some embodiments, the player 307 is a HTML5 Video Player using video.js. The multimedia player 238 also includes an output module 306a configured to display the output media stream 230. The output module 306a may trigger an on-screen event from a command received via interface 301. Data may travel among the various components shown in the diagram 300a of FIG. 3A via system bus 308.\n\n(38) FIG. 3B is a block diagram 300b of example computer components of the streaming media server 215 of FIG. 2A in embodiments of the present invention. The streaming media server 215 includes an interface 301b configured to retrieve an input media stream 212 from a media encoder 210. The streaming media server 215 includes storage 305b, which may host the content element file 218. The content element file 218 may alternatively be hosted remotely from the streaming media server 215, such as at another server on the network 340.\n\n(39) The streaming media server 215 can be seen in FIG. 3B to interface with a transcription service, as illustrated in the figure with a dotted line. The streaming media server 215 may transmit the encoded input content of the input media stream 212 to the transcription service. The transcription service is configured to assist with parsing of the input media stream 212, as encoded by the media encoder 210, into the individual content elements 217. Such parsing may include creation and indexing of a real-time speech-to-text transcription of the content of the input media stream 212. The transcription service may be a third-party transcription program such as, for example, Amazon Web Services (AWS) Transcribe or Google Cloud Speech-to-Text. The transcription service may thus identify individual content elements 217, assign a current time-stamp to the individual content elements 217, and store the individual content elements 217 and the respective assigned time-stamps in the content element file 218 to be transmitted back to the streaming media server 215. In some embodiments, the streaming media server 215 may interface with a third-party translation service (e.g., AWS Translate or Google Translate) (not shown in FIG. 3B). The streaming media server 215 may cause the content element file 218 to be transmitted to the translation service to produce a translated content element file in a specified language to be transmitted back to the streaming media server 215.\n\n(40) As can be seen in FIG. 3B, the streaming media server 215 further includes a real-time search engine 314. The real-time search engine 314 is configured to monitor the content element file for an instance of a potential content element as specified in a user alert request 213. The streaming media server 215 may thus load and execute comparison instructions representing the real-time search engine 314, in order to compare the potential content element with the stored individual content elements 217, and automatically select a given content element 226 of the stored individual content elements 217 upon determining that the given content element 226 substantially matches the potential content element according to the comparing. In some embodiments, a real-time search engine may be configured to curate a list of search results, allowing a user to choose from multiple media streams upon receipt of an alert 243.\n\n(41) The streaming media server 215 can be seen in FIG. 3B to also include a publisher 317 configured to enable the user to create, update, and load media content, such as audio, video, text, or graphic image content, and trigger a stream of commands on the loaded media content (such as, for loaded media content of a slide presentation, selecting a new slide). The multimedia player 238 also includes an output module 306b configured to display the output media stream 230. The output module 306b may trigger an on-screen event from a command received via interface 301b. \n\n(42) FIG. 3C is a block diagram 300c of example computer components of the streaming media server 215 of FIG. 2A in embodiments of the present invention. The streaming media server 215 of FIG. 3C can be seen to include an interface 301c, storage 305c, and an output module 306c as described with respect to corresponding components 301b, 305b, 306b of FIG. 3B. It can be seen in FIG. 3C that the streaming media server 215 also includes an optical character recognition (OCR) module 322. The OCR module 322 may be configured to identify the individual content elements 217. For example, the OCR module 322 may be configured to recognize written words presented upon lecture slides or presentation slides included within an associated input media stream 212. The streaming media server 215 may include an artificial intelligence module 323, which may use machine learning to support the streaming media server in parsing the encoded content of the input media stream 212, and in monitoring the content element file 218 for an instance of the potential content element.\n\n(43) In an example embodiment, an investor conference takes place covering alternative energy sources for automobiles, and the computer systems (e.g., streaming media server component), methods, or program products are configured to support video streams of presentations given at the conference. Given the present landscape within the field, it would be expected that such presentations would include a relatively high proportion of content relating to electric vehicles, while hydrogen-powered cars may be mentioned only occasionally. In the example embodiment, the computer systems (e.g., streaming media server component), methods, or program products receive an alert request from a client device of a user specifying that any time the word \u201chydrogen\u201d is spoken, or is shown in writing on a presentation slide, across a plurality of video streams respectively representing a plurality of presentation tracks at the investor conference, the computer systems (e.g., streaming media server component), methods, or program products issue an alert to the client device via text message. To continue, a presenter of a given presentation track mentions \u201chydrogen,\u201d or, alternatively, an audience member asks a question mentioning \u201chydrogen.\u201d The computer systems (e.g., streaming media server component), methods, or program products, in parsing the encoded input content of the input media stream, receive, within a content element file, a representation of the word \u201chydrogen\u201d from a transcription service, with a time stamp corresponding to the time at which the word was mentioned in the given presentation track. The time stamp may be, for example, \u201c10:43:02 UCT,\u201d including a combination of an hour, minutes, seconds, and time standard/time zone, which may include a notation of local adjustment such as daylight saving time. The example embodiment continues with the computer systems (e.g., streaming media server component), methods, or program products detecting the representation of \u201chydrogen\u201d in the content element file, and in response issue an alert to the client device via, for example, a text message. The alert may include a link to connect the client device, upon activation of the link by a user, to the input media stream of the given presentation track via an output media stream generated by the streaming media server.\n\n(44) Continuing to describe the example embodiment, the user may not wish to connect the client device to the input media stream of the given presentation track immediately. Furthermore, even if the user were to so connect immediately, the moment of interest may already have passed. As such, upon activation of the link in the alert by the user, for example, five minutes after the alert was received at the client device, the client device may be connected to the input media stream of the given presentation track at a playback time that is earlier than the present time, for example, corresponding to the beginning of the sentence in which \u201chydrogen\u201d was mentioned. Subsequently, the user may actuate various playback controls to adjust playback of the output media stream generated by the streaming media server as described hereinbelow. Such actuation of playback controls may enable the user to consume contextual information surrounding the topic of interest presented within the given presentation track.\n\n(45) Method of Streaming\n\n(46) FIG. 4A is a flowchart of an example method 400a of streaming selected media content to a client device 150 of a user according to embodiments of the present invention. The method 400a starts at step 405 by receiving an input media stream 212. The input media stream 212 may be received 405 from a media encoder 210. The media encoder 210 may capture and encode input content from a source device into the input media stream 212. The method 400a continues at step 410 by parsing input content of the input media stream 212 into individual content elements 217 with respective current playback time-stamps. The parsing step 410 may include transmitting the encoded input content of the input media stream 212 to a third-party processing service such as a transcription service. The transcription service may identify the individual content elements 217, which individual content elements may be words or phrases spoken aloud and captured within the input media stream 215. The transcription service may assign time-stamps to individual content elements 217 corresponding to a playback time at which the respective individual content elements 217 appear in the input media stream 215. The transcription service may store representations of the individual content elements 217 and associated time-stamps in a content element file 218. The transcription service may transmit the content element file back to the streaming media server 215. Alternatively, the streaming media server 215, may, for example via an OCR module 322 and/or artificial intelligence module 323, identify individual content elements 217 from the input media stream and store representations thereof, along with respective time-stamps corresponding to a playback time of the respective individual content elements 217, in a content element file 218 that may remain local to the streaming media server 215.\n\n(47) The method 400a continues by receiving an alert request 413 from a client device 150 of the user. The alert request received 413 may denote a specified potential content element. The method 400a continues by monitoring 420 the parsed input content for the potential content element. If the potential content element is not detected at step 425, the method 400a returns to step 420 and continues to monitor the parsed input content. If the potential content element is detected at step 425, the method 400a transcodes 430 the received input media stream 212 for content delivery as an output media stream 230 upon detection of the potential content elements. At step 425, the method 400a also transmits an alert 443 to the client device 150. The method 400a continues by receiving a user command 447 to connect the client device to the transcoded output media stream 430. The method 400a continues by loading 450 the transcoded output media stream 430 at a multimedia player 238. The method 400a continues by cueing playback 460 of the transcoded output media stream 430 to a playback time based on a playback time-stamp of the detected parsed content 410. The method 400a then starts playback 470 of the transcoded output media stream 430 from a moment in the output media stream 430 corresponding to the playback time-stamp of the detected parsed content 410. The detected parsed content 410 may herein be referred to interchangeably as the detected individual content element 217.\n\n(48) FIG. 4B is a flowchart of an example method 400c of monitoring 420 the content element file 218, to be used in embodiments 400a of a method of streaming selected media content to a client device 150. The method 400c begins at step 422 by loading and executing comparison instructions. The comparison instructions 422 represent a real-time search engine. The method 400c continues at step 425 with the real-time search engine comparing 425 a user indicated potential content element with stored individual content elements 217. The method 400c then selects a given content element 426 of the stored individual content elements 217 upon determining that a given content element 426 substantially matches the potential content element according to the comparison of step 425.\n\n(49) FIG. 5A is a flowchart of an example method 500a of adjusting playback of an output media stream to be used in conjunction with a method 400a of streaming selected media content to a client device 150 of a user. The method 500a begins at step 565 by cueing playback of the transcoded output media stream 430 to an initial playback time based on a time-stamp assigned to a selected given content element 426, such as the detected parsed content 410. The method 500a continues by starting playback 570 of the transcoded output media stream 430. The method 500a computationally selects a new playback time 575a of the transcoded output media stream 430. The new selected playback time 575a is different from the initial playback time. The new selected playback time 575a may be static or dynamic. The method 500a continues by adjusting playback 580 of the transcoded output media stream 430 such that a present playback time, beginning at the initial playback time, approaches the new selected playback time 575a. The method 500a continues at step 590a by monitoring the present playback time of the transcoded output media stream 430 as adjusted. Such monitoring 590a may include polling 595a the content element file 218 based on the present playback time as adjusted.\n\n(50) FIG. 5B is a flowchart of an example method 500b of adjusting playback of an output media stream to be used in conjunction with a method 400a of streaming selected media content to a client device 150 of a user. Such a method 500b may enable a user to consume the selected media content immediately, or after a period of delay following notification of the selected media content via an alert 443. Such a method 500b may also enable a user to consume contextual information preceding an individual content element 217 detected 425 according to an alert request 413 as appropriate. The method 500b begins at step 575b by computationally selecting a new playback time of the transcoded output media stream 430. The new selected playback time 575b is different from the initial playback time. The new selected playback time 575b may be static or dynamic. In some embodiments, the method 500b restarts 581 the transcoded output media stream 430 at a frame corresponding to the new selected playback time 575b. In these embodiments, the method continues at step 582 by presenting the transcoded output media stream 430 beginning with the restarted frame corresponding to the new selected playback time 575b. In some embodiments, the new selected playback time 575b is used by the method 500b to automatically rewind the transcoded output media stream 430 to an earlier playback time in an available timeline. In some embodiments, the method 500b may respond to user actuation of playback controls such as rewinding and/or forwarding to approach the new selected playback time 575b. In still other embodiments, the new selected playback time 575b is used by the method 500b to control a playback rate parameter 586. In still other embodiments, the new selected playback time 575b is used by the method 500b to update a displayed ancillary element 588 associated with the transcoded output media stream 430 to be displayed according to the monitored 590b present playback time. In any of the aforementioned embodiments, the method 500b continues by monitoring 590b the present playback time of the transcoded output media stream 430. The monitoring 590b may include polling 595b the content element file 218 based on the present playback time as adjusted.\n\n(51) While example embodiments have been particularly shown and described, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the embodiments encompassed by the appended claims."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 22,
      "claims_start": 21,
      "description_end": 21,
      "description_start": 13,
      "drawings_end": 12,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 22,
      "number_of_drawing_sheets": 10,
      "number_of_figures": 12,
      "page_count": 22,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 21,
      "specification_start": 13,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000005960073,
    "field_of_search_us": [
      "709/231"
    ],
    "group_art_unit": "2459",
    "guid": "US-11558444-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G44/584/115",
    "intl_class_current_primary": [
      {
        "intl_class": "H04L",
        "intl_subclass": "12/28",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H04L",
        "intl_subclass": "65/70",
        "version": "2022-01-01"
      },
      {
        "intl_class": "H04L",
        "intl_subclass": "65/75",
        "version": "2022-01-01"
      }
    ],
    "intl_class_issued": [
      "H04L12/28",
      "H04L65/70",
      "H04N21/24"
    ],
    "inventors": [
      {
        "city": "Quechee",
        "country": "US",
        "name": "Kolowich; Michael E.",
        "postal_code": "N/A",
        "state": "VT"
      },
      {
        "city": "Arlington",
        "country": "US",
        "name": "Kieft; Alexander J.",
        "postal_code": "N/A",
        "state": "MA"
      }
    ],
    "inventors_short": "Kolowich; Michael E. et al.",
    "legal_firm_name": [
      "Hamilton, Brook, Smith & Reynolds, P.C."
    ],
    "npl_references": [
      {
        "citation": "\u201chttps://www.telestream.net/wirecast/overview.html\u2014Telstra Wirecast Wirecast\u2014Live video streaming production software for Mac &amp; PC 10 pages\u2014retrieved from Internet Aug. 17, 2018.\u201d.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://helpx.adobe.com/media-encoder/user-guide.html\u2014Adobe Live Media Encoder \u201cAdobe Media Server Professional\u201d 4 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www newtek.com/tricaster/\u2014\u201cNewTek TriCaster\u201d, 9 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://zoom.us/webinar\u2014\u201cZoom Video Webinar\u201d, 3 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.pexip.com/pexip-infinity-connect\u2014\u201cPexip Infinity\u201d, 5 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.wowza.com/products/streaming-engine\u2014\u201cWowza Streaming Engine\u201d, 2 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.adobe.com/products/adobe-media-server-professional.html\u2014\u201cAdobe Media Server\u201d, 2 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.brightcove.com/en/live\u2014\u201cBrightcove Live Streaming Service\u201d, 7 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.brightcove.com/en/zencoder\u2014\u201cBrightcove Zencoder Live Transcoding\u201d, 6 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://aws.amazon.com/medialive/\u2014\u201cAWS Elemental MediaLive\u201d, 6 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.wowza.com/products/streaming-cloud\u2014\u201cWowza Streaming Cloud\u201d, 6 pages\u2014retrieved from Internet Aug. 17, 2018.",
        "cited_by_examiner": false
      },
      {
        "citation": "https://www.encoding.com/live/, 6 pages\u2014retrieved from Internet Aug. 20, 2018.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "Automatic discovery and reporting of streaming content of interest and connection of user to same",
    "primary_examiner": "Gilles; Jude Jean",
    "publication_date": "2023-01-17",
    "publication_number": "11558444",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "Khosla",
        "pub_month": "2000-06-01",
        "publication_number": "6080063"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Karmarkar",
        "pub_month": "2003-01-01",
        "publication_number": "6508709"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kuramura",
        "pub_month": "2010-10-01",
        "publication_number": "7823066"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Harvey et al.",
        "pub_month": "2010-11-01",
        "publication_number": "7824268"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Caldwell et al.",
        "pub_month": "2015-05-01",
        "publication_number": "9042708"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kim et al.",
        "pub_month": "2017-10-01",
        "publication_number": "9787113"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2020-02-01",
        "publication_number": "10560502"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2020-05-01",
        "publication_number": "10652293"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2021-05-01",
        "publication_number": "10999338"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2022-02-01",
        "publication_number": "11240279"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Ducheneaut et al.",
        "pub_month": "2006-05-01",
        "publication_number": "20060112325"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Graham",
        "pub_month": "2007-01-01",
        "publication_number": "20070005794"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Paquier",
        "pub_month": "2010-07-01",
        "publication_number": "20100166298"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Paquier",
        "pub_month": "2010-07-01",
        "publication_number": "20100166315"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Paquier",
        "pub_month": "2010-07-01",
        "publication_number": "20100166320"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Caldwell et al.",
        "pub_month": "2010-09-01",
        "publication_number": "20100247082"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bennett",
        "pub_month": "2011-03-01",
        "publication_number": "20110066746"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Atkins et al.",
        "pub_month": "2011-04-01",
        "publication_number": "20110083073"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Spilo",
        "pub_month": "2012-06-01",
        "publication_number": "20120144435"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Klappert et al.",
        "pub_month": "2014-01-01",
        "publication_number": "20140007170"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Lee et al.",
        "pub_month": "2014-01-01",
        "publication_number": "20140010515"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Nault",
        "pub_month": "2014-12-01",
        "publication_number": "20140379868"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Annamraju",
        "pub_month": "2015-06-01",
        "publication_number": "20150172727"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Siders",
        "pub_month": "2016-01-01",
        "publication_number": "20160014477"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Lee et al.",
        "pub_month": "2018-09-01",
        "publication_number": "20180270284"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2019-02-01",
        "publication_number": "20190068664"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Kieft",
        "pub_month": "2019-02-01",
        "publication_number": "20190068665"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2020-09-01",
        "publication_number": "20200304548"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2020-10-01",
        "publication_number": "20200322401"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kieft et al.",
        "pub_month": "2021-07-01",
        "publication_number": "20210227008"
      }
    ]
  },
  {
    "app_filing_date": "2021-07-22",
    "appl_id": "17382616",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Tianjin",
        "country": "CN",
        "name": "Tianjin University",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "TIANJIN UNIVERSITY",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "attorney_name": [
      "Shumaker, Loop & Kendrick, LLP",
      "Miller; James D."
    ],
    "composite_id": "1000005786482!US-US-11554343",
    "cpc_additional": [
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2257/504",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2253/108",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2253/204",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2259/4508",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2253/102",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "2259/40088",
        "version": "2013-01-01"
      }
    ],
    "cpc_inventive": [
      {
        "cpc_class": "A01G",
        "cpc_subclass": "9/246",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "53/0446",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A movable carbon capture system applied to agriculture-harmonious buildings, which includes a carbon capture unit and a high-concentration CO.sub.2 supply unit which are respectively integrated, wherein the carbon capture unit comprises a CO.sub.2 adsorption chamber and an air pump, and the high-concentration CO.sub.2 supply unit comprises a vacuum pump and an air storage cavity; an air inlet of the CO.sub.2 adsorption chamber is connected to the indoor environment, an exhaust port of the CO.sub.2 adsorption chamber is connected to an atmosphere outlet, an air outlet of the CO.sub.2 adsorption chamber is connected with an air inlet of the vacuum pump, an air outlet of the vacuum pump is connected with an air inlet of the air storage cavity, an air outlet of the air storage cavity is connected with a greenhouse air supply port, and the greenhouse air supply port is connected with a greenhouse.",
      "brief": "TECHNICAL FIELD\n\n(1) The present disclosure belongs to the technical field of carbon capture, and specifically relates to a movable carbon capture system combined with agricultural and indoor environment control function.\n\nBACKGROUND ART\n\n(2) As the pace of life is continuously accelerated and the pressure of life is continuously increased, the time for people to work intensively is continuously prolonged. CO.sub.2 is an important factor influencing the quality of indoor air, and if the concentration cannot be effectively controlled, people working in buildings for a long time are likely to feel uncomfortable. The volume concentration of carbon dioxide in fresh air is 0.036-0.0039%, the increase of the concentration of carbon dioxide can cause the increase of blood pressure and dyspnea, and the importance of improving the indoor air quality is self-evident.\n\n(3) On one hand, the technology of directly capturing carbon from the interior of the building can effectively reduce the CO.sub.2 concentration in the building and improve the indoor air quality. The carbon capture process in the adsorption method is simple, and is suitable for distributed carbon capture systems. The enriched CO.sub.2 is mainly used in the aspects of food processing, oil displacement and the like, and is not effectively utilized in other fields.\n\n(4) On the other hand, CO.sub.2 is a raw material for photosynthesis of plants, and plays an important role in promoting growth of the plants, so that it is of significant meaning to improve the CO.sub.2 concentration in the greenhouse. Common methods for increasing the CO.sub.2 concentration in the greenhouse include a chemical method, a combustion method, a biological method, application of a large amount of organic fertilizer and the like. However, the methods often have the problems of high cost, environmental pollution and the like.\n\n(5) Some inventors have tried to make innovations in terms of increasing CO.sub.2 concentration in the greenhouse. For example, the patent with the publication number of CN109601201A provides a system for applying CO.sub.2 to a greenhouse by using a normal-pressure hot-water boiler and a treatment method of the greenhouse, and necessary CO.sub.2 is provided for the growth of crops by utilizing flue gas discharged from a hot-water boiler system through efficient utilization of waste gas; and meanwhile, the flue gas exhausted by the hot-water boiler is treated, and cleaning treatment and utilization of the flue gas are achieved. The patent with the publication number of CN108753843A utilizes the byproduct CO.sub.2 generated in the biogas engineering purification process to solve the problem of CO.sub.2 deficiency in greenhouse production in winter, meanwhile, according to the crop photosynthesis law, staged and precise CO.sub.2 release is achieved, and the agricultural production efficiency is greatly improved; and through CO.sub.2 gas fertilization, the maximum resource utilization of organic wastes is realized, and the carbon pollution of large biogas engineering is reduced. The patent with the publication number of CN207383068U provides a greenhouse CO.sub.2 gas applying device, CO.sub.2 can be automatically produced by utilizing a lime powder box and a diluted hydrochloric acid box, the CO.sub.2 is stored in a storage tank, and the generated CO.sub.2 is introduced into a greenhouse. The patent with the publication number CN108031238A, the patent with the publication number CN109258241A, the patent with the publication number CN206193515U and the like are similar methods for increasing the concentration of CO.sub.2 at home and abroad. However, the innovation point of the above patents lies in that the CO.sub.2 concentration in the greenhouse is increased through industrial means, and that the CO.sub.2 in the closed space is a source of carbon fertilizer is rarely reported.\n\n(6) Other inventors have tried to make innovations in the use of CO.sub.2. For example, the patent with the publication number of CN109534959A provides a method for decomposing sodium phenolate by utilizing CO.sub.2, a water phase containing sodium bicarbonate is generated, the PH value of the water phase is controlled, primary crude phenol is mixed with water for secondary decomposition, and high sodium phenolate decomposition rate and crude phenol content are achieved. The patent with the publication number of CN208667504U provides an electric burning lime kiln for generating electricity by recycling CO.sub.2, CO.sub.2 gas generated in lime production is used for generating electricity, lime is calcined by using electric energy, the electric burning lime kiln is simple and novel in structure and convenient to use and operate, the CO.sub.2 gas in lime production can be collected and recycled, and zero emission of CO.sub.2 is basically achieved. The patent with the publication number CN105819445A provides a method for sealing CO.sub.2 in deep sea, energy is released by utilizing energy storage and phase change of CO.sub.2, CO.sub.2 collection, compression, transportation and injection equipment is driven to carry out CO.sub.2 capture, dry ice preparation, transportation and injection into the seabed, green energy is fully utilized, the cost of the CO.sub.2 sealing process is effectively reduced, and atmospheric pollution and greenhouse effect are reduced. However, the utilization of CO.sub.2 in the above patents is limited, is basically based on industrial application, and is not tightly combined with actual work and life.\n\nSUMMARY\n\n(7) The present disclosure aims to solve the technical problem that reduction of CO.sub.2 concentration in a building and promotion of plant growth are not flexibly and effectively combined in the prior art and provides a movable carbon capture system applied to agriculture-harmonious buildings, waste is turned into wealth, CO.sub.2 with lower concentration is captured from the interior of the building, enriched to higher concentration and stored, and meanwhile, and CO.sub.2 with proper concentration is supplied to the greenhouse, so that waste recycling of CO.sub.2 is realized.\n\n(8) In order to solve the technical problem, the present disclosure is realized through the following technical scheme:\n\n(9) A movable carbon capture system applied to agriculture-harmonious buildings comprises a carbon capture unit and a high-concentration CO.sub.2 supply unit, the carbon capture unit comprises a CO.sub.2 adsorption chamber and an air pump, and the high-concentration CO.sub.2 supply unit comprises a vacuum pump and an air storage cavity; an air inlet of the CO.sub.2 adsorption chamber is connected to the indoor environment through the air pump, and a first electric control valve is arranged on an air conveying pipeline between the air pump and the indoor environment; an exhaust port of the CO.sub.2 adsorption chamber is connected to an atmosphere outlet, and a third electric control valve is arranged between the exhaust port of the CO.sub.2 adsorption chamber and the atmosphere outlet; an air outlet of the CO.sub.2 adsorption chamber is connected with an air inlet of the vacuum pump, and a second electric control valve is arranged on an air conveying pipeline between the air outlet of the CO.sub.2 adsorption chamber and the air inlet of the vacuum pump; an air outlet of the vacuum pump is connected with an air inlet of the air storage cavity, and an air outlet of the air storage cavity is connected with a greenhouse air supply port; a fourth electric control valve is arranged on an air conveying pipeline between the air outlet of the air storage cavity and the greenhouse air supply port; and the greenhouse air supply port is connected with a greenhouse.\n\n(10) Further, an adsorption material filled in the CO.sub.2 adsorption chamber is one of zeolite 13X, activated carbon, amino modified silica gel and an organic metal framework.\n\n(11) Further, the desorption temperature in the CO.sub.2 adsorption chamber ranges from 80\u00b0 C. to 150\u00b0 C.\n\n(12) Further, the greenhouse is provided with a CO.sub.2 concentration detector which is used for controlling the concentration of CO.sub.2 in the greenhouse to be 800-1500 \u03bcmol/mol in summer and 600-1000 \u03bcmol/mol in winter.\n\n(13) Further, the carbon capture unit and the high-concentration CO.sub.2 supply unit are respectively integrated.\n\n(14) The present disclosure has the following beneficial effects:\n\n(15) According to the movable carbon capture system applied to agriculture-harmonious buildings in the present disclosure, the carbon capture unit captures CO.sub.2 from the interior of the building and stores the CO.sub.2, so that the indoor CO.sub.2 concentration can be reduced, the indoor air quality is effectively improved, and the human body comfort level is improved; meanwhile, the high-concentration CO.sub.2 supply unit supplies the stored CO.sub.2 and the captured CO.sub.2 to the greenhouse to serve as a carbon source, it can be guaranteed that the CO.sub.2 in the greenhouse is kept in a stable state with the appropriate concentration, indispensable carbon fertilizer is applied to plants, and therefore the purpose of improving the quality of crops is achieved. Therefore, from the aspect of improving the indoor air quality and meeting the requirement of plants in the greenhouse for CO.sub.2, a carbon capture and carbon supply system is formed by locally taking materials, the concentration of CO.sub.2 in the building is reduced, the plant growth is promoted, and the effect of two purposes is achieved. In addition, the carbon capture unit and the high-concentration CO.sub.2 supply unit in the system are respectively integrated, are flexible to move, are not limited to a certain area, can run at different places, and even can be used in a cross-area mode, so that the cost is saved, and the effect of multiple purposes is achieved.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A movable carbon capture system applied to an agriculture-harmonious building comprising: a carbon capture unit and a high-concentration CO.sub.2 supply unit, wherein the carbon capture unit comprises a CO.sub.2 adsorption chamber and an air pump, and the high-concentration CO.sub.2 supply unit comprises a vacuum pump and an air storage cavity; an air inlet of the CO.sub.2 adsorption chamber connected to an indoor environment through the air pump, and a first electric control valve is arranged on an air conveying pipeline between the air pump and the indoor environment; an exhaust port of the CO.sub.2 adsorption chamber is connected to an atmosphere outlet, and a third electric control valve is arranged between the exhaust port of the CO.sub.2 adsorption chamber and the atmosphere outlet; and an air outlet of the CO.sub.2 adsorption chamber connected with an air inlet of the vacuum pump, and a second electric control valve is arranged on an air conveying pipeline between the air outlet of the CO.sub.2 adsorption chamber and the air inlet of the vacuum pump; an air outlet of the vacuum pump is connected with an air inlet of the air storage cavity, and an air outlet of the air storage cavity is connected with a greenhouse air supply port; a fourth electric control valve is arranged on an air conveying pipeline between the air outlet of the air storage cavity and the greenhouse air supply port; and the greenhouse air supply port is connected with a greenhouse.  \n\n2. The movable carbon capture system according to claim 1, wherein an adsorption material filled in the CO.sub.2 adsorption chamber is one of zeolite 13X, activated carbon, amino modified silica gel, and an organic metal framework. \n\n3. The movable carbon capture system according to claim 1, wherein a desorption temperature in the CO.sub.2 adsorption chamber ranges from 80\u00b0 C. to 150\u00b0 C. \n\n4. The movable carbon capture system according to claim 1, wherein the greenhouse is provided with a CO.sub.2 concentration detector for controlling a concentration of CO.sub.2 in the greenhouse to be 800-1500 \u03bcmol/mol in summer and 600-1000 \u03bcmol/mol in winter. \n\n5. The movable carbon capture system according to claim 1, wherein the carbon capture unit and the high-concentration CO.sub.2 supply unit are respectively integrated.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) The drawing is a structural schematic diagram of a movable carbon capture system applied to agriculture-harmonious buildings in the present disclosure.\n\nREFERENCE SIGNS\n\n(2) 1, indoor environment; 2, air conveying pipeline; 3, first electric control valve; 4, air pump; 5, adsorption chamber; 6, second electric control valve; 7, third electric control valve; 8, vacuum pump; 9, air storage cavity; 10, atmosphere outlet; 11, greenhouse air supply port; 12, fourth electric control valve; and 13, CO.sub.2 concentration detector.\n\nDETAILED DESCRIPTION OF THE EMBODIMENTS\n\n(3) For the purpose that the summary, characteristics and effect of the present disclosure can be further understood, the following embodiments are exemplified and will be described in detail with reference to the attached FIGURES as follows:\n\n(4) As shown in the drawing, the embodiment provides a movable carbon capture system applied to agriculture-harmonious buildings, and the movable carbon capture system mainly comprises a carbon capture unit and a high-concentration CO.sub.2 supply unit. If CO.sub.2 in the building is used as a carbon source to supply carbon fertilizer to a greenhouse, the content of CO.sub.2 in the building can be effectively reduced, and the requirement of plants for CO.sub.2 can also be met to the maximum extent.\n\n(5) The carbon capture unit comprises a CO.sub.2 adsorption chamber 5 communicating with the interior of the indoor environment 1 in the building, an air pump 4 and an atmosphere outlet 10. An adsorption material filled in the CO.sub.2 adsorption chamber 5 can be zeolite 13X, activated carbon, amino modified silica gel, an organic metal framework (MOFs) and the like. The desorption temperature in the CO.sub.2 adsorption chamber 5 ranges from 80\u00b0 C. to 150\u00b0 C., and the temperature can fully utilize medium and low temperature exhaust heat generated in the fields of industry, agriculture, daily life and the like. An air inlet of the CO.sub.2 adsorption chamber 5 is connected with the air pump 4 and a first electric control valve 3 through an air conveying pipeline 2 in sequence, and is finally connected to the indoor environment 1. An exhaust port of the CO.sub.2 adsorption chamber 5 is connected with the atmosphere outlet 10, and a third electric control valve 7 is arranged on an air conveying pipeline 2 between the exhaust port of the CO.sub.2 adsorption chamber 5 and the atmosphere outlet 10.\n\n(6) The high-concentration CO.sub.2 supply unit comprises a vacuum pump 8 and an air storage cavity 9. An air inlet of the vacuum pump 8 is connected with an air outlet of the CO.sub.2 adsorption chamber 5, and a second electric control valve 6 is arranged on an air conveying pipeline 2 between the vacuum pump 8 and the CO.sub.2 adsorption chamber 5. An exhaust port of the vacuum pump 8 is connected with an air inlet of the air storage cavity 9, an exhaust port of the air storage cavity 9 is connected with a greenhouse air supply port 11, and a fourth electric control valve 12 is arranged on an air conveying pipeline 2 between an exhaust port of the air storage cavity 9 and the greenhouse air supply port 11.\n\n(7) The greenhouse air supply port 11 is connected to the greenhouse, the greenhouse is provided with a CO.sub.2 concentration detector 13, and the concentration of CO.sub.2 in the greenhouse can be controlled to be 800-1500 \u03bcmol/mol in summer and 600-1000 pmol/mol in winter.\n\n(8) The carbon capture unit and the high-concentration CO.sub.2 supply unit in the movable carbon capture system are respectively integrated, can randomly move, and are convenient for running at different places.\n\n(9) The working flow of the carbon capture unit in the present disclosure is as follows: after the first electric control valve 3 is opened, air in the indoor environment 1 in the building is sucked in by using the air pump 4, the air is fed into the CO.sub.2 adsorption chamber 5 through the air conveying pipeline 2, CO.sub.2 in the air is adsorbed by the adsorption material in the CO.sub.2 adsorption chamber 5, the second electric control valve 6 is closed, the third electric control valve 7 is opened, and the treated air is discharged into the atmosphere through the atmosphere outlet 10. The air pump 4 provides power for circular flow of air, and the adsorption chamber 5 guarantees that CO.sub.2 gas with high concentration is obtained. After running for a period of time, the first electric control valve 3 and the atmosphere outlet 10 are closed, and the CO.sub.2 adsorption chamber 5 starts to perform a desorption process to release CO.sub.2.\n\n(10) The working flow of the high-concentration CO.sub.2 supply unit in the present disclosure is as follows: the second electric control valve 6, the third electric control valve 7 and the fourth electric control valve 12 are closed, the vacuum pump 8 is used for vacuumizing the air storage cavity 9, the second electric control valve 6 is opened, and high-concentration CO.sub.2 in the CO.sub.2 adsorption chamber 5 flows into the air storage cavity 9 under the action of pressure difference. The CO.sub.2 concentration detector 13 is opened to detect the concentration of CO.sub.2 in the greenhouse, when the concentration of CO.sub.2 in the greenhouse is lower than a standard value, the fourth electric control valve 12 is opened, high-concentration CO.sub.2 flows into the greenhouse, and when the concentration of CO.sub.2 in the greenhouse reaches a certain value, the fourth electric control valve 12 is closed, so that the concentration of CO.sub.2 in the greenhouse is kept at a stable value, and the growth of crops is guaranteed.\n\n(11) In the system, the adsorption efficiency of CO.sub.2 in the atmosphere and the volume of the adsorption chamber depend on the property of the adsorption material in the adsorption chamber, the concentration of CO.sub.2 in the atmosphere, corresponding operation strategy requirements and the like.\n\n(12) The CO.sub.2 collected by the carbon capture unit is used in the plant growth process of the greenhouse, so that the utilization of chemical fertilizer is reduced, the quality of plants is effectively improved, and no secondary pollution is caused to the environment. According to the movable carbon capture system, the concentration of indoor carbon dioxide can be effectively alleviated, and the movable carbon capture system is very suitable for CO.sub.2 capture in densely populated buildings and has certain universal significance.\n\n(13) The embodiments of the present disclosure are described above with reference to the attached FIGURES, but the present disclosure is not limited to the foregoing embodiments. The foregoing embodiments are only illustrative rather than restrictive. Inspired by the present disclosure, those skilled in the art can still derive many specific variations without departing from the essence of the present disclosure and the protection scope of the claims. All these variations shall fall within the protection of the present disclosure."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 5,
      "claims_start": 5,
      "description_end": 5,
      "description_start": 3,
      "drawings_end": 2,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "number_of_claims": 5,
      "number_of_drawing_sheets": 1,
      "number_of_figures": 1,
      "page_count": 5,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 5,
      "specification_start": 3,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000005786482,
    "field_of_search_us": [
      "Non/e"
    ],
    "foreign_references": [
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": true,
        "country_code": "CN",
        "patent_number": "207383068",
        "pub_month": "2018-05-01"
      }
    ],
    "group_art_unit": "1772",
    "guid": "US-11554343-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G43/543/115",
    "intl_class_current_primary": [
      {
        "intl_class": "B01D",
        "intl_subclass": "53/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "B01D",
        "intl_subclass": "53/04",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A01G",
        "intl_subclass": "9/24",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "B01D53/00",
      "B01D53/04"
    ],
    "inventors": [
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Deng; Shuai",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Wu; Kailong",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Liu; Bowen",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Zhao; Li",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Li; Shuangjun",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Chen; Mengchao",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Chen; Lijin",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Tianjin",
        "country": "CN",
        "name": "Zhao; Jie",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "Deng; Shuai et al.",
    "legal_firm_name": [
      "Shumaker, Loop & Kendrick, LLP"
    ],
    "patent_title": "Movable carbon capture system applied to agriculture-harmonious buildings",
    "primary_examiner": "Pregler; Sharon",
    "publication_date": "2023-01-17",
    "publication_number": "11554343",
    "type": "USPAT"
  },
  {
    "app_filing_date": "2021-02-11",
    "appl_id": "17174037",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Menlo Park",
        "country": "US",
        "name": "Meta Platforms Technologies, LLC",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Menlo Park",
        "country": "US",
        "name": "Meta Platforms Technologies, LLC",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Baker Botts L.L.P."
    ],
    "composite_id": "1000006811649!US-US-11556169",
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "9/451",
        "version": "2018-02-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "3/011",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "3/0482",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "In one embodiment, a method includes rendering, for one or more displays of a VR display device, a first output image of a VR environment. The VR environment includes a personal UI associated with a first user. The personal UI has a first form factor and a first pose with respect to the first user, is a virtual object comprising one or more 2D virtual displays, and is operable to execute a plurality of applications. The method includes determining whether to adapt the personal UI based on a detected change in a context of the first user with respect to the VR environment. The method includes rendering a second output image of the VR environment, where the personal UI is adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user.",
      "brief": "TECHNICAL FIELD\n\n(1) This disclosure generally relates to databases and file management within network environments, and in particular relates to customizing user interfaces (UIs) in a virtual reality (VR) setting.\n\nBACKGROUND\n\n(2) Traditionally, a user in a virtual reality (VR) experience may have user interface (UI), like a dashboard or menu, that a user can access while in the VR experience. The UI may appear as a 2D menu or window that is overlaid on a field of view of a user in a VR setting. The UI may have a uniform appearance across different VR settings, thus running the risk of obstructing the user's view in certain VR settings. The UI may be hidden from view and pulled up following a prompt or command by the user. However, the user may lose UI functionality in such situations.\n\nSUMMARY OF PARTICULAR EMBODIMENTS\n\n(3) In particular embodiments, a user of an immersive virtual reality (VR) system (e.g., head-mounted VR goggles) may have their view of the VR environment partially or fully occluded by either the system user interface (UI) or application-specific UIs, and thus risk obstructing their view of the VR environment. The user may hide the UI, but the user would then lose access to the UI functionalities. Thus, one technical challenge may include maintaining access to the UI while the user is within the VR environment and across different VR experiences. One solution presented by the embodiments disclosed herein to address the technical challenge of maintaining access to the UI while the user is in different VR experiences is to adapt the UI to the context of the user with respect to the VR environment, such as by adapting the UI to have a form factor and/or pose (i.e., position and orientation with respect to the user) that best suits the user and the VR environment. The UI may be portable, in that the UI is accessible and maintains its functionalities as the user goes from one VR environment to another or switch from one application to another. And the UI may be personal, private, and customizable to the user's experience in the VR environment. As an example and not by way of limitation, a user may adjust the form factor and the pose of the UI to increase the UI productivity or to decrease the view obstruction of the UI, while still maintaining key UI functionalities. Although this disclosure describes maintaining the UI while the user is in different VR experiences by adapting and personalizing the UI in a particular manner, this disclosure contemplates maintaining the UI while the user is in different VR experiences in any suitable manner.\n\n(4) In particular embodiments, one or more computing systems may render, for one or more displays of a VR display device, a first output image of a VR environment. The VR environment may comprise a personal UI associated with a first user. The personal UI may have a first form factor and a first pose with respect to the first user. The personal UI may be a virtual object comprising one or more 2D virtual displays. The personal UI may be operable to execute a plurality of applications. The one or more computing systems may detect a change in a context of the first user with respect to the VR environment. The one or more computing systems may determine whether to adapt the personal UI based on the detected change in the context of the first user. The one or more computing systems may render, for the one or more displays of the VR display device, a second output image of the VR environment. The personal UI may be adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user.\n\n(5) Certain technical challenges exist for maintaining a personal UI while the user is in different VR environments and contexts. One technical challenge may include maintaining UI functionalities without view obstructing or disrupting the VR experience. The solution presented by the embodiments disclosed herein to address this challenge may be to adapt the personal UI based on the context of the user with respect to the VR environment. Another technical challenge may include detecting when a user's context has changed. The solution presented by the embodiments disclosed herein to address this challenge may be to determine a change in the pose of the user with respect to the VR environment and/or objects within the VR environment, receive an input by the user selecting an application that is associated with a change in the form factor and/or pose of the personal UI, or detect a change from one application to another.\n\n(6) Certain embodiments disclosed herein may provide one or more technical advantages. A technical advantage of the embodiments may include automatically adapting a form factor and a pose of the UI responsive to a detected change in context of the user with respect to the VR environment. Another technical advantage of the embodiments may include automatically adapting the personal UI in the VR environment to mimic the UI of a real-world object to help a user acclimate to using the personal UI. Certain embodiments disclosed herein may provide none, some, or all of the above technical advantages. One or more other technical advantages may be readily apparent to one skilled in the art in view of the figures, descriptions, and claims of the present disclosure.\n\n(7) The embodiments disclosed herein are only examples, and the scope of this disclosure is not limited to them. Particular embodiments may include all, some, or none of the components, elements, features, functions, operations, or steps of the embodiments disclosed herein. Embodiments according to the invention are in particular disclosed in the attached claims directed to a method, a storage medium, a system and a computer program product, wherein any feature mentioned in one claim category, e.g. method, can be claimed in another claim category, e.g. system, as well. The dependencies or references back in the attached claims are chosen for formal reasons only. However any subject matter resulting from a deliberate reference back to any previous claims (in particular multiple dependencies) can be claimed as well, so that any combination of claims and the features thereof are disclosed and can be claimed regardless of the dependencies chosen in the attached claims. The subject-matter which can be claimed comprises not only the combinations of features as set out in the attached claims but also any other combination of features in the claims, wherein each feature mentioned in the claims can be combined with any other feature or combination of other features in the claims. Furthermore, any of the embodiments and features described or depicted herein can be claimed in a separate claim and/or in any combination with any embodiment or feature described or depicted herein or with any of the features of the attached claims.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A method comprising, by one or more computing systems: rendering, for one or more displays of a virtual reality (VR) display device, a first output image of a VR environment, wherein the VR environment comprises a personal user interface (UI) associated with a first user, wherein the personal UI has a first form factor and a first pose with respect to the first user, wherein the personal UI is a virtual object comprising one or more 2D virtual displays, and wherein the personal UI is operable to execute a plurality of applications; detecting a change in a context of the first user within the VR environment and with respect to a second user entering the VR environment; determining whether to adapt the personal UI based on the detected change in the context of the first user within the VR environment; and rendering, for the one or more displays of the VR display device, a second output image of the VR environment, wherein the personal UI is adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user within the VR environment, wherein the second form factor and the second pose of the personal UI is predetermined and personalized based on settings inputted by the first user related to the second user.  \n\n2. The method of claim 1, wherein detecting the change in the context of the first user comprises: determining the first user has changed from a first pose to a second pose in the VR environment.  \n\n3. The method of claim 2, wherein determining the first user has changed from the first pose to the second pose comprises: determining whether a location of the user is within a threshold distance of an object in the VR environment; and determining whether an orientation of the user indicates the user is facing the object in the VR environment.  \n\n4. The method of claim 1, wherein detecting the change in the context of the first user comprises: receiving an input by the user selecting a particular application on the personal UI from the plurality of applications, wherein the particular application is associated with the second form factor and the second pose of the personal UI.  \n\n5. The method of claim 1, wherein detecting the change in the context of the first user comprises: detecting the first user has moved from a first virtual space to a second virtual space.  \n\n6. The method of claim 1, wherein detecting the change in the context of the first user comprises: detecting the first user has switched from using a first application to a second application on the personal UI.  \n\n7. The method of claim 1, wherein determining whether to adapt the personal UI based on the detected change in the context of the first user comprises: identifying a predetermined form factor and pose of the personal UI associated with an activity of the user in the VR environment.  \n\n8. The method of claim 1, wherein the personal UI comprises a plurality of 2D virtual displays, and wherein the plurality of 2D virtual displays are arranged in a tile-based layout comprising a plurality of preset orientations for the plurality of 2D virtual displays. \n\n9. The method of claim 8, wherein a first 2D virtual display of the plurality of 2D virtual displays is operable to execute a first set of applications of the plurality of applications, and wherein a second 2D virtual display of the plurality of 2D virtual displays is operable to execute a second set of applications of the plurality of applications. \n\n10. The method of claim 1, wherein the first form factor and the second form factor are selected from a group consisting of a virtual watch, a virtual smartphone, a virtual tablet, a virtual remote control, a virtual computer monitor, a virtual television, and a virtual movie screen. \n\n11. The method of claim 1, wherein the VR environment comprises a shared virtual space for the first user, the second user, and one or more third users. \n\n12. The method of claim 11, wherein privacy settings of the first user prevent the second and third users from viewing content on the personal UI associated with the first user. \n\n13. The method of claim 11, further comprising: casting content from the personal UI of the first user onto a shared virtual screen in the VR environment, wherein the content is viewable on the shared virtual screen by the first user and the second and third users.  \n\n14. The method of claim 1, wherein the plurality of applications correspond to applications on a real-world computing device associated with the first user. \n\n15. The method of claim 1, further comprising: receiving a user input comprising the settings for predetermining and personalizing the personal UI.  \n\n16. One or more computer-readable non-transitory storage media embodying software that is operable when executed to: render, for one or more displays of a virtual reality (VR) display device, a first output image of a VR environment, wherein the VR environment comprises a personal user interface (UI) associated with a first user, wherein the personal UI has a first form factor and a first pose with respect to the first user, wherein the personal UI is a virtual object comprising one or more 2D virtual displays, and wherein the personal UI is operable to execute a plurality of applications; detect a change in a context of the first user within the VR environment and with respect to a second user entering the VR environment; determine whether to adapt the personal UI based on the detected change in the context of the first user within the VR environment; and render, for the one or more displays of the VR display device, a second output image of the VR environment, wherein the personal UI is adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user within the VR environment, wherein the second form factor and the second pose of the personal UI is predetermined and personalized based on settings inputted by the first user related to the second user.  \n\n17. The media of claim 16, wherein the software is operable when executed to detect the change in the context of the first user to: determine the first user has changed from a first pose to a second pose in the VR environment.  \n\n18. The media of claim 16, wherein the software is operable when executed to determine the first user has changed from the first pose to the second pose to: determine whether a location of the user is within a threshold distance of an object in the VR environment; and determine whether an orientation of the user indicates the user is facing the object in the VR environment.  \n\n19. The media of claim 16, wherein the software is operable when executed to detect the change in the context of the first user to: receive an input by the user selecting a particular application on the personal UI from the plurality of applications, wherein the particular application is associated with the second form factor and the second pose of the personal UI.  \n\n20. The media of claim 16, wherein the software is operable when executed to detect the change in the context of the first user to: detect the first user has switched from using a first application to a second application on the personal UI.  \n\n21. A system comprising: one or more processors; and a non-transitory memory coupled to the processors comprising instructions executable by the processors, the processors operable when executing the instructions to: render, for one or more displays of a virtual reality (VR) display device, a first output image of a VR environment, wherein the VR environment comprises a personal user interface (UI) associated with a first user, wherein the personal UI has a first form factor and a first pose with respect to the first user, wherein the personal UI is a virtual object comprising one or more 2D virtual displays, and wherein the personal UI is operable to execute a plurality of applications; detect a change in a context of the first user within the VR environment and with respect to a second user entering the VR environment; determine whether to adapt the personal UI based on the detected change in the context of the first user within the VR environment; and render, for the one or more displays of the VR display device, a second output image of the VR environment, wherein the personal UI is adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user within the VR environment, wherein the second form factor and the second pose of the personal UI is predetermined and personalized based on settings inputted by the first user related to the second user.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) FIG. 1 illustrates an example virtual reality system worn by a user, in accordance with particular embodiments.\n\n(2) FIG. 2 illustrates an example of a passthrough feature, in accordance with particular embodiments.\n\n(3) FIG. 3 is an example illustration of a user in a VR environment.\n\n(4) FIG. 4 illustrates an example personal UI.\n\n(5) FIG. 5 illustrates example form factors for a personal UI.\n\n(6) FIG. 6 illustrates an example of a user in a VR environment.\n\n(7) FIG. 7 illustrates an example view of an arrangement of virtual displays for a personal UI.\n\n(8) FIG. 8A illustrates an example of a first user using a real-world computing device with a computing device UI in a real-world environment.\n\n(9) FIG. 8B illustrates an example of a personal UI for a first user in a VR environment.\n\n(10) FIG. 9 illustrates an example of casting content onto a shared virtual screen in a VR environment.\n\n(11) FIG. 10 illustrates an example method for rendering a personal UI in a VR environment.\n\n(12) FIG. 11 illustrates an example network environment associated with a social-networking system.\n\n(13) FIG. 12 illustrates an example computer system.\n\nDESCRIPTION OF EXAMPLE EMBODIMENTS\n\n(14) In particular embodiments, a user of an immersive virtual reality (VR) system (e.g., head-mounted VR goggles) may have their view of the VR environment partially or fully occluded or obstructed by either the system user interface (UI) or application-specific UIs, and thus risk obstructing their view of the VR environment. The user may hide the UI, but the user would then lose access to the UI functionalities. Thus, one technical challenge may include maintaining access to the UI while the user is within the VR environment and across different VR experiences. One solution presented by the embodiments disclosed herein to address the technical challenge of maintaining access to the UI while the user is in different VR experiences is to adapt the UI to the context of the user with respect to the VR environment, such as by adapting the UI to have a form factor and/or pose (i.e., position and orientation with respect to the user) that best suits the user and the VR environment. The UI may be portable, in that the UI is accessible and maintains its functionalities as the user goes from one VR environment to another or switch from one application to another. And the UI may be personal, private, and customizable to the user's experience in the VR environment. As an example and not by way of limitation, a user may adjust the form factor and the pose of the UI to increase the UI productivity or to decrease the view obstruction of the UI, while still maintaining key UI functionalities. Although this disclosure describes maintaining the UI while the user is in different VR experiences by adapting and personalizing the UI in a particular manner, this disclosure contemplates maintaining the UI while the user is in different VR experiences in any suitable manner.\n\n(15) FIG. 1 illustrates an example of a virtual reality system 100 worn by a user 102. In particular embodiments, the virtual reality system 100 may comprise a head-mounted VR display device 135, a controller 106, and one or more computing systems 110. The VR display device 135 may be worn over the user's eyes and provide visual content to the user 102 through internal displays (not shown). The VR display device 135 may have two separate internal displays, one for each eye of the user 102 (single display devices are also possible). As illustrated in FIG. 1, the VR display device 135 may completely cover the user's field of view. By being the exclusive provider of visual information to the user 102, the VR display device 135 achieves the goal of providing an immersive artificial-reality experience. One consequence of this, however, is that the user 102 may not be able to see the physical (real-world) environment surrounding him, as his vision is shielded by the VR display device 135. As such, the passthrough feature described herein may be technically advantageous for providing the user with real-time visual information about his physical surroundings.\n\n(16) FIG. 2 illustrates an example of the passthrough feature. A user 102 may be wearing a VR display device 135, immersed within a virtual reality environment. A real-world object 145 is in the physical environment surrounding the user 102. However, due to the VR display device 135 blocking the vision of the user 102, the user 102 is unable to directly see the real-world object 145. To help the user perceive his physical surroundings while wearing the VR display device 135, the passthrough feature captures information about the physical environment using, for example, one or more cameras 105 such as external-facing cameras 105A-B. The captured information may then be re-projected to the user 102 based on his viewpoints. In particular embodiments where the VR display device 135 has a right display 136A for the user's right eye and a left display 136B for the user's left eye, the virtual reality system 100 may individually render (1) a re-projected view 145A of the physical environment for the right display 135A based on a viewpoint of the user's right eye and (2) a re-projected view 145B of the physical environment for the left display 135B based on a viewpoint of the user's left eye.\n\n(17) Referring again to FIG. 1, the VR display device 135 may have external-facing cameras, such as the two forward-facing cameras 105A and 105B shown in FIG. 1. While only two forward-facing cameras 105A-B are shown, the VR display device 135 may have any number of cameras facing any direction (e.g., an upward-facing camera to capture the ceiling or room lighting, a downward-facing camera to capture a portion of the user's face and/or body, a backward-facing camera to capture a portion of what's behind the user, and/or an internal camera for capturing the user's eye gaze for eye-tracking purposes). The external-facing cameras may be configured to capture the physical environment around the user and may do so continuously to generate a sequence of frames (e.g., as a video). As previously explained, although images captured by the forward-facing cameras 105A-B may be directly displayed to the user 102 via the VR display device 135, doing so may not provide the user with an accurate view of the physical environment since the cameras 105A-B cannot physically be located at the exact same location as the user's eyes. As such, the passthrough feature described herein may use a re-projection technique that generates a 3D representation of the physical environment and then renders images based on the 3D representation from the viewpoints of the user's eyes.\n\n(18) The 3D representation may be generated based on depth measurements of physical objects observed by the cameras 105A-B. Depth may be measured in a variety of ways. In particular embodiments, depth may be computed based on stereo images. As an example and not by way of limitation, the two forward-facing cameras 105A-B may share an overlapping field of view and be configured to capture images simultaneously. As a result, the same physical object may be captured by both cameras 105A-B at the same time. As an example and not by way of limitation, a particular feature of an object may appear at one pixel pA in the image captured by camera 105A, and the same feature may appear at another pixel pB in the image captured by camera 105B. As long as the depth measurement system knows that the two pixels correspond to the same feature, the virtual reality system 100 could use triangulation techniques to compute the depth of the observed feature. As an example and not by way of limitation, based on the camera 105A's position within a 3D space and the pixel location of pA relative to the camera 105A's field of view, a line could be projected from the camera 105A and through the pixel pA. A similar line could be projected from the other camera 105B and through the pixel pB. Since both pixels are supposed to correspond to the same physical feature, the two lines should intersect. The two intersecting lines and an imaginary line drawn between the two cameras 105A and 105B form a triangle, which could be used to compute the distance of the observed feature from either camera 105A or 105B or a point in space where the observed feature is located.\n\n(19) In particular embodiments, the pose (e.g., position and orientation) of the VR display device 135 within the environment may be needed. For example, in order to render the appropriate display for the user 102 while he is moving about in a virtual environment, the virtual reality system 100 may need to determine his position and orientation at any moment. Based on the pose of the VR display device, the virtual reality system 100 may further determine the viewpoint of either of the cameras 105A and 105B or either of the user's eyes. In particular embodiments, the VR display device 135 may be equipped with inertial-measurement units (\u201cIMU\u201d). The data generated by the IMU, along with the stereo imagery captured by the external-facing cameras 105A-B, allow the virtual reality system 100 to compute the pose of the VR display device 135 using, for example, SLAM (simultaneous localization and mapping) or other suitable techniques.\n\n(20) In particular embodiments, the virtual reality system 100 may further have one or more controllers 106 that enable the user 102 to provide inputs. The controller 106 may communicate with the VR display device 135 or a separate one or more computing systems 110 via a wireless or wired connection. The controller 106 may have any number of buttons or other mechanical input mechanisms. In addition, the controller 106 may have an IMU so that the pose of the controller 106 may be tracked. The controller 106 may further be tracked based on predetermined patterns on the controller. As an example and not by way of limitation, the controller 106 may have several infrared LEDs or other known observable features that collectively form a predetermined pattern. Using a sensor or camera, the virtual reality system 100 may be able to capture an image of the predetermined pattern on the controller. Based on the observed orientation of those patterns, the system may compute the controller's position and orientation relative to the sensor or camera.\n\n(21) The virtual reality system 100 may further include a one or more computing systems 110. The one or more computing systems 110 may be a stand-alone unit that is physically separate from the VR display device 135 or the computer system 110 may be integrated with the VR display device 135. In embodiments where the one or more computing systems 110 is a separate unit, the one or more computing systems 110 may be communicatively coupled to the VR display device 135 via a wireless or wired link. The one or more computing systems 110 may be a high-performance device, such as a desktop or laptop, or a resource-limited device, such as a mobile phone. A high-performance device may have a dedicated GPU and a high-capacity or constant power source. A resource-limited device, on the other hand, may not have a GPU and may have limited battery capacity. As such, the algorithms that could be practically used by a virtual reality system 100 depends on the capabilities of its one or more computing systems 110.\n\n(22) In embodiments where the one or more computing systems 110 is a high-performance device, an embodiment of the passthrough feature may be designed as follows. Through the external-facing cameras 105A-B of the VR display device 135, a sequence of images of the surrounding physical environment may be captured. The information captured by the cameras 105A-B, however, may be misaligned with what the user's eyes may capture since the cameras could not spatially coincide with the user's eyes (e.g., the cameras may be located some distance away from the user's eyes and, consequently, have different viewpoints). As such, simply displaying what the cameras captured to the user may not be an accurate representation of what the user should perceive.\n\n(23) Instead of simply displaying what was captured, the passthrough feature may re-project information captured by the external-facing cameras 105A-B to the user. Each pair of simultaneously captured stereo images may be used to estimate the depths of observed features. As explained above, to measure depth using triangulation, the one or more computing systems 110 may find correspondences between the stereo images. As an example and not by way of limitation, the one or more computing systems 110 may determine which two pixels in the pair of stereo images correspond to the same observed feature. A high-performance one or more computing systems 110 may solve the correspondence problem using its GPU and optical flow techniques, which are optimized for such tasks. The correspondence information may then be used to compute depths using triangulation techniques. Based on the computed depths of the observed features, the one or more computing systems 110 could determine where those features are located within a 3D space (since the one or more computing systems 110 also knows where the cameras are in that 3D space). The result may be represented by a dense 3D point cloud, with each point corresponding to an observed feature. The dense point cloud may then be used to generate 3D models of objects in the environment. When the system renders a scene for display, the system could perform visibility tests from the perspectives of the user's eyes. As an example and not by way of limitation, the system may cast rays into the 3D space from a viewpoint that corresponds to each eye of the user. In this manner, the rendered scene that is displayed to the user may be computed from the perspective of the user's eyes, rather than from the perspective of the external-facing cameras 105A-B.\n\n(24) The process described above, however, may not be feasible for a resource-limited computing unit (e.g., a mobile phone may be the main computational unit for the VR display device). For example, unlike systems with powerful computational resources and ample energy sources, a mobile phone cannot rely on GPUs and computationally expensive algorithms (e.g., optical flow) to perform depth measurements and generate an accurate 3D model of the environment. Thus, to provide passthrough on resource-limited devices, an optimized process is needed.\n\n(25) In particular embodiments, the computing device may be configured to dynamically determine, at runtime, whether it is capable of or able to generate depth measurements using (1) the GPU and optical flow or (2) the optimized technique using video encoder and motion vectors, as described in further detail below. As an example and not by way of limitation, if the device has a GPU and sufficient power budget (e.g., it is plugged into a power source, has a full battery, etc.), it may perform depth measurements using its GPU and optical flow. However, if the device does not have a GPU or has a stringent power budget, then it may opt for the optimized method for computing depths.\n\n(26) FIG. 3 is an example illustration of a user 102 in a VR environment 125. The user 102 may be wearing a VR display device 135 (not illustrated). The user 102 may be represented by an avatar in the VR environment 125, as illustrated in FIG. 3. Using the VR display device 135, the user may view the VR environment 125 or a passthrough view of the real-world environment. Within the VR environment 125 and the passthrough view of the real-world environment, there may be a personal UI 115 comprising one or more virtual displays 140 (e.g., the virtual displays 140a-d).\n\n(27) FIG. 4 illustrates an example personal UI 115. The personal UI 115 may appear as a menu or dashboard for the user to execute one or more tasks, e.g., the user may use the personal UI 115 to execute one or more applications (from among the plurality of applications selectable by application icons 120), such as gaming applications, work applications, entertainment applications, call/chat applications, etc. The personal UI 115 may be a feature of the VR operating system (VROS) associated with the virtual reality system 100. The plurality of applications may correspond to applications accessible on a real-world computing device associated with the user, such as the user's smartphone, tablet, laptop computer, or other computing device. The VROS may have various built-in functionalities. As an example and not by way of limitation, the personal UI 115 of the VROS may provide access to a built-in web browser application and social media application that the user can access. If the user is in a virtual meeting, the user may quickly research a topic on the web browser on the personal UI 115 without having to exit the virtual meeting. If the user is playing a VR video game on a video game application and wants to post their high score, the user may access their social media application from their personal UI 115 and post their high score directly onto their social media, without having to leave the video game application.\n\n(28) FIG. 5 illustrates example form factors for a personal UI 115. The form factors (e.g., the shape, appearance, layout, etc.) of the personal UI 115 may appear as a virtual object personal UI 115a (e.g. a floating 2D screen), or mimic the form factor of real-world objects, such a smart watch personal UI 115b (e.g., appearing on the user's virtual wrist), a phone personal UI 115c, a tablet computer personal UI 115d, a monitor personal UI 115e, a laptop personal UI 115f, a TV screen personal UI (not illustrated), a movie theater screen personal UI (not illustrated), a curved display personal UI (not illustrated), or any other suitable form factor. That is, the form factors of the personal UI may include virtual watches, virtual smartphones, virtual tablets, virtual remote controls, virtual computer monitors, virtual televisions, and virtual movie screens, to name a few. Thus, a technical advantage of the embodiments may include automatically adapting the personal UI 115 in the VR environment 125 to mimic the UI of a real-world object to help a user acclimate to using the personal UI 115.\n\n(29) In particular embodiments, the virtual reality system 100 may render, for one or more displays of a VR display device 135, a first output image of a VR environment 125. The VR environment 125 may comprise a personal UI 115 associated with a first user (e.g., the user 102). The personal UI 115 may have a first form factor (e.g., shape, appearance, layout, etc.) and a first pose (e.g., position and orientation) with respect to the first user. As an example and not by way of limitation, the personal UI 115 may have a form factor of a floating virtual object (e.g., the virtual object personal UI 115a). The personal UI 115 may have a pose that is proximate and in front of the user 102, e.g., in front of the user and in the user's field of view. The personal UI 115 may be a virtual object comprising one or more 2D virtual displays. As an example and not by way of limitation, the personal UI 115 may comprise a smaller virtual display such as the virtual display 140a directly in front of the user 102, as well as one or more other virtual displays 140b-d. The personal UI 115 may be operable to execute a plurality of applications. As an example and not by way of limitation, the personal UI 115 may be operable to access and display various applications, including entertainment related applications (such as streaming services), social media and networking applications, gaming related applications, web browser applications, messaging, call, and video chat applications, work related applications (such as word processing applications), or other suitable applications. Although this disclosure describes rendering a first output image of a VR environment in a particular manner, this disclosure contemplates rendering an output image in any suitable manner.\n\n(30) In particular embodiments, the virtual reality system 100 may detect a change in a context of the first user 102 with respect to the VR environment 125. A technical challenge may include detecting when a user's context has changed. The solution presented by the embodiments disclosed herein to address this challenge may be to determine a change in the pose of the user 102 with respect to the VR environment 125 and/or objects within the VR environment 125, receive an input by the user 102 selecting an application that is associated with a change in the form factor and/or pose of the personal UI 115, or detect a change from one application to another. Detecting the change in the context of the first user 102 may include determining the first user 102 has changed from a first pose to a second pose in the VR environment 125. The virtual reality system 100 may detect a change in the pose of the user 102 using one or more sensors (such as position sensors, cameras 105, accelerometers, etc.). The user 102 may change their pose if they rotate around their spot or move to another spot in the VR environment 125. As an example and not by way of limitation, a first pose of the user 102 may be the user 102 standing in the middle of a room (e.g., in the middle of the room in the VR environment 125). A second pose of the user 102 may be the user 102 moving to a couch or a desk (e.g., the object 150) and sitting on the couch or sitting at the desk.\n\n(31) In particular embodiments, determining the first user 102 has changed from the first pose to the second pose may include the virtual reality system 100 determining whether a location of the user 102 is within a threshold distance of an object 150 in the VR environment 125, and determining whether an orientation of the user 102 indicates the user is facing the object 150 in the VR environment 125. The virtual reality system 100 may determine whether a location of the user 102 is within a threshold distance of an object 150 in the VR environment 125 using one or more sensors (such as position sensors, cameras 105, accelerometers, object detection filters, etc.) of the virtual reality system 100. The object 150 may be a VR object in the VR environment 125. The object 150 may be a mixed-reality (MR) object in the VR environment 125 may correspond to a real-world object in the real-world environment. As an example and not by way of limitation, the virtual reality system 100 may determine whether the user 102 has moved towards a couch, and further determine whether the user 102 has approached within a threshold distance of the couch (e.g., whether the user 102 has approached within 1 meter of the couch). The virtual reality system 100 may determine whether an orientation of the user 102 indicates the user 102 is facing the object in the VR environment 125 using one or more sensors (such as position sensors, cameras 105, accelerometers, object detection filters, etc.) of the virtual reality system 100. As an example and not by way of limitation, the virtual reality system 100 may determine whether the user 102 is facing the couch (e.g., the couch is within the user's field of view) or facing away. Then, if the virtual reality system 100 determines the user 102 is within the threshold distance of the object, and the user 102 is facing the object, the virtual reality system 100 may detect the user 102 has changed from a first pose to a second pose in the VR environment 125.\n\n(32) In particular embodiments, detecting the change in the context of the first user may include the virtual reality system 100 receiving an input by the user selecting a particular application on the personal UI 115 from the plurality of applications. As an example and not by way of limitation, the user 102 may use the personal UI 115 to select an application or switch between different applications by clicking an icon, button, switch, or other form of input on the personal UI 115. The virtual reality system 100 may then receive an indication the user 102 intends to select a particular application on the personal UI 115. The particular application may be associated with the second form factor and the second pose of the personal UI 115. The personal UI 115 may have a predetermined form factor and pose, preferred form factor and pose, or may otherwise be associated with a specific form factor and pose to enhance the user's VR experience by determining the form factor and pose of the personal UI that will least detract from the user's VR experience (e.g., determining the least view obstructive form factor and pose of the personal UI 115) and/or best enhance the user's VR experience (e.g., determining the most productive form factor and pose of the personal UI 115). As an example and not by way of limitation, if the virtual reality system 100 is executing a first application such as a word processing application, the first form factor of the personal UI 115 may be a multi-monitor setup to enable the user 102 to do work as the user may in an office setting, and the pose of the personal UI 115 may have the virtual monitors displayed proximate to and in front of the user. If the virtual reality system 100 switches to execute a second application such as a video streaming application, the form factor of the personal UI 115 may be a large virtual TV or movie theater screen, and the pose of the personal UI may have the virtual TV or movie theater screen placed further away from the user (mimicking the pose of a real-world TV or movie theater screen relative to a real-world user).\n\n(33) FIG. 6 illustrates an example of a user 102 in a VR environment 125. The user 102 may be represented by an avatar in the VR environment 125, as illustrated in FIG. 6. The VR environment 125 in FIG. 6 may be a new or different VR environment than the VR environment 125 in FIG. 3. In particular embodiments, detecting the change in the context of the first user may include detecting the first user has moved from a first virtual space to a second virtual space. As an example and not by way of limitation, if the user was in a first VR space that was a VR office space, and the user selects a video streaming application on the personal UI 115, the second VR space may be a VR TV or movie theater screen. As another example, if the user selects a gaming application, the second VR space may be a gaming VR space (such as \u201cinside\u201d a video game environment or landscape, or at a desk setup that includes a virtual widescreen monitor that is geared for a gaming experience). As another example, if the user selects a map or travel application, the second VR space may be a VR environment 125 that puts the user in a new location. As an example and not by way of limitation, if the user selects a \u201ctour of Paris\u201d application, the VR space may be a rendering of the Eiffel Tower and its surroundings in Paris. Based on these example changes, the virtual reality system 100 may then detect the user has switched from a first virtual space to a second virtual space. Although this disclosure describes detecting a change in context in particular manners, this disclosure contemplates detecting a change in context in any suitable manner.\n\n(34) In particular embodiments, detecting the change in the context of the first user may include the virtual reality system 100 detecting the first user has switched from using a first application to a second application on the personal UI 115. The virtual space (e.g., the VR environment 125) may be a result of the user switching between applications. As an example and not by way of limitation, if the user was using a work-related application (such as a word processing application), the user may have been in a first VR space that may be an office VR space. If the user then selects a video streaming application, the second VR space may be a VR movie theater. As another example and not by way of limitation, if the user selects a gaming application, the second VR space may be a gaming VR space (such as \u201cinside\u201d a video game, or at a desk setup includes a virtual widescreen monitor that is geared for a gaming experience). As another example, if the user selects a map or travel application, the second VR space may be a VR environment 125 that puts the user in a new location. As an example and not by way of limitation, if the user selects a \u201ctour of Paris\u201d application, the VR space may be a rendering of the Eiffel Tower and its surroundings in Paris.\n\n(35) One technical challenge may include maintaining personal UI 115 functionalities without view obstructing or disrupting the VR environment 125. The solution presented by the embodiments disclosed herein to address this challenge may be to adapt the personal UI 115 based on the context of the user 102 with respect to the VR environment 125. In particular embodiments, the virtual reality system 100 may determine whether to adapt the personal UI 115 based on the detected change in the context of the first user 102. Based on the detected change in the context of the user 102 with respect to the VR environment 125 (e.g., whether the user 102 has changed from a first pose to a second pose, whether the user selected a particular application that is associated with a form factor and a pose of the personal UI 115, whether the user 102 has moved from a first virtual space to a second virtual space, whether the user switched from a first application to a second application on the personal UI 115, etc.), the personal UI 115 may adapt to a new form factor and/or pose with respect to the user 102 in the VR environment 125. As an example and not by way of limitation, if the user 102 has changed from a first pose to a second pose, the virtual reality system 100 may adapt the personal UI 115 to the user's second pose (e.g., if the user 102 was standing and then sits down, the form factor and the pose of the personal UI 115 may be adapted to change its appearance, position, and orientation to keep the personal UI 115 in the field of view of the user 102). As another example and not by way of limitation, if the user 102 selected a particular application that is associated with a form factor and a pose of the personal UI 115, the virtual reality system 100 may determine the form factor and the pose of the personal UI 115 should adapt to the preferred form factor and pose of the personal UI, or the form factor and pose that is associated with the particular application. As an example and not by way of limitation, if the user selects a video streaming application, the virtual reality system 100 may choose to adapt the form factor and the pose of the personal UI 115 to mimic a real-world movie theater screen. As another example and not by way of limitation, if the user moves from one virtual space to another virtual space (e.g., from a work VR setting to a movie theater VR setting), the virtual reality system 100 may determine the personal UI 115 should adapt from a work VR setting (e.g., a dual monitor virtual display) to an entertainment VR setting (e.g., a virtual movie theater screen). As another example and not by way of limitation, if the user switches from a first application to a second application, the virtual reality system 100 may adapt the personal UI 115 to a form factor and pose that best suits the user 102 (e.g., the user's wants and needs) and/or the second application. As an example and not by way of limitation, if the user 102 switches from a work-related application to a travel-related application, the form factor and pose of the personal UI 115 may change from a form factor and pose more suited for a work setting (e.g., large dual-monitor virtual displays) to a form factor and pose more suited for a travel setting (e.g., a smaller smartphone or tablet computer virtual display that is less view obstructive for sightseeing). As another example and not by way of limitation, the personal UI 115 may be used to run a 2D application in a 3D VR environment 125. If the user wants to watch a movie, the user may cast the movie from the 2D virtual streaming application onto a virtual TV screen in a 3D VR environment 125. Although this disclosure describes detecting a change in a context in a particular manner, this disclosure contemplates detecting a change in a context in any suitable manner.\n\n(36) In particular embodiments, determining whether to adapt the personal UI 115 based on the detected change in the context of the first user 102 includes identifying a predetermined form factor and pose of the personal UI 115 associated with an activity of the user in the VR environment 125. That is, there may be a form factor and pose of the personal UI 115 that is predetermined based on the activity or application currently being executed by the virtual reality system 100. The predetermined form factor and pose of the personal UI 115 may be based on the form factor and pose of the personal UI that is the least view obstructive and/or the most productive form factor and pose of the personal UI 115, based on the activity currently being undertaken by the user 102. As an example and not by way of limitation, if the user 102 is in a virtual conference room or lecture hall, the form factor and pose of the personal UI 115 may be a view unobstructive virtual laptop directly in front of the user 102 for taking notes. As another example, if the user 102 is on a city tour or is in a virtual hangout room with another user, the form factor and pose of the personal UI 115 may take on the appearance of a view unobstructive virtual smartphone or tablet. As another example, if the user is in a work VR environment 125, the form factor and pose of the personal UI 115 may take on the appearance and layout of a virtual multi-monitor setup that enables the user 102 to work efficiently and productively. As another example, if the user 102 is in an entertainment VR setting (e.g., watching a movie), the form factor and pose of the personal UI 115 may appear as a large virtual movie screen that appears to be far away from the user 102 (mimicking a real-world movie theater experience). Although this disclosure describes identifying a predetermined form factor and pose of a personal UI 115 in a particular manner, this disclosure contemplates determining a form factor and pose of a UI in any suitable manner.\n\n(37) In particular embodiments, the virtual reality system 100 may render, for the one or more displays of the VR display device 135, a second output image of the VR environment 125. The personal UI 115 may be adapted to have a second form factor and a second pose with respect to the first user 102 responsive to determining to adapt the personal UI 115 based on the detected change in the context of the first user 102. As an example and not by way of limitation and as seen in FIG. 3, a first output image of the VR environment 125 has a personal UI 115 that includes multiple larger virtual displays. In FIG. 6, when the context of the user 102 with respect to the VR environment 125 has changed, the second output image of the VR environment 125 has a personal UI 115 that is adapted to the changed context. The personal UI 115 has a smaller, less view obstructive form factor and pose of the personal UI 115, which may allow the user to explore the new VR environment 125 without having the larger, view obstructive displays 140 in the user's view (e.g., the virtual displays 140a-b in FIG. 6 may be fewer in number and/or smaller than the virtual displays 140a-d in FIG. 3). As an example and not by way of limitation, if the user 102 is in a setting where it is important to be able to view the VR environment 125 (e.g., the user may be virtually sightseeing or travelling), the personal UI 115 may be adapted to have a smaller form factor (e.g., a smartwatch or smartphone form factor) and less view obstructive pose (e.g., a smartwatch on the user's virtual wrist or smartphone in the user's virtual hand) to allow the user 102 to explore their virtual surroundings. Thus, a technical advantage of the embodiments may include automatically adapting a form factor and a pose of the personal UI 115 responsive to a detected change in the context of the user 102 with respect to the VR environment 125. Although this disclosure describes rendering a second output image of a VR environment 125 in a particular manner, this disclosure contemplates rendering an output image in any suitable manner.\n\n(38) FIG. 7 illustrates an example view of an arrangement of virtual displays 140 for a personal UI 115. In particular embodiments, the personal UI 115 may comprise a plurality of 2D virtual displays 140. The plurality of 2D virtual displays 140 may be arranged in a tile-based layout comprising a plurality of preset orientations for the plurality of 2D virtual displays 140. That is, the plurality of 2D virtual displays may be arranged so that each virtual display 140 may \u201csnap\u201d to a position (i.e., automatically jump to a predetermined position when the user drags it to the proximity of the desired location). This may enable the user 102 (illustrated as a VR rendering of the user, e.g., an avatar of the user) to easily and efficiently arrange the plurality of virtual displays 140. As an example and not by way of limitation, the user 102 may arrange each virtual display 140 side-by-side next to one another, above or below one another, etc. The tile-based layout may prevent the plurality of virtual displays 140 from overlapping one another, e.g., having one virtual display 140 obstruct the view of another virtual display 140. Additionally, the user may adjust the virtual display 140 to \u201cstretch\u201d or extend the display to cover more than one tile of the tile-based layout. The user may adjust the virtual displays 140 in the tile-based system by dragging, adding, removing, or otherwise altering the layout of the virtual displays 140. Although this disclosure describes arranging virtual displays 140 in a particular manner, this disclosure contemplates arranging virtual displays in any suitable manner.\n\n(39) In particular embodiments, a first 2D virtual display 140 of the plurality of 2D virtual displays 140 may be operable to execute a first set of applications of the plurality of applications. A second 2D virtual display 140 of the plurality of 2D virtual displays 140 may be operable to execute a second set of applications of the plurality of applications. As an example and not by way of limitation, a first virtual display 140a may be operable to run a first application 121a, a second virtual display 140b may be operable to run a second application 121b, a third virtual display 140c may be operable to run a third application 121c, and etc. The applications 121 may be selected using the one or more application icons 120 of the personal UI 115 (see FIG. 4). Each virtual display 140 may be able to execute applications 121 independently of one another. As an example and not by way of limitation, a first virtual display 140a may be able to display a messaging application (e.g., an application 121a), while a second virtual display 140b may be able to operate a gaming application (e.g., an application 121b), and while a third virtual display 140c may be able to operate a photo-editing application (e.g., an application 121c). The virtual displays may operate these separate applications (e.g., applications 121a-c) side-by-side on the virtual displays (e.g., virtual displays 140a-c) the personal UI 115. Although this disclosure describes executing a plurality of applications on virtual displays in a particular manner, this disclosure contemplates executing applications on virtual displays in any suitable manner.\n\n(40) FIG. 8A illustrates an example of a first user 102 using a real-world computing device 117 (e.g., a tablet computer) having a real-world computing device UI 118 in a real-world environment 130. FIG. 8A is in the perspective of the first user 102. As an example and not by way of limitation, the first user 102 may be inside their own home and the VR display device 135 may display a passthrough view of the user's real-world environment 130. FIG. 8B illustrates an example of a personal UI 115 for a first user 102 in a VR environment 125. FIG. 8B is in the perspective of the first user 102. As an example and not by way of limitation, the VR display device 135 may display a rendered VR environment 125 where the first user 102 is at a virtual park.\n\n(41) FIG. 9 illustrates an example of casting content onto a shared virtual screen 141 in a VR environment 125. FIG. 9 is in the perspective of a first user 102a. The VR display device 135 may display a rendered VR environment 125 where the first user 102a is in a shared virtual space with a second user 102b. In particular embodiments, the virtual reality system 100 may cast content from the personal UI 115a of the first user 102a onto a shared virtual display such as the shared virtual screen 141 in the VR environment 125. The shared virtual screen 141 may be viewable by the first user 102a and the one or more second users 102b, and the content projected or displayed on the shared virtual screen 141 may be viewable by the first user 102a and the one or more second users 102b. While casting content from the personal UI 115a of the first user 102a onto a shared virtual screen to be viewed by the first user 102a and the one or more second users 102b, the information and content on the personal UI 115a of the first user 102a may remain hidden or blocked to the one or more second users 102b. The form factor of the shared virtual screen 141 may be the same relative to the first user 102a and the second user 102b in the VR environment 125. The pose of the shared virtual screen 141 may be the same relative to the first user 102a and relative to the second user 102b in the VR environment 125. The pose of the shared virtual screen 141 may be the same relative to the VR environment 125 (e.g., the first user 102a and the second user 102b may find the shared virtual screen 141 in the same corner of the VR environment 125). Although this disclosure describes casting content in a particular manner, this disclosure contemplates casting content in any suitable manner.\n\n(42) In particular embodiments, the VR environment 125 may comprise a shared virtual space for the first user 102a and one or more second users 102b, e.g., a virtual environment 125 for the first user 102a and the one or more second users 102b. The first user 102a and the one or more second users 102b may meet and interact in the shared virtual space such as the VR environment 125 illustrated in FIG. 9. The first user 102a and the one or more second users 102b may meet in a shared virtual space that is based on a real-world place (e.g., a real-world park) or a fictional space (e.g., an alien planet rendered in the VR environment 125). The first user 102a and the one or more second users 102b may interact with one another in the shared virtual space. As an example and not by way of limitation, if the first user 102a receives a video call from the second user 102b, the first user 102a and the second user 102b may enter a virtual meeting room and talking \u201cface to face\u201d instead of talking to one another through a 2D display of the video call application. As another example and not by way of limitation, the first user 102a and the one or more second users 102b may interact with one another and play a game of virtual tennis in the shared virtual space (e.g., where the users may use the controller 106 as tennis rackets). Although this disclosure describes users meeting in a shared virtual space in a particular manner, this disclosure contemplates users meeting in a shared virtual space in any suitable manner.\n\n(43) In particular embodiments, privacy settings of the first user 102a may prevent the one or more second users from viewing content on the personal UI 115a associated with the first user 102a. Similarly, privacy settings of the one or more second users 102b may prevent the first user 102a from viewing content on the personal UI 115b associated with the one or more second users 102b. The personal UI 115 of other users may appear as a floating blank object that is devoid of information or content (as illustrated in FIG. 9 where the personal UI 115b of the second user 102b appears as a floating object devoid of any information or content). As an example and not by way of limitation, the second user 102b will not be able to see documents and photos that are displayed on the personal UI 115a of the first user 102a. That is, the form factor and pose of another user's personal UI 115 may appear as a standard form factor and pose. That is, if the personal UI 115a of the first user 102a has a form factor of a large TV screen and a pose that is far away from the first user 102a, the second user 102b may only see the personal UI 115a of the first user 102a as a blank floating object that has a standard form factor and a pose that is proximate to the first user 102a. Privacy settings may be adjusted to allow other users to view items of information or content on another user's personal UI 115. Although this disclosure describes utilizing privacy settings in a particular manner, this disclosure contemplates privacy settings in any suitable manner.\n\n(44) FIG. 10 illustrates an example method 1000 for rendering a personal UI in a VR environment. The method may begin at step 1010, where one or more computing systems may render, for one or more displays of a VR display device, a first output image of a VR environment. The VR environment may comprise a personal UI associated with a first user. The personal UI may have a first form factor and a first pose with respect to the first user. The personal UI may be a virtual object comprising one or more 2D virtual displays. The personal UI may be operable to execute a plurality of applications. At step 1020, the one or more computing systems may detect a change in a context of the first user with respect to the VR environment. At step 1030, the one or more computing systems may determine whether to adapt the personal UI based on the detected change in the context of the first user. At step 1040, the one or more computing systems may render, for the one or more displays of the VR display device, a second output image of the VR environment. The personal UI may be adapted to have a second form factor and a second pose with respect to the first user responsive to determining to adapt the personal UI based on the detected change in the context of the first user. Particular embodiments may repeat one or more steps of the method of FIG. 10, where appropriate. Although this disclosure describes and illustrates particular steps of the method of FIG. 10 as occurring in a particular order, this disclosure contemplates any suitable steps of the method of FIG. 10 occurring in any suitable order. Moreover, although this disclosure describes and illustrates an example method 1000 for rendering a personal UI in a VR environment including the particular steps of the method of FIG. 10, this disclosure contemplates any suitable method for rendering a personal UI in a VR environment including any suitable steps, which may include all, some, or none of the steps of the method of FIG. 10, where appropriate. Furthermore, although this disclosure describes and illustrates particular components, devices, or systems carrying out particular steps of the method of FIG. 10, this disclosure contemplates any suitable combination of any suitable components, devices, or systems carrying out any suitable steps of the method of FIG. 10.\n\n(45) FIG. 11 illustrates an example network environment 1100 associated with a VR or social-networking system. Network environment 1100 includes a client system 1130, a VR or social-networking system 1160, and a third-party system 1170 connected to each other by a network 1110. Although FIG. 11 illustrates a particular arrangement of client system 1130, VR or social-networking system 1160, third-party system 1170, and network 1110, this disclosure contemplates any suitable arrangement of client system 1130, VR or social-networking system 1160, third-party system 1170, and network 1110. As an example and not by way of limitation, two or more of client system 1130, VR or social-networking system 1160, and third-party system 1170 may be connected to each other directly, bypassing network 1110. As another example, two or more of client system 1130, VR or social-networking system 1160, and third-party system 1170 may be physically or logically co-located with each other in whole or in part. Moreover, although FIG. 11 illustrates a particular number of client systems 1130, VR or social-networking systems 1160, third-party systems 1170, and networks 1110, this disclosure contemplates any suitable number of client systems 1130, VR or social-networking systems 1160, third-party systems 1170, and networks 1110. As an example and not by way of limitation, network environment 1100 may include multiple client system 1130, VR or social-networking systems 1160, third-party systems 1170, and networks 1110.\n\n(46) This disclosure contemplates any suitable network 1110. As an example and not by way of limitation, one or more portions of network 1110 may include an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area network (LAN), a wireless LAN (WLAN), a wide area network (WAN), a wireless WAN (WWAN), a metropolitan area network (MAN), a portion of the Internet, a portion of the Public Switched Telephone Network (PSTN), a cellular telephone network, or a combination of two or more of these. Network 1110 may include one or more networks 1110.\n\n(47) Links 1150 may connect client system 1130, social-networking system 1160, and third-party system 1170 to communication network 1110 or to each other. This disclosure contemplates any suitable links 1150. In particular embodiments, one or more links 1150 include one or more wireline (such as for example Digital Subscriber Line (DSL) or Data Over Cable Service Interface Specification (DOCSIS)), wireless (such as for example Wi-Fi or Worldwide Interoperability for Microwave Access (WiMAX)), or optical (such as for example Synchronous Optical Network (SONET) or Synchronous Digital Hierarchy (SDH)) links. In particular embodiments, one or more links 1150 each include an ad hoc network, an intranet, an extranet, a VPN, a LAN, a WLAN, a WAN, a WWAN, a MAN, a portion of the Internet, a portion of the PSTN, a cellular technology-based network, a satellite communications technology-based network, another link 1150, or a combination of two or more such links 1150. Links 1150 need not necessarily be the same throughout network environment 1100. One or more first links 1150 may differ in one or more respects from one or more second links 1150.\n\n(48) In particular embodiments, client system 1130 may be an electronic device including hardware, software, or embedded logic components or a combination of two or more such components and capable of carrying out the appropriate functionalities implemented or supported by client system 1130. As an example and not by way of limitation, a client system 1130 may include a computer system such as a desktop computer, notebook or laptop computer, netbook, a tablet computer, e-book reader, GPS device, camera, personal digital assistant (PDA), handheld electronic device, cellular telephone, smartphone, augmented/virtual reality device, other suitable electronic device, or any suitable combination thereof. This disclosure contemplates any suitable client systems 1130. A client system 1130 may enable a network user at client system 1130 to access network 1110. A client system 1130 may enable its user to communicate with other users at other client systems 1130.\n\n(49) In particular embodiments, client system 1130 (e.g., an HMD) may include a passthrough engine 1132 to provide the passthrough feature described herein, and may have one or more add-ons, plug-ins, or other extensions. A user at client system 1130 may connect to a particular server (such as server 1162, or a server associated with a third-party system 1170). The server may accept the request and communicate with the client system 1130.\n\n(50) In particular embodiments, VR or social-networking system 1160 may be a network-addressable computing system that may host an online Virtual Reality environment or social network. VR or social-networking system 1160 may generate, store, receive, and send social-networking data, such as, for example, user-profile data, concept-profile data, social-graph information, or other suitable data related to the online social network. Social-networking or VR system 1160 may be accessed by the other components of network environment 1100 either directly or via network 1110. As an example and not by way of limitation, client system 1130 may access social-networking or VR system 1160 using a web browser, or a native application associated with social-networking or VR system 1160 (e.g., a mobile social-networking application, a messaging application, another suitable application, or any combination thereof) either directly or via network 1110. In particular embodiments, social-networking or VR system 1160 may include one or more servers 1162. Each server 1162 may be a unitary server or a distributed server spanning multiple computers or multiple datacenters. Servers 1162 may be of various types, such as, for example and without limitation, web server, news server, mail server, message server, advertising server, file server, application server, exchange server, database server, proxy server, another server suitable for performing functions or processes described herein, or any combination thereof. In particular embodiments, each server 1162 may include hardware, software, or embedded logic components or a combination of two or more such components for carrying out the appropriate functionalities implemented or supported by server 1162. In particular embodiments, social-networking or VR system 1160 may include one or more data stores 1164. Data stores 1164 may be used to store various types of information. In particular embodiments, the information stored in data stores 1164 may be organized according to specific data structures. In particular embodiments, each data store 1164 may be a relational, columnar, correlation, or other suitable database. Although this disclosure describes or illustrates particular types of databases, this disclosure contemplates any suitable types of databases. Particular embodiments may provide interfaces that enable a client system 1130, a social-networking or VR system 1160, or a third-party system 1170 to manage, retrieve, modify, add, or delete, the information stored in data store 1164.\n\n(51) In particular embodiments, social-networking or VR system 1160 may store one or more social graphs in one or more data stores 1164. In particular embodiments, a social graph may include multiple nodes\u2014which may include multiple user nodes (each corresponding to a particular user) or multiple concept nodes (each corresponding to a particular concept)\u2014and multiple edges connecting the nodes. Social-networking or VR system 1160 may provide users of the online social network the ability to communicate and interact with other users. In particular embodiments, users may join the online social network via social-networking or VR system 1160 and then add connections (e.g., relationships) to a number of other users of social-networking or VR system 1160 to whom they want to be connected. Herein, the term \u201cfriend\u201d may refer to any other user of social-networking or VR system 1160 with whom a user has formed a connection, association, or relationship via social-networking or VR system 1160.\n\n(52) In particular embodiments, social-networking or VR system 1160 may provide users with the ability to take actions on various types of items or objects, supported by social-networking or VR system 1160. As an example and not by way of limitation, the items and objects may include groups or social networks to which users of social-networking or VR system 1160 may belong, events or calendar entries in which a user might be interested, computer-based applications that a user may use, transactions that allow users to buy or sell items via the service, interactions with advertisements that a user may perform, or other suitable items or objects. A user may interact with anything that is capable of being represented in social-networking or VR system 1160 or by an external system of third-party system 1170, which is separate from social-networking or VR system 1160 and coupled to social-networking or VR system 1160 via a network 1110.\n\n(53) In particular embodiments, social-networking or VR system 1160 may be capable of linking a variety of entities. As an example and not by way of limitation, social-networking or VR system 1160 may enable users to interact with each other as well as receive content from third-party systems 1170 or other entities, or to allow users to interact with these entities through an application programming interfaces (API) or other communication channels.\n\n(54) In particular embodiments, a third-party system 1170 may include one or more types of servers, one or more data stores, one or more interfaces, including but not limited to APIs, one or more web services, one or more content sources, one or more networks, or any other suitable components, e.g., that servers may communicate with. A third-party system 1170 may be operated by a different entity from an entity operating social-networking or VR system 1160. In particular embodiments, however, social-networking or VR system 1160 and third-party systems 1170 may operate in conjunction with each other to provide social-networking services to users of social-networking or VR system 1160 or third-party systems 1170. In this sense, social-networking or VR system 1160 may provide a platform, or backbone, which other systems, such as third-party systems 1170, may use to provide social-networking services and functionality to users across the Internet.\n\n(55) In particular embodiments, a third-party system 1170 may include a third-party content object provider. A third-party content object provider may include one or more sources of content objects, which may be communicated to a client system 1130. As an example and not by way of limitation, content objects may include information regarding things or activities of interest to the user, such as, for example, movie show times, movie reviews, restaurant reviews, restaurant menus, product information and reviews, or other suitable information. As another example and not by way of limitation, content objects may include incentive content objects, such as coupons, discount tickets, gift certificates, or other suitable incentive objects.\n\n(56) In particular embodiments, social-networking or VR system 1160 also includes user-generated content objects, which may enhance a user's interactions with social-networking or VR system 1160. User-generated content may include anything a user can add, upload, send, or \u201cpost\u201d to social-networking or VR system 1160. As an example and not by way of limitation, a user communicates posts to social-networking or VR system 1160 from a client system 1130. Posts may include data such as status updates or other textual data, location information, photos, videos, links, music or other similar data or media. Content may also be added to social-networking or VR system 1160 by a third-party through a \u201ccommunication channel,\u201d such as a newsfeed or stream.\n\n(57) In particular embodiments, social-networking or VR system 1160 may include a variety of servers, sub-systems, programs, modules, logs, and data stores. In particular embodiments, social-networking or VR system 1160 may include one or more of the following: a web server, action logger, API-request server, relevance-and-ranking engine, content-object classifier, notification controller, action log, third-party-content-object-exposure log, inference module, authorization/privacy server, search module, advertisement-targeting module, user-interface module, user-profile store, connection store, third-party content store, or location store. Social-networking or VR system 1160 may also include suitable components such as network interfaces, security mechanisms, load balancers, failover servers, management-and-network-operations consoles, other suitable components, or any suitable combination thereof. In particular embodiments, social-networking or VR system 1160 may include one or more user-profile stores for storing user profiles. A user profile may include, for example, biographic information, demographic information, behavioral information, social information, or other types of descriptive information, such as work experience, educational history, hobbies or preferences, interests, affinities, or location. Interest information may include interests related to one or more categories. Categories may be general or specific. As an example and not by way of limitation, if a user \u201clikes\u201d an article about a brand of shoes the category may be the brand, or the general category of \u201cshoes\u201d or \u201cclothing.\u201d A connection store may be used for storing connection information about users. The connection information may indicate users who have similar or common work experience, group memberships, hobbies, educational history, or are in any way related or share common attributes. The connection information may also include user-defined connections between different users and content (both internal and external). A web server may be used for linking social-networking or VR system 1160 to one or more client systems 1130 or one or more third-party system 1170 via network 1110. The web server may include a mail server or other messaging functionality for receiving and routing messages between social-networking or VR system 1160 and one or more client systems 1130. An API-request server may allow a third-party system 1170 to access information from social-networking or VR system 1160 by calling one or more APIs. An action logger may be used to receive communications from a web server about a user's actions on or off social-networking or VR system 1160. In conjunction with the action log, a third-party content-object log may be maintained of user exposures to third-party content objects. A notification controller may provide information regarding content objects to a client system 1130. Information may be pushed to a client system 1130 as notifications, or information may be pulled from client system 1130 responsive to a request received from client system 1130. Authorization servers may be used to enforce one or more privacy settings of the users of social-networking or VR system 1160. A privacy setting of a user determines how particular information associated with a user may be shared. The authorization server may allow users to opt in to or opt out of having their actions logged by social-networking or VR system 1160 or shared with other systems (e.g., third-party system 1170), such as, for example, by setting appropriate privacy settings. Third-party content-object stores may be used to store content objects received from third parties, such as a third-party system 1170. Location stores may be used for storing location information received from client systems 1130 associated with users. Advertisement-pricing modules may combine social information, the current time, location information, or other suitable information to provide relevant advertisements, in the form of notifications, to a user.\n\n(58) FIG. 12 illustrates an example computer system 1200. In particular embodiments, one or more computer systems 1200 perform one or more steps of one or more methods described or illustrated herein. In particular embodiments, one or more computer systems 1200 provide functionality described or illustrated herein. In particular embodiments, software running on one or more computer systems 1200 performs one or more steps of one or more methods described or illustrated herein or provides functionality described or illustrated herein. Particular embodiments include one or more portions of one or more computer systems 1200. Herein, reference to a computer system may encompass a computing device, and vice versa, where appropriate. Moreover, reference to a computer system may encompass one or more computer systems, where appropriate.\n\n(59) This disclosure contemplates any suitable number of computer systems 1200. This disclosure contemplates computer system 1200 taking any suitable physical form. As example and not by way of limitation, computer system 1200 may be an embedded computer system, a system-on-chip (SOC), a single-board computer system (SBC) (such as, for example, a computer-on-module (COM) or system-on-module (SOM)), a desktop computer system, a laptop or notebook computer system, an interactive kiosk, a mainframe, a mesh of computer systems, a mobile telephone, a personal digital assistant (PDA), a server, a tablet computer system, an augmented/virtual reality device, or a combination of two or more of these. Where appropriate, computer system 1200 may include one or more computer systems 1200; be unitary or distributed; span multiple locations; span multiple machines; span multiple data centers; or reside in a cloud, which may include one or more cloud components in one or more networks. Where appropriate, one or more computer systems 1200 may perform without substantial spatial or temporal limitation one or more steps of one or more methods described or illustrated herein. As an example and not by way of limitation, one or more computer systems 1200 may perform in real time or in batch mode one or more steps of one or more methods described or illustrated herein. One or more computer systems 1200 may perform at different times or at different locations one or more steps of one or more methods described or illustrated herein, where appropriate.\n\n(60) In particular embodiments, computer system 1200 includes a processor 1202, memory 1204, storage 1206, an input/output (I/O) interface 1208, a communication interface 1210, and a bus 1212. Although this disclosure describes and illustrates a particular computer system having a particular number of particular components in a particular arrangement, this disclosure contemplates any suitable computer system having any suitable number of any suitable components in any suitable arrangement.\n\n(61) In particular embodiments, processor 1202 includes hardware for executing instructions, such as those making up a computer program. As an example and not by way of limitation, to execute instructions, processor 1202 may retrieve (or fetch) the instructions from an internal register, an internal cache, memory 1204, or storage 1206; decode and execute them; and then write one or more results to an internal register, an internal cache, memory 1204, or storage 1206. In particular embodiments, processor 1202 may include one or more internal caches for data, instructions, or addresses. This disclosure contemplates processor 1202 including any suitable number of any suitable internal caches, where appropriate. As an example and not by way of limitation, processor 1202 may include one or more instruction caches, one or more data caches, and one or more translation lookaside buffers (TLBs). Instructions in the instruction caches may be copies of instructions in memory 1204 or storage 1206, and the instruction caches may speed up retrieval of those instructions by processor 1202. Data in the data caches may be copies of data in memory 1204 or storage 1206 for instructions executing at processor 1202 to operate on; the results of previous instructions executed at processor 1202 for access by subsequent instructions executing at processor 1202 or for writing to memory 1204 or storage 1206; or other suitable data. The data caches may speed up read or write operations by processor 1202. The TLBs may speed up virtual-address translation for processor 1202. In particular embodiments, processor 1202 may include one or more internal registers for data, instructions, or addresses. This disclosure contemplates processor 1202 including any suitable number of any suitable internal registers, where appropriate. Where appropriate, processor 1202 may include one or more arithmetic logic units (ALUs); be a multi-core processor; or include one or more processors 1202. Although this disclosure describes and illustrates a particular processor, this disclosure contemplates any suitable processor.\n\n(62) In particular embodiments, memory 1204 includes main memory for storing instructions for processor 1202 to execute or data for processor 1202 to operate on. As an example and not by way of limitation, computer system 1200 may load instructions from storage 1206 or another source (such as, for example, another computer system 1200) to memory 1204. Processor 1202 may then load the instructions from memory 1204 to an internal register or internal cache. To execute the instructions, processor 1202 may retrieve the instructions from the internal register or internal cache and decode them. During or after execution of the instructions, processor 1202 may write one or more results (which may be intermediate or final results) to the internal register or internal cache. Processor 1202 may then write one or more of those results to memory 1204. In particular embodiments, processor 1202 executes only instructions in one or more internal registers or internal caches or in memory 1204 (as opposed to storage 1206 or elsewhere) and operates only on data in one or more internal registers or internal caches or in memory 1204 (as opposed to storage 1206 or elsewhere). One or more memory buses (which may each include an address bus and a data bus) may couple processor 1202 to memory 1204. Bus 1212 may include one or more memory buses, as described below. In particular embodiments, one or more memory management units (MMUs) reside between processor 1202 and memory 1204 and facilitate accesses to memory 1204 requested by processor 1202. In particular embodiments, memory 1204 includes random access memory (RAM). This RAM may be volatile memory, where appropriate. Where appropriate, this RAM may be dynamic RAM (DRAM) or static RAM (SRAM). Moreover, where appropriate, this RAM may be single-ported or multi-ported RAM. This disclosure contemplates any suitable RAM. Memory 1204 may include one or more memories 1204, where appropriate. Although this disclosure describes and illustrates particular memory, this disclosure contemplates any suitable memory.\n\n(63) In particular embodiments, storage 1206 includes mass storage for data or instructions. As an example and not by way of limitation, storage 1206 may include a hard disk drive (HDD), a floppy disk drive, flash memory, an optical disc, a magneto-optical disc, magnetic tape, or a Universal Serial Bus (USB) drive or a combination of two or more of these. Storage 1206 may include removable or non-removable (or fixed) media, where appropriate. Storage 1206 may be internal or external to computer system 1200, where appropriate. In particular embodiments, storage 1206 is non-volatile, solid-state memory. In particular embodiments, storage 1206 includes read-only memory (ROM). Where appropriate, this ROM may be mask-programmed ROM, programmable ROM (PROM), erasable PROM (EPROM), electrically erasable PROM (EEPROM), electrically alterable ROM (EAROM), or flash memory or a combination of two or more of these. This disclosure contemplates mass storage 1206 taking any suitable physical form. Storage 1206 may include one or more storage control units facilitating communication between processor 1202 and storage 1206, where appropriate. Where appropriate, storage 1206 may include one or more storages 1206. Although this disclosure describes and illustrates particular storage, this disclosure contemplates any suitable storage.\n\n(64) In particular embodiments, I/O interface 1208 includes hardware, software, or both, providing one or more interfaces for communication between computer system 1200 and one or more I/O devices. Computer system 1200 may include one or more of these I/O devices, where appropriate. One or more of these I/O devices may enable communication between a person and computer system 1200. As an example and not by way of limitation, an I/O device may include a keyboard, keypad, microphone, monitor, mouse, printer, scanner, speaker, still camera, stylus, tablet, touch screen, trackball, video camera, another suitable I/O device or a combination of two or more of these. An I/O device may include one or more sensors. This disclosure contemplates any suitable I/O devices and any suitable I/O interfaces 1208 for them. Where appropriate, I/O interface 1208 may include one or more device or software drivers enabling processor 1202 to drive one or more of these I/O devices. I/O interface 1208 may include one or more I/O interfaces 1208, where appropriate. Although this disclosure describes and illustrates a particular I/O interface, this disclosure contemplates any suitable I/O interface.\n\n(65) In particular embodiments, communication interface 1210 includes hardware, software, or both providing one or more interfaces for communication (such as, for example, packet-based communication) between computer system 1200 and one or more other computer systems 1200 or one or more networks. As an example and not by way of limitation, communication interface 1210 may include a network interface controller (NIC) or network adapter for communicating with an Ethernet or other wire-based network or a wireless NIC (WNIC) or wireless adapter for communicating with a wireless network, such as a WI-FI network. This disclosure contemplates any suitable network and any suitable communication interface 1210 for it. As an example and not by way of limitation, computer system 1200 may communicate with an ad hoc network, a personal area network (PAN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), or one or more portions of the Internet or a combination of two or more of these. One or more portions of one or more of these networks may be wired or wireless. As an example, computer system 1200 may communicate with a wireless PAN (WPAN) (such as, for example, a BLUETOOTH WPAN), a WI-FI network, a WI-MAX network, a cellular telephone network (such as, for example, a Global System for Mobile Communications (GSM) network), or other suitable wireless network or a combination of two or more of these. Computer system 1200 may include any suitable communication interface 1210 for any of these networks, where appropriate. Communication interface 1210 may include one or more communication interfaces 1210, where appropriate. Although this disclosure describes and illustrates a particular communication interface, this disclosure contemplates any suitable communication interface.\n\n(66) In particular embodiments, bus 1212 includes hardware, software, or both coupling components of computer system 1200 to each other. As an example and not by way of limitation, bus 1212 may include an Accelerated Graphics Port (AGP) or other graphics bus, an Enhanced Industry Standard Architecture (EISA) bus, a front-side bus (FSB), a HYPERTRANSPORT (HT) interconnect, an Industry Standard Architecture (ISA) bus, an INFINIBAND interconnect, a low-pin-count (LPC) bus, a memory bus, a Micro Channel Architecture (MCA) bus, a Peripheral Component Interconnect (PCI) bus, a PCI-Express (PCIe) bus, a serial advanced technology attachment (SATA) bus, a Video Electronics Standards Association local (VLB) bus, or another suitable bus or a combination of two or more of these. Bus 1212 may include one or more buses 1212, where appropriate. Although this disclosure describes and illustrates a particular bus, this disclosure contemplates any suitable bus or interconnect.\n\n(67) Herein, a computer-readable non-transitory storage medium or media may include one or more semiconductor-based or other integrated circuits (ICs) (such, as for example, field-programmable gate arrays (FPGAs) or application-specific ICs (ASICs)), hard disk drives (HDDs), hybrid hard drives (HHDs), optical discs, optical disc drives (ODDs), magneto-optical discs, magneto-optical drives, floppy diskettes, floppy disk drives (FDDs), magnetic tapes, solid-state drives (SSDs), RAM-drives, SECURE DIGITAL cards or drives, any other suitable computer-readable non-transitory storage media, or any suitable combination of two or more of these, where appropriate. A computer-readable non-transitory storage medium may be volatile, non-volatile, or a combination of volatile and non-volatile, where appropriate.\n\n(68) Herein, \u201cor\u201d is inclusive and not exclusive, unless expressly indicated otherwise or indicated otherwise by context. Therefore, herein, \u201cA or B\u201d means \u201cA, B, or both,\u201d unless expressly indicated otherwise or indicated otherwise by context. Moreover, \u201cand\u201d is both joint and several, unless expressly indicated otherwise or indicated otherwise by context. Therefore, herein, \u201cA and B\u201d means \u201cA and B, jointly or severally,\u201d unless expressly indicated otherwise or indicated otherwise by context.\n\n(69) The scope of this disclosure encompasses all changes, substitutions, variations, alterations, and modifications to the example embodiments described or illustrated herein that a person having ordinary skill in the art would comprehend. The scope of this disclosure is not limited to the example embodiments described or illustrated herein. Moreover, although this disclosure describes and illustrates respective embodiments herein as including particular components, elements, feature, functions, operations, or steps, any of these embodiments may include any combination or permutation of any of the components, elements, features, functions, operations, or steps described or illustrated anywhere herein that a person having ordinary skill in the art would comprehend. Furthermore, reference in the appended claims to an apparatus or system or a component of an apparatus or system being adapted to, arranged to, capable of, configured to, enabled to, operable to, or operative to perform a particular function encompasses that apparatus, system, component, whether or not it or that particular function is activated, turned on, or unlocked, as long as that apparatus, system, or component is so adapted, arranged, capable, configured, enabled, operable, or operative. Additionally, although this disclosure describes or illustrates particular embodiments as providing particular advantages, particular embodiments may provide none, some, or all of these advantages."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 27,
      "claims_start": 26,
      "description_end": 26,
      "description_start": 15,
      "drawings_end": 14,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 21,
      "number_of_drawing_sheets": 12,
      "number_of_figures": 13,
      "page_count": 27,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 26,
      "specification_start": 15,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006811649,
    "field_of_search_cpc": [
      "G06F 3/011",
      "G06F 9/451",
      "G06F 3/0482"
    ],
    "group_art_unit": "2143",
    "guid": "US-11556169-B2",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G69/561/115",
    "intl_class_current_primary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "3/01",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "3/0482",
        "version": "2013-01-01"
      },
      {
        "intl_class": "G06F",
        "intl_subclass": "9/451",
        "version": "2018-01-01"
      }
    ],
    "intl_class_issued": [
      "G06F3/01",
      "G06F3/0482"
    ],
    "inventors": [
      {
        "city": "South San Francisco",
        "country": "US",
        "name": "Wallen; Nicholas",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Los Gatos",
        "country": "US",
        "name": "Luther; Matthew",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Francisco",
        "country": "US",
        "name": "Melchiori; Paulo",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Francisco",
        "country": "US",
        "name": "Mesguich Havilio; Amir",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Burlingame",
        "country": "US",
        "name": "Choo; Amber",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Hayward",
        "country": "US",
        "name": "Mullen; Jesse John",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Wallen; Nicholas et al.",
    "legal_firm_name": [
      "Baker Botts L.L.P."
    ],
    "npl_references": [
      {
        "citation": "International Search Report and Written Opinion for International Application No. PCT/US2022/016110, dated Apr. 29, 2022, 12 pages.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "Adaptable personal user interfaces in cross-application virtual reality settings",
    "primary_examiner": "Spratt; Beau D",
    "publication_date": "2023-01-17",
    "publication_number": "11556169",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": true,
        "patentee_name": "Liu",
        "pub_month": "2013-02-01",
        "publication_number": "20130044128"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Mattingly",
        "pub_month": "2018-02-01",
        "publication_number": "20180040044"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Valdivia",
        "pub_month": "2018-04-01",
        "publication_number": "20180098059"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Roman et al.",
        "pub_month": "2018-07-01",
        "publication_number": "20180207522"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kocharlakota et al.",
        "pub_month": "2019-05-01",
        "publication_number": "20190139321"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Furtwangler",
        "pub_month": "2019-11-01",
        "publication_number": "20190340818"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Tran",
        "pub_month": "2020-01-01",
        "publication_number": "20200020165"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Murali",
        "pub_month": "2020-07-01",
        "publication_number": "20200218342"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "DiVerdi",
        "pub_month": "2020-07-01",
        "publication_number": "20200241730"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Mirhosseini et al.",
        "pub_month": "2020-08-01",
        "publication_number": "20200258278"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Oser",
        "pub_month": "2021-01-01",
        "publication_number": "20210005022"
      }
    ]
  },
  {
    "app_filing_date": "2021-08-23",
    "appl_id": "17409385",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Dumfries",
        "country": "US",
        "name": "QR-me, LLC",
        "state": "VA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Dumfries",
        "country": "US",
        "name": "QR-ME, LLC",
        "postal_code": "N/A",
        "state": "VA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Mitchell; William Gray"
    ],
    "composite_id": "1000005939959!US-US-11556727",
    "cpc_inventive": [
      {
        "cpc_class": "G06K",
        "cpc_subclass": "7/1417",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06K",
        "cpc_subclass": "19/06037",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "The invention is a system allowing an individual user to display a readable unique QR code, which code can be read by the mobile devices of customers, whose devices will thereby be connected to an online user profile.",
      "brief": "CROSS REFERENCE TO RELATED APPLICATIONS\n\n(1) Not applicable.\n\nSTATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT\n\n(2) No federal government funds were used in researching or developing this invention.\n\nNAMES OF PARTIES TO A JOINT RESEARCH AGREEMENT\n\n(3) Not applicable.\n\nSEQUENCE LISTING INCLUDED AND INCORPORATED BY REFERENCE HEREIN\n\n(4) Not applicable.\n\nBACKGROUND\n\nField of the Invention\n\n(5) The invention is a system allowing an individual user to display a readable holographically displayed, projection displayed, or physically displayed QR code within proximity to the user, which code can be read visually, or by the mobile devices of potential customers or interested individuals, whose devices will then be connected to the user's online profile.\n\nBackground of the Invention\n\n(6) The use of QR codes directing users in public settings to information available on websites is well known. Such codes are increasingly used in retail and hospitality settings, where patrons are presented with the codes on signage, handout materials or otherwise and directed to scan the code with their mobile devices to access and/or download items such as product specifications, menus, pricing, etc.\n\n(7) In its standard usage, the term QR code is abbreviated from Quick Response code, and is a type of matrix, two-dimensional barcode. A barcode is a machine-readable optical label that contains information about the item to which it is attached. In practice, QR codes often contain data for a locator, identifier, or tracker that points to a website or application. A QR code uses four standardized encoding modes (numeric, alphanumeric, byte/binary, and kanji) to store data efficiently. A QR code consists of black squares arranged in a square grid on a white background, which can be read by an imaging device, such as a camera, and processed using Reed-Solomon error correction until the image can be appropriately interpreted. The required data is then extracted from patterns that are present in both horizontal and vertical components of the image.\n\n(8) The Quick Response system has become popular due to its fast readability and greater storage capacity compared to standard UPC barcodes. Applications include product tracking, item identification, time tracking, document management, and general marketing. In this particular application, a tattoo, or branding on an item of clothing can be utilized to communicate the individual's unique QR Code, which can and will be read by interested individuals with a simple QR reader application that is readily available as a Smart Phone app.\n\n(9) Holography is a technique that enables a wavefront to be recorded and later re-constructed. Holography is best known as a method of generating three-dimensional images, but it also has a wide range of other applications. In principle, it is possible to make a hologram for any type of wave.\n\n(10) A hologram, also known as a holograph, is made by superimposing a second wavefront (normally called the reference beam) on the wavefront of interest, thereby generating an interference pattern which is visable and recorded as a physical medium. When only the second wavefront illuminates the interference pattern, it is diffracted to recreate an original wavefront. Holograms can also be computer-generated by modelling the two wavefronts and adding them together digitally. Until recently, the resulting digital image has then been printed or projected onto a suitable screen, mask or film and illuminated by a suitable source to reconstruct the wavefront of interest.\n\n(11) The creation of three-dimensional projection now opens new possibilities for the use of QR codes. In general, a 3D projection (or graphical projection) is a design technique used to display a three-dimensional (3D) object on a two-dimensional (2D) surface. These projections rely on visual perspective and aspect analysis to project a complex object for viewing capability on a simpler plane.\n\n(12) 3D projections use the primary qualities of an object's basic shape to create a map of points, that are then connected to one another to create a visual element. The result is a graphic that contains conceptual properties to interpret that the figure or image as not actually flat (2D), but rather, as a solid object (3D) being viewed on a 2D display.\n\n(13) 3D objects are largely displayed on two-dimensional mediums (i.e. screens). As such, graphical projections are a commonly used design element; notably, in engineering drawing, drafting, and computer graphics. Projections can be calculated through employment of mathematical analysis and formulae, or by using various geometric and optical techniques.\n\n(14) While holographic projections have typically required the use of some type of screen-type feature, technology now exists to create holographic displays utilizing light diffraction to create virtual three-dimensional images that require neither screens nor special glasses for viewers to see the image. New holographic display technologies include:\n\n(15) Laser plasma. Laser plasma displays utilize a series of lasers that focus light in desired positions in order to create plasma excitations with the oxygen and nitrogen molecules in the air. This type of holographic display is capable of producing images in thin air, without the need for any sort of screen or external refraction media. The laser plasma display is able to depict very bright and visible objects, but it lacks in terms of resolution and picture quality.\n\n(16) Micromagnetic piston display. The piston display, invented by Belgian company IMEC in 2011, utilizes a MEMS (micro-electro-mechanical system) based structure. In this type of display, thousands of microscopic pistons are able to be manipulated up and down to act as pixels, which in turn reflect light with a desired wavelength to represent an image. This developing technology is currently in the prototype phase, as IMEC is still developing the mechanism that will mobilize their \u201cpixels\u201d more effectively. Some of the limitations of this type of this display include the high cost, difficulty of creating large screens, and its susceptibility to mechanical failures due to the relatively large amount of moving parts (microscopic pistons).\n\n(17) Holographic television display. The holographic television display uses a three-dimensional camera (originally, a Microsoft Kinect\u00ae camera) as a relatively effective way to capture subjects in a three-dimensional space. The image is then processed by a PC graphics card and replicated with a series of laser diodes. The produced image is fully 3-dimensional and can be viewed from all 360 degrees to gain spatial perspective. The technology is now deep into commercial development and is expected to soon be offered with pricing similar to that of ordinary consumer HDTVs.\n\n(18) Touchable holograms. Touchable hologram technology is the closest modern representation of the holographic displays that one might see in sci-fi movies. This display is unique in that it can detect a user's touch by sensing movements in the air. The device then provides haptic feedback to the user by sending an ultrasonic air blast in return. In Intel's demonstration of this technology, the display was showcased representing a touchless, responsive piano. A possible implementation for this technology would be interactive displays in public kiosks; because this type of display does not require a user to physically touch a screen, it ensures that bacteria and viruses do not get transmitted from person to person.\n\n(19) In particular, the RED Hydrogen One smart phones base their holographic projection capabilities on a technology called \u201cDiffractive Lightfield Backlighting\u201d, which functions by placing a \u201cnanostructured light guide plate\u201d beneath an LCD phone screen. This provides four simultaneous images to be viewed together, and the resulting projected image is thus sometimes referred to as a \u201c4V\u201d or \u201c4 View\u201d image.\n\n(20) The more recent IKIN RYZ hologram accessory boasts an ability to convert 2D images into a 3D hologram, projected into the air directly adjacent to the device. The system uses a set of APIs and a SDK, along with a proprietary software technology called \u201cneuroadaptive AI\u201d, which comprises a series of neural networks. The device will also use a scalar lens technology allowing holographic images to be displayed well in normal ambient light onto a dual-pane display screen attached to a mobile phone.\n\n(21) Pok\u00e9mon Go uses a free downloadable app, and has successfully sent millions of people worldwide on digital scavenger hunts to hunt down and collect cartoon characters. It uses the camera and GPS system on an Android or iPhone handset to digitally superimpose animated creatures on top of whatever scenery appears on the smartphone's screen when its camera scans the surroundings. This use of augmented reality is often termed \u201clocation-based superimposition.\u201d Essentially, this augmented reality provides smartphone-based technology overlaying digital images atop the real world based upon location.\n\n(22) Google Glass is a similar technology that allows users to see text and images floating before their eyes. This enables users to superimpose images over reality (e.g., visualize a sofa in the living room before buying it, or visualize arrows on the ground in a virtual scavenger hunt). This type of application is generally termed \u201cmixed reality.\u201d At present, new technologies are raising expectations for augmented and mixed reality beyond holding your phone out to see a physical object on the street corner.\n\n(23) The field of holography is highly active and newer and better versions of personal holographic displays are being developed and marketed quickly.\n\n(24) What is needed is a system and method for generating a unique, personal QR Code for an individual, preferably using holographic display projections, thus allowing access to customers or other parties in the proximity of the QR Code user, either by scanning a visually available Code or by providing such code directly from the user's device to the customer's device and thereby directing the customer to the user's webpage.\n\nBRIEF SUMMARY OF THE INVENTION\n\n(25) In a preferred embodiment, a system comprising a projection device with a projector held or worn by a user, a customer mobile device with a camera and mobile application connecting the customer mobile device to a server algorithm located on a memory residing on one or more internet servers, wherein the projector projects a visible image containing a unique QR Code that is read by the camera, providing the customer with a link to a user profile on the customer mobile device.\n\n(26) In another preferred embodiment, the system described above, wherein the projection device is a smart phone or similar mobile device.\n\n(27) In another preferred embodiment, the system described above, wherein the visible image is projected onto the user's clothing or skin or displayed virtually.\n\n(28) In another preferred embodiment, the system described above, wherein the projection device is a mobile device and the visible image is a holographic display.\n\n(29) In another preferred embodiment, the system described above, wherein the holographic display is projected onto a two-pane or three-dimensional screen screen.\n\n(30) In another preferred embodiment, the system described above, wherein the holographic display is projected onto a two-dimensional screen using 3D projection.\n\n(31) In another preferred embodiment, the system described above, wherein the holographic display is projected into the air.\n\n(32) In another preferred embodiment, the system described above, wherein the user profile contains links to web pages containing one or more of product, service, vendor or purchase information.\n\n(33) In an alternate preferred embodiment, an alternate system comprising a projection device held or worn by a user and a customer mobile device, a server algorithm located on a memory residing on one or more internet servers, wherein each of the projection device and customer mobile device comprise a memory containing a mobile application for connection to the server algorithm and GPS capability, whereby the mobile application on the customer device receives an alert and notifies the customer of the proximity of the projection device and the user and provides a photo of the user and a link to a user profile page.\n\n(34) In another preferred embodiment, the alternate system described above, wherein the projection device is a smart phone or similar mobile device.\n\n(35) another preferred embodiment, the alternate system described above, wherein the algorithm uses location-based superimposition to provide the customer with a digital image signifying the user together with the alert.\n\n(36) In another preferred embodiment, a method of using the system disclosed herein above, comprising the steps of: 1. downloading a mobile application onto the projection device; 2. obtaining a unique QR Code for the user; 3. creating a user profile on the mobile application; 4. associating the unique QR Code with the user profile; 5. projecting the unique QR Code as a visible image in proximity to the user; 6. reading the unique QR Code with the camera of the customer mobile device; and 7. linking the customer to the user profile on the customer mobile device.\n\n(37) In another preferred embodiment, the method previously disclosed, wherein the visual image of step 5 is a holographic or similar multidimensional image.\n\n(38) In another preferred embodiment, the method previously disclosed, further comprising the step: 8. further linking the customer to a third party vendor web site via the user profile.",
      "claim_statement": "I claim:",
      "claims": "1. A system comprising a projection device with a projector held or worn by a user, a customer mobile device with a camera and mobile application connecting the customer mobile device to a server algorithm located on a memory residing on one or more internet servers, wherein the projector projects a visible image containing a unique QR Code that is read by the camera, providing the customer with a link to a user profile on the customer mobile device, and wherein the algorithm uses location-based superimposition to provide the user with a digital image signifying the user together with an alert. \n\n2. The system of claim 1, wherein the projection device is a smart phone or similar mobile device. \n\n3. The system of claim 1, wherein the visible image is projected onto the user's clothing or skin or displayed virtually. \n\n4. The system of claim 1, wherein the projection device is a mobile device and the visible image is a holographic display. \n\n5. The system of claim 4, wherein the holographic display is projected onto a two-pane or three-dimensional screen screen. \n\n6. The system of claim 4, wherein the holographic display is projected onto a two-dimensional screen using 3D projection. \n\n7. The system of claim 4, wherein the holographic display is projected into the air. \n\n8. The system of claim 1, wherein the user profile contains links to web pages containing one or more of product, service, vendor or purchase information. \n\n9. A system comprising a projection device held or worn by a user and a customer mobile device, a server algorithm located on a memory residing on one or more internet servers, wherein each of the projection device and customer mobile device comprise a memory containing a mobile application for connection to the server algorithm and GPS, whereby the mobile application on the customer device receives an alert and notifies the customer of the proximity of the projection device and the user and provides a photo of the user and a link to a user profile page, wherein the algorithm uses location-based superimposition to provide the customer with a digital image signifying the user together with the alert. \n\n10. The system of claim 9, wherein the projection device is a smart phone or similar mobile device and the projection device projects a visible image containing a QR Code. \n\n11. A method of using the system of claim 9, comprising the steps of: (1) downloading a mobile application onto the projection device; (2) obtaining a unique QR Code for the user; (3) creating a user profile on the mobile application; (4) associating the unique QR Code with the user profile; (5) projecting the unique QR Code as a visible image in proximity to the user; (7) reading the unique QR Code with the camera of the customer mobile device; a (8) linking the customer to the user profile on the customer mobile device.  \n\n12. The method of claim 11, wherein the visual image of step 5 is a holographic or similar multidimensional image. \n\n13. The method of claim 11, further comprising the step 9 the customer to a third party vendor website via the user profile.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) FIG. 1 is a line drawing of a holographic display QR Code user broadcasting his personal QR Code.\n\n(2) FIG. 2 is a line drawing of a passerby customer reading a user QR Code using the customer's mobile device.\n\n(3) FIG. 3 is a line drawing of a customer mobile device scanning a user and projecting QR code link to the user's profile.\n\n(4) FIG. 4 is a flowchart evidencing steps in a method of use of the QR Code system described herein.\n\nDETAILED DESCRIPTION OF THE INVENTION\n\n(5) The invention constitutes a QR Code-based communication system with methods of manufacture and use wherein a user creates and manages a personal QR Code associated with his or her person, which code is either visually or holographically readable to customers in his or her proximity. The user will also create a profile on a software platform to which the QR Code can be linked, with such platform profile allowing a linkage between the code, when scanned, and preloaded personal information about the user, as well as links to advertisers and/or product and services endorsed by or affiliated with the user. A point of innovation in each iteration of the disclosed system and method is the use of a unique, personalized QR Code for influencer/displayer users and a related customer-reader interface that can be accessed via a mobile application.\n\n(6) To begin operating under the disclosed system, a user will open a profile on the web-based software platform, either through an application downloaded to a mobile device, a computer or other similar device. The user will be prompted to input personal information to identify the user as well as list products, services and companies or other individuals liked or used by the user. The user may use the platform to link with the profiles of other individuals and companies, which links will then be available to any customer reading the user's QR Code.\n\n(7) In a preferred embodiment, upon creating a profile, the user will be prompted to create a personal QR Code, either through a service within the platform itself or by linking to an outside code creation resource such as www.QRStuff.com, where the user will be guided through a step-by-step process for QR code creation.\n\n(8) Once created, the user's unique QR Code will become available for broadcast by the user to customers in the user's proximity. Options for such broadcast will include projection of the code onto the user or an item held or worn by the user, such as a patch or piece of clothing with a sufficient surface, or through a holographic image projected onto a three-dimensional (e.g., two paned) screen, a superimposed QR Code that remains in proximity with the users smart phone and which can be read with the QR-me reader, a QR Code broadcast that can be matched with the correct displayer by a chime or color change signaling the correct person and then accessed, or a holographic image projected into the air or onto a screen, created by and visible in proximity to a mobile device carried or worn by the user.\n\n(9) In a preferred embodiment, a system comprising an IKIN RYZ-styled holographic image or Google Glass image display will project the holographic QR Code image within near proximity of the influencer/displayers smart phone. Any individual with the QR-me reader application on their smart phone will be able to read the holographic QR Code and follow the links (e.g., to a website, Instagram, FaceBook, or other social media and/or private link) to whatever information the influencer/displayer wants to share. Otherwise, a projected holographic image of the QR Code (using current light projection display technology), or printed rendition of the personal QR Code will be used to accomplish the same end.\n\n(10) With the light projection display technology, the user will intentionally project his or her QR code onto their shirt or some fixed object, and any individual with a QR reader on their phone will be able to access the information and immediately load the information directed by that QR Code. With the printed rendition of the QR Code the displayer will order press-on, or iron-on decals from QR-me that will be placed on an item of clothing that will permit anyone with a QR reader to access the QR file.\n\n(11) In one iteration, the projection device will be located in an area close to but removed from the user's torso, such as on the user's arm or on a handheld bag, such that the projection device is able to project the QR Code onto the user's arm, chest and/or back. In such an embodiment, the user's skin, shirt or jacket itself may function as a type of screen, or may incorporate a patch or other screen-like surface. In one particular version, the projector device will be a smart watch or bracelet comprising a projection feature which projects an image directly onto the adjacent portion of the user's arm.\n\n(12) In another iteration, the projection device will be configured to produce a holographic display on a three-dimensional screen, such as a two-paned screen integrated into or attached to a mobile device. Alternatively, the three-dimensional screen may be located on the user's body or otherwise be in near enough proximity to the projection device to form a holographic or similar multidimensional image. In such an embodiment, the projection device will incorporate a projector feature that projects 2, 4 or more images simultaneously, which are captured on a multilevel screen and thus provide a three-dimensional visual image.\n\n(13) In a preferred embodiment, each individual will have the ability to customize the appearance of his or her own QR code projection. Possible customizations include variations in color, size, or the introduction of text and/or animation to the projected code.\n\n(14) For example, AT&T and Verizon each offer a holographic smartphone that will project stored images into the air above the screen of the smartphone, using technology as described herein. The Red Hydrogen One\u00ae smartphone is the first phone from video equipment company Red. The Android\u00ae phone features a holographic display that projects 3D images that can be viewed without special glasses.\n\n(15) In another preferred embodiment, the QR code will be three-dimensional, embodied as a cube or another 3D geometric shape or image. This will make the image scannable from multiple angles to passersby, or potentially be viewable from the side but only scannable from a single angle, such as the front. Such a 3D projection can be made using standard 3D projection technology, or holographically as described herein.\n\n(16) Once projected, the user's unique QR code image will become readable to customers via their own mobile devices using existing QR reader technology, just as QR codes currently employed by shops and restaurants are now readable to provide customers with immediate access to product, store or menu information. Within the described system, however, the customer will instead be forwarded via a web link to the system software plaform connecting him or her to the user's profile and thereby to all other profiles or advertisements connected to such profile. Such profile will constitute one or more web pages containing information and images related to the user and the user's preferred products, services, designs, vendors, etc., with hyperlinks for jumping to associated third party websites and pages. As holographic display technology advances, the size and quality of holographic images broadcast by mobile devices will also increase, allowing for holographic QR Codes to be identified by customers and read by customer devices at ever greater distances.\n\n(17) Since customers have already become accustomed to the action of using their mobile devices to read QR Codes and obtain links in public spaces, they will recognize the presence of a QR Code on or near the user's person. Thus, should the customer be interested in the appearance of the user, whether due to dress, hair, cosmetics, fitness or otherwise, the customer may be inclined to scan the user's unique QR code to obtain information on what apparel the user wears, what diet or fitness regimen the user employs, what gym or hair salon the user visits, or any other of a myriad of potential products and services that might be attractive to the customer. By accessing the user's profile and the links contained therein, the customer will not only be able to ascertain the brands of the various products and services related to the user, but will also have the option of being forwarded directly to vendor profiles and/or websites enabling the customer to immediately make appointments or purchases.\n\n(18) In one embodiment, the user's unique QR Code will be generated by a smart phone or other mobile device held in the user's hand, such device sometimes referred to herein as the \u201cprojection device\u201d. In other embodiments, the projection device may be worn on the user's person. Examples of wearable devices may be a smart watch, a smart phone inserted into an armband, or similar devices worn on neck cords, shoulder pads, as well as pockets or straps incorporated into the user's clothing, a bag carried by the user, or a similar arrangement.\n\n(19) While a projected QR Code image that is visible to the naked eye may be used, it is also possible for the projected QR Code to be virtual in nature. In one preferred embodiment, the unique QR Code may be stored in the application software and available for air drop or similar wireless communication to any customer who scans the crowd in search of a QR Code. When the QR reader of the customer mobile device aligns with the user, the smartphone will signal alignment through a written alert, sound or color change that user's information profile can be accessed. Such an embodiment may also provide a notification to a customer whenever a user is in proximity, regardless of active scanning by the customer.\n\n(20) Such embodiment may use location-based superimposition, similar to that employed by Pokemon Go and in lieu of QR cCodes, whereby alignment of the customer mobile device with the individual user, and detection of the user's mobile device, accesses the user's mobile device GPS location and prompts an image or other notification to appear on the customer mobile device screen, such image including or appearing with a link to the user's profile page. Preferably, such image will be unique to the user. The connection between the customer's and user's mobile devices will be initiated via the application software, and may employ communication features such as air drop, Bluetooth or similar features already commonly available in current mobile devices.\n\n(21) Regarding the web-based software platform itself, in one embodiment, the platform will be executed using one or more internet servers, each comprising a processor and a memory containing an algorithm for establishing user profiles, such algorithm providing for connection with and access by a mobile application downloadable to individual mobile devices and personal computers.\n\n(22) In a preferred embodiment, a user will receive a notice or alert whenever his or her unique QR code is used to access the associated user profile. The mobile application software may also provide customers with the option to provide users with a customer photo to enable users to approach and converse with customers in person.\n\nDETAILED DESCRIPTION OF THE FIGURES\n\n(23) FIG. 1 shows a two-dimensional unique QR code 13 printed on a screen 11, such screen embodied as a shirt.\n\n(24) FIG. 2 shows a two-dimensional unique QR code 13 projected by projection device 10, embodied as a smart watch/bracelet worn on the wrist of user 1, onto a screen consisting of the user's arm.\n\n(25) FIG. 3 is a line drawing of a customer 2 holding a customer mobile device 21 scanning a unique QR Code 13 of user 1 and projecting QR code 13 onto the device screen 23, thereby initiating a link to the user's online profile.\n\n(26) FIG. 4 is a flow chart providing the sequential steps required for a method of using a unique QR code by an individual user to provide a customer access via the customer's mobile device to link to a user profile via the internet. In a preferred embodiment, the user profile will contain additional marketing information and/or links to other web pages containing marketing information for shopping and the purchase of products.\n\nLIST OF REFERENCE NUMBERS\n\n(27)  1 user 2 customer 10 projection device 11 screen 12 projection device projector 13 unique QR Code 14 3D unique QR Code 21 customer mobile device 22 camera 23 mobile device screen 30 user online profile\n\n(28) The references recited herein are incorporated herein in their entirety, particularly as they relate to teaching the level of ordinary skill in this art and for any disclosure necessary for the commoner understanding of the subject matter of the claimed invention. It will be clear to a person of ordinary skill in the art that the above embodiments may be altered or that insubstantial changes may be made without departing from the scope of the invention. Accordingly, the scope of the invention is determined by the scope of the following claims and their equitable equivalents."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 9,
      "claims_start": 8,
      "description_end": 8,
      "description_start": 5,
      "drawings_end": 4,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "number_of_claims": 13,
      "number_of_drawing_sheets": 3,
      "number_of_figures": 4,
      "page_count": 9,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 8,
      "specification_start": 5,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000005939959,
    "field_of_search_cpc": [
      "G06K 7/1417",
      "G06K 19/06037"
    ],
    "group_art_unit": "2887",
    "guid": "US-11556727-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G27/567/115",
    "intl_class_current_primary": [
      {
        "intl_class": "G06K",
        "intl_subclass": "7/14",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06K",
        "intl_subclass": "19/06",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06K7/14",
      "G06K19/06"
    ],
    "inventors": [
      {
        "city": "Dumfries",
        "country": "US",
        "name": "Geisler; James",
        "postal_code": "N/A",
        "state": "VA"
      }
    ],
    "inventors_short": "Geisler; James",
    "patent_title": "Personal user QR code-holographic system",
    "primary_examiner": "Johnson; Sonji N",
    "publication_date": "2023-01-17",
    "publication_number": "11556727",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": true,
        "patentee_name": "Thornton",
        "pub_month": "2020-09-01",
        "publication_number": "10762812"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "De Mattei",
        "pub_month": "2012-08-01",
        "publication_number": "20120204307"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Fletcher",
        "pub_month": "2013-05-01",
        "publication_number": "20130126596"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Macia",
        "pub_month": "2015-09-01",
        "publication_number": "20150254902"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Shors",
        "pub_month": "2019-12-01",
        "publication_number": "20190377330"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Falconer",
        "pub_month": "2020-03-01",
        "publication_number": "20200090224"
      }
    ]
  },
  {
    "app_filing_date": "2021-12-09",
    "appl_id": "17546565",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "San Francisco",
        "country": "US",
        "name": "Fitbit LLC",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "San Francsco",
        "country": "US",
        "name": "FITBIT LLC",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Dority & Manning, P.A."
    ],
    "composite_id": "1000006076133!US-US-11554372",
    "cpc_additional": [
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2300/0681",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2200/12",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2200/0605",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2200/16",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2300/0887",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2300/0864",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2300/126",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2400/043",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2200/0689",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G01N",
        "cpc_subclass": "33/54346",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G01N",
        "cpc_subclass": "33/54326",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2300/0663",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "2200/04",
        "version": "2013-01-01"
      }
    ],
    "cpc_inventive": [
      {
        "cpc_class": "B01L",
        "cpc_subclass": "3/502715",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "3/50273",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "3/502753",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01L",
        "cpc_subclass": "3/502707",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "This present disclosure provides devices, systems, and methods for performing point-of-care analysis of a target analyte in a biological fluid via a binding assay. The present disclosure includes a cartridge for collecting the target analyte contained in a fluid sample and performing an assay. The cartridge includes an assay stack having a first separation layer, a second separation layer, and a detection membrane. The cartridge also includes a plurality of first complexes comprising a capture molecule and a magnetic bead and a plurality of second complexes comprising a detection molecule and a detection label. Further, the detection membrane includes a substrate that interacts with the detection label to elicit a quantifiable response in the presence of the target analyte. The quantifiable response corresponds to an amount of detection antibody present in the detection membrane, and the amount of detection antibody present corresponds to an amount of the target analyte present.",
      "background": "PRIORITY CLAIM\n(1) The present application is a continuation of PCT International Patent Application No. PCT/US2021/041988 having a filing date of Jul. 16, 2021. Applicant claims priority to and the benefit of said application and incorporates said application herein by reference in its entirety.",
      "brief": "FIELD\n\n(1) The present disclosure relates generally to a point-of-care (POC) testing system. More particularly, the present disclosure relates to systems and methods for performing a binding assay without any wash steps, incubation steps, or moving parts.\n\nBACKGROUND\n\n(2) Point-of-care (POC) testing refers to performing medical diagnostic tests at the time and place that the patient is being treated. POC testing is advantageous over traditional diagnostic testing where patient samples are sent out to a laboratory for further analysis, because the results of traditional diagnostic tests may not be available for hours, if not days or weeks, making it difficult for a caregiver to assess the proper course of treatment in the interim.\n\n(3) Typically, when measuring certain chemical analytes in biological fluids such as blood, binding assays such as immunoassays are the gold standard for detecting such chemical analytes. However, binding assays are rarely, if ever, used in POC diagnostics because they conventionally require several wash steps and several incubation steps. This makes the binding assays difficult to incorporate into POC testing systems due to the complexity in conducting the binding assays properly and accurately in a POC environment.\n\n(4) For instance, designing POC testing systems for in-home use is particularly challenging, because such systems are often operated by people with limited training or no training at all. Current systems can often require the user to follow multiple steps of operations of multiple separated parts, where user-introduced errors can easily cause inaccurate or failed assays.\n\n(5) Further, in most POC testing systems for blood samples, certain sample preparation steps need to be performed prior to a final chemical reaction that provides the test result. These sample preparation steps may include complex preparation steps such as plasma separation, cell lysis, incubation, wash steps, or others, depending on the assay. The time required to complete such complex preparation steps may be comparable to the time required for blood to undergo undesirable clotting, which further introduces error into the assay results. While many attempts to solve this problem have been proposed or implemented, these solutions often employ complex fluidics or moving parts to create the necessary incubation times and wash steps, and such mechanisms result in increases in cost, failure rate, and complexity.\n\n(6) Thus, it would be desirable to have a POC system that can detect a target analyte using a binding assay that addresses the aforementioned problems.\n\nSUMMARY\n\n(7) Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.\n\n(8) One example aspect of the present disclosure is directed to a cartridge for collecting a target analyte contained in a biological fluid sample and performing an assay on the target analyte. The cartridge includes an assay stack having a first separation layer. The assay stack also includes a plurality of first complexes having a capture molecule and a magnetic bead; a plurality of second complexes having a detection molecule and a detection label; a second separation layer; and a detection membrane. The detection membrane includes a substrate that interacts with the detection label to elicit a quantifiable response in the presence of the target analyte. The quantifiable response corresponds to an amount of detection molecule present in the detection membrane, and the amount of detection molecule present in the detection membrane corresponds to an amount of the target analyte present in the fluid sample.\n\n(9) Another aspect of the present disclosure is directed to a method of fabricating a cartridge. The method includes, in no particular order, the steps of: applying a plurality of first complexes comprising a capture molecule and a magnetic bead and a plurality of second complexes comprising a detection molecule and a detection label to a first separation layer; allowing the plurality of first complexes and the plurality of second complexes to dry on the first separation layer; applying a substrate to a detection membrane; allowing the substrate to dry on the detection membrane; and positioning a second separation layer between the first separation layer and the detection membrane. Further, the substrate is configured to interact with the detection label to elicit a quantifiable response in the presence of a target analyte in a fluid sample that is introduced to the cartridge, the quantifiable response corresponds to an amount of detection molecule present in the detection membrane, and the amount of detection molecule present in the detection membrane corresponds to an amount of the target analyte present in the fluid sample.\n\n(10) Still another aspect of the present disclosure is directed to a system for collecting a target analyte contained in a fluid sample and performing an assay on the target analyte. The system includes an assay stack, wherein the assay stack comprises a first separation layer; a plurality of first complexes comprising a capture molecule and a magnetic bead; a plurality of second complexes comprising a detection molecule and a detection label; a second separation layer; and a detection membrane, wherein the detection membrane includes a substrate that interacts with the detection label to elicit a quantifiable response in the presence of the target analyte, wherein the quantifiable response corresponds to an amount of detection molecule present in the detection membrane, and wherein the amount of detection molecule present in the detection membrane corresponds to an amount of the target analyte present in the fluid sample; and an electromagnet for pulling a third complex comprising the target analyte bound to one of the first complexes and one of the second complexes through the second separation layer to the detection membrane.\n\n(11) Still another aspect of the present disclosure is directed to an in-vitro use of the proposed cartridge for performing an assay on a target analyte in an isolated fluid sample.\n\n(12) Yet another aspect of the present disclosure is directed to a use of a cartridge in a diagnostic method for performing an assay on a target analyte in an isolated fluid sample.\n\n(13) These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A cartridge for collecting a target analyte contained in a fluid sample and performing an assay on the target analyte, wherein the cartridge comprises: an assay stack, wherein the assay stack comprises: a first separation layer; a plurality of first complexes comprising a capture molecule and a magnetic bead; a plurality of second complexes comprising a detection molecule and a detection label; a second separation layer, wherein the second separation layer comprises a hydrophobic membrane, a low molecular weight cut-off membrane, or a combination thereof; and a detection membrane, wherein the detection membrane includes a substrate that interacts with the detection label to elicit a quantifiable response in the presence of the target analyte, wherein the quantifiable response corresponds to an amount of detection molecule present in the detection membrane, wherein the amount of detection molecule present in the detection membrane corresponds to an amount of the target analyte present in the fluid sample, wherein the second separation layer allows for a third complex comprising the target analyte bound to: (a) one of the first complexes and (b) one of the second complexes to pass to the detection membrane in the presence of an activated electromagnet.   \n\n2. The cartridge according to claim 1, wherein the second separation layer prevents passage of any unbound second complexes to the detection membrane. \n\n3. The cartridge according to claim 1, wherein the second separation layer includes a hydrophilic treatment. \n\n4. The cartridge according to claim 3, wherein the hydrophilic treatment is a coating that comprises a surfactant. \n\n5. The cartridge according to claim 1, wherein the target analyte is contained within a fluid sample selected from the group consisting of blood, saliva, sweat, urine, lymph, tears, synovial fluid, breast milk, serum, plasma, bile, or a component thereof. \n\n6. The cartridge according to claim 5, wherein the fluid sample is blood and the first separation layer is a plasma separation membrane that prevents erythrocytes from contacting the second separation layer. \n\n7. The cartridge according to any claim 1, wherein the detection label comprises a peroxidase enzyme. \n\n8. The cartridge according to claim 7, wherein the substrate comprises a reagent for the peroxidase enzyme. \n\n9. The cartridge according to claim 1, wherein the cartridge is configured to perform an assay on the target analyte without any wash steps or moving parts. \n\n10. The cartridge according to claim 1, wherein at least one component of the cartridge is compressible, thereby allowing for an uncompressed state and a compressed state of the cartridge. \n\n11. The cartridge according to claim 10, further comprising a metering stack configured to receive and distribute the fluid sample containing the target analyte along a channel of the cartridge, wherein the channel has a bottom that comprises a porous or mesh material, further wherein the metering stack includes one or more venting holes in communication with the channel. \n\n12. The cartridge according to claim 11, wherein a spacer material is disposed between the metering stack and the assay stack, wherein the spacer material provides a gap between the metering stack and the assay stack that prevents the target analyte from flowing from the metering stack into the assay stack when the cartridge is in the uncompressed state. \n\n13. The cartridge according to claim 11, wherein the porous or mesh material permits the target analyte to flow from the metering stack to the assay stack upon compression of the at least one component of the cartridge.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:\n\n(2) FIG. 1 provides a schematic drawing of a system comprising a cartridge and an assay reader according to one embodiment of the disclosure;\n\n(3) FIGS. 2A-2C illustrate an embodiment of the cartridge utilized in the system;\n\n(4) FIG. 3 illustrates various layers of a metering stack contained within the cartridge;\n\n(5) FIG. 4 illustrates various layers of an assay stack contained within the cartridge;\n\n(6) FIG. 5A shows a longitudinal cross-sectional view of an assay reader according to one embodiment of the present disclosure;\n\n(7) FIG. 5B shows a longitudinal cross-sectional view of an assay reader with an inserted cartridge according to one embodiment of the present disclosure;\n\n(8) FIG. 6A shows a transverse cross-sectional view of an assay reader according to one embodiment of the present disclosure;\n\n(9) FIG. 6B shows a transverse cross-sectional view of the assay reader with an inserted cartridge according to one embodiment of the present disclosure;\n\n(10) FIG. 7 shows a block diagram of the sensor system of the assay reader, according to an exemplary implementation of the present disclosure;\n\n(11) FIGS. 8A-8F illustrate a cartridge that includes a metering stack and an assay stack during various stages of the immunoassay process following the introduction to the cartridge of a fluid to be analyzed for the presence of a target analyte;\n\n(12) FIG. 9 shows a flow chart illustrating a method of using the assay system according to an exemplary implementation of the present disclosure; and\n\n(13) FIG. 10 shows a flow chart illustrating a method of manufacturing a cartridge according to one exemplary implementation of the present disclosure.\n\n(14) Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.\n\nDETAILED DESCRIPTION\n\n(15) Any of the features, components, or details of any of the arrangements or embodiments disclosed in this application, including without limitation any of the cartridge embodiments and any of the testing or assay embodiments disclosed below, are interchangeably combinable with any other features, components, or details of any of the arrangements or embodiments disclosed herein to form new arrangements and embodiments.\n\n(16) Generally, the present disclosure is directed to a device and a system for rapid POC detection of a target analyte contained in a biological fluid sample and the subsequent analysis of the target analyte via an immunoassay or other binding type assay that does not require any wash steps and that does not require any moving parts. The binding assay may also be performed without any incubation steps in some embodiments. The present disclosure also provides methods and systems for using the device to analyze the fluid sample via an immunoassay or other binding type assay to quantify the level of the target analyte that is present in the fluid sample.\n\n(17) The device can be in the form of a cartridge that includes an assay stack. The assay stack includes a first separation layer, a second separation layer, and a detection membrane containing a substrate that interacts with the detection label to elicit a quantifiable response. The second separation layer can be arranged between the first separation layer and the detection membrane. A plurality of first complexes that each include a capture molecule and a magnetic bead and a plurality of second complexes that each include a detection molecule and a detection label can be dried onto the first separation layer, where it is to be understood that the capture molecule and the detection molecule are chosen based on their ability to bind with the target analyte. Upon contact of a fluid sample with the first separation layer, any target analyte present in the fluid sample will couple with the first complexes and the second complexes to form one or more third complexes. In an exemplary embodiment, an electromagnet can be activated to pull any third complexes through the second separation layer to the detection membrane, while any unbound second complexes remain in the second separation layer. It is also to be understood that any unbound first complexes will also be pulled through the second separation layer to the detection membrane. However, because such unbound first complexes will not be coupled to a target analyte, detection molecule, or detection label, the presence of the unbound first complexes in the detection membrane will not affect the accuracy of the binding assay. Thereafter, the substrate can interact with the detection label to elicit a quantifiable response (e.g., colorimetric, fluorescent, electrochemical, etc.) in the presence of the target analyte. The quantifiable response can correspond to an amount of detection molecule present in the detection membrane, and the amount of detection molecule present in the detection membrane can correspond to an amount of the target analyte present in the fluid sample. It is to be understood that any binding assay known to one of ordinary skill in the art can be utilized in the systems and devices of the present disclosure, such as, but not limited to sandwich assays, competition assays, or labeled-antigen assays. Further, although immunoassays are described in the embodiments below, other detection and capture molecules in addition to antibodies are also contemplated by the present disclosure.\n\n(18) The proposed solution allows for providing a compact POC testing system capable of an in-vitro assay of an isolated (biological) fluid sample without wash and incubation steps and thus without the need for physical washing or complex moving parts in the POC system. A cartridge constructed as proposed with an assay stack comprising a second separation layer sandwiched between a first separation layer and a detection membrane in this context allows for a cost-efficient and\u2014compared to conventional POC testing systems\u2014less complex detection of a target analyte in a fluid sample. In combination with a proposed assay reader, the detection may be automated in an easy way, since any target analyte present in the fluid sample may join to the plurality of first complexes and the plurality of second complexes to create a third complex and the third complex can then be pulled through the assay stack at a specified point in time upon activation of an electromagnet of the assay reader. Based on the detection membrane allowing for qualitatively or even quantitatively determining the amount of target analyte in the sample fluid based on a resulting signal (e.g., a color change), also an automated assessment on the presence of target analyte in the fluid sample is possible.\n\n(19) Further, it should be understood that when the electromagnet is not activated, the first and second complexes have time to interact with the target analyte in the fluid sample before moving through the assay stack to the detection membrane once the electromagnet is activated to pull any third complexes through to the detection membrane. This allows for precise control of the fluid sample incubation time, where such precise control is not possible in many other assay platforms, much less with a physical washing step.\n\n(20) With reference now to the figures, example embodiments of the present disclosure will be discussed in further detail. First, the components of the cartridge and assay reader will be discussed, followed by the components used to perform an immunoassay as contemplated by the present disclosure.\n\n(21) FIG. 1 shows a POC testing system according to one exemplary embodiment of the present disclosure. The POC testing system comprises a cartridge 100 and an assay reader 110. As described herein, cartridge 100 is used to collect the biological sample that may potentially contain a target analyte. The collection process also distributes the target analyte within cartridge 100. After the target analyte is collected in cartridge 100, the user inserts cartridge 100 into assay reader 110. As described herein, the act of inserting cartridge 100 into assay reader 110 results in the compression of cartridge 100, thereby causing the target analyte to be distributed to a plurality of assay pads. In this way, the act of inserting cartridge 100 into assay reader 110 commences one or more assay reactions that provide information regarding the contents of the target analyte. However, it is also to be understood that other insertion approaches are contemplated that do not require compression. Further, it is to be understood that while multiple assays can be utilized to determine the contents of a target sample, each assay is generally specific for one particular target analyte. As described herein, assay reader 110 is equipped with a detection system that is used to detect the results of the one or more assay reactions that occur at one or more assay pads of cartridge 100. The detection system is not particularly limited and may be a detection system which causes a measurable signal change as the result of an assay reaction. Non-limiting examples of suitable detection systems include colorimetric, fluorescence, electrochemical, and optical detection systems as described herein and any other detection system that would be understood by one of ordinary skill in the art.\n\n(22) FIG. 2A illustrates a top, perspective view of an embodiment of cartridge 100 in the form of a cartridge 200. In FIG. 2A, cartridge 200 includes a housing 201 attached to a handle 202. In general, cartridge 200 is designed to be easy to handle by the user and to provide a protective shell for the microfluidic distribution system and assay components housed within cartridge 200. In general, suitable materials for housing 201 and handle 202 include polyolefinic compounds, such as polyethylene, polypropylene, and other polymeric resins or compounds known in the medical device manufacturing art. During sample collection, cartridge 200 is brought into contact with a target analyte in a fluid sample (e.g., blood). The target analyte is drawn into channel 203 and via channel opening 204 by capillary action. In some embodiments, channel 203 comprises a plurality of receiving chambers 205 located along channel 203. In some embodiments, each receiving chamber is positioned between two venting holes, which facilitate the division of the target analyte in the channel into multiple aliquots which flow to the assay pads in the assay stack. It should be recognized that the channel opening 204 can function as a venting hole and that neighboring receiving chambers can share a common venting hole between them. The venting holes, in combination with the porous or mesh material described herein, prevent unwanted bubble formation as the target analyte is drawn into the receiving chambers. FIG. 2B illustrates a bottom view of an embodiment of the cartridge 200. In FIG. 2B, the bottom portion of housing 201 comprises a plurality of assay detection ports 206 aligned with channel opening 204. The assay detection ports 206 permit the assay results to be interrogated, for example, by optical detection methods as described herein. In addition, the bottom portion of housing 201 may comprise plurality of holes 207, which are additional assay detection ports that may be used with assay components and microfluidic channels that are arranged in a corresponding configuration.\n\n(23) FIG. 2C provides an exploded view of the components of the cartridge 200, according to one embodiment of the present disclosure. In FIG. 2C, the outer shell of cartridge 200 includes the handle 202, bottom housing portion 227, and a cap 223 that is equipped with a slot 228. The bottom housing portion 227 can be a cuboid shape enclosure with one open side. The enclosure shape of the bottom housing portion 227 protects the components within the interior chamber and can avoid accidental actuation of the system. The cap 223 can fit to the open side of the bottom housing portion 227 and have a shape and size that corresponds to the open side of the bottom housing portion 227. When the bottom housing portion 227 and cap 223 of the housing are assembled together, an interior chamber can be formed for enclosing other components of the cartridge within the interior chamber. In other embodiments, the cap 223 and bottom housing portion 227 do not form an enclosure with an interior chamber and can be rigid structures positioned on the top of a metering stack and bottom of an assay stack, which are described herein.\n\n(24) In preferred embodiments, bottom housing portion 227 and cap 223 can be formed of a material to provide a rigid structure to the cartridge 200. For example, the bottom housing portion 227 and the cap 223 can be a plastic material, as described herein. The bottom housing portion 227 and cap 223 can be moveable or non-moveable with relation to each other. In some embodiments, when cartridge 200 is inserted into an assay reader, the components within the interior chamber are compressed to cause at least one portion of the collected target analyte to be delivered to a plurality of assay components. The compression can be caused by the user closing a lid of the assay reader, for example. However, it is also to be understood that other approaches for insertion of the cartridge 200 into an assay reader are contemplated that do not require compression.\n\n(25) In some embodiments, the cartridge does not include a cap and bottom housing portion. In such embodiments, the cartridge does not include the housing 201 (see e.g., FIG. 2A) and the metering stack and assay stack can be inserted into an assay reader without an enclosure around it.\n\n(26) As shown in FIG. 2C, cartridge 200 can include a metering stack 224, a spacer material 225, and an assay stack 226. The metering stack 224 can be used to collect a sample of a biological fluid (e.g., blood) and the assay stack 226 comprises assay components necessary for a binding assay (e.g., an immunoassay) to be carried out as discussed in detail herein. As used herein, the term \u201cmetering\u201d refers to collecting a liquid sample of a biological fluid and delivering one or more predetermined volumes of at least a portion of the fluid to the assay components for further analysis via the assay components contained in the assay stack. When assembled into a cartridge, the metering stack 224, a spacer material 225, and an assay stack 226 can be arranged in a stack.\n\n(27) The spacer material 225 is a compressible layer that may be positioned between the metering stack 224 and assay stack 226 as shown in FIG. 2C. In an embodiment, the spacer material 225 may be a flexible material that can be compressed in the vertical direction when the cartridge is inserted into the assay reader and the metering stack 224 is moved into contact with or close proximity to the assay stack 226. In some embodiments, the spacer material 225 can be a flexible material, such as foam, rubber, porous polymer, metal, cotton, or other bending, folding, or moving mechanisms such as a clamp or spring. In some embodiments, the metering and assay stacks are initially separated by an air gap maintained by the spacer material 225. In certain embodiments, spacer material 225 is physically affixed to another layer, such as metering stack 224 or assay stack 226 before the layers of the cartridge are brought together. Typically, the metering and assay stacks remain separated throughout the sample collection process. In such embodiments, the separation between the metering stack and the assay stack can prevent a chemical reaction from starting during the target analyte collection step. When the spacer material 225 is compressed, the metering stack 224 and assay stack 226 can come into contact with or brought into close proximity to each other.\n\n(28) In preferred embodiments, when the metering stack is fully filled with a biological fluid, the cartridge is inserted into an assay reader. Preferably, the material that is used for the top surface of channel 230 is sufficiently transparent so that a user can determine by visual inspection when the channel 230 is filled and the cartridge is ready for insertion into the assay reader. The assay reader is configured to accept the cartridge and comprises a mechanism that compresses the spacer material, thereby pushing the metering stack and assay stack together when the cartridge is inserted into the assay reader. The compression of the spacer material causes a predetermined volume of at least a portion of the collected fluid to flow to assay components in the assay stack. In this way, the act of compressing the metering stack and assay stack together can, in certain embodiments, provide a well-defined point in time that marks the start of the immunoassay or other binding type assay through the components in the assay stack. However, it is also to be understood that other insertion approaches are contemplated that do not require compression of the metering stack and assay stack together as would be understood by one of ordinary skill in the art.\n\n(29) In some embodiments, the biological fluid containing the target analyte is blood, and the cartridge can be used to collect a sample of blood from a skin prick and deliver the sample to the assay stack consistently with minimal user intervention. The user, with a regular pricking lancet, can elicit bleeding in a suitable body site such as a fingertip, palm, hand, forearm, stomach area, etc. Once a drop of blood of sufficient volume is on the skin, the user can collect it by touching the tip of the cartridge to the blood drop. Once the metering stack is fully filled with blood, the user can insert the cartridge into the assay reader, which triggers the delivery of the blood sample to the assay stack. In some embodiments, this can be performed by a patient, administrator, or healthcare provider. The blood collection and testing as described herein does not have to be performed by a trained heath care professional.\n\n(30) In addition, the cartridge design can allow for dispensing different pre-defined volumes of blood sample to multiple assay locations, without using any moving parts such as pumps or valves in the cartridge or in the assay reader. This increases the accuracy and flexibility of a multiplexed quantitative POC analysis, while reducing the complexity and cost of the cartridge and the assay reader.\n\n(31) Typically, as illustrated in FIG. 2C, the metering stack 224 includes a channel 230 to contain the target analyte (e.g., an analyte of interest contained in a blood sample). In certain embodiments, the channel 230 can hold a volume of biological fluid containing a target analyte in the range of about 0.5 to about 100 \u03bcl, about 5 \u03bcl to about 90 \u03bcl, about 10 to about 80 \u03bcl, about 20 \u03bcl to about 60 \u03bcl, or about 30 \u03bcl to about 50 \u03bcl. The volume of the target analyte can be controlled by the dimensions of the channel, including the shape, width, length, and depth of the channel, as described herein. In some embodiments, the depth of the channel can be in the range of about 5 \u03bcm to about 3 mm, about 10 \u03bcm to about 2 mm, or about 250 \u03bcm to about 1 mm. In some embodiments, the width of the channel can be in the range of about 100 \u03bcm to about 10 mm, about 250 \u03bcm to about 5 mm, about 500 \u03bcm to about 3 mm, or about 750 \u03bcm to about 1 mm. In certain preferred embodiments, the dimensions of the channel are selected such that the target analyte is drawn into the channel by capillary action.\n\n(32) Preferably, the metering stack 224 is designed to direct the target analyte fluid to flow into the channel 230 and into any receiving chamber(s) that may be present. In some embodiments, the channel 230 can be formed of or coated with a hydrophilic material, non-limiting examples of which include 93210 hydrophilic PET (Adhesives Research, Glen Rock Pa.) or 9984 Diagnostic Microfluidic Surfactant Free Fluid Transport Film, 9960 Diagnostic Microfluidic Hydrophilic Film, or 9962 Diagnostic Microfluidic Hydrophilic Film (3M Oakdale, Minn.). The channel 230 can also have one or more porous or mesh material(s) along at least some portions of the channel 230 that allows at least a portion of the biological fluid containing the target analyte to be dispensed from the channel 230 of the metering stack 224 to contact assay components in the assay stack. In one non-limiting embodiment, the metering stack layer includes a porous or mesh material that can be positioned such that the porous or mesh material is aligned with the channel portion on the metering stack's top surface and the assay distribution ports and assay components on the metering stack's bottom surface. In some embodiments, the porous or mesh material is selected such that the pores in such material separate the target analyte into a portion that is to be delivered to the assay components and a portion that is not delivered to the assay components. For example, when the biological fluid containing the target analyte is blood, the pores of the porous or mesh material may be of a size that is suitable for separating erythrocytes from other blood components, such as plasma. In this way, when the cartridge is inserted into the assay reader to perform the assays, only plasma is delivered to the assay components for analysis. Of course, combinations of porous or mesh materials may be used such that the entire biological fluid is delivered to some of the assay components, while only portions of the biological fluid may be delivered to other assay components. For example, the combination of porous or mesh materials may allow only plasma to reach some assay components but allow for the delivery of all blood components to other assay components. In certain embodiments, the channel can include a porous or mesh material at the bottom of the channel. The porous or mesh material at the bottom of the channel can be a hydrophilic material or a material coated with a hydrophilic coating or treatment. In some embodiments, the porous or mesh material can have a pore size between about 1 \u03bcm to about 500 \u03bcm. Advantageously, when the biological fluid containing the target analyte is blood, the pores of the porous or mesh material can be sized to allow the porous or mesh material to hold the blood sample in the channel without dripping during blood collection and to be absorbed by the assay stack during the blood dispensing step which occurs upon insertion of the cartridge into the assay reader. In some embodiments, the porous or mesh material can also be used to release air and prevent bubble formation during the time that channel is filled with the biological fluid\n\n(33) FIG. 3 illustrates an exploded view of a metering stack 304 according to one exemplary embodiment of the present disclosure, where such metering stack 304 can be used as metering stack 224 in the embodiment of FIGS. 2A to 2C. In FIG. 3, the metering stack 304 is formed by assembling multiple layers. The first layer 341 can be a plastic sheet with a first side 342, which is in communication with the surrounding environment when the cartridge is located outside the assay reader, and a second side 343 that faces the assay stack. In some embodiments, the first layer 341 may be a cover layer or top layer of the metering stack. In preferred embodiments, first layer 341 may have a hydrophilic surface or coating on second side 343. Non-limiting examples of suitable hydrophilic surfaces coatings include polyvinylpyrrolidone-polyurethane interpolymer, poly(meth)acrylamide, maleic anhydride polymers, cellulosic polymers, polyethylene oxide polymers, and water-soluble nylons or derivatives thereof, to name just a few. The presence of the hydrophilic surface or coating on second side 343 helps to draw the target analyte into the channel, since most, if not all, of the target analytes are aqueous mixtures, such as blood. The first layer 341 may include venting holes 311 positioned to align with the channel 310 defined by the layers below. In FIG. 3, for example, the venting holes 311 are aligned with the receiving chambers of channel 310 to allow air that otherwise would be trapped as an air bubble in the receiving chamber during channel filling to escape efficiently into the surrounding environment. It should be noted that the channel opening can also serve as a vent hole, if desired. In certain preferred embodiments, the first layer 341 comprises polyethylene terephthalate (PET) with a hydrophilic coating on the second side 343 and venting holes 311.\n\n(34) The second layer 344 is positioned below the first layer 341 on the second side or assay facing side of the first layer 341. The second layer 344 itself can be a combination of one or more layers as illustrated in FIG. 3. Regardless of whether the second layer is comprised of one layer or more than one layer, the second layer essentially defines the shape and size of channel in the metering stack, including any receiving chambers that may be part of the channel. For example, the second layer 344 can be formed from one or more layers of polymeric material cut to define the volume and shape of the channel 310 that can contain the target analyte. Other non-limiting methods of forming the channel 310 include injection-molding, stamping, machining, casting, laminating, and 3-D printing. Combinations of such fabrication techniques are also expressly contemplated by the present disclosure. In the embodiment shown in FIG. 3, second layer 344 has a first side 347 facing the first layer 341 and an opposite, second side 348 that faces the assay stack. Furthermore, second layer 344 comprises adhesive layer 345 and plastic layer 346. Adhesive layer 345 fastens the first layer 341 to plastic layer 346. In some embodiments, the second layer 344 can be a combination of one or more plastic layer(s) 346 and adhesive layers 345. Preferably, adhesive layer 345 or plastic layer 346 or both are fabricated from materials which present a hydrophilic surface to the interior surfaces of the channel 310 in order to facilitate the distribution of the target analyte within channel 310. In some embodiments, the hydrophilic plastic sheet(s) can include a PET material with a channel 310 cut into it. If desired, channel 310 may include one or more receiving chambers as shown in FIG. 3. Thus, the thickness and geometry of channel 310 can control the volume of sample to be collected. The hydrophilic interior surfaces of the channel 310 allow the metering stack to collect blood sample by capillary force. In some embodiments, the first layer 341 and the second layer 344 can be one integrated layer used in the metering stack 304.\n\n(35) In FIG. 3, third layer 349 can be formed from a hydrophobic adhesive layer. Non-limiting examples of suitable materials for fabricating third layer 349 include 3M 200MP adhesive or 3M 300MP adhesive (3M, Oakdale, Minn.). In preferred embodiments, the same channel geometry as channel 310 is cut into the third layer to match channel 310 cut in the second layer. In some embodiments, the third layer 349 can have a first side 351 facing the second layer 344 and a second side 352. In some embodiments, the third layer 349 can define the hydrophilic region in a fourth layer 350 positioned below or on the second side 352 of the third layer.\n\n(36) In some embodiments, the fourth layer 350 can be a hydrophilic mesh or porous material. In some embodiments, substantially all of the fourth layer 350 can include the mesh or porous material as shown in FIG. 3. In other embodiments, the hydrophilic mesh or porous material can be a portion of the fourth layer 350. In some embodiments, such as the example shown in FIG. 3, the fourth layer 350 can have a first side 353 facing the third layer 349 and an opposite assay stack-facing second side 354. The hydrophobic third layer 349 can be positioned above the fourth layer 350. The hydrophobic third layer 349 can be a hydrophobic adhesive layer to define a wettable region of the mesh or porous material of the fourth layer 350.\n\n(37) The method used to fabricate the metering stack is not particularly limited, so long as it is compatible with the general manufacturing requirements for medical devices. In certain embodiments, the layers that constitute the metering stack are first fastened together as large multilayer sheet or strip which is then subjected to stamping or cutting processes to form the metering stack, including the channel and any receiving chambers that may be present. In some embodiments, the first layer 341 and second layer 344 can be combined in one piece of plastic material with a hydrophilic surface forming the channel. In some embodiments, the third layer 349 and fourth layer 350 can be combined in one piece of patterned mesh made by printing or other method to define the hydrophilic porous area. In some embodiments, the third layer is not used in the metering stack. Various other combinations of two or more layers, as well as additional layers, are contemplated by various embodiments.\n\n(38) In the binding assay or POC systems of the present disclosure, the assay reactions occur in the assay stack. In general, an assay stack comprises one or more \u201cassay components.\u201d As used herein, the term \u201cassay component\u201d refers to one or more of the active component and a passive supporting element or mask, including but not limited to the multiplexed assay pads. The number of assay pads in a particular assay component is not particularly limited and is scalable to meet the assay requirements needed to diagnose the condition of the patients for whom the assay stack is designed. In preferred embodiments, the top layers of the assay pads of a given assay component align vertically with the appropriate regions of the channel in the metering stack above to ensure that a predetermined volume of a biological fluid, sufficient to perform the assay associated with the particular target analyte of interest, is delivered to the assay pad. The assay pad can act as a wick that draws the sample through the mesh of the metering stack into the assay stack, for example through capillary action, gravity, etc. Therefore, once the metering stack and the assay stack are in contact with or within close proximity to each other, the biological fluid to be analyzed is directed to move into the assay pad, where it may encounter one or more chemical reagents required to perform the assay associated with the particular assay component. If desired, the assay stack may comprise additional layers that contain the chemicals required for the completion of the assay. The number of layers required can depend on the number of chemical reactions that need to take place in order to complete the assay. In various embodiments, layers of the assay stack can be made of variously shaped and variously-sized pads of different porous membrane materials, non-limiting examples of which include nylon, polyethersulphone (PES), nitrocellulose, cellulose filter paper, and glass fiber.\n\n(39) The type of assays that may be formed using the assay systems of the present disclosure are not particularly limited and can be any assay for which the required reagents can be stably incorporated into one or more assay pads and which can cause a change that can be detected by the assay reader. In some embodiments, the assay reactions cause a color change, which may be detected using the colorimetric detection methods as described herein. Still other assay reactions may result in another optical change, a fluorescence change, an electrochemical change, or any other detectable change that may occur in a detection membrane of the assay stack. In certain embodiments, the assays may be porous material-based lateral flow assays, vertical flow assays, and/or a combination of lateral and vertical flow assays. In general, the target analyte is contained within a biological fluid, non-limiting examples of which include blood, plasma, serum, saliva, sweat, urine, lymph, tears, synovial fluid, breast milk, and bile, or a component thereof, to name just a few. In certain preferred embodiments, the biological fluid is blood or a component thereof (e.g., blood plasma). For example, in one embodiment, the assay systems of the present disclosure are useful for providing patients with POC information regarding target analytes in their blood composition. Non-limiting examples of analytes that can be measured in blood include thyroid markers (e.g., T3, free T4, thyroid stimulating hormone, etc.), inflammatory markers (e.g., C-reactive protein, etc.), vitamins (detected via a competitive assay structure), metabolic syndrome markers, glucose, glycated hemoglobin, glycated albumin, and serological levels of antibodies against a disease (detected by a labeled antigen architecture). Non-limiting examples of analyte that can be measured in urine include total protein, leukocyte esterase and myoglobin.\n\n(40) FIG. 4 illustrates an exemplary assay stack 406 according to one embodiment of the present disclosure, where such assay stack 406 can in particular be used as assay stack 226 in the embodiment of FIGS. 2A to 2C. In FIG. 4, the assay stack 406 is formed of multiple layers, including one or more of the layers with active components and a passive supporting element or mask. More specifically, in FIG. 4, assay stack 406 comprises assay stack cover layer 410 that features a cut-out portion 411 that is aligned with the channel in the overlying assay stack. Generally, assay stack cover layer 410 is fabricated from a polymeric material that provides rigidity to the assay stack and provides ease of handling during manufacturing of the cartridge. Furthermore, the cut-out portion 411 allows the biological fluid to flow past the assay stack cover layer 410 towards the under assay components when the cartridge is inserted into the assay reader, as described herein. As shown, the assay stack 406 comprises a first separation layer 461 (e.g., a plasma separation membrane) which can be the top-most layer facing the metering stack. The first separation layer 461 may be used to separate components of the biological fluid to prevent undesirable components from reaching the underlying assay components. For example, when the biological fluid is blood, the first separation layer 461 may be a plasma separation membrane that prevents erythrocytes from reaching the assay components after the cartridge has been inserted into the assay reader. This is advantageous because the strong spectral absorption by the hemoglobin present in erythrocytes may overwhelm the color changes that occurs at the assay pad after the assay is performed. Such a plasma separation membrane can be made of a variety of materials, non-limiting examples of which include an asymmetric polysulphone membrane, glass fiber, or cellulose. In some embodiments, the fabrication of the plasma separation membrane can include surface treatments for improved wettability and/or other properties. The plasma separation membrane can be one continuous piece of membrane for all of the assay components, or multiple discontinuous pieces of membrane material that may be the same or different (or some combination thereof) for each of the assay pads in the assay component in the assay stack FIG. 4. When the first separation layer 461 is discontinuous, cross-talk between neighboring assays can be prevented. In some embodiments, some of the assay pads of an assay component have a corresponding plasma separation membrane, while other assay pads do not have such a layer. Other additional components utilized in the immunoassay systems contemplated by the present disclosure are discussed in more detail with respect to FIGS. 8A-8F.\n\n(41) In FIG. 4, assay stack 406 includes assay component 420, which features mask support layer 430 with a plurality of cut-outs 431 that are configured to receive and immobilize assay pads 440 (e.g., hydrophobic membranes) when the assay stack 406 is assembled. Preferably, cut-outs 431 are positioned laterally in mask support layer 430 such that each of the assay pads 440 (e.g., separation layers including a low molecular weight cut off membrane, a hydrophobic membrane, or a combination thereof) are aligned with both the channel and the porous or mesh material of the metering stack above in order to receive predetermined volumes of target analyte sufficient to perform the assay reaction associated with the given assay pad. As shown in FIG. 4, in some embodiments, the assay stack 406 can include a second assay component 462 positioned below the first separation layer 461 (e.g., a plasma separation membrane) and first assay component 420. The second assay component 462 comprises a mask support layer 450 with a plurality of cut-outs 451 that are configured to receive and immobilize assay pads 463 when the assay stack 406 is assembled. Preferably, cut-outs 451 are positioned to align assay pads 463 with assay pads 440 (e.g., hydrophobic membranes) such that the biological fluid containing the target analyte will flow from assay pads 440 into assay pads 463. Assay pads 463 (e.g., detection membranes such as but not limited to color generation membranes) may comprise chemical reagents that are necessary to complete the assay reactions that are initiated once the target analyte flows through the assay pads 440 (e.g., hydrophobic membranes) of assay component 420. In some embodiments, assay pads 463 serve as a detection indicator layer that provides information corresponding to the results of the assay performed. For example, assay pads 463 (e.g., color generation membranes) can include a visual indicator, such as a color change, to indicate the results of the assays, although it is to be understood that the detection membranes contemplated by the present disclosure also contemplate fluorescent and electrochemical changes or responses. Furthermore, while assay stack 406 in FIG. 4 contains only two assay components 420 and 462, it should be understood that the assay stack 406 may contain additional assay components with assay pads that are impregnated with the chemical reagents required to complete and/or report the results of a particular assay. For instance, the assay stack 406 can include any number of assay components necessary to perform the analysis of the blood sample. Because some assays require more chemical steps than others, assay components may comprise more non-functional assay pads which only serve to draw the completed assay products to the bottom of the assay stack, where the results may be detected by the assay reader, as described herein.\n\n(42) Assay stack 406 in FIG. 4 also includes an assay bottom layer 470, which is typically fabricated from a polymeric material to provide mechanical strength and ease of handling of assay stack 406 during the manufacturing process. In addition, assay bottom layer 470 typically comprises a plurality of detection ports 471 which are aligned with the assay pads of the assay stack and sized to permit interrogation of the assay results by the assay reader.\n\n(43) FIG. 5A shows a schematic drawing of an assay reader, in longitudinal cross-section, according to one non-limiting embodiment of the present disclosure. In FIG. 5A, assay reader 500 includes cartridge receiving chamber 510 which houses the cartridge when it is inserted as indicated by arrow 505. Tab 515 runs longitudinally along assay reader 500 and extends into cartridge receiving chamber 510. Tab 515 is configured to insert into a slot at the top of the cartridge, such as slot 228 in FIG. 2C, when the cartridge is inserted into the assay reader. In addition, the spacing 525 between the bottom edge of tab 515 and support surface 520 is set such that when the cartridge is inserted, tab 515 compresses the metering stack and the assay stack together, thereby causing the target analyte to flow from the metering stack into the assay stack and initiating the assay reactions. In certain embodiments, the assay reader may comprise a snap-fit mechanism that locks the cartridge in place once it has been fully inserted into the assay reader. This is advantageous because it prevents the user from accidentally removing the cartridge from the assay reader before the assays are complete, which could adversely affect the accuracy of the assay results. In some embodiments, assay reader 500 also comprises sensors 542a and 542b, which detect and time the insertion of the cartridge. For example, as the cartridge is inserted into cartridge receiving chamber 510 and begins to engage with tab 515, the bottom surface of the cartridge may pass over sensor 542a, which is detected by appropriate electronics as the beginning of the insertion of the cartridge. The second sensor 542b is located further inside the assay reader 500 and detects the presence of the cartridge when the cartridge is fully inserted as well as the time at which full insertion occurred. Assay reader 500 may then compare the overall time for insertion of the cartridge to determine if the insertion of the cartridge was timely and proper. In this way, the assay reader will not perform any assay readings in situations where (1) the cartridge was only partially inserted, or (2) the cartridge was partially inserted, removed, and inserted again. Either case could give inaccurate assay readings, due to the incomplete compression of the metering stack and assay stack, resulting in incomplete delivery of the required amount of target analyte to the assay pads in the assay stack.\n\n(44) In the exemplary embodiment shown in FIG. 5A, assay reader 500 detects the results of the assay by detecting the color change of the assay pad caused by the assay reactions. To achieve this, assay reader 500 comprises a plurality of light sources (not shown in this cross-sectional drawing) and light detection elements 550 arrayed within assay reader 500 such that they align with the assay pads of the cartridge when the cartridge is fully inserted. In order for light detection elements 550 to be able to detect the color of the assay pads, support surface 520 may be equipped with one or more apertures or be fabricated from a transparent material that allows light to penetrate therethrough. However, it is also to be understood that the assay reader 500 can alternatively include components to detect electrochemical or fluorescent changes in a detection membrane portion of the assay stack. Regardless of the changes in the detection membrane that may be measured, the assay reader 500 also includes one or more electromagnets 552 that, when activated, facilitate the transport of the target analyte through the various layers of the assay stack when part of a complex (e.g., an immunocomplex) that includes, inter alia, a magnetic bead as discussed in more detail with respect to FIGS. 8A-8F. FIG. 5B shows a schematic illustration of a longitudinal cross-section of assay reader 500 with cartridge 502 fully inserted. Cartridge 502, which may correspond to cartridge 100 or 200 of FIG. 1 or FIGS. 2A to 2C, includes metering stack 504 and assay stack 506, which are compressed together by tab 515 such that the target analyte is delivered from the metering stack 504 to the assay pads 530. Assay pads 530 are aligned with light detection elements 550. Note, however, that assay reader 500 may include an additional light detection element 550a without a corresponding assay pad 530. The presence of additional light detection elements, such as light detection element 550a, allow the assay reader to be used with different types of cartridges for different assays, particularly cartridges that may be designed to perform more assays, as well as to identify the different types of cartridges for the different assays.\n\n(45) FIG. 6A shows a schematic drawing of a transverse cross-section of the assay reader shown in FIG. 5 in the form of an assay reader 600 that may be used to detect color changes. In FIG. 6A, the assay reader 600 includes a tab 615 that extends into cartridge receiving chamber 610 to engage with a slot on the cartridge. Such engagement then compresses the metering stack and the assay stack against support surface 620, initiating the assay reactions. Light sources 660a and 660b provide light for detecting the assay results and are positioned near light detection device 650. Specifically, as illustrated in FIG. 6A, light sources 660a and 660b provide light to analyze the assay pad corresponding to light detection device 650. In general, it is advantageous to dedicate one or more light sources to each light detection element in order to ensure that the photon flux onto the light detection element is sufficient to obtain an accurate reading. In some embodiments, the light sources dedicated to a particular light detection element have the same output spectrum. In other embodiments, however, the light sources corresponding to a given light detection element produce different output spectra. For instance, the light sources may be light emitting diodes (LEDs) that produce different colors of light. For example, when the target analyte is blood, it may be useful to use light sources that can generate bichromatic pairs (600 nm/570 nm) to detect the presence of undesirable hemolysis. In general, it is advantageous to include optical elements to direct the light and/or reduce the amount of light scattering in the assay reader. In some embodiments, the optical elements are apertures that only allow light emanating from the light source that is line-of-sight to the respective assay pad to reach the assay pad. For example, in FIG. 6A, light source 660a is limited by aperture defining members 670a and 671a such that only the light from light source 660a that passes through aperture 673a will reach the assay pad and subsequently be detected by light detection device 650. Similarly, light source 660b is limited by aperture defining members 670b and 671b, such that only the light from light source 660b that passes through aperture 673b will reach the assay pad and subsequently be detected by light detection device 650. In preferred embodiments, aperture defining members 670a, 670b, 671a, and 671b are fabricated from a black matte material to reduce the amount of undesirable scattering when light sources 660a and 660b are turned on. Furthermore, in this embodiment, light detection device 650 located in a housing that is comprised of aperture defining members 671a and 671b that only permit light that passes through aperture 672 to reach light detection device 650. If desired, the aperture 672 may be fitted with a filter to admit only light of a predetermined wavelength or wavelength range for detection by light detection device 650. This may be useful, for example, when the light sources are equipped to provide only white light for colorimetric analysis. In addition, the light from light sources 660a and 660b and the light to be detected by light detection device 650 may be directed or manipulated using optical elements such as lenses, filters, shutters, fiber optics, light guides, and the like without departing from the spirit and the scope of the present disclosure. The assay reader 600 also includes one or more electromagnets 652 that, when activated, facilitate the transport of the target analyte through the various layers of the assay stack when part of a complex (e.g., an immunocomplex) that includes, inter alia, a magnetic bead as discussed in more detail with respect to FIGS. 8A-8F.\n\n(46) FIG. 6B shows a schematic illustration of the operation of the assay reader described in FIG. 6A. In FIG. 6B, a cartridge comprising metering stack 604 and assay stack 606 are inserted into cartridge receiving chamber 610 of assay reader 600. Tab 615 compresses metering stack 604 and assay stack 606 against support surface 620 to cause the target analyte to flow from the channel 612 into assay pad 630. As noted previously, assay reader 600 may be fitted with sensors to confirm that the cartridge has been inserted correctly and in a timely manner. Assay reader 600 may also be pre-programmed before sample collection, either by the user or during the manufacturing process, to illuminate the assay pads at the appropriate time based on the type of cartridge being used. In this way, assay reader 600 collects assay data from assay pad 630 only when the assay is completed. Alternatively, if desired, assay reader 600 may be configured to collect assay data from assay pad 630 during the entire assay reaction after the cartridge has been inserted. As shown in FIG. 6B, light source 660a provides light beam 680a, which impinges on the bottom face of assay pad 630 to produce reflected light beam 661. Similarly, light source 660b produces light beam 680b, which may impinge on the bottom of the assay pad 630 to produce reflected light beam 661 at the same time as light source 660a or a different time, depending on the requirements of the assays being detected.\n\n(47) FIG. 7 shows a block diagram 700 of a sensor configuration inside an assay reader according to one exemplary embodiment of the present disclosure. In FIG. 7, four assay pads (identified by reference numerals 741, 742, 743, and 744) have completed their assay reactions with the target analyte, undergone the respective color changes, and are ready for colorimetric analysis. Note that, if desired, this configuration can also be used to collect data from the four assay pads to monitor the progress of the assay reactions. Input signal 701 from a first microcontroller serial-peripheral interface bus (MCU SPI Bus) enters digital-to-analog converter unit 710, which comprises individual digital-to-analog converters 711, 712, 713, and 714 that independently control current sources 721, 722, 723, and 724. These current sources, in turn, power light sources 731, 732, 733, and 734, respectively. In some embodiments, input signal 701 may be sent by a timing circuit at a predetermined time after the insertion of the cartridge into the assay reader. In such embodiments, the predetermined time corresponds to the known time or times for the assay reactions in the assay pads to reach completion. In some preferred embodiments, the light sources 731, 732, 733, and 734 are activated at the same time to measure the assay-induced color change of assay pads 741, 742, 743, and 744 simultaneously in a multiplexed mode. However, this present disclosure also contemplates operating all of the light sources separately and sequentially, or some simultaneously and some separately, depending on the timing requirements of the assays in the cartridge.\n\n(48) In this non-limiting example, each of light sources 731, 732, 733, and 734 includes individual three light emitting diodes (LEDs) which may be the same or different colors, depending on the requirements of the assay and any optical elements that may be present in the assay reader. For example, in certain embodiments, the three LEDs in a particular light source (e.g., 731) may be red, green, and blue (RGB LEDs), such that the light impinging on the assay pad is white light when all three LEDS are activated. Of course, the light sources are not limited to any particular number or type of LEDs or other light generating devices. More generally, the light sources that are useful in the assay readers of the present disclosure are not particularly limited, so long as they provide light of suitable wavelength(s) and brightness for the light detection element to make an accurate reading of the colored light reflected from the assay pad. In certain non-limiting embodiments, the light sources are light emitting diodes (LEDs), organic light emitting diodes (OLEDs), active matrix organic light emitting diodes (AMOLEDs), or lasers. For example, the light source may be only one LED that has sufficient brightness and the proper wavelength to allow colorimetric analysis of an assay reaction in a given assay pad. In certain embodiments, the light sources may produce light of specific wavelengths. As one non-limiting example, when the biological fluid containing the target analyte is blood (with erythrocytes removed), a bichromatic light source that produces light at 570 nm and 600 nm may be used to detect the presence of heme on a non-functional (i.e., assay reagent-free) assay pad, which is indicative of undesirable hemolysis in the patient. Alternatively, the light source may be a broadband source that is paired with one or more narrow bandpass filters to select light of certain desired wavelength(s). Typically, the light sources produce light in the visible region of the electromagnetic spectrum (i.e., wavelength between 400-700 nm) although this present disclosure also contemplates light sources that produce electromagnetic radiation in the infrared (700 nm to 10.sup.6 nm) or ultraviolet regions (10 nm-400 nm) of the electromagnetic spectrum, so long as they are paired with the appropriate light detection devices. Combinations of different light sources are also expressly contemplated by the present disclosure.\n\n(49) In FIG. 7, element 740 is a schematic representation of optical elements that optionally may be present in the optical path between the light sources 731, 732, 733, and 734 and assay pads 741, 742, 743, and 744. When desired, one or more optical elements may be located between the light source and its corresponding assay pad to direct the light, focus the light, reduce undesirable scattering, select one or more wavelengths for assay detection, or some combination thereof. Non-limiting examples of such optical elements include apertures, lenses, light guides, bandpass filters, optical fibers, shutters, and the like. Similarly, element 745 represent optical elements that optionally may be present in the optical path between assay pads 741, 742, 743, and 744 and corresponding light detection devices 751, 752, 753, and 754. These optical elements may be used to manipulate the light upstream of the light detector devices in a manner similar to that described for element 740. It is to be understood that different types and numbers of optical elements may be used for each combination of light source, assay pad, and light detection device. Light detecting devices 751, 752, 753, and 754 detect the light from the assay pads 741, 742, 743, and 744. In this non-limiting example, the light detecting devices are photodiodes. More generally, the type of light detection device is not particularly limited, provided that it is capable of detecting the light that is reflected from the assay pads used for colorimetric measurement of the assay results. Other examples of suitable light detection elements include photodiode arrays, CCD chips, and CMOS chips. The outputs from light detection devices (e.g., photodiodes) 751, 752, 753, and 754 are sent to transimpedance amplifier/low pass filter elements 761, 762, 763, and 764, which convert the current signal from the photodiodes to a voltage output, while filtering unwanted signal components. The output from transimpedance amplifier/low pass filter elements 761, 762, 763, and 764 are sent to analog-to-digital converter unit 770, which comprises multiplexer unit 771, gain 772, and analog-to-digital converter 773. The output of analog-to-digital converter unit 770 may be sent to a component 780, which may be a second MCU SPI bus, a transmitter, or a processor. In certain embodiments, the transmitter allows for hardwired or wireless connectivity (e.g., Bluetooth or Wi-Fi) with a personal computer, mobile device, or computer network. In one particularly useful embodiment, the assay results are transmitted to the user's mobile device or personal computer, where they are displayed in a graphical user interface (GUI). If desired, the GUI may display prior assay results, in addition to the current results, in order to provide the user with information regarding the overall trends in the results of the assays. For example, if the user is diabetic, the GUI may plot the glucose levels measured by the assay reader as a function of time to allow the user to determine whether blood glucose level is being properly controlled. In addition, the assay results may be transmitted from the user's mobile device or computer to a computer network, such as one belonging to the user's physician. In this way, the assay systems of the present disclosure can allow a user's physician to monitor a patient closely, by providing up-to-date medical information from the assay results obtained by the assay reader.\n\n(50) It should be noted that the optical detection systems described in the foregoing correspond to some exemplary embodiments of the system, but that the present disclosure expressly contemplates other types of detection systems as well. In general, any detection system which corresponds to a signal change caused by an assay reaction may be used in connection with the assay reader of the present disclosure. Thus, for example, in certain embodiments, the detection system is an optical detection system that is based on chemiluminescence. In such embodiments, light sources such as LEDS and OLEDS are not required to detect a color change caused by the assay reaction in the assay pads. Rather, the signal change may be caused by the reaction of an oxidative enzyme, such as luciferase, with a substrate which results in light being generated by a bioluminescent reaction. In another exemplary embodiment, the signal change caused by the assay reaction may be detected by electrochemical reaction.\n\n(51) FIGS. 8A-8F illustrate a further embodiment of cartridges 100, 200, or 502 in the form of a cartridge 800 that includes a metering stack 802 and an assay stack 804 during various stages of performing an immunoassay after a fluid sample 814 to be analyzed for the presence of a target analyte 816 has been introduced to the cartridge 800. The metering stack 802 is configured to receive and distribute the target analyte 816 along a channel, where the channel has a bottom that comprises a porous or mesh material and one or more venting holes in communication with the channel, as described in detail above. As also described above, a spacer material is disposed between the metering stack and the assay stack, wherein the spacer material provides a gap between the metering stack and the assay stack that prevents the target analyte from flowing from the metering stack into the assay stack when the cartridge is in an uncompressed state. Additionally, the porous or mesh material permits the target analyte 816 to flow from the metering stack 802 to the assay stack 804 upon compression of the cartridge 800.\n\n(52) Once introduced to the metering stack 802, the fluid sample 814 containing the target analyte 816 passes to the first separation layer 806, and ultimately, the target analyte 816 reaches a detection membrane 812 (e.g., a color generation membrane) via a second separation layer 808 (e.g., a hydrophobic membrane, a low molecular weight cut-off membrane, or a combination thereof) as discussed in more detail below. When the fluid sample 814 is blood, the first separation layer 806 can be referred to as the plasma separation membrane. Further, when the fluid sample 814 is blood, the first separation layer 806 can include pores 840 that have a pore size large enough to allow the target analyte 816 to be pulled through additional layers of the cartridge 800 but that also have a pore size small enough (e.g., less than about 2 micrometers) to prevent passage of any erythrocytes through additional layers of the cartridge 800, which could affect the accuracy of the assay results. This is because there is a strong spectral absorption by the hemoglobin present in erythrocytes that may, for example, overwhelm the color changes that occur after the assay is performed.\n\n(53) Additionally, as shown in FIG. 8A, the first separation layer 806 may also include a plurality of first complexes 822 that each include a capture molecule 818 (e.g., a capture antibody in the case of an immunoassay) and a magnetic bead 820 that are conjugated and a plurality of second complexes 828 that each include a detection molecule 824 (e.g., a detection antibody in the case of an immunoassay) and a detection label 826 that are conjugated. Ultimately, as shown in FIG. 8B, any target analyte 816 present in the fluid sample 814 can join to the plurality of first complexes 822 and the plurality of second complexes 828 to create a third complex 834 (e.g., an immunocomplex in the case of an immunoassay) that can be pulled through the assay stack 804 upon activation of an electromagnet 852, causing the electromagnet 852 to emit an electromagnetic force or signal 853 (see FIGS. 8C and 8D), where the specific structure and components of the first separation layer 806 essentially replace the wash and incubation steps that are typically employed in a standard assay, although it is to be understood that the complexes may be located on a different layer besides the first separation layer 806 in some embodiments, where washing and incubation steps would still be eliminated. In any event, regardless of where the first complexes 822 and the second complexes 828 are initially deposited on the assay stack 804, the need for physical washing with fluid or complex moving parts is eliminated in the assay system contemplated by the present disclosure. The electromagnet 852 may be part of an assay reader as previously discussed in connection with FIGS. 1 and 5A to 7.\n\n(54) The capture molecule 818 can be a capture molecule (e.g., a capture antibody in the case of an immunoassay) that specifically binds to the target analyte 816. Capture molecules 818 for a target analyte 816 are readily known by one having ordinary skill in the art and may be produced by routine techniques or are readily available commercially. Further, the magnetic beads 820 to which the capture molecules 818 are coupled can be ferromagnetic particles which are readily conjugated to biomolecules such as the capture molecule 818. The magnetic beads 820 can have a diameter ranging from about 10 nanometers to about 10 micrometers, such as from about 20 nanometers to about 7.5 micrometers, such as from about 30 nanometers to about 5 micrometers. Suitable magnetic beads are well known by one having ordinary skill in the art and are available from commercial suppliers. The magnetic beads 820 can include iron oxide particles, such as magnetite (Fe.sub.3O.sub.4), although it is to be understood that any other iron oxide particles can be used so long as the magnetic beads 820 have superparamagnetic properties in that the beads exhibit magnetic behavior only in the presence of an external magnetic field. This property is dependent on the small size of the particles in the magnetic beads 820 and enables the magnetic beads 820 to be separated in suspension, along with the capture molecules 818 to which the magnetic beads 820 are coupled. Since the magnetic beads 820 do not attract each other outside of a magnetic field, the magnetic beads 820 can therefore be used without any concern about unwanted clumping. The capture molecule 818 can be coupled to the magnetic bead 820 directly or indirectly via a linker molecule that can be bound to the capture molecule 818 and the magnetic bead 820 either covalently or non-covalently. In any event, suitable methods for forming the first complex 822 containing the capture molecule 818 (e.g., a capture antibody) and the magnetic bead 820 are well known by one having ordinary skill in the art.\n\n(55) Further, like the capture molecule 818, the detection molecule 824 (e.g., the detection antibody in the case of an immunoassay) is also a molecule that specifically binds to the target analyte. Detection molecules 824 for a target analyte 816 are readily known by one having ordinary skill in the art and may be produced by routine techniques or are readily available commercially. The detection molecule 824 is linked to the detection label 826. The detection label 826 can begin a chemical reaction with the reagent or substrate 830 located in the detection membrane 812 (e.g., a color generation membrane, a fluorescence generation membrane, an electrochemical signal generation membrane, etc.) to produce a detectable signal as discussed in more detail below. For example, the detection label 826 may catalyze the oxidation of the substrate 830. The oxidized form of the substrate 830 may then provide a detectable signal in the form of a color change, a fluorescence change, or an electrochemical change. Suitable detection labels 826 are well known in the art and can include peroxidase, glucose oxidase, and alkaline phosphatase. In one particular embodiment, the detection label 826 can be a peroxidase enzyme, such as horseradish peroxidase (HRP) or, in another embodiment, the detection label 826 can be \u03b2-galactosidase. The detection molecule 824 can be coupled to the detection label 826 directly or indirectly via a linker molecule that can be bound to the detection molecule 824 and the detection label 826 either covalently or non-covalently. In any event, suitable methods for forming the second complex 828 containing the detection molecule 824 (e.g., a detection antibody) and the detection label 826 are well known by one having ordinary skill in the art. In addition, it should be understood that when the assay is a sandwich assay, the capture molecule 818 and the detection molecule 824 can be selected specifically for the target analyte 816 and are paired to ensure that different epitopes of the target analyte 816 are targeted so that both molecules can bind to the target analyte 816 to create a complex 834 that includes the capture molecule 818, the detection molecule 824, and the target analyte 816 (along with the magnetic bead 820 and the detection label 826). Further, it should also be understood that other assay architectures fall within the scope of the present disclosure, such as, but not limited to competitive and labelled-antigen architectures.\n\n(56) The assay stack 804 also includes a second separation layer 808 (e.g., a hydrophobic membrane, a low molecular weight cut-off membrane, or a combination thereof) that is positioned adjacent to the first separation layer 806. The second separation layer 808 can include pores 842 having a pore size large enough to allow for the third complex 834 comprising the target analyte 816 bound to one of the first complexes 822 and one of the second complexes 828 to pass to the detection membrane 812 in the presence of an activated electromagnet 852. The second separation layer 808 can also include pores 842 having a pore size small enough to prevent passage of any unbound second complexes 836 to the detection membrane 812 (see FIGS. 8B-8F) in the presence of an activated electromagnet 852, where the passage of such unbound second complexes 836 could decrease the accuracy of the assay since excess detection labels 826 that are not coupled to a target analyte 816 as part of the third complex 834 could be allowed to pass through to the detection membrane 812 and potentially interact with the substrate 830, which could result in the assay indicating the presence of higher concentrations of the target analyte 816 in the fluid sample 814 than is actually present. Further, it is to be understood that the passage of unbound first complexes 838 to the detection membrane 812 in the presence of the activated electromagnet 852 is acceptable as the unbound first complexes 838 do not include detection labels 826.\n\n(57) In some embodiments, the pores 842 can have a pore size that has a molecular weight cut-off of about 150,000 Daltons or less, such as about 125,000 Daltons or less, such as about 100,000 Daltons or less, to prevent the passage of the unbound second complexes 836 containing the detection molecule 824 and the detection label 826 through the pores 842 as the molecules (e.g., antibodies) can have a molecular weight of about 150,000 Daltons or more. Moreover, it is to be understood that although the first complexes 822 also include molecules (e.g., antibodies) that may have a molecular weight above about 150,000 Daltons or more (e.g., capture molecules 818), the presence of the magnetic beads 820 in the first complexes 822 upon activation of the electromagnet 852 provides sufficient force to allow the second complexes 828 to pass through the second separation layer 808 to the detection membrane 812.\n\n(58) In addition to the pore size of the second separation layer 808, the second separation layer 808 can also include a hydrophilic treatment 810 (e.g., a coating) that can be applied in order to tune the second separation layer 808 so that it has the desired molecular weight cut-off based on the specific detection molecules 824 utilized in the second complexes 828. In one embodiment, the hydrophilic treatment 810 can include one or more surfactants. Any suitable surfactant known by one of ordinary skill in the art can be utilized to form the hydrophilic treatment 810, including, but not limited nonionic surfactants (e.g., surfactants having a hydrophilic polyethylene oxide chain and an aromatic hydrocarbon lipophilic group such as Triton X-100, surfactants containing polysorbate molecules containing a hydrophilic head group of oligo(ethylene glycol) chains and a hydrophobic tail of fatty acid ester moiety such as Tween 20, Tween 40, and Tween 80, etc.), anionic surfactants (e.g., sodium laureth sulfate, sodium dodecyl sulfate, etc.), and cationic surfactants (e.g., methyl triethanolammonium). In any event, it is to be understood that the combination of low molecular weight cut-off membrane materials, hydrophobic membrane materials, hydrophilic treatments or coatings, etc. to form the second separation layer 808 can be optimized by one of ordinary skill in the art based on the magnetic field strength of the electromagnet signal 853 of the electromagnet 852, the size of the magnetic beads 820, and the molecular weight cutoffs of the materials utilized.\n\n(59) After any formed third complexes 834 containing any target analyte 816 present and sandwiched between a first complex 822 and a second complex 828 as well as any unbound first complexes 838 containing a capture molecule 818 and a magnetic bead 820 pass through the pores 842 in the second separation layer 808 in response to a magnetic force or signal 853 emitted by one or more electromagnets 852, the detection labels 826 in each of the third complexes 834 can react with the substrate 830 present in the detection membrane 812. See FIGS. 8D-8F. The reaction between the substrate 830 and any detection label 826 present can elicit a quantifiable response 844 (e.g., a colorimetric response, a fluorescent response, an electrochemical response, etc.) in the presence of the target analyte 816, wherein the quantifiable response 844 corresponds to an amount of detection molecule 824 present in the detection membrane 812 as shown in FIGS. 8E and 8F. Further, the amount of detection molecule 824 present in the detection membrane 812 corresponds to an amount of the target analyte 816 present in the fluid sample 814. In one embodiment, the substrate 830 can include one or more reagents for the detection label 826. For example, in one embodiment, the substrate 830 can include one or more reagents which are catalyzable by the detection label 826 attached to the detection molecule 824 to provide a detectable signal within the detection membrane 812. For instance, the substrate 830 can include a first reagent and/or a second reagent, where the second reagent can be oxidizing agent or a precursor thereof for the first reagent. Further, a reaction between the first reagent and the oxidizing agent can be catalyzable by the detection label 826 to provide a detectable signal in the detection membrane 812.\n\n(60) The choice of first and second reagents can depend on the detection label 826 that is part of the second complex 828. The first reagent may be reactable with the second reagent in the presence of the detection label 826. Suitable first reagents can include tetramethylbenzidine (TMB), alpha guaiaconic acid, 2,2\u2032-azino-bis(3-ethylbenzothiazolidine-6-sulphonic acid), hydroquinone, phenylenediamine, o-dianisidine, o-tolidine (dimethylbenzidine), 6-methoxyquinoline, and 3,3\u2032-diaminobenzidine, 3-amino-9-ethylcarbazole, or a combination thereof. The second reagent can be an oxidizing agent or a precursor thereof and can be reactable with the first reagent in the presence of the detection label 826. Suitable second reagents for the detection of a detection label 826 that includes peroxidase can include hydrogen peroxide or a precursor thereof. For example, the second reagent can include urea peroxide or sodium perborate. Therefore, the first reagent can be a compound that reacts with hydrogen peroxide in the presence of the peroxidase detection label 826. Further, suitable second reagents for the detection of a glucose oxidase detection label 826 can include glucose or a precursor thereof. In some preferred embodiments, the substrate 830 for the detection of a peroxidase detection label 826 can include tetramethylbenzidine (TMB) and perborate (PER). In some embodiments, the substrate 830 can include single reagent (i.e., a first reagent only). A reaction between the reagent and the detection label 826 can provides a detectable signal without the need for a second reagent. This may be useful, for example, in the detection of an alkaline phosphatase detection label 826. Suitable reagents for the detection of alkaline phosphatase include 1-naphthyl-phosphate; 5-bromo-4-chloro-3-indolyl phosphate (BCIP); hydroquinone diphosphate; phenolphthalein phosphate; 4-aminophenyl phosphate; 3-idoxyl phosphate; and phenyl phosphate. However, it should be understood the substrate 830 can include other reagents known to one having ordinary skill in the art based on the particular detection label 826 being utilized.\n\n(61) In any event, the quantifiable response 844 (e.g., a colorimetric response, a fluorescent response, an electrochemical response, etc.) in the detection membrane 812 can be detected by one or more detection devices, which, for example, can include light detection devices 854 as shown in FIG. 8F when the quantifiable response is a color change. Meanwhile, one or more light sources 831 provide light for detecting the quantifiable response 844 (e.g., color change) in the detection membrane 812 (e.g., color generation membrane) and can be positioned near light detection device 864. In the case of a colorimetric response, the one or more light sources 831 provide light to analyze the quantifiable response 844 (e.g., color change) in the detection membrane 812 (e.g., color generation membrane) corresponding to light detection device 854. As described above, it is advantageous to dedicate one or more light sources to each light detection element in order to ensure that the photon flux onto the light detection element is sufficient to obtain an accurate reading. In some embodiments, the light sources 831 dedicated to a particular light detection device 854 have the same output spectrum. In other embodiments, however, the light sources 831 corresponding to a given light detection devices 854 produce different output spectra. For instance, the light sources may be light emitting diodes (LEDs) that produce different colors of light. In general, it is advantageous to include optical elements to direct the light and/or reduce the amount of light scattering in the assay reader. If desired, the light detection device 854 may be fitted with a filter to admit only light of a predetermined wavelength or wavelength range for detection by light detection device 854. This may be useful, for example, when the light sources 831 are equipped to provide only white light for colorimetric analysis. In addition, the light from light sources 831 and the light to be detected by light detection device 854 may be directed or manipulated using optical elements such as lenses, filters, shutters, fiber optics, light guides, and the like without departing from the spirit and the scope of the present disclosure. More generally, the light sources 831 that are useful in the assay readers of the present disclosure are not particularly limited, so long as they provide light of suitable wavelength(s) and brightness for the light detection devices 854 to make an accurate reading of the colored light reflected from the detection membrane 812.\n\n(62) It should also be understood that the detection membrane 812 can include one or more stabilizing agents 832 as shown in FIGS. 8A-8F. Such stabilizing agents 832 can include neo silk protein saver; mannitol, trehalose, or other sugars; polypropylene glycol-polyethylene glycol block copolymers or other hydrophilic-hydrophobic block copolymers; or a combination thereof.\n\n(63) FIG. 9 shows a flowchart that illustrates a method 900 of using of the assay system according to one embodiment of the present disclosure to perform a plurality of assays. The method 900 includes step 910, which involves receiving a fluid sample that may contain a target analyte or analyte of interest into a channel in a cartridge. Step 920 involves inserting the cartridge into an assay reader, thereby compressing the cartridge to expose a target analyte stored in the channel to an assay stack in the cartridge to initiate one or more assay reactions. Step 930 involves detecting one or more signal changes associated with the plurality of assay reactions. The method 900 can include any additional steps that would be understood by one of ordinary skill in the art to detect the one or more signal changes via the various components of the metering stack and assay stack described in detail above.\n\n(64) FIG. 10 shows a flowchart that illustrates a method 1000 of fabricating a cartridge according to one embodiment of the present disclosure. The method includes the steps of obtaining a first separation layer, applying a plurality of first complexes comprising a capture molecule and a magnetic bead and a plurality of second complexes comprising a detection molecule and a detection label to the plasma separation membrane, and allowing the plurality of first complexes and the plurality of second complexes to dry; (step 1010) and obtaining a second separation layer (step 1020). The method 1000 also includes the step of obtaining a detection membrane, applying a substrate to the detection membrane, and allowing the substrate to dry (step 1030). Further, the method 1000 also includes the step of positioning the second separation layer between the first separation layer and the detection membrane. In this method the substrate interacts with the detection label to elicit a quantifiable response in the presence of a target analyte in a fluid sample that is introduced to the cartridge. Further, the quantifiable response corresponds to an amount of detection molecule present in the detection membrane; and the amount of detection molecule present in the color detection membrane corresponds to an amount of the target analyte present in the fluid sample.\n\n(65) While various embodiments of the present disclosure have been described above, it should be understood that they have been presented by way of example only, and not by way of limitation. Likewise, the various diagrams may depict an example architectural or other configuration for the disclosure, which is done to aid in understanding the features and functionality that can be included in the disclosure. The disclosure is not restricted to the illustrated example architectures or configurations, but can be implemented using a variety of alternative architectures and configurations. Additionally, although the disclosure is described above in terms of various exemplary embodiments and implementations, it should be understood that the various features and functionality described in one or more of the individual embodiments are not limited in their applicability to the particular embodiment with which they are described. They instead can be applied, alone or in some combination, to one or more of the other embodiments of the disclosure, whether or not such embodiments are described, and whether or not such features are presented as being a part of a described embodiment. Thus, the breadth and scope of the present disclosure should not be limited by any of the above-described exemplary embodiments.\n\n(66) Unless otherwise defined, all terms (including technical and scientific terms) are to be given their ordinary and customary meaning to a person of ordinary skill in the art, and are not to be limited to a special or customized meaning unless expressly so defined herein. It should be noted that the use of particular terminology when describing certain features or aspects of the disclosure should not be taken to imply that the terminology is being re-defined herein to be restricted to include any specific characteristics of the features or aspects of the disclosure with which that terminology is associated. Terms and phrases used in this application, and variations thereof, especially in the appended claims, unless otherwise expressly stated, should be construed as open ended as opposed to limiting. As examples of the foregoing, the term \u2018including\u2019 should be read to mean \u2018including, without limitation,\u2019 \u2018including but not limited to,\u2019 or the like; the term \u2018comprising\u2019 as used herein is synonymous with \u2018including,\u2019 \u2018containing,\u2019 or \u2018characterized by,\u2019 and is inclusive or open-ended and does not exclude additional, unrecited elements or method steps; the term \u2018having\u2019 should be interpreted as \u2018having at least;\u2019 the term \u2018includes\u2019 should be interpreted as \u2018includes but is not limited to;\u2019 the term \u2018example\u2019 is used to provide exemplary instances of the item in discussion, not an exhaustive or limiting list thereof; adjectives such as \u2018known\u2019, \u2018normal\u2019, \u2018standard\u2019, and terms of similar meaning should not be construed as limiting the item described to a given time period or to an item available as of a given time, but instead should be read to encompass known, normal, or standard technologies that may be available or known now or at any time in the future; and use of terms like \u2018preferably,\u2019 \u2018preferred,\u2019 \u2018desired,\u2019 or \u2018desirable,\u2019 and words of similar meaning should not be understood as implying that certain features are critical, essential, or even important to the structure or function of the present disclosure, but instead as merely intended to highlight alternative or additional features that may or may not be utilized in a particular embodiment of the present disclosure. Likewise, a group of items linked with the conjunction \u2018and\u2019 should not be read as requiring that each and every one of those items be present in the grouping, but rather should be read as \u2018and/or\u2019 unless expressly stated otherwise. Similarly, a group of items linked with the conjunction \u2018or\u2019 should not be read as requiring mutual exclusivity among that group, but rather should be read as \u2018and/or\u2019 unless expressly stated otherwise.\n\n(67) Where a range of values is provided, it is understood that the upper and lower limit, and each intervening value between the upper and lower limit of the range is encompassed within the embodiments.\n\n(68) With respect to the use of substantially any plural and/or singular terms herein, those having skill in the art can translate from the plural to the singular and/or from the singular to the plural as is appropriate to the context and/or application. The various singular/plural permutations may be expressly set forth herein for sake of clarity. The indefinite article \u201ca\u201d or \u201can\u201d does not exclude a plurality. A single processor or other unit may fulfill the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. Any reference signs in the claims should not be construed as limiting the scope.\n\n(69) It will be further understood by those within the art that if a specific number of an introduced claim recitation is intended, such an intent will be explicitly recited in the claim, and in the absence of such recitation no such intent is present. For example, as an aid to understanding, the following appended claims may contain usage of the introductory phrases \u201cat least one\u201d and \u201cone or more\u201d to introduce claim recitations. However, the use of such phrases should not be construed to imply that the introduction of a claim recitation by the indefinite articles \u201ca\u201d or \u201can\u201d limits any particular claim containing such introduced claim recitation to embodiments containing only one such recitation, even when the same claim includes the introductory phrases \u201cone or more\u201d or \u201cat least one\u201d and indefinite articles such as \u201ca\u201d or \u201can\u201d (e.g., \u201ca\u201d and/or \u201can\u201d should typically be interpreted to mean \u201cat least one\u201d or \u201cone or more\u201d); the same holds true for the use of definite articles used to introduce claim recitations. In addition, even if a specific number of an introduced claim recitation is explicitly recited, those skilled in the art will recognize that such recitation should typically be interpreted to mean at least the recited number (e.g., the bare recitation of \u201ctwo recitations,\u201d without other modifiers, typically means at least two recitations, or two or more recitations). Furthermore, in those instances where a convention analogous to \u201cat least one of A, B, and C, etc.\u201d is used, in general such a construction is intended in the sense one having skill in the art would understand the convention (e.g., \u201ca system having at least one of A, B, and C\u201d would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and/or A, B, and C together, etc.). In those instances where a convention analogous to \u201cat least one of A, B, or C, etc.\u201d is used, in general such a construction is intended in the sense one having skill in the art would understand the convention (e.g., \u201ca system having at least one of A, B, or C\u201d would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and/or A, B, and C together, etc.). It will be further understood by those within the art that virtually any disjunctive word and/or phrase presenting two or more alternative terms, whether in the description, claims, or drawings, should be understood to contemplate the possibilities of including one of the terms, either of the terms, or both terms. For example, the phrase \u201cA or B\u201d will be understood to include the possibilities of \u201cA\u201d or \u201cB\u201d or \u201cA and B.\u201d\n\n(70) All numbers expressing quantities of ingredients, reaction conditions, and so forth used in the specification are to be understood as being modified in all instances by the term \u2018about.\u2019 Accordingly, unless indicated to the contrary, the numerical parameters set forth herein are approximations that may vary depending upon the desired properties sought to be obtained. At the very least, and not as an attempt to limit the application of the doctrine of equivalents to the scope of any claims in any application claiming priority to the present application, each numerical parameter should be construed in light of the number of significant digits and ordinary rounding approaches.\n\n(71) All of the features disclosed in this specification (including any accompanying exhibits, claims, abstract and drawings), and/or all of the steps of any method or process so disclosed, may be combined in any combination, except combinations where at least some of such features and/or steps are mutually exclusive. The disclosure is not restricted to the details of any foregoing embodiments. The disclosure extends to any novel one, or any novel combination, of the features disclosed in this specification (including any accompanying claims, abstract and drawings), or to any novel one, or any novel combination, of the steps of any method or process so disclosed.\n\n(72) While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 33,
      "claims_start": 32,
      "description_end": 32,
      "description_start": 20,
      "drawings_end": 19,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 13,
      "number_of_drawing_sheets": 17,
      "number_of_figures": 19,
      "page_count": 33,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 32,
      "specification_start": 20,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006076133,
    "field_of_search_cpc": [
      "B01L 3/50273",
      "B01L 3/502715",
      "B01L 3/502753",
      "B01L 3/502707",
      "B01L 2200/04",
      "B01L 2200/0605",
      "G01N 33/54326",
      "G01N 33/54346"
    ],
    "foreign_references": [
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "JP",
        "patent_number": "2014149172",
        "pub_month": "2014-08-01"
      },
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "WO",
        "patent_number": "WO 2018/097796",
        "pub_month": "2018-05-01"
      },
      {
        "citation_classification": "N/A",
        "citation_cpc": "N/A",
        "cited_by_examiner": false,
        "country_code": "WO",
        "patent_number": "WO 2019/074760",
        "pub_month": "2019-04-01"
      }
    ],
    "group_art_unit": "1796",
    "guid": "US-11554372-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G72/543/115",
    "intl_class_current_primary": [
      {
        "intl_class": "B01L",
        "intl_subclass": "3/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G01N",
        "intl_subclass": "33/543",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "B01L3/00",
      "G01N33/543"
    ],
    "inventors": [
      {
        "city": "Woodacre",
        "country": "US",
        "name": "Watkins; Herschel",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Francisco",
        "country": "US",
        "name": "McKeating; Kristy",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Watkins; Herschel et al.",
    "legal_firm_name": [
      "Dority & Manning, P.A."
    ],
    "npl_references": [
      {
        "citation": "Clotilde et al., \u201cMultiplex Immunoassay: a Planar Array on a Chip Using the MagArray\u2122 Technology\u201d, ELISA: Methods and Protocols, Methods in Molecular Biology, vol. 1318, pp. 119-126.",
        "cited_by_examiner": false
      },
      {
        "citation": "Dutta et al., \u201cWash-free, label-free immunoassay for rapid electrochemical detection of PfHRP2 in whole blood samples\u201d, Scientific Reports, www.nature.com/scientificreports, Nov. 20, 2018, 8 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Afriat et al., \u201cDevelopment of a point-of-care technology for bacterial identification in milk\u201d, Taianta, vol. 219, Nov. 1, 2020, 8 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Eltzov et al., \u201cColorimetric stack pad immunoassay for bacterial identification\u201d, Biosensors and Bioelectronics, vol. 87, Jan. 15, 2017, pp. 572-578.",
        "cited_by_examiner": false
      },
      {
        "citation": "Eltzov et al., \u201cMiniaturized Flow Stacked Immunoassay for Detecting <i>Escherichia coli </i>in a Single Step\u201d, Anal. Chem., 2016, 88, 12, pp. 6441-6449.",
        "cited_by_examiner": false
      },
      {
        "citation": "Harpaz et al., \u201cEnhanced Colorimetric Signal for Accurate Signal Detection in Paper-Based Biosensors\u201d, Diagnostics 2020, 10, 28, 15 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Harpaz et al., \u201cPoint-of-Care-Testing in Acute Stroke Management: an Unmet Need Ripe for Technological Harvest\u201d, Biosensors 2017, 7, 30, 39 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "International Search Report and Written Opinion for Application No. PCT/US2021/041988, dated Apr. 12, 2022, 17 pages.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "Binding assay with no wash steps or moving parts using magnetic beads",
    "primary_examiner": "Sines; Brian J.",
    "publication_date": "2023-01-17",
    "publication_number": "11554372",
    "related_apps": [
      {
        "child_patent_country": "US",
        "child_patent_number": "17546565",
        "filing_date": "2021-07-16",
        "number": "PCT/US2021/041988",
        "parent_status_code": "PENDING"
      }
    ],
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "Hiratsuka",
        "pub_month": "1989-09-01",
        "publication_number": "4868131"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Mai",
        "pub_month": "2019-05-01",
        "publication_number": "10293340"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Sakaino",
        "pub_month": "2007-08-01",
        "publication_number": "20070178521"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Gregory et al.",
        "pub_month": "2010-12-01",
        "publication_number": "20100311186"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dilleen et al.",
        "pub_month": "2015-08-01",
        "publication_number": "20150241423"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Johannsen et al.",
        "pub_month": "2021-02-01",
        "publication_number": "20210055289"
      }
    ]
  },
  {
    "app_filing_date": "2021-03-22",
    "appl_id": "17208845",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Morgan Hill",
        "country": "US",
        "name": "ANRITSU COMPANY",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Morgan Hill",
        "country": "US",
        "name": "Anritsu Company",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Kilpatrick Townsend & Stockton, LLP"
    ],
    "composite_id": "1000005522387!US-US-11558129",
    "cpc_inventive": [
      {
        "cpc_class": "H04B",
        "cpc_subclass": "17/14",
        "version": "2015-01-15"
      },
      {
        "cpc_class": "H04B",
        "cpc_subclass": "17/309",
        "version": "2015-01-15"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "Systems and methods for calibrating VNA modules which dynamically assigns match utilization to improve overall calibration accuracy and reduce problems from a non-optimal set of calibration components and simplify user input requirements during calibration.",
      "background": "PRIORITY\n(1) This application claims the benefit of priority to U.S. Provisional Application No. 62/993,215 filed Mar. 23, 2020 titled \u201cSYSTEM AND METHOD FOR CALIBRATING VECTOR NETWORK ANALYZER MODULES\u201d which application is incorporated by reference in its entirety.",
      "brief": "TECHNICAL FIELD\n\n(1) The present invention relates generally to vector network analyzers and more particularly, the present invention relates to systems and methods for calibrating vector network analyzers.\n\nBACKGROUND\n\n(2) A common task in RF and microwave engineering involves the analysis of circuits using a network analyzer (VNA). The VNA is a reflectometer-based electronic instrument that can be used to measure the frequency response (magnitude and phase) of a device under test (DUT) such as an electrical network, component, circuit, or sub-assembly. This VNA can evaluate nearly all types of RF and microwave devices, including, filters, amplifiers, and complex multifunction and integrated systems. A Vector Network Analyzer contains both a source, used to generate a known stimulus signal, and a set of receivers, used to determine changes to this stimulus caused by the device-under-test or DUT. The stimulus signal is injected into the DUT and the Vector Network Analyzer measures both the signal that's reflected from the input side, as well as the signal that passes through to the output side of the DUT. The Vector Network Analyzer receivers measure the resulting signals and compare them to the known stimulus signal. The measured results are then processed by either an internal or external PC and sent to a display.\n\n(3) Vector Network Analyzer's perform two types of measurements\u2014transmission and reflection. Transmission measurements pass the Vector Network Analyzer stimulus signal through the device under test, which is then measured by the Vector Network Analyzer receivers on the other side. The most common transmission S-parameter measurements are S21 and S12 (Sxy for greater than 2-ports). Swept power measurements are a form of transmission measurement. Some other examples of transmission measurements include gain, insertion loss/phase, electrical length/delay and group delay. Comparatively, reflection measurements measure the part of the VNA stimulus signal that is incident upon the DUT, but does not pass through it. Instead, the reflection measurement measures the signal that travels back towards the source due to reflections. The most common reflection S-parameter measurements are S11 and S22 (Sxx for greater than 2-ports).\n\n(4) Typically a VNA makes use of a frequency sweeping source or stimulus, directional couplers, and one or more receivers that provide ratioed amplitude and phase information such as reflection and transmission coefficients. The VNA utilizes scattering (S)-parameters to evaluate the characteristics of the device under test (DUT) with a high level of precision. S-parameters are a desirable method for measurement because they are relatively easy to derive at high frequencies and are directly related to parameters of interest including gain, return loss, and reflection coefficient. These parameters derived by use of the VNA are essential during design and testing of RF and microwave devices. The measurements made by the VNA can be analyzed to characterize the properties of the DUT.\n\n(5) A VNA must be calibrated in order to make accurate measurements in a particular test configuration. The internal frequency response of the VNA can be calibrated at the factory, however any cables connected externally will have some frequency response that must be calibrated out for high-quality measurements in the particular test configuration. In general calibration uses vector error correction in which error terms are characterized using known standards so that errors can be removed from actual measurements. The process of removing these errors requires the errors and measured quantities to be measured vectorially. The error measurements made during calibration permit the errors to be mathematically eliminated from the measurement results of the DUT.\n\n(6) Existing calibration systems and methods have a variety of weaknesses. One significant problem is where data collection during calibration does not provide enough information to accurately solve for the error coefficients. This results in an undetermined set of equations. If there are frequencies where this happens for the available lines, a singularity occurs and large spikes or instabilities in the calibrated data are possible. With existing calibration systems and methods, effort and calculations are required by the user to ensure that calibration problems do not happen, User expertise is required to determine which standards parameters are needed increasing the complexity of calibration processes.\n\n(7) Accordingly, it would be desirable to provide enhanced VNA calibration systems and methods which overcome the weakness of the prior art and ensure that calibration accurately determines error coefficients for VNAs while reducing complexity for the user.\n\nSUMMARY OF THE INVENTION\n\n(8) The present disclosure describes enhanced VNA calibration systems and methods which overcome the weakness of the prior art and ensure that calibration accurately determines error coefficients for VNAs while reducing complexity for the user.\n\n(9) In embodiments, a new calibration system and method is provided which dynamically assigns match utilization to improve overall calibration accuracy and reduce problems from a non-optimal set of calibration components and simplify user input requirements during calibration.\n\n(10) Further objects and advantages of the present invention will become apparent to those skilled in the art from the following detailed description of the various embodiments, when read in light of the accompanying drawings.",
      "claim_statement": "The invention claimed is:",
      "claims": "1. A method for calibrating a vector network analyzer (VNA), the method comprising: performing line-based calibration measurements; performing match-based calibration measurements; and dynamically assigning match utilization to improve overall calibration accuracy, wherein dynamically assigning match utilization comprises automatically determining the application of line-based calibration and match-based calibration.  \n\n2. The method of claim 1, further comprising storing error coefficients derived from said calibration in memory in said VNA. \n\n3. The method of claim 2, further comprising measuring characteristics of a device using the VNA and correcting said measured characteristics using said error coefficients stored in said memory. \n\n4. The method of claim 2, wherein dynamically assigning match utilization to improve overall calibration accuracy comprises calculating said error coefficients. \n\n5. The method of claim 2, wherein dynamically assigning match utilization to improve overall calibration accuracy comprises calculating error coefficients and automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration. \n\n6. The method of claim 2, wherein dynamically assigning match utilization to improve overall calibration accuracy comprises automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration, wherein LRM calibration is utilized in frequency ranges where TRL calibration approaches a singularity. \n\n7. The method of claim 2, wherein dynamically assigning match utilization to improve overall calibration accuracy comprises automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration, wherein LRM calibration is utilized in frequency ranges where TRL calibration is within user-specified limits of a singularity. \n\n8. The method of claim 1 wherein match-based calibration is utilized in frequency ranges where line-based calibration approaches a singularity. \n\n9. A vector network analyzer (VNA) comprising: a calibration module configured to perform steps comprising: performing line-based calibration measurements; and performing match-based calibration measurements, wherein match-based calibration is utilized in frequency ranges where line-based calibration approaches a singularity.  \n\n10. The VNA of claim 9, wherein the calibration module is configured to perform further steps comprising: calculating error coefficients; and storing said error coefficients in memory in said VNA.  \n\n11. The VNA of claim 9 further comprising dynamically assigning match utilization to improve overall calibration accuracy. \n\n12. The VNA of claim 11, wherein dynamically assigning match utilization comprises automatically determining the application of line-based calibration and match-based calibration. \n\n13. A method for operating a vector network analyzer (VNA), the method comprising: performing line-based calibration measurements; performing match-based calibration measurements; and wherein match-based calibration is utilized in frequency ranges where line-based calibration approaches a singularity.  \n\n14. The method of claim 13, further comprising: calculating error coefficients by dynamically assigning match utilization; and storing said error coefficients in memory in said VNA.  \n\n15. The method of claim 14, further comprising measuring characteristics of a device using the VNA and correcting said measured characteristics using said error coefficients stored in said memory. \n\n16. The method of claim 14, wherein calculating error coefficients by dynamically assigning match utilization comprises automatically determining the application of line-based calibration and match-based calibration. \n\n17. The method of claim 14, wherein calculating error coefficients by dynamically assigning match utilization comprises automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration. \n\n18. The method of claim 14, wherein calculating error coefficients by dynamically assigning match utilization comprises automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration, wherein LRM calibration is utilized in frequency ranges where TRL calibration approaches a singularity. \n\n19. The method of claim 14, wherein calculating error coefficients by dynamically assigning match utilization comprises automatically determining the application of Thru-reflect-line (TRL) calibration and Line-reflect-match (LRM) calibration, wherein LRM calibration is utilized in frequency ranges where TRL calibration is within user-specified limits of a singularity. \n\n20. The method of claim 13 further comprising dynamically assigning match utilization to improve overall calibration accuracy, wherein dynamically assigning match utilization comprises automatically determining the application of line-based calibration and match-based calibration.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n(1) FIG. 1A shows a VNA calibration system according to an embodiment.\n\n(2) FIG. 1B shows a DUT characterization system using a calibrated VNA according to an embodiment.\n\n(3) FIG. 2 shows a method for calibrating a VNA and characterizing a DUT according to an embodiment.\n\n(4) FIG. 3 illustrates VNA calibration results according to an embodiment.\n\n(5) FIG. 4 illustrates VNA calibration results according to an embodiment.\n\nDETAILED DESCRIPTION\n\n(6) The following description is of the best modes presently contemplated for practicing various embodiments of the present invention. The description is not to be taken in a limiting sense but is made merely for the purpose of describing the general principles of the invention. The scope of the invention should be ascertained with reference to the claims. In the description of the invention that follows, like numerals or reference designators will be used to refer to like parts or elements throughout.\n\n(7) In the following description, numerous specific details are set forth to provide a thorough description of the invention. However, it will be apparent to those skilled in the art that the invention may be practiced without these specific details. In other instances, well-known features have not been described in detail so as not to obscure the invention.\n\n(8) Calibration\n\n(9) Calibration is critical to making good VNA S-parameter measurements. While the VNA is a highly-linear receiver and has sufficient spectral purity in its sources to make good measurements, there are a number of imperfections that limit measurements done without calibrations. These imperfections include:\n\n(10) 1. Match\u2014Because the VNA is such a broadband instrument, the raw match is decent but not excellent. Even a 20-dB match, which is physically very good, can lead to errors of greater than 1 dB. Correcting for this raw match greatly reduces the potential error.\n\n(11) 2. Directivity\u2014A key component of a VNA is a directional coupler. This device allows the instrument to separate the signal incident on the DUT from the signal reflected back from the DUT. While the couplers used in the VNA are of very high quality, there is a certain amount of coupled signal, even when a perfect termination is connected. This is related to directivity and can impact measurements of very small reflection coefficients.\n\n(12) 3. Frequency Response\u2014While the internal frequency response of the VNA could be calibrated at the factory, any cables connected externally will have some frequency response that must be calibrated out for high-quality measurements.\n\n(13) Calibration is a tool for correcting for these imperfections, as well as other defects. There are an enormous number of possible calibration algorithms and many of them are implemented within VNAs. The choice between them is largely determined by the media the engineer is working in, the calibration standards available and the desired accuracy/effort trade off. Each of these calibrations has an associated error model that describes what measurement imperfection is being corrected. The error models define error coefficients which fall into several categories that roughly describe the physical effect measurement imperfection that the coefficients are responsible for correcting.\n\n(14) FIG. 1A shows a VNA calibration system according to an embodiment. As shown, in FIG. 1A, VNA 10 comprises a dynamic calibration module (DCM) 15 and two ports 11 and 12. Ports 11 and 12 are connected by line 31 and 32 to the connectors of calibration component 30. This connection depends on the type of component. For wafer standards for example probes are contacted with pads on the wafer. Calibration components may comprises one or more defined standards, lines, wafers or the like having known properties and developed for calibration of VNAs. VNA manufacturers typically provide calibration kits comprising calibration components for defined standards or line-based calibration or other methods. Kits can comprise discreet components, or integrated devices such as wafers integrating several calibration components. The kit components have known properties such that measurements of the components can be used to calibrate a VNA.\n\n(15) After connection the calibration component, a calibration routine is performed by the DCM 15 or VNA 10. VNA 10 and/or DCM 15 comprises a processor and memory configured to control measurements performed by the VNA, store data resulting from such measurements, and analyze such data in order to determine error coefficients for the VNA. The routine takes measurements of the calibration component, analyzes the results as compared to know properties of the calibration component 30, and determines error coefficients for the VNA. The error coefficients are then stored in memory of the VNA. In an embodiment DCM 15 implements a VNA calibration process that dynamically assigns match utilization to improve overall calibration accuracy and can help reduce problems from a non-optimal set of calibration components and simplify user input requirements.\n\n(16) FIG. 1B shows a DUT characterization system using a calibrated VNA according to an embodiment. As shown in FIG. 1B, after the VNA 10 is calibrated, the VNA is disconnected from the calibration component and lines 21 and 22 are connected to connectors 41 and 42 of a device under test (DUT) 40. The VNA may then makes measurements of the VNA, which DUT measurements can be corrected using the error coefficients previously stored in the memory of the VNA 10. The VNA 10 may then characterize the DUT 40 using the corrected DUT measurements.\n\n(17) Although the examples of FIGS. 1A, 1B show a 2-port VNA, the calibration analysis discussed below may also be applied to VNAs having different port configurations, including for example 4-prt VNAs.\n\n(18) FIG. 2 shows a method for calibrating a VNA and characterizing a DUT according to an embodiment. At step 200, the VNA is connected to the calibration components. At step 210 the VNA performs measurements of the calibration components. At step 220 the VNA performs analysis of the measurements of the calibration components made by the VNA as compared to the known properties of the calibration components. In an embodiment the measurement and/or analysis steps implement a VNA calibration process that dynamically assigns match utilization to improve overall calibration accuracy and can help reduce problems from a non-optimal set of calibration components and simplify user input requirements. Based on this analysis the error coefficients for the VNA can be calculated. At step 230 the error coefficients are stored in memory of the VNA as calibration parameters. Calibration of the VNA is complete (steps 200, 210 220, 230).\n\n(19) At step 240 the VNA is connected to the DUT in place of the calibration component. At step 250 the VNA performs measurements of the DUT. These DUT measurements can be corrected using the error coefficients previously stored in the memory of the VNA. At step 260 the VNA characterizes the DUT based on the DUT measurements as corrected by the error coefficients are stored in memory of the VNA. Characterization of the DUT is now complete (steps 240, 250, 260).\n\n(20) Calibration Analysis\n\n(21) As described above, many VNA calibration component systems and methods exist. Calibrations systems and methods fall into two broad classes (at least among those methods that fully solve for the error coefficients) Defined Standards and Line-based methods.\n\n(22) Defined standards: The S-parameters of each calibration standard are specified. All the standards are measured using the VNA, and these measurements are used to solve for the error coefficients of the VNA. A match standard may or may not be part of the standards set but, if present, it is always used.\n\n(23) Line-based methods: Thru-reflect-line (TRL) and related methods fall into this class and are based on assumed ideality of transmission lines (single mode, all differences between lines are based on complex propagation only, etc.). Line-reflect-match (LRM), Line-reflect-reflect-match (LRRM) and related methods are part of this family that use a match standard (either defined by the user or partially solved for using line data) and the match data is always used to solve for directivity (and sometimes other) terms. Multiline TRL (mTRL) is also part of this family where a (potentially) large number of lines are used and the result optimized in a least-squares sense based on self-consistency of the measurements. The mTRL method does automatically underweight lines that do not contribute to the measurement but it is possible that the entire set of lines is not optimal. The TRL family of calibrations is generally band-limited. Multiple line variants (mTRL) help with this but the line set may still be non-optimal for the frequency range of interest.\n\n(24) All of the conventional methods have their weaknesses and in the line-based methods, one problem is when the collection of lines do not provide enough information to accurately solve for the error coefficients of the VNA. This happens when the line length differences are near 0 or a multiple of half-wavelengths since the S-parameters of the line are the same (if loss is low enough). This results in an undetermined set of equations. If there are frequencies where this happens for the available lines, a singularity occurs and large spikes or instabilities in the calibration data are possible.\n\n(25) With existing calibrations, effort and calculations are required by the user to ensure that the above problem does not happen (figuring out which standards parameters are needed, sometimes creating new standards, etc.). TRL (Thru, Reflect, Line) represents a family of calibration techniques that measure two transmission standards and one reflection standard to determine the 2-port 12-term error coefficients. For example, TRM (Thru, Reflect, Match), LRL (Line, Reflect, Line), LRM (Line, Reflect, Match) are all included in this family. mTRL helps to some degree since more lines are available (reducing the chances of singularities) but it is still possible that the problem will occur.\n\n(26) To illustrate the problems with conventional calibration, consider an extreme example involving a 2-port coaxial calibration covering 70 kHz-44 GHz. The user has selected TRL with two lines (0 and 16 mm) which is unfortunate for this frequency range since the lines have \u02dcequal S-parameters at very low frequencies and at half-wavelength multiples (\u02dc9.4, 18.8, 28.2 and 27.6 GHz). A DUT is measured after performing this calibration and the resulting measurements are the solid curves shown in FIGS. 3 and 4. As shown in the FIGS. 3 and 4 the results are significantly in error in the vicinity of these singularities (\u02dc9.4, 18.8, 28.2 and 27.6 GHz). This will result in poor calibration of the VNA in the region of the singularities.\n\n(27) FIG. 3 is a plot (solid line) of reflection measurements for the DUT after calibration of the VNA. The DUT is expected to have values in the \u221240 to \u221250 dB range at 40 GHz. The difference in the mean values in the FIG. 3 plot are believed to be due to connector repeatability rather than from algorithmic effects.\n\n(28) FIG. 4 is a plot (solid line) of insertion loss measurements for the DUT after the calibration of the VNA described above. The values of deviation at the singularities are not very repeatable as they are sensitive to milli-dB variations in the raw parameters. Further, some post-processing schemes are sensitive to sudden data changes as they introduce passivity or causality violations. Such violations can cause simulations, using the corrected S-parameter data as input, to fail to converge and complicate the users' design or analysis work.\n\n(29) In order to correct the problems with the standard calibration analysis, the dynamic calibration module (DCM 15 of FIG. 1A implements an improved calibration analysis system. The improved calibration process dynamically assigns match utilization to improve overall calibration accuracy and can help reduce problems from a non-optimal set of calibration components and simplify user input requirements. The improved calibration analysis system is an enhancement to VNA calibration routines, particularly those relying on multiple lengths of transmission lines or multiple offset lengths of reflects to cover a frequency range.\n\n(30) The system is enhanced by including the use of an alternate calibration path (dependent on a match or non-full-reflect standard) when singularities are approached. The match-based calibration variant takes over when the original length set has numerical issues and results in a calibration with better residual errors. Accordingly, the analysis performed by the DCM dynamically switches between line and match based calibration over the frequency range to reduce numerical issue based problems in the calibration.\n\n(31) An LRM-variant is included in the calibration system and method such that an LRM-variant is included in the calibration when the line lengths (of offset lengths) in the TRL calibration approach a singularity. The LRM-variant automatically takes over the calibration from the TRL calibration in the problem frequency range. The degree of approach to the singularity may be user selectable such that the range of frequencies over which LRM is responsible in place of TRL can be selected by the user. The match model may be predefined of fit using other available standards data. According the DCM results in improved calibration data for the VNA by automatically selecting either LRM or TRL at different regions of the frequency range over which calibration is required.\n\n(32) As shown in FIGS. 3 and 4, the VNA calibration was repeated using the dynamic match selection with the algorithm looking for phase differences between line lengths coming within 10 degrees of problem areas. Such algorithm can be implemented in the DCM 15 of VNA 10. A simple match model was used. The resulting measurement of the same DUT is shown in the dashed lines of FIGS. 3 and 4. FIG. 3 shows a plot (dashed line) of reflection measurements for the DUT after calibration of the VNA using the enhanced method. FIG. 4 show a plot (dashed line) of insertion loss measurements for the DUT after the calibration of the VNA using the enhanced method. As shown by the dashed lines in FIGS. 3 and 4, the errors in the singular regions have been improved (as compared to the solid lines) and the data continuity in the problem regions is better than 0.01 dB in loss and 0.1 dB in reflection (and repeatability was much better). The residual tracking error has been reduced by more than two orders of magnitude.\n\n(33) Effectively, the VNA calibration was performed using a combination of TRL and LRM methodologies with dynamic switching between the two regimes over the wavelength range of the calibration. The looked for phase differences between line lengths coming within 10 degrees of problem areas (singularities) and to determine what in frequency ranges LRM should be used. Note that if the match standard (LRM) had been used for the whole frequency range, the result might have been less optimal since the model accuracy of the match standard is not often not as good as the fundamental characteristic impedance accuracy of the transmission lines.\n\n(34) Importantly for on-wafer calibrations, the match element is normally readily available and consumes little real estate (compared to adding several more transmission line sections).\n\n(35) From a convenience and practical speed point-of-view, the user also did not have to perform calculations off-line to determine when additional components would be used.\n\n(36) Although the examples of FIGS. 1A, 1B, 3 and 4 relate to a 2-port VNA, the calibration analysis discussed below may also be applied to VNAs having different port configurations, including for example 4-port VNAs.\n\n(37) In some embodiments, the present invention includes a computer program product, for example DCM 15, which can comprise a storage medium or computer readable medium (media) having instructions stored thereon/in which can be used to program a computer to perform any of the processes of the present invention. The storage medium can include, but is not limited to, any type of disk including floppy disks, optical discs, DVD, CD-ROMs, microdrive, and magneto-optical disks, ROMs, RAMs, EPROMs, EEPROMs, DRAMs, VRAMs, flash memory devices, magnetic or optical cards, nanosystems (including molecular memory ICs), or any type of media or device suitable for storing instructions and/or data. In and embodiment DCM 16 comprises a VNA control systems including a processor configured to control the VNA to perform the processes of the present invention including dynamic match calibration and subsequent testing of a DUT using such calibration.\n\n(38) The previous description of the preferred embodiments is provided to enable any person skilled in the art to make or use the embodiments of the present invention. While the invention has been particularly shown and described with reference to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention. It is intended that the scope of the invention be defined by the following claims and their equivalents."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 3,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 11,
      "claims_start": 10,
      "description_end": 10,
      "description_start": 7,
      "drawings_end": 6,
      "drawings_start": 4,
      "front_page_end": 3,
      "front_page_start": 1,
      "number_of_claims": 20,
      "number_of_drawing_sheets": 3,
      "number_of_figures": 5,
      "page_count": 11,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 10,
      "specification_start": 7,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000005522387,
    "field_of_search_cpc": [
      "H04B 17/309"
    ],
    "group_art_unit": "2641",
    "guid": "US-11558129-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G29/581/115",
    "intl_class_current_primary": [
      {
        "intl_class": "H04B",
        "intl_subclass": "17/309",
        "version": "2015-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H04B",
        "intl_subclass": "17/14",
        "version": "2015-01-01"
      }
    ],
    "intl_class_issued": [
      "H04B17/309",
      "H04B17/14"
    ],
    "inventors": [
      {
        "city": "San Jose",
        "country": "US",
        "name": "Martens; Jon",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Jose",
        "country": "US",
        "name": "Chock; Gary",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Jose",
        "country": "US",
        "name": "Tu; Jamie",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Jose",
        "country": "US",
        "name": "Vayner; Elena",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "San Jose",
        "country": "US",
        "name": "Gandavarapu; Amruth Sai",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Sunnyvale",
        "country": "US",
        "name": "Damle; Mrunal",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Martens; Jon et al.",
    "legal_firm_name": [
      "Kilpatrick Townsend & Stockton, LLP"
    ],
    "npl_references": [
      {
        "citation": "Akmal, M. et al., \u201cAn Enhanced Modulated Waveform Measurement System for the Robust Characterization of Microwave Devices under Modulated Excitation\u201d, Proceedings of the 6th European Microwave Integrated Circuits Conference, Oct. 10-11, 2011, Manchester, UK, \u00a9 2011, pp. 180-183.",
        "cited_by_examiner": false
      },
      {
        "citation": "Cunha, Telmo R. et al., \u201cCharacterizing Power Amplifier Static AM/PM with Spectrum Analyzer Measurements\u201d, IEEE \u00a9 2014, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Fager, Christian et al., \u201cPrediction of Smart Antenna Transmitter Characteristics Using a New Behavioral Modeling Approach\u201d IEEE \u00a92014, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Fager, Christian et al., \u201cAnalysis of Nonlinear Distortion in Phased Array Transmitters\u201d 2017 International Workshop on Integrated Nonlinear Microwave and Millmetre-Wave Circuits (INMMiC), Apr. 20-21, 2017, Graz, Austria, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Martens, J. et al., \u201cTowards Faster, Swept, Time-Coherent Transient Network Analyzer Measurements\u201d 86th ARFTG Conf. Dig., Dec. 2015, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Martens, J., \u201cMatch correction and linearity effects on wide-bandwidth modulated AM-AM and AM-PM measurements\u201d 2016 EuMW Conf. Dig., Oct. 2016, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Nopchinda, Dhecha et al., \u201cEmulation of Array Coupling Influence on RF Power Amplifiers in a Measurement Setup\u201d, IEEE \u00a9 2016, 4 pages.",
        "cited_by_examiner": false
      },
      {
        "citation": "Pedro, Jose Carlos et al., \u201cOn the Use of Multitone Techniques for Assessing RF Components' Intermodulation Distortion\u201d, IEEE Transactions On Microwave Theory and Techniques, vol. 47, No. 12, Dec. 1999, pp. 2393-2402.",
        "cited_by_examiner": false
      },
      {
        "citation": "Ribeiro, Diogo C. et al., \u201cD-Parameters: A Novel Framework for Characterization and Behavorial Modeling of Mixed-Signal Systems\u201d, IEEE Transactions On Microwave Theory and Techniques, vol. 63, No. 10, Oct. 2015, pp. 3277-3287.",
        "cited_by_examiner": false
      },
      {
        "citation": "Roblin, Patrick, \u201cNonlinear RF Circuits and Nonlinear Vector Network Analyzers; Interactive Measurement and Design Techniques\u201d, The Cambridge RF and Microwave Engineering Series, Cambridge University Press \u00a9 2011, entire book.",
        "cited_by_examiner": false
      },
      {
        "citation": "Rusek, Fredrik et al., \u201cScaling Up MIMO; Opportunities and challenges with very large arrays\u201d, IEEE Signal Processing Magazine, Jan. 2013, pp. 40-60.",
        "cited_by_examiner": false
      },
      {
        "citation": "Senic, Damir et al., \u201cEstimating and Reducing Uncertainty in Reverberation-Chamber Characterization at Millimeter-Wave Frequencies\u201d, IEEE Transactions on Antennas and Propagation, vol. 64, No. 7, Jul. 2016, pp. 3130-3140.",
        "cited_by_examiner": false
      },
      {
        "citation": "Senic, Damir et al., \u201cRadiated Power Based on Wave Parameters at Millimeter-wave Frequencies for Integrated Wireless Devices\u201d, IEEE \u00a9 2016, 4 pages.",
        "cited_by_examiner": false
      },
      {}
    ],
    "patent_title": "System and method for calibrating vector network analyzer modules",
    "primary_examiner": "Liao; Hsinchun",
    "publication_date": "2023-01-17",
    "publication_number": "11558129",
    "related_apps": [
      {
        "filing_date": "2020-03-23",
        "number": "62993215"
      }
    ],
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "1998-09-01",
        "publication_number": "5801525"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "1998-09-01",
        "publication_number": "5812039"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Finch",
        "pub_month": "1999-06-01",
        "publication_number": "5909192"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "1999-11-01",
        "publication_number": "5977779"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "2000-04-01",
        "publication_number": "6049212"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Wong",
        "pub_month": "2001-09-01",
        "publication_number": "6291984"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kapetanic",
        "pub_month": "2001-11-01",
        "publication_number": "6316945"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Wong",
        "pub_month": "2001-12-01",
        "publication_number": "6331769"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Chio",
        "pub_month": "2002-12-01",
        "publication_number": "6496353"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Constantine",
        "pub_month": "2003-01-01",
        "publication_number": "6504449"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "2003-01-01",
        "publication_number": "6509821"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "2003-02-01",
        "publication_number": "6525631"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kapetanic",
        "pub_month": "2003-03-01",
        "publication_number": "6529844"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Wong",
        "pub_month": "2003-04-01",
        "publication_number": "6548999"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Dunsmore",
        "pub_month": "2003-11-01",
        "publication_number": "6643597"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2003-11-01",
        "publication_number": "6650123"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2003-12-01",
        "publication_number": "6665628"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Mori",
        "pub_month": "2003-12-01",
        "publication_number": "6670796"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Stickle",
        "pub_month": "2004-01-01",
        "publication_number": "6680679"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Truesdale",
        "pub_month": "2004-03-01",
        "publication_number": "6700366"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Abou-Jaoude",
        "pub_month": "2004-03-01",
        "publication_number": "6700531"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Kapetanic",
        "pub_month": "2004-03-01",
        "publication_number": "6714898"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2004-07-01",
        "publication_number": "6766262"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2004-12-01",
        "publication_number": "6832170"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Marshall",
        "pub_month": "2004-12-01",
        "publication_number": "6834180"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2005-01-01",
        "publication_number": "6839030"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2005-04-01",
        "publication_number": "6882160"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2005-05-01",
        "publication_number": "6888342"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2005-05-01",
        "publication_number": "6894581"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2005-07-01",
        "publication_number": "6917892"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2005-08-01",
        "publication_number": "6928373"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2005-09-01",
        "publication_number": "6943563"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2006-02-01",
        "publication_number": "7002517"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "2006-03-01",
        "publication_number": "7011529"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bridge",
        "pub_month": "2006-03-01",
        "publication_number": "7016024"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2006-03-01",
        "publication_number": "7019510"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2006-05-01",
        "publication_number": "7054776"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2006-06-01",
        "publication_number": "7068046"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2006-08-01",
        "publication_number": "7088111"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Oldfield",
        "pub_month": "2006-09-01",
        "publication_number": "7108527"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2006-10-01",
        "publication_number": "7126347"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Buchwald",
        "pub_month": "2007-02-01",
        "publication_number": "7173423"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Stickle",
        "pub_month": "2007-10-01",
        "publication_number": "7284141"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2007-12-01",
        "publication_number": "7304469"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Feldman",
        "pub_month": "2007-12-01",
        "publication_number": "7307493"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2009-03-01",
        "publication_number": "7509107"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2009-03-01",
        "publication_number": "7511577"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2009-04-01",
        "publication_number": "7521939"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2009-06-01",
        "publication_number": "7545151"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2010-03-01",
        "publication_number": "7683602"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2010-03-01",
        "publication_number": "7683633"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2010-04-01",
        "publication_number": "7705582"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2010-06-01",
        "publication_number": "7746052"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2010-07-01",
        "publication_number": "7764141"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2011-01-01",
        "publication_number": "7872467"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2011-04-01",
        "publication_number": "7924024"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Aboujaoude",
        "pub_month": "2011-06-01",
        "publication_number": "7957462"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Tiernan",
        "pub_month": "2011-07-01",
        "publication_number": "7983668"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2011-09-01",
        "publication_number": "8027390"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2011-11-01",
        "publication_number": "8058880"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Barber",
        "pub_month": "2012-03-01",
        "publication_number": "8145166"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2012-04-01",
        "publication_number": "8156167"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Brown",
        "pub_month": "2012-04-01",
        "publication_number": "8159208"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Huang",
        "pub_month": "2012-05-01",
        "publication_number": "8169993"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2012-05-01",
        "publication_number": "8185078"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2012-10-01",
        "publication_number": "8278944"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2012-10-01",
        "publication_number": "8294469"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2012-11-01",
        "publication_number": "8305115"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2012-11-01",
        "publication_number": "8306134"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2013-04-01",
        "publication_number": "8410786"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2013-04-01",
        "publication_number": "8417189"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Aboujaoude",
        "pub_month": "2013-06-01",
        "publication_number": "8457187"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2013-07-01",
        "publication_number": "8493111"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2013-07-01",
        "publication_number": "8498582"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Varjonen",
        "pub_month": "2013-09-01",
        "publication_number": "8538350"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2013-11-01",
        "publication_number": "8593158"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-01-01",
        "publication_number": "8629671"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2014-01-01",
        "publication_number": "8630591"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-03-01",
        "publication_number": "8666322"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2014-05-01",
        "publication_number": "8718586"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-06-01",
        "publication_number": "8760148"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-08-01",
        "publication_number": "8816672"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Barber",
        "pub_month": "2014-08-01",
        "publication_number": "8816673"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-11-01",
        "publication_number": "8884664"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2014-12-01",
        "publication_number": "8903149"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2014-12-01",
        "publication_number": "8903324"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Dorenbosch",
        "pub_month": "2015-01-01",
        "publication_number": "8942109"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Brown",
        "pub_month": "2015-08-01",
        "publication_number": "9103856"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2015-08-01",
        "publication_number": "9103873"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Warwick",
        "pub_month": "2015-10-01",
        "publication_number": "9153890"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2015-11-01",
        "publication_number": "9176174"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2015-11-01",
        "publication_number": "9176180"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2015-12-01",
        "publication_number": "9210598"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2016-01-01",
        "publication_number": "9239371"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Noujeim",
        "pub_month": "2016-03-01",
        "publication_number": "9287604"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Robertson",
        "pub_month": "2016-05-01",
        "publication_number": "9331633"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Emerson",
        "pub_month": "2016-05-01",
        "publication_number": "9337941"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2016-06-01",
        "publication_number": "9366707"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Truesdale",
        "pub_month": "2016-09-01",
        "publication_number": "9455792"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Lundquist",
        "pub_month": "2017-01-01",
        "publication_number": "9560537"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Huang",
        "pub_month": "2017-02-01",
        "publication_number": "9571142"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2017-03-01",
        "publication_number": "9588212"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2017-03-01",
        "publication_number": "9594370"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2017-03-01",
        "publication_number": "9606212"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Warwick",
        "pub_month": "2017-06-01",
        "publication_number": "9680245"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Warwick",
        "pub_month": "2017-06-01",
        "publication_number": "9685717"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Elder-Groebe",
        "pub_month": "2017-07-01",
        "publication_number": "9696403"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2017-08-01",
        "publication_number": "9733289"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Martens",
        "pub_month": "2017-09-01",
        "publication_number": "9753071"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2017-09-01",
        "publication_number": "9768892"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-01-01",
        "publication_number": "9860054"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-05-01",
        "publication_number": "9964585"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-05-01",
        "publication_number": "9967085"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-05-01",
        "publication_number": "9977068"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-06-01",
        "publication_number": "10003453"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-06-01",
        "publication_number": "10006952"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-08-01",
        "publication_number": "10064317"
      },
      {
        "cited_by_examiner": false,
        "patentee_name": "Bradley",
        "pub_month": "2018-10-01",
        "publication_number": "10116432"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Shamir",
        "pub_month": "2004-07-01",
        "publication_number": "20040135726"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Martens",
        "pub_month": "2004-08-01",
        "publication_number": "20040153265"
      }
    ]
  },
  {
    "app_filing_date": "2017-06-12",
    "appl_id": "15620625",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Seattle",
        "country": "US",
        "name": "Amazon Technologies, Inc.",
        "state": "WA",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "assignees": [
      {
        "city": "Seattle",
        "country": "US",
        "name": "Amazon Technologies, Inc.",
        "postal_code": "N/A",
        "state": "WA",
        "type_code": "02"
      }
    ],
    "attorney_name": [
      "Patterson + Sheridan, LLP"
    ],
    "composite_id": "1000002717343!US-US-11556879",
    "cpc_inventive": [
      {
        "cpc_class": "G09B",
        "cpc_subclass": "19/003",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/06398",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/08",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06T",
        "cpc_subclass": "7/20",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/0639",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USPT",
    "derwent_week_int": 0,
    "document": {
      "abstract": "Method and apparatus for evaluating user movement in a fulfillment center. A plurality of sub-tasks for performing a fulfillment operation are determined. Embodiments retrieve a training set of motion data that includes a plurality of training motion data samples, each specifying motion data over a respective window of time, and train a first data model to classify instances of motion data into portions, based on the plurality of sub-tasks. At least one data model is trained to assess performance of the fulfillment operation, using the training set of motion data. An instance of motion data describing motion performed during the fulfillment operation is received, the instance of motion data is divided into a plurality of portions, using the first data model, and a measure of quality is generated, by analyzing the plurality of portions of the instance of motion data using the trained at least one data model.",
      "brief": "BACKGROUND\n\n(1) The present description relates to motion analysis, and more specifically, to using data models to assess captured motion data to determine the efficacy of a performance of a fulfillment operation.\n\n(2) Modern inventory systems, such as those in mail-order warehouses, supply chain distribution centers, and custom-order manufacturing facilities, face significant challenges in responding to requests for inventory items. As inventory systems grow, the challenges of simultaneously completing many packing, storing, and other inventory-related tasks become non-trivial. For instance, in product distribution centers (e.g., fulfillment centers), vast quantities of products are processed for shipment to consumers traditionally using manual labor and/or mechanical handling equipment.\n\n(3) Inventory systems that are tasked with responding to large numbers of diverse inventory requests typically exhibit inefficient utilization of system resources, including space and equipment, and may further exhibit inefficient performance of fulfillment operations by employees. In many situations, employees may not realize they are performing a given task in a less than optimal manner or in a manner that may result in injury to the employee over time. As a result, inventory systems may experience lower throughput, unacceptably long response times, and an ever-increasing backlog of unfinished tasks, and employee safety may be negatively impacted.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A method, comprising: determining a plurality of sub-tasks for carrying out a fulfillment operation; retrieving a training set of motion data that includes a plurality of training motion data samples, each specifying motion data over a respective window of time; training a first data model to classify instances of motion data into temporal chunks, based on the plurality of sub-tasks; training at least one data model to assess performance of the fulfillment operation, using the training set of motion data; determining an instance of motion data describing motion performed in carrying out the fulfillment operation, comprising: capturing, via one or more camera devices, images of a user performing the fulfillment operation; tracking, based on the captured images, movement of one or more markers on a motion capture device worn by the user; and generating the instance of motion data based on the movement of the one or more markers;  dividing the instance of motion data into a plurality of temporal chunks, using the first data model; generating a measure of quality for the instance of motion data, by analyzing the plurality of temporal chunks of the instance of motion data using the trained at least one data model, wherein generating the measure of quality comprises generating a quality score for at least one first sub-task of the plurality of sub-tasks; and upon determining that the quality score for the first sub-task satisfies a predetermined condition, rendering, to the user via an immersive reality device, during performance of the fulfillment operation, a plurality of frames depicting motion of the performance of the first sub-task based on one of the plurality of temporal chunks of the instance of motion data corresponding to the first sub-task of the plurality of sub-tasks.  \n\n2. The method of claim 1, wherein the training set of motion data comprises a plurality of positive motion data samples and a plurality of negative motion data samples. \n\n3. The method of claim 1, wherein training the at least one data model to assess performance of the fulfillment operation, using the training set of motion data, comprises training a plurality of data models to assess the performance of the fulfillment operation, wherein each of the plurality of data models corresponds to a respective one of the plurality of sub-tasks. \n\n4. The method of claim 1, wherein the quality score for the at least one first sub-task of the plurality of sub-tasks comprises a plurality of scores, each corresponding to a different performance metric for the first sub-task. \n\n5. The method of claim 1, wherein training a first data model to classify instances of motion data into temporal chunks, based on the plurality of sub-tasks, further comprises: receiving, for each of the plurality of training motion data samples, a respective time window within the training motion data samples that corresponds to each of the plurality of sub-tasks, wherein the first data model is trained to classify instances of motion data into temporal chunks, based on the time windows for each of the plurality of training motion data samples and each of the plurality of sub-tasks.  \n\n6. The method of claim 1, wherein the fulfillment operation comprises at least one of a pick operation and a stow operation. \n\n7. The method of claim 6, wherein the plurality of sub-tasks further comprise at least logging into a workstation, selecting a workflow, waiting on a pod to arrive, scanning a container, picking a product out of the container, identifying a target bin for the product, placing the product in the target bin, and scanning the target bin. \n\n8. The method of claim 1, wherein the predetermined condition comprises the quality score being less than a predefined threshold score associated with the first sub-task. \n\n9. A method, comprising: receiving an instance of motion data describing motion performed by an operator in carrying out a fulfillment operation, the motion data having been captured by a motion capture system tracking, via one or more first motion capture devices, movement of a second motion capture device worn by the operator during performance of the fulfillment operation; determining, for at least one portion of the instance of motion data, which sub-task of a plurality of sub-tasks of the fulfilment operation corresponds to the at least one portion of the instance of motion data, based on a first data model; analyzing the instance of motion data using at least one data model trained with historically collected motion data; determining a measure of quality for the instance of motion data, based on the analysis, wherein determining the measure of quality comprises determining a quality score for the sub-task corresponding to the at least one portion of the instance of motion data; and upon determining that the quality score for the sub-task satisfies a predetermined condition, rendering, to the operator via an immersive reality device, during the performance of the fulfillment operation, a plurality of frames depicting motion of the performance of the sub-task based on the at least one portion of the instance of motion data.  \n\n10. The method of claim 9, further comprising: retrieving a training set of motion data that includes a plurality of training motion data samples, each specifying motion data over a respective window of time; and training the first data model to classify instances of motion data into portions, based on the plurality of sub-tasks.  \n\n11. The method of claim 10, further comprising: training a plurality of data models to assess performance of the fulfillment operation, using the training set of motion data.  \n\n12. The method of claim 11, wherein analyzing the instance of motion data using the at least one data model trained with historically collected motion data further comprises: assessing performance of each portion of the instance of motion data, using a respective one of the plurality of data models; and generating an overall quality assessment of the performance of the fulfillment operation described by the instance of motion data, based on the performance assessment of the portions of the instance of motion data.  \n\n13. The method of claim 9, wherein: the one or more first motion capture devices comprise camera devices; and tracking the movement of the second motion capture device comprises capturing, via the camera devices, images of one or more markers on the second motion capture device during performance of the fulfillment operation.  \n\n14. The method of claim 9, wherein tracking the movement of the second motion capture device comprises: detecting, via the first motion capture device, one or more signals emitted by the second motion capture device during performance of the fulfillment operation; and determining, for each of the one or more signals, a respective set of location coordinates corresponding to a different position of the second motion capture device over a period of time during the performance of the fulfillment operation.",
      "description": "BRIEF DESCRIPTION OF DRAWINGS\n\n(1) FIG. 1 illustrates an inventory system having multiple regions and mobile drive units that carry inventory holders about the regions.\n\n(2) FIG. 2 shows a side view of a mobile drive unit and an inventory holder employed in the inventory system.\n\n(3) FIG. 3 is a block diagram illustrating a training system, according to one embodiment described herein.\n\n(4) FIG. 4A-D illustrate a user performing a fulfillment operation, according to embodiments described herein.\n\n(5) FIG. 5 is a flow diagram illustrating a method of training a data model to assess the performance of a fulfillment operation, according to one embodiment described herein.\n\n(6) FIG. 6 is a flow diagram illustrating a method of evaluating user efficiency in performing a fulfillment operation, according to one embodiment described herein.\n\n(7) FIG. 7 illustrates an evaluation summary report, according to one embodiment described herein.\n\n(8) FIG. 8A-B illustrate an augmented reality environment that includes a virtual avatar performing a fulfillment operation, according to one embodiment described herein.\n\n(9) FIG. 9 illustrates reality environment that includes a virtual avatar performing a fulfillment operation, according to one embodiment described herein.\n\n(10) FIG. 10 illustrates a workflow for providing an evaluation summary and feedback for a performance of a fulfillment operation, according to one embodiment described herein.\n\n(11) FIG. 11 is a flow diagram illustrating a method for generating virtual content for providing feedback for a performance of a fulfillment operation, according to one embodiment described herein.\n\nDETAILED DESCRIPTION\n\n(12) Inventory systems are utilized by many entities for storing and managing inventory. For example, some retailers may utilize a warehouse of racks that store inventory items in various bins. When an order for a specific inventory item needs to be filled by the retailer, a worker typically retrieves the inventory item from the bin where the inventory item is stored.\n\n(13) Inventory systems according to one embodiment described herein utilize one or more mobile drive units to automatically retrieve inventory holders from warehouse storage. The inventory holders may be entire racks of shelves, with each shelf having bins for various inventory items. Mobile drive units may be self-powered robotic devices configured to move freely about the warehouse. Racks may be transported by mobile drive units to a pick station for performing inventory operations. The station may include a human operator and/or automated equipment to remove desired inventory items from the inventory holders and complete orders. In some operations, the items are removed from inventory holders and placed on order holders, which too can be maneuvered by the mobile drive units. Throughout this document, reference is made to inventory holders as the racks that are moved about by the mobile drive units. It is noted that inventory holders is used in the general sense as structures that hold inventory items, items that are part of an order, packaging elements for the orders, and essentially any other item or element that might be used by the inventory and fulfillment system. As such, inventory holders may also be referred to as holders, order holders, container holders, bins, and so forth.\n\n(14) Mobile drive units may move about the warehouse and/or retrieve inventory holders in response to commands and/or instructions received from an automated guidance system. For example, a management module may control administration and coordination of certain elements and/or tasks within the warehouse. The management module may receive orders for inventory items in the warehouse and coordinate task assignments for fulfilling the orders, including providing instructions to mobile drive units to transport racks with requested inventory items to an inventory station for completion of an order. The management module may also provide guidance at the inventory station for how the order is to be assembled.\n\n(15) In a distribution warehouse for an office supply store, the management module may receive an order for a printer, several ink cartridges, and a box of paper. The management module may determine the locations of these items within the racks of storage in the distribution warehouse. The items may, for instance, be on the shelves of three separate racks. The management module may issue tasks for three different mobile drive units to retrieve each rack respectively holding the printer, ink cartridges, and paper, and for the mobile drive units to transport the racks holding those items to an inventory station where the three items may be picked from the racks and packed into a box for shipment. In doing so, the management module may coordinate the various tasks such that all of the orders received by the management module are fulfilled in an efficient manner.\n\n(16) The mobile drive units are continuously moving the inventory holders into and out from the pick stations to place the appropriate items near the pickers at the right time. For existing pick stations, current inventory systems face a challenge of increasing the lines per hour so that pickers may retrieve more items per hour, thereby improving efficiency and throughput of the inventory system.\n\n(17) Employee training constitutes a significant expense within a fulfillment system environment. For a newly hired employee, it may take days if not weeks until that employee is operating at an average level of efficiency. Moreover, such training may need to be done by an already trained employee, thereby lowering the trained employees productivity during the training process. Even once an employee is trained, it can be difficult to continuously monitor their performance and to address any issues that arise. Typically such assessments are done by periodic audits. However, such processes may allow issues to persist for a significant amount of time, and such audits may not catch all the issues in a given employee's performance. For example, a particular employee may tend to lift heavy items with poor technique (e.g., using his lower back), which can result in injury to the employee over time. Even if such an issue is caught during an audit, the employee may have already sustained injury by that time.\n\n(18) In some cases, performance issues can be subtle and an employee may not realize there is a more optimal way to perform a given process. As an example, an employee may be able to perform a given fulfillment operation slightly faster if the employee gets into position to retrieve an item from an inventory holder, before the mobile drive unit carrying the inventory holder arrives at the pick station. While such slight gains in performance may appear insignificant to the employee and may be difficult to detect even during an audit, these performance gains may result in a substantial cost savings when viewed in the aggregate across all of the employees in multiple fulfillment centers.\n\n(19) As such, embodiments provide techniques for monitoring employee movement using motion capture devices and analyzing the collected motion data to assess the employee's performance. In one embodiment, a motion capture system can monitor behavior of fulfillment center employees performing a fulfillment operation to collect instances of motion data. For example, each of the collected instances of motion data can correspond to one of the fulfillment center employees performing a given task over a window of time (e.g., a pick operation at a pick station for a single item). A motion analysis component could then train one or more data models using the collected instances of motion data, for use in assessing employee performance of the fulfillment operation. For example, the collected instances of motion data could be scored based on their efficiency, and the motion analysis component could use the collected instances of motion data and their efficiency scores to train a data model to predict efficiency scores when given an additional instance of motion data as input. A given employee performing the fulfillment operation could then be monitored using the motion capture devices and the motion data collected could be analyzed using the trained data model to calculate an efficiency score for the employee.\n\n(20) FIG. 1 shows one illustrative example of an inventory system 100 that may be used to implement a technique for efficient shuffling of inventory holders. The inventory system 100 may be arranged in a facility or warehouse (e.g., distribution facility, fulfillment center, etc.) that is logically organized into areas or regions associated with various functions. In the illustrated example, the warehouse includes a storage region 102, a pick station 104, and an inventory pier 106. In practice, depending upon the size of the inventory system 100, the warehouse may hold more than one of the storage regions 102, pick stations 104, and inventory piers 106, or the warehouse may be configured without the storage region 102, or the pick station 104, or the inventory pier 106.\n\n(21) The inventory system 100 includes a management module 110, multiple mobile drive units 112, inventory holders 114 and a training system 150. Only some of the inventory holders 114 are shown referenced with the number 114 for ease of illustration. The mobile drive units 112 are independent, self-powered robotic devices that may move freely about the warehouse, under their own direction or through coordination by the management module 110. The mobile drive units 112 may be used at various times to transport the inventory holders 114 around the warehouse among the regions. For instance, the mobile drive units 112 may transport the inventory holders 114 between the storage region 102 and the pick station 104 or inventory pier 106.\n\n(22) Each inventory holder 114 may be implemented as a physical structure to hold various inventory items. The inventory holder 114 has a physical length, width, and height that may be standardized or varied within the inventory system. As used herein, the inventory holders 114 may be configured to hold essentially any type or size of item or be used for any number of purposes, including, but not limited to, carrying pallets, storing shipping supplies, holding garbage, supporting empty boxes waiting for inventory, supporting filled boxes with items once orders are fulfilled, and so on. Furthermore, as used herein, inventory holders also include holders for other types of products or items and hence include order holders.\n\n(23) In one implementation, the inventory holder 114 may be formed as a rack having multiple shelves to support various types of inventory items. For instance, the inventory holders 114 may include multiple storage bins with each storage bin capable of holding a different type of inventory item. The inventory holders 114 are capable of being carried, rolled, or otherwise moved by the mobile drive units 112. Each inventory holder 114 may have a plurality of faces, and each bin may be accessible through specific faces. The rack is free-standing when at rest, but can be lifted and moved by the mobile drive units 112. The mobile drive units 112 may be configured to rotate inventory holders 114 at appropriate times to present particular faces of inventory holders 114 and the associated bins to an operator or other components of inventory system 10. One example is described below in more detail with reference to FIG. 2.\n\n(24) One or more mobile drive units 112 are provided in the storage region 102 to shuffle inventory holders 114 among the storage locations 120 and to transport the inventory holders between the storage region and other regions in the warehouse. Two loaded drive units 112(1) and 112(2) are shown in the storage area 102 carrying associated inventory holders 114(1) and 114(2), respectively, down aisles between the sets of predefined storage locations 120. An unloaded drive unit 112(3) is also shown moving through an aisle between the predefined storage locations 120.\n\n(25) The pick station region 104 is designed with multiple locations 122 and 124 to accommodate associated resting inventory holder 114. In FIG. 1, the pick station 104 has five locations arranged in two linear rows. A first line of three pick locations 122 is mapped next to a picking area in which a picker 126 picks inventory from the inventory holders 114 at the pick locations 122 and loads them into boxes or containers supported by another inventory holder 114(4) mounted on a mobile drive unit 112(4). In this illustration, the picker 126 is a human, although the picker may alternatively or additionally involve a mechanical picking apparatus. A second line of two staging locations 124 is mapped adjacent to, but spaced from, the first line of pick locations 122. The staging locations 124 temporarily hold inventory holders 114 on the way to and from the pick locations 122 of the pick station 104. It is noted that five locations are merely representative, and that pick stations 104 may be designed with more or fewer than five locations.\n\n(26) Order holders may, for example, be racks of shelving that are configured to hold boxes or containers that will be used to fill orders for inventory items. Each box or container may be associated with a particular order. For instance, an order holder may store a cardboard box that is to be used for shipping an order for several ink cartridges and several reams of paper. Order holders with orders needing such items may visit positions along the inventory pier corresponding to inventory holders storing inventory items needed by the orders. Operators may remove items from the inventory holders and place them into appropriate boxes or containers in the order holder. If necessary, the order holder may then be taken to an inventory station to fill items remaining on the orders in the boxes or containers of the order holder.\n\n(27) To illustrate, in the office supply example discussed above, paper may be a high volume inventory item and ink cartridges may be a highly popular item. Accordingly, a pallet of paper and an inventory holder storing various ink cartridges may be stationed at the inventory pier. An order for several packages of paper and an ink cartridge may be filled by moving an order holder a location along the inventory pier opposite to the pallet storing the paper where an operator may transfer the paper to the order holder. The order holder may then move to a location opposite the inventory holder storing the ink cartridges where the same or a different operator may transfer the ink cartridge to the order holder. If the requested printer is not already stored at the inventory pier, a mobile drive unit may transport the order holder to an inventory station, where another mobile drive unit may transport an inventory holder containing the printer so that the order can be filled.\n\n(28) In FIG. 1, the inventory pier 106 is shown with multiple designated pier locations 130 to accommodate the inventory holders 114. Five pier locations 130 are illustrated to accommodate five corresponding holders 114, although there may be more or fewer pier locations per inventory pier 106. One mobile drive unit 112(6) is shown at the inventory pier 106 to shuffle inventory holders 114 among the pier locations 130 according to the techniques described herein. Two loaded mobile drive units 112(7) and 112(8) are shown at rest next to a pier 132 and holding associated inventory holders 114(7) and 114(8) (or more specifically in this case, order holders) for access by a picker 134 (again, represented as a human although a mechanical device may be used). In this example, the inventory holders 114(7) and 114(7) may be holding packages or containers for order fulfillment, where the picker 134 removes selected items from the inventory holders 114 positioned in pier locations 130 and loads the items into order containers on the inventory holders 114(7) and 114(8).\n\n(29) In some implementations, multiple mobile drive units may be used to bring inventory holders to and from the pier locations 130 of the inventory pier 106. Incoming inventory holders may be placed in temporary locations while a single mobile drive unit, such as unit 112(6), removes an existing inventory holder that has been recently picked by the picker 134 from a pier location 130 and replaces it with a new inventory holder with new items of interest for the picker 134.\n\n(30) In one implementation, the management module 110 orchestrates movement of the mobile drive units 112, directing them to various regions within the warehouse. The management module 110 coordinates transport of the various inventory holders among the regions in the warehouse. Furthermore, the management module 110 may be used to instruct the mobile drive units to perform the shuffle processes within a particular region (e.g., storage region 102, pick station 104, inventory pier 106, etc.). Generally, the shuffle process involves directing a mobile drive unit 112 to reposition a first inventory holder from its current location within the region to a temporary location within the region that is adjacent or proximal to the current location. The mobile drive unit 112 leaves the first inventory holder in the temporary location and subsequently positions a second inventory holder into the location vacated by the first inventory holder. The mobile drive unit 112 then lifts the first inventory unit to transport the first inventory holder away.\n\n(31) The management module 110 may use any form of communication to direct the mobile drive units. In one implementation, the management module 110 and the mobile drive units are configured to communicate using wireless technologies, such as a wireless local area network (WLAN). As one example, some embodiments of mobile drive unit 112 may communicate with management module 110 and/or with one another using Wi-Fi (IEEE 802.11), Bluetooth (IEEE 802.15), Infrared Data Association standards, or any other appropriate wireless communication protocol. As another example, in a tracked inventory system 100, tracks or other guidance element upon which mobile drive units 112 move may be wired to facilitate communication between mobile drive units 112 and the management module 110 and/or other components of inventory system 100.\n\n(32) In addition to directing the mobile drive units, the management module 110 may receive and/or generate requests to initiate any of a number of particular operations involving the mobile drive units 112, inventory holders 114, or other elements of inventory system 100. The management module 110 may select components of inventory system 100 to perform various operations and communicate commands, instructions, and/or other appropriate information to the selected components to facilitate completion of these operations. The management module 110 may receive orders for various inventory items and coordinate and administer various appropriate tasks to fill the orders. For example, an order may specify particular inventory items that have been purchased by a customer and that are to be retrieved from inventory system 10 for shipment to the customer. The management module 110 may receive the orders from any appropriate system and generates task assignments based, in part, on the orders including requests for inventory items. Based on the orders, the management module 110 may identify particular operations to be completed involving inventory items stored or to be stored within inventory system 100.\n\n(33) After generating one or more task assignments, the management module 110 selects appropriate components to complete particular tasks and transmits task assignments to selected components, such as the mobile drive units, to trigger completion of the relevant tasks. The relevant components then execute their assigned tasks. Each task assignment defines one or more tasks to be completed by a particular component. These tasks may relate to the retrieval, storage, replenishment, and counting of inventory items and/or the management of mobile drive units 112, inventory holders 114, or other components of inventory system 100. Depending on the component and the task to be completed, a particular task assignment may identify locations, components, and/or actions associated with the corresponding task and/or any other appropriate information to be used by the relevant component in completing the assigned task.\n\n(34) The training system 150 is generally configured to evaluate the performance of an employee performing a fulfillment operation (e.g., a pick operation at the pick station 104). Of note, while the training system 150 is depicted within a fulfillment center in FIG. 1, more generally the training system 150 can be located in any suitable environment, including not only the fulfillment center but also remote computing environments (e.g., a cloud computing environment). In such an embodiment, motion data collected within the fulfillment environment could be transmitted to the remote computing environment for analysis by the training system 150. In evaluating an employee's performance, the training system 150 could determine a plurality of sub-tasks for carrying out a fulfillment operation. For instance, in a given fulfillment center, a pick operation could include the operations of logging into a workstation, selecting the pick workflow on the workstation, scanning an empty tote, confirming the tote location in the tote rack, waiting on the inventory holder to arrive, fetching an item from the begin in the inventory holder, scanning the item and putting the item in the tote and pressing a light on the tote rack.\n\n(35) The training system 150 could retrieve a training set of motion data that includes a plurality of training motion data samples, each specifying motion data over a respective window of time. For example, employees working in the pick station 104 could be monitored performing pick operations over a period of time, using motion capture devices (e.g., motion capture cameras, motion capture devices worn by the employees, etc.). The training system 150 could then train a first data model to classify instances of motion data into portions, based on the plurality of sub-tasks. For instance, a user could specify, for each of the instances of motion data, which temporal portion of the instance of motion data corresponds to which sub-task. As an example, the user could specify that the motion data from time T.sub.1 to time T.sub.2 corresponds to the user logging into the workstation, the motion data from time T.sub.2 to time T.sub.3 corresponds to the user selecting the pick workflow on the workstation, and so on. Using such information, the training system 150 could train a data model to divide an instance of motion data into portions corresponding to the various sub-tasks of the workflow. In doing so, the training system 150 could use information in addition to the motion data, such as the performance of various actions on the workstation (e.g., when the workstation received the command from the user to login) and within the pick station 104 (e.g., when the user scanned the empty tote using an electronic scanning device).\n\n(36) Additionally, the training system 150 can train at least one data model to assess performance of the fulfillment operation, using the training set of motion data. The training system 150 could then monitor an employee's actions while the employee performs the fulfillment operation using one or more motion capture devices, in order to collect an instance of motion data describing motion performed in carrying out the fulfillment operation. The training system 150 could divide the instance of motion data into a plurality of portions, using the first data model. For example, the training system 150 could determine a temporal window within the instance of motion data that corresponds to each of the sub-tasks for the fulfillment operation. The training system 150 could then generate a measure of quality for the instance of motion data, by analyzing the plurality of portions of the instance of motion data using the trained at least one data model.\n\n(37) As will be discussed in further detail below, the performance of the fulfillment operation can be assessed in a variety of different ways. In one embodiment, the training system 150 is configured to train a data model(s) to calculate an efficiency score for each of the sub-tasks and for each of multiple metrics. For example, the training system 150 could produce a report that evaluates the employee's performance of each sub-task on the metrics of ergonomics, safety, efficiency, quality, and task time analysis. In a particular embodiment where the employee is wearing an immersive reality headset (e.g., a virtual reality headset, an augmented reality headset, etc.), the training system 150 can render frames depicting an optimal motion for performing the fulfillment operation, using the trained data model. For instance, the training system 150 could render frames depicting a virtual avatar performing the optimal motion. Doing so enables the employee to visualize the optimal way of performing the task and to learn the optimal movement by mimicking the actions of the virtual avatar.\n\n(38) FIG. 2 illustrates a side-view illustration of a mobile drive unit 112 and inventory holders 114 employed in the inventory system 100. In this illustration, one inventory holder 114(1) is at rest in a first location next to a human picker 202. The inventory holder 114(1) has multiple shelves that support a variety of items 204. The inventory holder 114(1) has open side faces to facilitate stocking of the items 204 onto the shelves and picking of items from the shelves.\n\n(39) The inventory items 204 represent any objects suitable for storage, retrieval, and/or processing in an automated inventory system 100. For example, a given inventory item may represent a single stock-keeping unit (SKU) of a unique inventory item. Thus, inventory items may refer to a particular item and/or may refer to a quantity of items having a particular SKU as appropriate. As one example, the inventory system 100 may represent a retail distribution warehouse that stores bulk inventory items 204 for retail facilities, such as grocery stores and/or merchandise warehouses. As another example, the inventory system 100 may represent an e-commerce warehouse facility, and inventory items 204 may represent merchandise stored in the warehouse facility.\n\n(40) A mobile drive unit 112 is also shown moving a second inventory holder 114(2) into a second location next to the first location. The mobile drive unit 112 is positioning a second inventory holder 114(2) into a location for easy access by the picker 202 (such as at the inventory pier 102) or to place it in an on-deck location to be shuffled to the first location after the picker has completed picking items 204 from the first inventory holder 114(1) (such as at the pick station 104).\n\n(41) As shown, the mobile drive unit 112 is configured with multiple drive wheels 208 and stabilizer wheels 210. The drive wheels 208 could be driven using a motor that is within the body of the mobile drive unit 206 and which is in turn powered by a power source (not shown) within the body of the mobile drive unit 206.\n\n(42) FIG. 2 also includes a motion capture system 220, which is in communication with motion capture camera devices 230(1)-(3). Of note, while three motion capture camera devices are shown in FIG. 2, more generally the motion capture system 220 can be in communication and collect data from any number of motion capture devices. Additionally while motion capture camera devices are illustrated in FIG. 2, more generally any motion capture devices can be used to collect motion data from the user 202, consistent with the functionality described herein. For example, the user 202 could wear one or more wearable motion caption devices (e.g., a motion capture body suit, motion capture gloves, motion capture headwear, etc.) and the motion capture system 220 could communicate over a data communications network with such devices to collect motion data for the user 202.\n\n(43) Generally, the motion capture system 220 can monitor the movement of the user 202 using the motion capture camera devices 230(1)-(3) and can collect instances of motion data describing the movement of the user 202. In one embodiment, each instance of motion data describes the movement of the user 202 during a single instance of a fulfillment operation. For example, in the depicted embodiment, each instance of motion data can include motion capture sensor data for a single picking fulfillment operation. As discussed above, such a fulfillment operation can include a number of sub-tasks that are performed by the user 202.\n\n(44) Generally, the training system 150 can use the collected motion data for a number of different purposes. For example, where the user 202 is a top performing employee of the fulfillment center, the collected motion data could be used to train one or more data models for use in evaluating user performance of the fulfillment operation. Those trained data models can then be used as reference models against which to evaluate other motion data. For example, where the user 202 is a new employee undergoing training, the collected motion data could be analyzed using one of the already trained data model (e.g., derived from the motion data of the top performing employee) to evaluate the new user's performance. In doing so, the training system 150 could analyze the user's performance of each of the sub-tasks that make up the fulfillment operation (e.g., a picking operation) and can provide feedback to the user accordingly.\n\n(45) In one embodiment, the training system 150 is configured to collect motion data over a predetermined number of performances of the fulfillment operation, and to analyze the data as a whole to determine performance trends for the user. As an example, the training system 150 could analyze the motion data collected for the user 202 during performance of multiple fulfillment operations (e.g., 100 picking operations) and could determine that, for a substantial number of fulfillment operations, the user 202 was exhibiting unsafe bending and lifting behaviors, relative to the behaviors described in the trained data model. The training system 150 could provide such feedback to the user, so that the user can adjust his performance of the fulfillment operation going forward. In one embodiment, the training system 150 could determine a number of fulfillment operations during which the user exhibited the most unsafe behavior, and could provide a virtual depiction of the user's movement during those fulfillment operations to better help the user understand the problem. As an example, the training system 150 could apply the motion data collected by monitoring the user during the fulfillment operations where the user exhibited the most unsafe behavior to a virtual avatar, in order to reenact the user's unsafe behavior. Doing so not only informs the user of the problem, but helps the user visualize and better understand the unsafe behavior.\n\n(46) FIG. 3 is a block diagram illustrating a training system, according to one embodiment described herein. As shown, the training system 150 includes a motion analysis system, the motion capture system 220 and an immersive reality device 390, interconnected via network 365. The motion capture system 220 includes motion capture devices 375, motion data aggregation component 380 and a network interface 385. Generally, the network interface 380 may be any type of network communications allowing the motion capture system 220 to communicate with other computers via a data communications network (e.g., network 365). The motion capture devices 375 generally represent any electronic devices capable of collecting data describing a user's motion over a window of time. For example, the motion capture devices 375 could be the motion capture cameras 230(1)-(3) shown in FIG. 2. Additionally, the motion capture devices 375 could include wearable motion capture devices (e.g., motion capture suits, gloves, headwear, and so on) that are configured with markers (e.g., reflective markers, active optical markers, etc.) that can easily be detected by the motion capture camera devices. In such an embodiment, the motion data aggregation component 380 could analyze the image data collected by the camera devices to generate motion data describing the user's movement over time.\n\n(47) In one embodiment, the motion capture devices 375 include an electromyography (EMG) sensor device that is configured to measure the electrical activity of the user's muscle tissue as the user performs the fulfillment operation. Such data could be analyzed (e.g., by the motion analysis component 315) to determine which muscles a user is activating while performing various sub-tasks of a fulfillment operation. As an example, the motion analysis component 315 could consider such data in determining an ergonomics metric score for the user's performance of a given sub-task, where a user may get a higher ergonomics score when lifting a package predominantly using his leg muscles (e.g., as indicated by the collected EMG sensor data) and a lower ergonomics score when lifting a package predominantly using the muscles in his lower back.\n\n(48) The motion analysis system 310 includes a processor 312, memory 314, storage 340, I/O devices 355, and a network interface 360. Processor 312 is intended to be representative of a single CPU, multiple CPUs, a single CPU having multiple processing cores, and the like. The memory 314 is included to be representative of a random access memory.\n\n(49) As shown, memory 314 includes a motion analysis component 315 and an operating system 335. The motion analysis component 315 includes a data model training component 320, a task analysis component 325 and an efficiency evaluation component 330. Storage 340 includes training data 345, task definitions 350, and data models 353. The storage 340 may be a disk drive storage device. Although shown as a single unit, the storage 340 may be a combination of a fixed and/or removable storage, such as fixed disc drives, removable memory cards, optical storage, network attached storage (NAS), or storage-area-network (SAN). The network interface 360 may be any type of network communications allowing the motion analysis system 310 to communicate with other computers via a data communications network (e.g., network 365).\n\n(50) The training data 345 represents data collected for use in training the data models 353. Such training data 345 can include motion data collected by monitoring employees performing a fulfillment operation (e.g., a picking operation, a stow operation, etc.) within a fulfillment environment (e.g., a distribution warehouse). In one embodiment, the training data 345 includes both positive examples of performing the fulfillment operation (e.g., data collected by monitoring employees performing the fulfillment operation in an optimal fashion) and negative examples of performing the fulfillment operation (e.g., data collected by monitoring employees performing the fulfillment operation incorrectly). In one embodiment, whether a given instance of motion data is a positive or negative example can be explicitly specified (e.g., by an administrator of the motion analysis system 310). For example, when initially building the data models 353, positive samples of motion data could be collected from top performing employees, while negative samples of motion data could be collected from employees intentionally performing certain sub-tasks incorrectly (e.g., purposefully moving slowly, purposefully using poor lifting technique, and so on).\n\n(51) In a particular embodiment, the data model training component 320 can classify the instances of motion data within the training data 345 as either positive or negative examples, based on an employee history of the user that was monitored to collect the instance of motion data. For example, the data model training component 320 could analyze data describing a particular user's employment history within the fulfillment center and could determine that the particular user has always received excellent performance reviews and the user's performance has otherwise been rated very highly within the fulfillment center. As a result, the data model training component 320 could deem all instances of motion data collected for the particular user as positive examples. Likewise, for another user who has exhibited a historically poor performance within the fulfillment center, the motion analysis component 315 could deem instances of motion data collected for the other user as negative examples.\n\n(52) Generally, the data model training component 320 can use the positive and negative samples of motion data within the training data 345 to train one or more data models 353 to evaluate an instance of motion data, in order to determine whether the instance of motion data represents an optimal performance of a fulfillment operation. For example, the data model training component 320 could use the training data 345 to train a machine learning classifier data model 353 that maps motion data collected during the performance of a fulfillment operation to measures of quality. In a particular embodiment, at least one of the machine learning data models 353 is generated according to a regression algorithm and is configured to generate a performance score that represents an estimated measure of user performance for a given metric and for a given sub-task of a fulfillment operation.\n\n(53) The data models 353 can represent any form of machine learning or deep learning models, consistent with the functionality described herein. Examples of such machine learning and deep learning algorithms include, without limitation, Na\u00efve Bayes classification, Logistic regression models and classifiers, Decision Tree regression models and classifiers, Support Vector Machine (SVM) classifiers, and so on. Additional example include neural network classifiers and regression models (including deep neural network classifiers and regression models), Decision Forest classifiers and regression models, Perceptron classifiers, Linear and Bayesian linear regression models, and Bayes point machine classifiers. More generally, any suitable machine learning model, known or unknown, can be used, consistent with the functionality described herein.\n\n(54) The task definitions 350 generally represent the predefined sub-tasks that make up a fulfillment operation. For example, the task definitions 350 for a picking operation could include the tasks of (i) logging into a station, (ii) selecting the pick workflow, (iii) scanning an empty tote, (iv) confirming the tote location in the tote rack, (v) waiting on the inventory holder to arrive, (vi) fetching an item from a bin on the inventory holder, (vii) scanning the item and (viii) putting the item in the tote and press a lighted button on the tote rack. As another example, the task definitions 350 for a stow operation could include (i) logging into a station, (ii) selecting the stow workflow, (iii) waiting on the inventory holder to arrive, (iv) scanning a container, (v) picking a product out of the container, (vi) identifying a target bin for the product on the inventory holder, (vii) making room for the product and placing the product in the bin, and (viii) scanning the bin. Such a workflow could be repeated for multiple products by, for example, returning to step (iv) and scanning another container.\n\n(55) In one embodiment, the task definitions 350 further specify an ordering for the specified tasks. For example, the task definitions 350 could specify that the aforementioned tasks for a particular picking operation must be performed in sequential order from (i) to (viii). In a particular embodiment, the task definitions can specify 350 that two or more of the specified tasks can optionally be performed in any order relative to one another. As an example, in the aforementioned stow operation, an exemplary task definition 350 could specify that the tasks of placing the product in the bin and scanning the bin could optionally be performed in any order, relative to one another. In such an embodiment, the data models 353 may be trained to learn an optimal ordering for performing the tasks, based on positive samples within the training data 345.\n\n(56) In a particular embodiment, the efficiency evaluation component 330 is configured to detect when the user has not performed one or more of the tasks defined by the task definitions 350 for a given fulfillment operation. For example, the efficiency evaluation component 330 could perform a pattern matching analysis for each of the tasks for the fulfillment operation to determine whether a portion of an instance of motion data collected by monitoring a user matches a predefined pattern of motion data for a given task. If the efficiency evaluation component 330 determines that no portion of the instance of motion data sufficiently matches the predefined pattern (e.g., based on a confidence value for the best matching portion not exceeding a predefined threshold), the efficiency evaluation component 330 could determine that the user has omitted one of the tasks for the fulfillment operation. The efficiency evaluation component 330 could then provide feedback to the user, e.g., informing the user that the task was omitted. In one embodiment, the efficiency evaluation component 330 is configured to depict the optimal performance of the omitted task using the immersive reality device 390, so that the user can better understand how to correct his performance of the fulfillment operation.\n\n(57) Generally, the motion analysis component 315 is configured to receive motion data describing a user's movement during the performance of a fulfillment operation and to provide evaluation results and/or feedback on how well the user performed the operation. For example, the motion analysis component 315 could receive an instance of motion data describing motion performed in carrying out a fulfillment operation. The efficiency evaluation component 330 could analyze the instance of motion data using at least one data model trained with historically collected motion data. For example, such data model(s) could be trained by the data model training component 320 using the training data 345. The efficiency evaluation component 330 could then determine a measure of quality for the instance of motion data, based on the analysis.\n\n(58) In one embodiment, the efficiency evaluation component 330 is configured to provide feedback on the user's performance of the fulfillment operation using the immersive reality device 390. The immersive reality device 390 could represent, for example, an augmented reality or a virtual reality headset. As shown, the immersive reality device 390 includes a rendering component 392 and one or more display devices 394. For example, the efficiency evaluation component 330 could determine that the user performed a particular sub-task poorly (e.g., where the measure of quality calculated for the sub-task is less than a predefined threshold level of quality) and, in response, could transmit instructions to the rendering component 392 to generate a graphical depiction of the user's movement in performing the sub-task. In doing so, the efficiency evaluation component 330 could transmit a temporal chunk of motion data corresponding to the user's performance of the sub-task in question to the immersive reality device 390, and the rendering component 392 could render frames depicting the motion data being applied to a virtual avatar (e.g., an avatar that is configured with movement constraints that depict the realistic movement of human limbs and joints). The rendered frames could then be output using the display device(s) 394, so that the user see how he performed the sub-task and better understand why his performance was sub-optimal.\n\n(59) In one embodiment, the rendering component 392 is configured to render frames depicting a virtual avatar performing the sub-task in an optimal manner. Such an optimal manner could be learned, for example, by combining (e.g., averaging) all of the positive samples of motion data for performing the sub-task in the training data 345. The rendering component 392 could apply such optimal motion data to a virtual avatar (e.g., having predefined human joints and predefined joint movement constraints to depict realistic human movement) to depict the avatar performing the sub-task in the optimal manner.\n\n(60) In a particular embodiment, the rendering component 392 could also apply the motion data collected by monitoring the user perform the sub-task to a second avatar (e.g., also having the predefined human joints and joint movement constraints) and could render frames depicting the second avatar performing the user's movement. By doing so, the training system 150 can visualize how the user's behavior differs from the optimal behavior of a user performing the sub-task. In one embodiment, the rendering component 392 renders the first avatar and the second avatar together (e.g., adjacent to each other, overlayed on top of each other, etc.), so that the user can directly visualize the differences between the two movements.\n\n(61) FIG. 4A-D illustrate a user performing a fulfillment operation, according to embodiments described herein. As shown in FIG. 4A, the environment 400 includes an inventory holder on a mobile drive unit 410 that is shown in dashed lines to indicate the inventory holder has not yet arrived, a user 415 and a display device 420. The inventory holder 410, when it arrives, will contain a product 412. The user 415 is configured with a motion capture device 417. In the depicted example, the motion capture device 417 is shown as a wrist band which may contain one or more indicators (e.g., reflective indicators, active illuminated indicators, etc.) which can be easily detected by one or more motion capture camera devices (not shown). While the motion capture device 417 is illustrated as a wrist band in the environment 400, more generally any motion capture device(s) may be used to capture data describing the user's 415 motion, consistent with the functionality described herein.\n\n(62) In one embodiment, the motion capture device 417 comprises an EMG sensor device that is configured to collect electromyography data describing muscle activations of the user 415. In a particular embodiment, the motion capture device 417 represents an ultrasonic device as described in U.S. patent application Ser. No. 15/083,083, entitled \u201cULTRASONIC BRACELET AND RECEIVER FOR DETECTING POSITION IN 2D PLANE,\u201d which is hereby incorporated by reference in its entirety. In such an embodiment, the motion capture device 417 could emit ultrasonic pulses that can be detected by three or more ultrasonic transducers (not shown) within the fulfillment environment. A management module could then determine a set of location coordinates (e.g., X, Y and Z) corresponding to the three-space position of the motion capture device 417 worn by the user, based on time differentials when an ultrasonic pulse was emitted by the motion capture device 417. The respective set of coordinates corresponding to each emitted ultrasonic pulse could then be tracked over a period of time to determine motion data describing the user's movement while performing a fulfillment operation. More generally, any sort of sensor device(s) suitable for user in collecting data describing the user's movement during performance of a fulfillment activity could be used, consistent with the functionality described herein.\n\n(63) In the depicted example, the display screen 420 is depicting a graphical user interface (GUI) that indicates that, when the inventory holder 410 arrives, the product 412 will be in bin 3F. For purposes of this example, assume that bin 3F refers to a bin on the middle, bottom shelf of the inventory holder 410. In this example, as the user 415 sees that the product 412 will be on a bottom shelf once the inventory holder 410 arrives, the user 415 could perform an optimal behavior and could get into position to retrieve the product 412, once the inventory holder 410 arrives. As such, as shown in FIG. 4B, the environment 425 depicts the user 427 in a kneeling position, awaiting the arrival of the inventory holder 410. As discussed above, while the optimal behavior of getting into position before the inventory holder 410 arrives may save an amount of time that is not noticeable to the user (e.g., seconds, if not a fraction of a second), such a time savings can be significant when viewed across all fulfillment operations performed by all employees in all fulfillment centers for a business.\n\n(64) FIG. 4C depicts the arrival of the inventory holder 410, which is now shown in solid lines. As previously mentioned, as the user 427 is already in position to retrieve the item 412 from the Bin 3F, the user can more quickly retrieve the item 412 from the inventory holder 410. FIG. 4D depicts the user 427 now in a standing position holding the item 412, and ready to continue with the remainder of the sub-tasks that make up the fulfillment operation.\n\n(65) For purposes of this example, assume that the user movement shown in FIG. 4A-D represents a highly optimal movement for performing the fulfillment operation, in terms of one or more metrics (e.g., efficiency of movement, ergonomics of movement, time taken to complete the task, etc.). The motion capture system 220 could monitor the movement of the user across the examples shown in FIG. 4A-D and could collect data describing the user's motion using motion capture devices 375. For example, motion capture cameras could be placed throughout the fulfillment environment to capture images of the user as the user performs the fulfillment operation. The motion data aggregation component 380 could analyze the images and could track the movement of one or more markers on the wearable motion capture device 417, as the user performs the fulfillment operation. The motion data aggregation component 380 could then generate an instance of motion data describing the user's movement.\n\n(66) Generally, this process could be performed for multiple different iterations of the fulfillment operation. The collected instances of motion data could be transmitted to the motion analysis component 315 (e.g., using the network 365) for use in generating one or more data models for analyzing motion data. For example, the data model training component 320 could use the collected instances of motion data as positive examples for training one or more of the data models 353. In one embodiment, the task analysis component 325 is configured to divide each instance of collected motion data into a plurality of temporal portions, where each temporal portion corresponds to a respective sub-task defined by the task definitions 350. The data model training component 320 could then train a respective one of the data models 353 for each of the sub-tasks, using the corresponding temporal portions of motion data.\n\n(67) The motion analysis component 315 could use the trained data models 353 for training and evaluation of employees performing the fulfillment operation. For example, the motion analysis component 315 could determine an optimal movement for performing the fulfillment operation under particular conditions (e.g., when the item to be retrieved is located in Bin 3F) using the collected instances of motion data. The motion analysis component 315 could then transmit data describing the optimal movement to the immersive reality device 390. Upon receiving the data, the rendering component 392 could apply the optimal movement data to a virtual avatar and could render one or more frames for display on the display device(s) 394, depicting the virtual avatar performing the fulfillment operation under the particular conditions in the optimal manner. Doing so enables employees to visualize how the fulfillment operation can be performed in a more effective manner, which can assist in training new employees and improving the performance of existing employees.\n\n(68) As another example, the trained data models 353 could be used to evaluate a particular employee's performance of the fulfillment operation. For example, the motion capture system 220 could collect motion data using the motion capture devices 375 while the particular employee performs the fulfillment operation, and an instance of motion data could be generated by the motion data aggregation component 380 and transmitted to the motion analysis component 315 for analysis. The task analysis component 325 could divide the instance of motion data into a plurality of temporal portions based on the sub-tasks defined by the task definitions 350, and the efficiency evaluation component 330 could analyze each temporal portion using a data model 353 trained to evaluate motion data for the corresponding sub-task of the fulfillment operation. The efficiency evaluation component 330 could then generate a respective quality score for each of the plurality of sub-tasks, by analyzing a respective one of the plurality of portions using a corresponding one of the plurality of data models. The efficiency evaluation component 330 could provide an evaluation report to the particular user to help the user better understand his strengths and where his performance needs improvement.\n\n(69) FIG. 5 is a flow diagram illustrating a method of training a data model to assess the performance of a fulfillment operation, according to one embodiment described herein. As shown, the method 500 begins at block 510, where a motion capture system 220 monitors behavior of users performing fulfillment operations within a fulfillment environment to collect a plurality of instances of motion data. In doing so, the motion capture system 220 can collect data from a plurality of motion capture devices 375, such as motion capture camera devices, wearable motion capture equipment and so on. The plurality of instances of motion data can be transmitted to a motion analysis system 310 for use in training the one or more data models.\n\n(70) A task analysis component 325 can divide each instance of motion data into a plurality of temporal chunks, based on a set of predefined tasks specified in the task definitions 350 (block 515). For example, the task analysis component 325 could determine that a portion of a first instance of motion data from time T.sub.1 to time T.sub.2 corresponds to a first sub-task of the fulfillment operation, a second portion of data from time T.sub.2 to time T.sub.3 corresponds to a second sub-task, and so on. In one embodiment, the task analysis component 325 is configured to train a machine learning classifier to analyze an instance of motion data and to determine the temporal chunks that correspond to the sub-tasks of the fulfillment operation. For instance, an administrator could provide a number of positive samples of manually divided instances of motion data captured from users performing the fulfillment operation and the task analysis component 325 could use such information to train the machine learning classifier to divide subsequent instances of motion data into temporal chunks corresponding to the set of predefined tasks.\n\n(71) The data model training component 320 determines a subset of the plurality of instances of motion data that are indicative of positive samples (block 520), and further determines a subset of the plurality of instances of motion data that are indicative of negative samples (block 525). For example, an administrator could manually designate each of the plurality of instances of motion data as either a positive sample or a negative sample. As another example, the data model training component 320 could be configured to determine whether a given instance of motion data is a positive sample or a negative sample, based on attributes of the user from which the instance of motion data was collected. For example, the data model training component 320 could access data describing the user's performance history within the fulfillment center and upon determining that the user has historically received very positive performance reviews, could consider the data collected from monitoring the user as a positive sample.\n\n(72) The data model training component 320 then trains one or more data models, using the determined subsets (block 530), and the method 500 ends. For instance, the data model training component 320 could train a neural network to evaluate instances of motion data in order to rate the performance of the user from which the motion data was collected. For example, such a neural network could receive as inputs the temporal chunks of motion data corresponding to the set of predefined sub-tasks, and could output one or more measures of quality describing how well the user performed each sub-task, relative to the provided positive samples of motion data. In a particular embodiment, the data model training component 320 is configured to train a neural network to output multiple measures of quality for each of the sub-tasks, e.g., a rating of how ergonomically the task was performed, a rating of how safely the task was performed, a rating of how efficiently the task was performed, etc.\n\n(73) In one embodiment, the data model training component 320 further trains the one or more data models using attributes of the monitored fulfillment operations. Such attributes could include, for example, which bin of an inventory holder is specified for stowing an item for a given fulfillment operation. Additionally, the attributes could include one or more system events corresponding to the fulfillment operation. As an example, a sensor event could be generated when the user scans a particular bin using a scanning device as part of a task in performing a fulfillment operation. As another example, a sensor device(s) (e.g., a motion detection sensor) could be configured to monitor each of the bins of an inventory holder and, upon detecting motion within a given bin (e.g., resulting from a user stowing a product in the bin), could generate a sensor event specifying the bin. As yet another example, a system event could be generated during the performance of a fulfillment operation specifying a type of the product being retrieved or stowed (e.g., a garment, a small box, a large box, etc.). The data model training component 320 could use such information when training the model to provide a context for the instances of motion data. That is, the optimal movement for performing a given fulfillment operation can vary, depending on various attributes, and such information can be used in training the data model(s) to provide a more accurate and reliable evaluation of user movement. As an example, the optimal motion for retrieving a garment from a top bin of an inventory holder could differ substantially from the optimal motion for retrieving a large, heavy box from a bottom bin of the inventory holder.\n\n(74) FIG. 6 is a flow diagram illustrating a method of evaluating user efficiency in performing a fulfillment operation, according to one embodiment described herein. The method 600 begins at block 610, where the motion capture system 220 monitors behavior of a user performing a fulfillment operation in order to collect an instance of motion data. As discussed above, the motion capture system 220 could monitor the user's behavior using motion capture devices 375, such as motion capture camera devices, wearable motion capture equipment, and so on.\n\n(75) The task analysis component 325 divides the instance of motion data into a plurality of temporal chunks, based on a set of predefined sub-tasks that make-up an iteration of the fulfillment operation (block 615). As discussed above, the task analysis component 325 could analyze the instance of motion data using a trained data model(s) to determine the temporal chunk that corresponds to each of the sub-tasks. Additionally, the efficiency evaluation component 330 determines attributes of the fulfillment task that was performed by the user when the instance of motion data was collected (block 620). For example, such attributes could specify, for example, which bin of an inventory holder contains an item to be retrieved as part of a pick fulfillment operation. Such information can be relevant, for instance, in determining the optimal movement of the user in retrieving the item, as the user's movement can differ dramatically when retrieving an item from a top bin of the inventory holder as opposed to retrieving an item from a bottom bin of the inventory holder.\n\n(76) In the depicted embodiment, the efficiency evaluation component 330 analyzes each temporal chunk using a respective one or more data models to determine a measure of efficiency for the temporal chunk (block 625). In one embodiment, the efficiency evaluation component 330 analyzes the temporal chunks using a neural network that is configured to output one or more scores for one or more metrics for each of the temporal chunks. For example, in a particular embodiment, such a neural network is configured to output, for each of the temporal chunks, an ergonomics score, a safety score, an efficiency score, a quality score and a task time analysis score. In one embodiment, the efficiency evaluation component 330 is configured to use a respective one or more machine learning data models to analyze temporal chunks for each of the plurality of sub-tasks. For example, for a sub-task that requires the employee to pick a product out of a container, the efficiency evaluation component 330 could analyze the temporal chunk of motion data corresponding to the sub-task using a respective one or more machine learning models that are configured to analyze motion data for the specific sub-task according to one or more metrics. In a particular embodiment, the efficiency evaluation component 330 is configured to use a respective machine learning model for each metric and for each sub-task when analyzing the temporal chunks of motion data.\n\n(77) Returning to the depicted embodiment, the efficiency evaluation component 330 provides an indication of user efficiency for the user, based on the analysis of the temporal chunks (block 630), and the method 600 ends. In a particular embodiment, the efficiency evaluation component 330 is configured to output an evaluation summary report that reflects the results of the analysis. FIG. 7 illustrates one such evaluation summary report, according to one embodiment described herein. As shown, the report 700 contains a respective score for each sub-task of a picking fulfillment operation and for each of a plurality of metrics. In the depicted example, the efficiency evaluation component 330 has analyzed the temporal chunk of motion data for each of the sub-tasks according to the metrics of ergonomics, safety, efficiency, quality and task time analysis. Based on such a report, a user reviewing the report could determine that while the monitored user was highly efficient when performing the sub-task of making room for the product and placing the product in a bin of an inventory holder, the monitored user demonstrated relatively poor ergonomics and safety when performing the sub-task. For example, such scores may be the result of the bending over too quickly in a manner that could potentially injure the monitored user's back. Of note, while the scores in the depicted example are on a scale of 1 to 100, more generally any technique for evaluating the user's performance of a sub-task according to a given metric can be used, consistent with the functionality described herein. Examples of such scoring systems include, without limitation, other numeric scoring schemes, letter grades (e.g., A, B, C, D, or F), categories (e.g., Exceptional, Satisfactory, Needs Improvement) and so on.\n\n(78) In one embodiment, the motion analysis component 315 can provide a graphical representation of a highly optimal manner of performing a given fulfillment task. For example, according to a particular embodiment, the motion analysis component 315 is configured to determine an optimal instance of motion data for performing a given fulfillment task under defined conditions (e.g., where an item has a defined size and shape, where an item is located in a particular bin of an inventory holder, etc.), using the training data 345 and/or one or more trained data models 353. The motion analysis component 315 could then provide the optimal instance of motion data to an immersive reality device 390, which could apply the motion data to a virtual avatar and could render frames depicting the virtual avatar performing the task. FIG. 8A-B illustrate an augmented reality environment that includes a virtual avatar performing a fulfillment operation, according to one embodiment described herein.\n\n(79) As shown in FIG. 8A, the environment 425 depicts the inventory holder 410 containing the package 412, and shows that the inventory holder 410 has not yet arrived at the pick station (as reflected by the dotted lines). In the illustrated embodiment, the employee is wearing an augmented reality headset (i.e., an immersive reality device 390), and a rendering component 392 for the immersive reality device 390 has rendered a virtual avatar (also shown in dotted lines) performing the fulfillment operation in an optimal manner. That is, the rendering component 392 has applied the optimal instance of motion data to a virtual avatar (e.g., a virtual character having a human shape and defined limbs, joints and movement constraints), so that the user can visualize the performance of the fulfillment operation in the highly optimal manner. In the depicted embodiment, the virtual avatar is shown as kneeling before the inventory holder 410 arrives (e.g., the user position 427 in FIG. 4B), so as to be able to retrieve the package 412 once the inventory holder 410 arrives at the station.\n\n(80) As part of a training exercise, the user could mimic the performance of the virtual avatar, as shown in FIG. 8B, where the user has knelt down to substantially match the position of the virtual avatar. In one embodiment, the motion analysis component 315 can continue to track the user's movement during the training exercise, by analyzing motion data collected by the motion capture system 220. Upon determining that the user's movement substantially deviates from the optimal movement being performed by the virtual avatar, the motion analysis component 315 could halt the performance of the fulfillment activity by the virtual avatar (e.g., pausing the playback of the motion data in the rendered frames), such that the virtual avatar remains frozen in the optimal position at a given moment in time. The motion analysis component 315 could continue monitoring the user's movement until the motion analysis component 315 determines that the user's position substantially matches the optimal position of the virtual avatar (e.g., as shown in FIG. 8B). In response, the motion analysis component 315 could resume playback of the optimal movement by the virtual avatar in the rendered frames. Doing so provides the user time to understand and mimic the virtual avatar's movement.\n\n(81) In one embodiment, the immersive reality device 390 could augment the speed of the virtual avatar's movement to enable the user to better mimic the virtual avatar's movement. For example, upon determining that the user's movement has substantially deviated from the optimal movement being performed by the virtual avatar, the immersive reality device 390 could render frames with a slow motion effect, thereby providing additional time for the user to observe and mimic the optimal movement being performed by the virtual avatar.\n\n(82) In a particular embodiment, the immersive reality device 390 is configured to provide an indication of real-time feedback to the user as the user performs fulfillment operations. As an example, the immersive reality device 390 could depict an efficiency meter that ranges from green to red, with green indicating high efficiency movement and red indicating low efficiency movement. The efficiency evaluation component 330 could evaluate the user's behavior in real-time by analyzing the motion data collected by monitoring the user using the trained data models 353. The immersive reality device 390 could then receive real-time indications of the user's efficiency and could update the depicted efficiency meter accordingly. As an example, the user could continue performing fulfillment operations until a low-efficiency movement is detected and the efficiency meter shows an efficiency score in red. The immersive reality device 390 could then display a sequence of frames providing feedback to the user, e.g., frames depicting a first virtual avatar performing the optimal movement and a second virtual avatar performing the user's movement that was determined to be inefficient. Of note, while the aforementioned example describes an efficiency meter having a sliding metric ranging from red to green, such an example is provided for illustrative purposes only and without limitation, and more generally, any suitable technique for providing real-time feedback to the user can be used, consistent with the functionality described herein.\n\n(83) In some embodiments, the training system 150 can provide a fully virtual training experience for performing fulfillment operations. An example of such a virtual experience is shown in FIG. 9, which illustrates reality environment that includes a virtual avatar performing a fulfillment operation, according to one embodiment described herein. As shown, the user is wearing a virtual reality headset (i.e., an immersive reality device) and is positioned atop an active virtual reality motion platform. Generally, the active virtual reality motion platform enables the user to perform a variety of physical actions (e.g., walking, kneeling, running, jumping, etc.) while substantially remaining in place atop the active virtual reality motion platform. In such an environment, the user could move throughout a virtual fulfillment station, picking up packages, scanning containers, and so on, without physically leaving the active virtual reality motion platform. Doing so can provide a training environment for employees while maintaining a small physical footprint.\n\n(84) In the depicted embodiment, the rendering component 392 of the virtual reality headset has rendered frames depicting the virtual avatar in the kneeling position, based on the optimal instance of motion data received from the motion analysis component 315. Of note, however, in such an embodiment, a virtual depiction of the inventory holder 410 carrying the package 412 will be rendered by the rendering component 392. Likewise, once the package 412 is retrieved, the user would place the virtual package within a virtual container. Advantageously, such a virtual training environment allows employees to be trained and evaluated in an isolated environment. Moreover, by depicting the optimal movement being performed by the virtual avatar within the virtual environment, the employee being trained and/or evaluated can more easily see and understand how to improve his performance of the fulfillment operation.\n\n(85) FIG. 10 illustrates a workflow for providing an evaluation summary and feedback for a performance of a fulfillment operation, according to one embodiment described herein. As shown, the workflow 1000 begins at block 1010, where the motion capture system 220 monitors the movement of the user while the user performs a fulfillment operation (e.g., a pick operation, a stow operation, etc.). The motion data aggregation component 380 can analyze the data collected by the motion capture devices 375 (e.g., motion capture cameras) and can generate an instance of motion data which describes the user's movement in performing the fulfillment operation over a window of time (block 1015). The task analysis component 325 can analyze the instance of motion data and can divide the motion data into temporal chunks, where each chunk corresponds to a respective sub-task defined by the task definitions 350 (block 1020). For example, the task analysis component 325 could use one or more data models 353 trained to recognize the patterns of user motion when performing each of the sub-tasks.\n\n(86) In the depicted workflow 1000, the efficiency evaluation component 330 analyzes each of the temporal chunks using a respective data model 353 that is trained to analyze motion data for the corresponding sub-task (block 1025). Of note, while in the depicted embodiment a separate data model is used for analyzing each temporal chunk, such a depiction is provided for illustrative purposes only and more generally any machine learning schema can be used, consistent with the functionality described herein. For example, a neural network could be trained to analyze the instance of motion data as a whole and to output a respective one or more scores for each of the sub-tasks.\n\n(87) As shown, the data models each produces a set of evaluation results based on the analysis of the temporal chunks (block 1030). For example, such evaluation results could include a score for one or more metrics (e.g., as shown in FIG. 7 and discussed above). The evaluation results are then consolidated as evaluation summary and feedback, as shown in block 1035.\n\n(88) FIG. 11 is a flow diagram illustrating a method for generating virtual content for providing feedback for a performance of a fulfillment operation, according to one embodiment described herein. As shown, the method 1100 begins at block 1105, where the motion analysis component 315 assigns a user a particular fulfillment operation. For example, the user could be wearing a virtual reality headset and standing atop an active virtual reality motion platform, as shown in FIG. 9 and discussed above, and the motion analysis component 315 could facilitate the rendering of frames (e.g., by transmitting instructions to the rendering component 392) instructing the user to perform a pick operation to retrieve an item from a particular bin of an inventory holder. As an example, the rendering component 392 could render a series of frames that include a virtual depiction of a display screen indicating that the next item will be positioned in Bin 3F when the virtual inventory holder arrives.\n\n(89) As shown in the method 1100, the motion capture system 220 captures motion data as the user performs the assigned task in the virtual environment (block 1110). The method 1100 could then return to block 1105, where the user is assigned another fulfillment operation and additional motion data is collected, until the motion analysis component 315 determines that sufficient motion data has been collected to evaluate the user's performance (e.g., once a predefined number of fulfillment operations have been completed by the user in the virtual environment). The motion analysis component 315 can then analyze the captured motion data against one or more trained data models 353 to evaluate the user's performance (block 1115).\n\n(90) In the depicted embodiment, the motion analysis component 315 then generates virtual content to coach the user in the areas of evaluation (block 1120). For example, as discussed above, the motion analysis component 315 could transmit optimal motion data for performing a given sub-task to the immersive reality device 390 and the rendering component 392 could generate a sequence of frames depicting a virtual avatar performing the fulfillment operation by applying the optimal motion data to the virtual avatar. In a particular embodiment, upon identifying a particularly ineffective instance of motion data collected by the user (e.g., an instance of motion data for which the user was assigned a rating below a predefined threshold rating), the motion analysis component 315 could transmit the particularly ineffective instance of motion data to the immersive reality device 390, and the rendering component 392 could apply the motion data to a second virtual avatar. Doing so enables the user to visualize how his movement differed from the optimal movement for performing the fulfillment operation, and may help the user to understand the inefficiencies in his own movement. The rendering component 392 could then output the rendered frames for display using the display device(s) 394 (block 1125), and the method 1100 ends.\n\n(91) In the preceding, reference is made to embodiments presented in this disclosure. However, the scope of the present disclosure is not limited to specific described embodiments. Instead, any combination of the described features and elements, whether related to different embodiments or not, is contemplated to implement and practice contemplated embodiments. Furthermore, although embodiments disclosed herein may achieve advantages over other possible solutions or over the prior art, whether or not a particular advantage is achieved by a given embodiment is not limiting of the scope of the present disclosure. Thus, the preceding aspects, features, embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim(s).\n\n(92) As will be appreciated by one skilled in the art, the embodiments disclosed herein may be embodied as a system, method or computer program product. Accordingly, aspects may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \u201ccircuit,\u201d \u201cmodule\u201d or \u201csystem.\u201d Furthermore, aspects may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.\n\n(93) Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium is any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus or device.\n\n(94) A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.\n\n(95) Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing.\n\n(96) Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the \u201cC\u201d programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n\n(97) Aspects of the present disclosure are described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments presented in this disclosure. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.\n\n(98) These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks.\n\n(99) The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.\n\n(100) The flowchart and block diagrams in the Figures illustrate the architecture, functionality and operation of possible implementations of systems, methods and computer program products according to various embodiments. In this regard, each block in the flowchart or block diagrams may represent a module, segment or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.\n\n(101) In view of the foregoing, the scope of the present disclosure is determined by the claims that follow."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 30,
      "claims_start": 29,
      "description_end": 29,
      "description_start": 18,
      "drawings_end": 17,
      "drawings_start": 3,
      "front_page_end": 2,
      "front_page_start": 1,
      "number_of_claims": 14,
      "number_of_drawing_sheets": 15,
      "number_of_figures": 15,
      "page_count": 30,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 29,
      "specification_start": 18,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000002717343,
    "field_of_search_cpc": [
      "G06Q 10/06"
    ],
    "foreign_references": [
      {
        "citation_classification": "N/A",
        "citation_cpc": "A63B 24/0006",
        "cited_by_examiner": true,
        "country_code": "WO",
        "patent_number": "WO-2016187673",
        "pub_month": "2016-12-01"
      }
    ],
    "group_art_unit": "3623",
    "guid": "US-11556879-B1",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/G79/568/115",
    "intl_class_current_primary": [
      {
        "intl_class": "G06Q",
        "intl_subclass": "10/06",
        "version": "2012-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06T",
        "intl_subclass": "7/20",
        "version": "2017-01-01"
      },
      {
        "intl_class": "G06Q",
        "intl_subclass": "10/08",
        "version": "2012-01-01"
      },
      {
        "intl_class": "G09B",
        "intl_subclass": "19/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06Q10/06",
      "G06T7/20"
    ],
    "inventors": [
      {
        "city": "Kansas City",
        "country": "US",
        "name": "Timmons; Aaron",
        "postal_code": "N/A",
        "state": "KS"
      },
      {
        "city": "Winchester",
        "country": "US",
        "name": "Cohn; Jonathan Evan",
        "postal_code": "N/A",
        "state": "MA"
      }
    ],
    "inventors_short": "Timmons; Aaron et al.",
    "legal_firm_name": [
      "Patterson + Sheridan, LLP"
    ],
    "npl_references": [
      {
        "citation": "Ng, A. \u201cPart V: Support Vector Machines.\u201d CS229 Lecture Notes, Stanford University. Web Archive, Apr. 25, 2014, &lt;//web.archive.org/web/20140425063301/http://cs229.stanford.edu/notes/cs229-notes3.pdf&gt;, Accessed Apr. 27, 2020. (Year: 2014).",
        "cited_by_examiner": true
      },
      {
        "citation": "Hsu, C., Lin, C. \u201cA Comparison of Methods for Multi-class Support Vector Machines.\u201d IEEE Transactions on Neural Networks, vol. 13, No. 2, Mar. 2002, pp. 415-425. (Year: 2002).",
        "cited_by_examiner": true
      },
      {
        "citation": "Mahmoud Moussa, Monitoring Employee Behavior Through the Use of Technology and Issues of Employee Privacy in America, SAGE Open, Apr.-Jun. 2015 (Year: 2015).",
        "cited_by_examiner": true
      },
      {}
    ],
    "patent_title": "Motion data driven performance evaluation and training",
    "primary_examiner": "Gills; Kurtis",
    "publication_date": "2023-01-17",
    "publication_number": "11556879",
    "type": "USPAT",
    "us_references": [
      {
        "cited_by_examiner": true,
        "patentee_name": "Tang",
        "pub_month": "2012-08-01",
        "publication_number": "8244603"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Shakes",
        "pub_month": "2014-04-01",
        "publication_number": "8688598"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Sestini",
        "pub_month": "2015-09-01",
        "publication_number": "9129250"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Janert",
        "pub_month": "2005-12-01",
        "publication_number": "20050278062"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Dobbins",
        "pub_month": "2009-08-01",
        "publication_number": "20090213114"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Yang",
        "pub_month": "2012-05-01",
        "publication_number": "20120122062"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Saiki",
        "pub_month": "2013-07-01",
        "publication_number": "20130173212"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Postlethwaite",
        "pub_month": "2013-08-01",
        "publication_number": "20130209976"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Gaddipati",
        "pub_month": "2016-03-01",
        "publication_number": "20160081594"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Kaleal, III",
        "pub_month": "2016-03-01",
        "publication_number": "20160086500"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Dyer",
        "pub_month": "2016-05-01",
        "publication_number": "20160125348"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Modi",
        "pub_month": "2016-08-01",
        "publication_number": "20160239769"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Reichow",
        "pub_month": "2016-09-01",
        "publication_number": "20160275805"
      },
      {
        "cited_by_examiner": true,
        "patentee_name": "Kodeswaran",
        "pub_month": "2017-09-01",
        "publication_number": "20170278034"
      }
    ]
  },
  {
    "app_filing_date": "2022-06-23",
    "appl_id": "17848319",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Cupertino",
        "country": "US",
        "name": "Apple Inc.",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "1000006480386!PG-US-20230008865",
    "cpc_additional": [
      {
        "cpc_class": "H04R",
        "cpc_subclass": "2420/07",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04R",
        "cpc_subclass": "2420/03",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04R",
        "cpc_subclass": "2499/11",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04R",
        "cpc_subclass": "2430/01",
        "version": "2013-01-01"
      }
    ],
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "3/165",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04R",
        "cpc_subclass": "3/12",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A method performed by a first electronic device, the method includes, while engaged in a call with a second electronic device, initiating a joint media playback session in which the first and second electronic devices independently stream media content for synchronous playback; driving a speaker with a mix of a downlink signal of the call and an audio signal of the media content at an overall volume level; receiving a user-adjustment at a single volume control for the first electronic device to reduce the overall volume level; in response to the user adjustment, applying a first gain adjustment to the downlink signal and a second gain adjustment to the audio signal; and driving the speaker with a mix of the downlink signal and the audio signal at the reduced volume level.",
      "background": "CROSS-REFERENCE TO RELATED APPLICATION\n[0001] This application claims the benefit of and priority to U.S. Provisional Patent Application Ser. No. 63/220,928 filed Jul. 12, 2021, which is hereby incorporated by this reference in its entirety.",
      "brief": "FIELD\n\n[0002] An aspect of the disclosure relates to an audio system that controls volume. Other aspects are also described.\n\nBACKGROUND\n\n[0003] Many devices today, such as a smartphone, are capable of various types of telecommunication with other devices. For example, a smartphone may perform a phone call with another device. When a telephone number is dialed, the smartphone connects to a cellular network, which may then connect the smartphone with another device (e.g., another smart phone or a landline). In addition, the smartphone may also be able to conduct a video conference call in which video data and audio data are exchanged with another device.\n\nSUMMARY\n\n[0004] An aspect of the disclosure is a method performed by a first electronic device (e.g., a local device). For instance, while engaged in a (e.g., telephony (or \u201caudio only\u201d) or video) call with a second electronic device (e.g., a remote device), the local device initiates a joint media playback session in which both devices independently stream media content for synchronous playback. The local device drives a speaker with a mix of a downlink signal of the call and an audio signal of the media content at an overall volume level. The local device receives a user-adjustment at a single volume control (e.g., a master volume control) for the local device to reduce the overall volume level, and in response to the user-adjustment, applies a first gain adjustment to the downlink signal and a second gain adjustment to the audio signal, and drives the speaker with a mix of the signals at a reduced volume level.\n\n[0005] In one aspect, the single volume control is a master volume control of the first electronic device that is configured to provide bi-directional control for either incrementally increasing or decreasing the overall volume level. In another aspect, the master volume control is a physical control that is a part of the first electronic device. In some aspects, the master volume control is a user interface (UI) item that is displayed on a display screen of the first electronic device. In one aspect, the single volume control is an input including a gesture made by a user of the first electronic device.\n\n[0006] In one aspect, the single volume control includes several volume settings, each volume setting defining a different overall volume level of the first electronic device, the user-adjustment at the single volume control changes a current volume setting of the signal volume control to a new volume setting that is associated with the reduced overall volume level. In some aspects, the downlink signal is associated with a first volume-to-gain curve that associates the plurality of volume settings to a first plurality of gains and the audio signal of the media content is associated with a second volume-to-gain curve that associates the plurality of volume settings to a second plurality of gains, the method further includes, in response to receiving the user-adjustment, using the first volume-to-gain curve to determine the first gain adjustment based on a first gain that is associated with the new volume setting, and using the second volume-to-gain curve to determine the second gain adjustment based on a second gain that is associated with the new volume setting. In some aspects, the first and second volume-to-gain curves are linear functions of gain with respect to the plurality of volume settings of the single volume control, the first volume-to-gain curve has a greater slope than a slope of the second volume-to-gain curve such that at each volume setting a gain on the first volume-to-gain curve is lower than a gain on the second volume-to-gain curve. In one aspect, the first and second volume-to-gain curves are non-linear functions of gain with respect to volume settings of the single volume control.\n\n[0007] In one aspect, prior to the applying of the first and second gain adjustments the audio signal has a greater signal level than a signal level of the downlink signal, the second gain adjustment is greater than the first gain adjustment such that 1) a signal level of a gain-adjusted audio signal and a signal level of a gain-adjusted downlink signal are lower than the signal level of the downlink signal and 2) the signal level of the gain-adjusted downlink signal is greater than the signal level of the gain-adjusted audio signal. In another aspect, the user-adjustment is a first user-adjustment, the first gain adjustment is a first attenuation, and the second gain adjustment is a second attenuation, the method further includes receiving a second user-adjustment of the single volume control for the first electronic device to increase the reduced overall volume level back to the overall volume level; applying 1) a first gain to the gain-adjusted downlink signal and 2) a second gain to the gain-adjusted audio signal, the first and second gains increase signals levels of the gain-adjusted downlink signal and audio signal, respectively.\n\n[0008] In one aspect, the first gain is proportional to the first attenuation and the second gain is proportional to the second attenuation. In another aspect, the second gain increases a signal level of the gain-adjusted audio signal more than the second attenuation reduced a signal level of the audio signal when applied in response to receiving the first user-adjustment. In some aspects, when the second user-adjustment of the single volume control increases the overall volume level of the first electronic device to a maximum volume level, the applied second gain increases the signal level of the gain-adjusted audio signal higher than the applied first gain increase the signal level of the gain-adjusted downlink signal. In one aspect, the method further includes determining the first and second gain adjustments based on the streamed media content of the joint media playback session. In some aspects, determining the first and second gain adjustments includes using the user-adjustment at the single volume control to perform a table lookup into a data structure that associates, for different user-adjustments, a gain for the downlink signal and a gain for the audio signal of the streamed media content.\n\n[0009] In one aspect, the method further includes determining whether a microphone signal produced by a microphone of the first electronic device includes speech of a user of the first electronic device based on output from a voice activity detector (VAD), the first gain adjustment and the second gain adjustment are applied to the downlink signal and the audio signal, respectively, in response to determining that the microphone signal includes the speech. In one aspect, the first gain adjustment is different than the second gain adjustment. In some aspects, the method further includes, in response to determining that the microphone signal does not include speech the first and second gain adjustments are the same. In another aspect, the first gain adjustment and the second gain adjustment are applied in response to the microphone including speech for a period of time before the user-adjustment is received at the volume control.\n\n[0010] In one aspect, the application of the first and second gain adjustments reduce signal levels of the downlink signal and audio signal, respectively, the method further includes prior to receiving the user-adjustment, determining whether the downlink signal of the call includes speech based on output from a voice activity detector (VAD); and in response to determining that the downlink signal includes speech, applying a third gain adjustment to the audio signal to reduce a signal level of the audio signal. In another aspect, the second gain adjustment reduces the signal level of the audio signal more when the downlink signal includes speech than when the downlink signal does not include speech. In some aspects, the downlink signal is a first downlink signal, the method further includes while engaged in the call and the joint media playback session with the second electronic device and a third electronic device, receiving the first downlink signal from the second electronic device, a second downlink signal from the third electronic device, and the audio signal of the media content; in response to the user-adjustment, applying the first gain adjustment to the first downlink signal, the second gain adjustment to the audio signal, and a third gain adjustment to the second downlink signal, the third gain adjustment adjusts a signal level of the second downlink signal differential than the first gain adjustment adjusts a signal level of the first downlink signal.\n\n[0011] According to another aspect of the disclosure, a method performed by the local device includes initiating a call with a remote device, and during the call, initiating a joint media playback session in which both devices independently stream media content on a display of the local device. The local device receives 1) a downlink signal associated with the call and 2) an audio signal associated with the media content. The local device receives a user-adjustment of a volume control and based on the user-adjustment, applies 1) a first gain adjustment to the downlink signal and 2) a second gain adjustment that is different than the first gain adjustment to the audio signal. The local device drives a speaker with the downlink signal and the audio signal.\n\n[0012] In one aspect, the audio signal is a first audio signal, the method further includes displaying visual content of an extended reality (XR) presentation on the display of the first electronic device; driving the speaker with a mixed signal including the downlink signal, the first audio signal associated with the media content, and a second audio signal of an object of the XR presentation; and determining the first and second gain adjustments based on the object within the XR presentation. In another aspect, in response to receiving the user-adjustment of the volume control, applying a third gain adjustment to the second audio signal of the object of the XR presentation. In some aspects, determining the first and second gain adjustments includes, determining, using sensor data from one or more sensors of the first electronic device, that the user wants sound of the object to be emphasized over sound contained within the downlink signal and the first audio signal, the first gain adjustment and the second gain adjustment attenuate the downlink signal and the first audio signal, respectively, more than the third gain adjustment attenuates the second audio signal.\n\n[0013] In one aspect, determining that the user intends to emphasize the sound of the object includes determining that a gaze of at least one eye of the user is focused on the object within the XR presentation. In another aspect, the sensor data is motion data produced by a motion sensor of the first electronic device, determining that the user intends to emphasize the sound of the object includes determining, based on the motion data, that a user of the first electronic device is tilting the display screen about a center axis that runs through the display screen in a direction towards the object displayed on the display screen with respect to the center axis.\n\n[0014] In one aspect, the call is initiated by a telephony application that is being executed by the first electronic device, the audio signal is a first audio signal from a media application that is being executed by the first electronic device that, the method further includes receiving a second audio signal from a separate application that is being executed by the first electronic device; determining the first gain adjustment, the second gain adjustment, and a third gain adjustment to be applied to the downlink signal, the first audio signal, and the second audio signal, respectively, based on an order of which the first electronic device begins to execute the telephony application, the media application, and the separate application. In another aspect, the second audio signal is attenuated less than at least one of the first audio signal and the downlink signal.\n\n[0015] The above summary does not include an exhaustive list of all aspects of the disclosure. It is contemplated that the disclosure includes all systems and methods that can be practiced from all suitable combinations of the various aspects summarized above, as well as those disclosed in the Detailed Description below and particularly pointed out in the claims. Such combinations may have particular advantages not specifically recited in the above summary.",
      "claim_statement": "What is claimed is:",
      "claims": "1. A method performed by a first electronic device, the method comprising: while engaged in a call with a second electronic device, initiating a joint media playback session in which the first and second electronic devices independently stream media content for synchronous playback; driving a speaker with a mix of a downlink signal of the call and an audio signal of the media content at an overall volume level; receiving a user-adjustment at a single volume control for the first electronic device to reduce the overall volume level; in response to the user-adjustment, applying a first gain adjustment to the downlink signal and a second gain adjustment to the audio signal; and driving the speaker with a mix of the downlink signal and the audio signal at the reduced volume level.   \n\n2. The method of claim 1, wherein the single volume control is a master volume control of the first electronic device that is configured to provide bi-directional control for either incrementally increasing or decreasing the overall volume level. \n\n3. The method of claim 1, wherein prior to the applying of the first and second gain adjustments the audio signal has a greater signal level than a signal level of the downlink signal, wherein the second gain adjustment is greater than the first gain adjustment such that 1) a signal level of a gain-adjusted audio signal and a signal level of a gain-adjusted downlink signal are lower than the signal level of the downlink signal and 2) the signal level of the gain-adjusted downlink signal is greater than the signal level of the gain-adjusted audio signal.  \n\n4. The method of claim 1 further comprises determining the first and second gain adjustments based on the streamed media content of the joint media playback session. \n\n5. The method of claim 1 further comprising determining whether a microphone signal produced by a microphone of the first electronic device includes speech of a user of the first electronic device based on output from a voice activity detector (VAD), wherein the first gain adjustment and the second gain adjustment are applied to the downlink signal and the audio signal, respectively, in response to determining that the microphone signal includes the speech.  \n\n6. The method of claim 5, wherein the first gain adjustment and the second gain adjustment are applied in response to the microphone including speech for a period of time before the user-adjustment is received at the volume control. \n\n7. The method of claim 1, wherein the first gain adjustment is different than the second gain adjustment. \n\n8. A first electronic device, comprising: a speaker; a processor; and non-transitory machine-readable medium having instructions which when executed by the processor causes the electronic device to while engaged in a call with a second electronic device, initiate a joint media playback session in which the first and second electronic devices independently stream media content for synchronous playback, drive the speaker with a mix of a downlink signal of the call and an audio signal of the media content at an overall volume level, receive a user-adjustment at a single volume control for the first electronic device to reduce the overall volume level, and in response to the user-adjustment, apply a first gain adjustment to the downlink signal and a second gain adjustment to the audio signal; and drive the speaker with a mix of the downlink signal and the audio signal at the reduced volume level.    \n\n9. The first electronic device of claim 8, wherein the single volume control is a master volume control of the first electronic device that is configured to provide bi-directional control for either incrementally increasing or decreasing the overall volume level. \n\n10. The first electronic device of claim 8, wherein prior to the applying of the first and second gain adjustments the audio signal has a greater signal level than a signal level of the downlink signal, wherein the second gain adjustment is greater than the first gain adjustment such that 1) a signal level of a gain-adjusted audio signal and a signal level of a gain-adjusted downlink signal are lower than the signal level of the downlink signal and 2) the signal level of the gain-adjusted downlink signal is greater than the signal level of the gain-adjusted audio signal.  \n\n11. The first electronic device of claim 8, wherein the non-transitory machine-readable medium has further instructions to determine the first and second gain adjustments based on the streamed media content of the joint media playback session. \n\n12. The first electronic device of claim 8, wherein the non-transitory machine-readable medium has further instructions to determine whether a microphone signal produced by a microphone of the first electronic device includes speech of a user of the first electronic device based on output from a voice activity detector (VAD), wherein the first gain adjustment and the second gain adjustment are applied to the downlink signal and the audio signal, respectively, in response to determining that the microphone signal includes the speech.  \n\n13. The first electronic device of claim 12, wherein the first gain adjustment and the second gain adjustment are applied in response to the microphone including speech for a period of time before the user-adjustment is received at the volume control. \n\n14. The first electronic device of claim 8, wherein the first gain adjustment is different than the second gain adjustment. \n\n15. A method performed by a first electronic device, the method comprises: initiating a call with a second electronic device; during the call, initiating a joint media playback session in which the first and second electronic independently stream media content on a display of the first electronic device; receiving 1) a downlink signal associated with the call and 2) an audio signal associated with the media content; receiving a user-adjustment of a volume control; based on the user-adjustment, applying 1) a first gain adjustment to the downlink signal of the call and 2) a second gain adjustment that is different than the first gain adjustment to the audio signal associated with the media content; and driving a speaker with the downlink signal and the audio signal associated with the media content.  \n\n16. The method of claim 15, wherein the audio signal is a first audio signal, wherein the method further comprises: displaying visual content of an extended reality (XR) presentation on the display of the first electronic device; driving the speaker with a mixed signal comprising the downlink signal, the first audio signal associated with the media content, and a second audio signal of an object of the XR presentation; and determining the first and second gain adjustments based on the object within the XR presentation.  \n\n17. The method of claim 16, wherein in response to receiving the user-adjustment of the volume control, applying a third gain adjustment to the second audio signal of the object of the XR presentation. \n\n18. The method of claim 17, wherein determining the first and second gain adjustments comprises, determining, using sensor data from one or more sensors of the first electronic device, that the user wants sound of the object to be emphasized over sound contained within the downlink signal and the first audio signal, wherein the first gain adjustment and the second gain adjustment attenuate the downlink signal and the first audio signal, respectively, more than the third gain adjustment attenuates the second audio signal. \n\n19. The method of claim 18, wherein determining that the user intends to emphasize the sound of the object comprises determining that a gaze of at least one eye of the user is focused on the object within the XR presentation. \n\n20. The method of claim 18, wherein the sensor data is motion data produced by a motion sensor of the first electronic device, wherein determining that the user intends to emphasize the sound of the object comprises determining, based on the motion data, that a user of the first electronic device is tilting the display screen about a center axis that runs through the display screen in a direction towards the object displayed on the display screen with respect to the center axis.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0016] The aspects are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \u201can\u201d or \u201cone\u201d aspect of this disclosure are not necessarily to the same aspect, and they mean at least one. Also, in the interest of conciseness and reducing the total number of figures, a given figure may be used to illustrate the features of more than one aspect, and not all elements in the figure may be required for a given aspect.\n\n[0017] FIG. 1 shows an audio system that includes a local device and one or more remote devices that engage in a call while performing a joint media playback session according to one aspect.\n\n[0018] FIG. 2 shows a block diagram of the local device that initiates a joint playback media session while engaged in a call with the one or more remote devices, and of an audio output device that wirelessly communicates with the local device according to one aspect.\n\n[0019] FIG. 3 shows an example of a local device and a remote device engaged in a video call while performing a joint playback media session to synchronously playback video and audio content according to one aspect.\n\n[0020] FIG. 4 is a block diagram of the local device that performs volume control operations according to one aspect.\n\n[0021] FIG. 5 shows examples of volume-to-gain curves according to one aspect.\n\n[0022] FIG. 6 is a flowchart of one aspect of a process for using a volume control to adjust the overall volume level of the audio system.\n\nDETAILED DESCRIPTION\n\n[0023] Several aspects of the disclosure with reference to the appended drawings are now explained. Whenever the shapes, relative positions and other aspects of the parts described in a given aspect are not explicitly defined, the scope of the disclosure here is not limited only to the parts shown, which are meant merely for the purpose of illustration. Also, while numerous details are set forth, it is understood that some aspects may be practiced without these details. In other instances, well-known circuits, structures, and techniques have not been shown in detail so as not to obscure the understanding of this description. Furthermore, unless the meaning is clearly to the contrary, all ranges set forth herein are deemed to be inclusive of each range's endpoints.\n\n[0024] In one aspect, an extended reality (XR) environment (setting or presentation) refers to a wholly or partially simulated environment that people sense and/or interact with via an electronic system. In XR, a subset of a person's physical motions, or representations thereof, are tracked, and, in response, one or more characteristics of one or more virtual objects simulated in the XR environment are adjusted in a manner that comports with at least one law of physics. For example, a XR system may detect a person's head turning and, in response, adjust graphical content and an acoustic field presented to the person in a manner similar to how such views and sounds would change in a physical environment. In some situations, (e.g., for accessibility reasons), adjustments to characteristic(s) of virtual object(s) in a XR environment may be made in response to representations of physical motions (e.g., vocal commands).\n\n[0025] A person may sense and/or interact with a XR object using any one of their senses, including sight, sound, touch, taste, and smell. For example, a person may sense and/or interact with audio objects that create 3D or spatial audio environment that provides the perception of point audio sources in 3D space. In another example, audio objects may enable audio transparency, which selectively incorporates ambient sounds from the physical environment with or without computer-generated audio. In some XR environments, a person may sense and/or interact only with audio objects.\n\n[0026] Examples of XR include virtual reality and mixed reality. A virtual reality (VR) environment refers to a simulated environment that is designed to be based entirely on computer-generated sensory inputs for one or more senses. In contrast to a VR environment, which is designed to be based entirely on computer-generated sensory inputs, a mixed reality (MR) environment refers to a simulated environment that is designed to incorporate sensory inputs from the physical environment, or a representation thereof, in addition to including computer-generated sensory inputs (e.g., virtual objects). Examples of mixed realities include augmented reality and augmented virtuality. An augmented reality (AR) environment refers to a simulated environment in which one or more virtual objects are superimposed over a physical environment, or a representation thereof.\n\n[0027] There are many different types of electronic systems that enable a person to sense and/or interact with various XR environments. Examples include head mounted systems (or head mounted devices (HMDs)), projection-based systems, heads-up displays (HUDs), vehicle windshields having integrated display capability, windows having integrated display capability, displays formed as lenses designed to be placed on a person's eyes (e.g., similar to contact lenses), headphones/earphones, speaker arrays, input systems (e.g., wearable or handheld controllers with or without haptic feedback), smartphones, tablets, and desktop/laptop computers. A head mounted system may have one or more speaker(s) and an integrated opaque display. Alternatively, a head mounted system may be configured to accept an external opaque display (e.g., a smartphone). The head mounted system may incorporate one or more imaging sensors to capture images or video of the physical environment, and/or one or more microphones to capture audio of the physical environment. Rather than an opaque display, a head mounted system may have a transparent or translucent display. The transparent or translucent display may have a medium through which light representative of images is directed to a person's eyes. The display may utilize digital light projection, OLEDs, LEDs, uLEDs, liquid crystal on silicon, laser scanning light source, or any combination of these technologies. The medium may be an optical waveguide, a hologram medium, an optical combiner, an optical reflector, or any combination thereof. In one embodiment, the transparent or translucent display may be configured to become opaque selectively. Projection-based systems may employ retinal projection technology that projects graphical images onto a person's retina. Projection systems also may be configured to project virtual objects into the physical environment, for example, as a hologram or on a physical surface.\n\n[0028] FIG. 1 shows an audio system 1 that includes a local device and one or more remote devices that engage in a call while performing a joint media playback session according to one aspect. As described herein, this may allow users of the devices to listen to (and/or watch) media content (e.g., on the devices) while participating in a conversation with one another. The audio system includes a local (or first electronic) device 2, a remote (or second electronic) device 3, a network 4 (e.g., a computer network, such as the Internet), a media content server 5, and an (e.g., optional) audio output device 6. In one aspect, the system may include more or less elements. For example, the audio system may not include an audio output device (and/or the output device may be a part of (or integrated) into the local device. In the case in which the audio system does not include an audio output device, the local device may perform audio signal processing and/or audio output operations (e.g., driving one or more speakers of the local device to output sound), as described herein. In one aspect, the system may have one or more remote devices, where all of the devices are engaged in the (e.g., conference) call and the joint media playback session with one another and with the local device, as described herein. In another aspect, the audio system may include one or more remote (electronic) servers that are communicatively coupled with at least some of the devices of the audio system 1, and may be configured to perform at least some of the operations described herein.\n\n[0029] In one aspect, the local device (and/or the remote device) may be any electronic device (e.g., with electronic components, such as a processor, memory, etc.) that is capable of engaging in a call, such as a telephony (\u201cvoice-only\u201d or \u201caudio-only) call) or a video (e.g., conference) call, while performing a joint media playback session with one or more other devices (e.g., one or more remote devices) in which (at least some of) the devices (e.g., simultaneously) playback media content. For example, the media content may include a musical composition, a movie, etc., of which the local device and one or more remote devices may simultaneously play back, while engaged in a call. As a result, users of the devices may be able to hear sounds (and/or see images or video) of the media and/or hear sounds (and/or see images or video) of the (video) call (e.g., simultaneously). In some aspects, the media content may be interactive content, such as a video game in which users of the local and remote device(s) participant. In another aspect, the media content may include an XR environment in which each device that is engaged within the joint media playback session may participate. For example, the local device may participate in the XR environment by displaying image data of the XR environment on one or more display screens and using one or more audio signals that include sounds of the XR environment to drive one or more speakers of the local device.\n\n[0030] As described herein, an XR environment (or presentation) refers to a wholly or partially simulated environment that people sense and/or interact with via an electronic device. For example, the XR environment may include AR content, MR content, VR content, and/or the like. There are many different types of electronic systems that enable a person to sense and/or interact with various XR environments. Examples include head mountable systems, projection-based systems, heads-up displays (HUDs), vehicle windshields having integrated display capability, windows having integrated display capability, displays formed as lenses designed to be placed on a person's eyes (e.g., similar to contact lenses), headphones/earphones, speaker arrays, input systems (e.g., wearable or handheld controllers with or without haptic feedback), smartphones, tablets, and desktop/laptop computers.\n\n[0031] In some aspects, the local device may be a desktop computer, a laptop computer, a digital media player, etc. In one aspect, the device may be a portable electronic device (e.g., being handheld operable), such as a tablet computer, a smart phone, etc. In another aspect, the device may be a head-mounted device, such as smart glasses, or a wearable device, such as a smart watch. In one aspect, the remote device(s) may be the same type of device as the local device (e.g., both devices being smart phones). In another aspect, at least some of the remote devices may be different, such as some being desktop computers, while others are smart phones.\n\n[0032] As illustrated, the local device 2 is (e.g., communicatively) coupled to the remote device 3 and/or the media content server 5 via the computer network (e.g., Internet) 4. Specifically, the local and remote devices may be configured to establish and engage in a telephony (or voice-only) call in which the devices that are engaged within the call exchange audio data. For instance, each device transmits at least one microphone signal as an uplink audio signal to the other devices engaged in the call, and receives at least one audio signal as a downlink audio signal from the other devices for playback by one or more speakers. In one aspect, the network may include a Public Switched Telephone Network (PSTN), over which the local device and the remote device(s) may be capable of placing outgoing calls and/or receiving incoming calls. In another aspect, the local device may be configured to establish an Internet Protocol (IP) telephony (or Voice over IP (VoIP)) call with one or more remote devices via the network (e.g., the Internet). In particular, the local device may use any signaling protocol (e.g., Session Initiation Protocol (SIP)) to establish a communication session and use any communication protocol (e.g., Transmission Control Protocol (TCP), Real-time Transport Protocol (RTP), etc.) to exchange audio data during a call. For example, when a call is initiated (e.g., by a telephony application (e.g., application 29 shown in FIG. 2) executing within the local device), the local device may transmit one or more microphone signals captured by one or more microphones (e.g., as an uplink audio signal) as audio data (e.g., in IP packets) to one or more remote devices, and receive one or more (e.g., downlink audio) signals from the remote devices for driving one or more speakers of the local device, via the network. In another aspect, the local device may be configured to establish a wireless (e.g., cellular) call. In which case, the network 4 may include one or more cell towers, which may be part of a communication network (e.g., a 4G Long Term Evolution (LTE) network) that supports data transmission (and/or voice calls) for electronic devices, such as mobile devices (e.g., smartphones).\n\n[0033] In another aspect, the local and remote devices may be configured to establish and engage in a video call with one or more remote devices 3. In which case, the local device may establish the video call (e.g., similarly to a VoIP, using SIP to initiate the session and RTP to transmit data), and when established exchange video and/or audio data with one or more remote devices. For instance, the local device may include one or more cameras which capture video that is encoded using any video codec (e.g., H.264), and transmitted to the remote devices for decoding and display on one or more display screens. More about calls is described herein.\n\n[0034] In some aspects, the media content server 5 may be a stand-alone server computer or a cluster of server computers configured to stream media content to electronic devices, such as the local and remote devices. In which case, the server may be a part of a cloud computing system that is capable of streaming data as a cloud-based service that is provided to one or more subscribers (e.g., of the local and/or remote device(s)). In some aspects, the server may be configured to stream any type of media (or multi-media) content, such as audio content (e.g., musical compositions, audiobooks, podcasts, etc.), still images, video content (e.g., movies, television productions, etc.), etc. In one aspect, the server may use any audio and/or video encoding format and/or any method for streaming the content to one or more devices.\n\n[0035] In one aspect, the media content server 5 may be configured to simultaneously stream media content to one or more devices in order to allow the devices to engage in a joint media playback session. For example, the server may receive a request from a device (e.g., local device 2) to stream a piece of media content that may include audio content (e.g., a musical composition) and/or video content (e.g., a video signal associated with a movie) with another device (e.g., remote device 3). In one aspect, the request may be transmitted by the local device (and/or the remote device(s)) in response to the device receiving user input to begin to playback the media content. In which case, the server may establish a communication link with both the local device and the remote device(s) that are already engaged in a (e.g., telephony and/or video) call. Once established, the server may encode audio content using any codec (e.g., MP3, AAC, etc.) and/or may encode video content using any codec, and transmit the encoded content to each device to be decoded and output. In another aspect, the local device may transmit a message to the remote device, requesting to initiate a joint media playback session. In response, the remote device may communicate with the media content server to retrieve the media content and to synchronize playback with the local device. In one aspect, devices that participate within the joint media playback session may output media content in sync, such that the content is output and experienced by users at the same time. In some aspects, any timing synchronization method may be used (e.g., by the devices participating within the session and/or the server) to ensure that the media is streamed simultaneously and in sync. More about the joint media playback session is described herein.\n\n[0036] As illustrated, the audio output device 6 may be any electronic device that includes at least one speaker and is configured to output sound by driving the speaker. For instance, as illustrated the device is a wireless headset (e.g., in-ear headphones or earbuds) that are designed to be positioned on (or in) a user's ears, and are designed to output sound into the user's ear canal. In some aspects, the earphone may be a sealing type that has a flexible ear tip that serves to acoustically seal off the entrance of the user's ear canal from an ambient environment by blocking or occluding in the ear canal. As shown, the output device includes a left earphone for the user's left ear and a right earphone for the user's right ear. In this case, each earphone may be configured to output at least one audio channel of media content (e.g., the right earphone outputting a right audio channel and the left earphone outputting a left audio channel of a two-channel input of a stereophonic recording, such as a musical work). In another aspect, the output device may be any electronic device that includes at least one speaker and is arranged to be worn by the user and arranged to output sound by driving the speaker with an audio signal. As another example, the output device may be any type of headset, such as an over-the-ear (or on-the-ear) headset that at least partially covers the user's ears and is arranged to direct sound into the ears of the user.\n\n[0037] In some aspects, the audio output device may be a head-worn device, as illustrated herein. In another aspect, the audio output device may be any electronic device that is arranged to output sound into an ambient environment. Examples may include a stand-alone speaker, a smart speaker, a home theater system, or an infotainment system that is integrated within a vehicle.\n\n[0038] In one aspect, the output device may be a wireless device that may be communicatively coupled to the local device in order to exchange audio data. For instance, the local device may be configured to establish the wireless connection with the audio output device via a wireless communication protocol (e.g., BLUETOOTH protocol or any other wireless communication protocol). During the established wireless connection, the local device may exchange (e.g., transmit and receive) data packets (e.g., Internet Protocol (IP) packets) with the audio output device, which may include audio digital data in any audio format. In particular, the local device may be configured to establish and communicate with the audio output device over a bi-directional wireless audio connection (e.g., which allows both devices to exchange audio data), for example to conduct a hands-free call or to use voice commands. Examples of a bi-directional wireless communication protocol include, without limitation the Hands-Free Profile (HFP) and the Headset Profile (HSP), both of which are BLUETOOTH communication protocols. In another aspect, the local device may be configured to establish and communication with the output device over a uni-directional wireless audio connection, such as (e.g., Advanced Audio Distribution Profile (A2DP) protocol), which allows the local device to transmit audio data to one or more audio output devices. More about these wireless audio connections is described herein.\n\n[0039] In another aspect, the local device 2 may communicatively couple with the audio output device 6 via other methods. For example, both devices may couple via a wired connection. In this case, one end of the wired connection may be (e.g., fixedly) connected to the audio output device, while another end may have a connector, such as a media jack or a universal serial bus (USB) connector, which plugs into a socket of the audio source device. Once connected, the local device may be configured to drive one or more speakers of the audio output device with one or more audio signals, via the wired connection. For instance, the local device may transmit the audio signals as digital audio (e.g., PCM digital audio). In another aspect, the audio may be transmitted in analog format.\n\n[0040] In some aspects, the local device 2 and the audio output device 6 may be distinct (separate) electronic devices, as shown herein. In another aspect, the local device may be a part of (or integrated with) the audio output device. For example, as described herein, at least some of the components of the local device (such as a controller) may be part of the audio output device, and/or at least some of the components of the audio output device may be part of the local device. In this case, each of the devices may be communicatively coupled via traces that are a part of one or more printed circuit boards (PCBs) within the audio output device.\n\n[0041] FIG. 2 shows a block diagram of the local device 2 that initiates a joint playback media session while engaged in a (e.g., voice or video) call with the one or more remote devices 3, and shows of an audio output device 6 that wirelessly communicates with the local device according to one aspect. The local device 2 includes a controller 20, a network interface 21, a speaker 22, a display screen (or display) 25, memory 26, a volume control 12, and one or more sensors 10, which include a microphone 23, a camera 24, an inertial measurement unit (IMU) 11. In one aspect, the local device may include more or less elements as described herein. For instance, the device may include two or more of at least some of the elements, such as having two or more microphones 23 and/or two or more speakers 22.\n\n[0042] In one aspect, the one or more sensors 10 are configured to detect the environment (e.g., in which the local device is located) and produce sensor data based on the environment. For instance, the camera 24 is a complementary metal-oxide-semiconductor (CMOS) image sensor that is capable of capturing digital images including image data that represent a field of view of the camera 24, where the field of view includes a scene of an environment in which the device 2 is located. In some aspects, the camera may be a charged-coupled device (CCD) camera type. The camera is configured to capture still digital images and/or video that is represented by a series of digital images. In one aspect, the camera may be positioned anywhere about/on the local device. In some aspects, the device may include multiple cameras (e.g., where each camera may have a different field of view).\n\n[0043] The microphone 23 may be any type of microphone (e.g., a differential pressure gradient micro-electro-mechanical system (MEMS) microphone) that is configured to convert acoustical energy caused by sound wave propagating in an acoustic environment into an input microphone signal. In some aspects, the microphone may be an \u201cexternal\u201d (or reference) microphone that is arranged to capture sound from the acoustic environment. In another aspect, the microphone may be an \u201cinternal\u201d (or error) microphone that is arranged to capture sound (and/or sense pressure changes) inside a user's ear (or ear canal). The IMU is configured to produce motion data that indicates the position and/or orientation of the local device. In one aspect, the local device may include additional sensors, such as (e.g., optical) proximity sensors that are designed to produce sensor data that indicates an object is at a particular distance from the sensor (and/or the local device).\n\n[0044] In one aspect, the sensors 10 may be a part of (or integrated into) the local device. In another aspect, sensors may be separate electronic devices that are communicatively coupled with the controller (e.g., via the network interface 21).\n\n[0045] The speaker 22 may be an electrodynamic driver that may be specifically designed for sound output at certain frequency bands, such as a woofer, tweeter, or midrange driver, for example. In one aspect, the speaker 22 may be a \u201cfull-range\u201d (or \u201cfull-band\u201d) electrodynamic driver that reproduces as much of an audible frequency range as possible. In some aspects, the local device may include one or more speakers, where at least some of the speakers may be the same or different (e.g., one being a woofer while another is a tweeter).\n\n[0046] The display screen 25 is designed to present (or display) digital images or videos of video (or image) data. In one aspect, the display screen may use liquid crystal display (LCD) technology, light emitting polymer display (LPD) technology, or light emitting diode (LED) technology, although other display technologies may be used in other aspects. In some aspects, the display may be a touch-sensitive display screen that is configured to sense user input as input signals. In some aspects, the display may use any touch sensing technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies.\n\n[0047] The volume control 12 is configured to adjust a volume level of sound output of the local device in response to receiving a user-adjustment (e.g., user input) at the control. In one aspect, the volume control may be a \u201cmaster\u201d volume control that is configured to control the overall volume level (e.g., sound output level of the speaker 22) of the local device. In one aspect, the volume control may be a \u201chardware\u201d volume control that may be a dedicated volume input control, such as one or more buttons, a rotatable knob, or a physical slider. In some aspects, the volume control may be any type of physical input device that can adjust the overall volume level. In one aspect, the volume control may be a single volume control that includes several volume settings (or positions), where each setting defines a different volume level (e.g., a different sound output level (e.g., dB SPL)) of the local device. In particular, the volume control may (e.g., in response to a user-adjustment) incrementally increase or decrease the volume level based on a user adjusting the control's volume setting or position. For example, when the volume control is a rotatable volume knob, the control may have several (e.g., 18) volume settings, where each successive volume setting may correspond to a degree of rotation and may increase the overall volume by a particular gain value. In this case, each volume setting may correspond to a 20\u00b0 rotation about a center axis. For instance, a first volume setting may be 0\u00b0, where the overall volume is muted, a second volume setting may be 20\u00b0 (e.g., where the overall volume increases by a particular gain value), and so on. Thus, the knob produces a control signal that either incrementally increases or decreases the volume based on how much the knob is twisted and in what direction (e.g., turning clockwise increases the volume, whereas turning counter-clockwise decreases the volume). In one aspect, the volume control may be a master volume control that is configured to provide bi-directional control for either incrementally increasing or decreasing an overall volume level of (e.g., sound output of) the device. In one aspect, the control may be a part of the local device (e.g., integrated on the device). In another aspect, the volume control may be a part of an electronic device that is communicatively coupled with the local device.\n\n[0048] In another aspect, the volume control may be a \u201csoftware\u201d volume control, such as user interface (UI) item that is displayed on (e.g., a graphical user interface (GUI) within) the display screen 25 of the local device. For example, the volume control may be a slider that may be translated (e.g., moved in at least one direction) along a predefined slidable range. When user input is received to adjust (or translate) the position of the slider (e.g., by the user touching the slider on the display screen and dragging it in one or more directions), the volume control adjusts the overall volume level based on the position of the slider. In one aspect, similar to the example of the physical control, the UI item may include several volume settings, where each position of the slider may correspond to a different volume level for the device. In this case, since the slider has a pre-defined slidable range, or a slidable distance from one side, each volume setting may correspond to a distance along the slidable range. In another aspect, the volume settings may correspond to a percentage (e.g., from 0 to 100 percent) that may correspond to the slider's distance from a starting position along the slidable range. In another aspect, the volume control may include several volume settings as numerical values (e.g., 1-10), where the volume settings may be changed by a user-adjustment to the volume control (e.g., selecting, dragging, twisting, etc.).\n\n[0049] In some aspects, the volume control may be any input by a user of the device. For example, the input may include a gesture (e.g., a hand gesture, a finger gesture, a head gesture, etc.) made by the user and detected by the device (e.g., by the IMU 11 detecting motion of the local device that is caused by the hand gesture). In another aspect, the volume control may be a voice command that is received via the microphone 23. More about the volume control 12 is described herein.\n\n[0050] The controller 20 may be a special-purpose processor such as an application-specific integrated circuit (ASIC), a general purpose microprocessor, a field-programmable gate array (FPGA), a digital signal controller, or a set of hardware logic structures (e.g., filters, arithmetic logic units, and dedicated state machines). The controller is configured to perform audio signal processing operations and/or networking operations. For instance, the controller 20 may be configured to engage in a call and simultaneously perform a joint media playback session to stream (e.g., exchange) media content with one or more remote devices, via the network interface 21. In another aspect, the controller may be configured to perform audio signal processing operations upon audio data of the media content and/or audio data (e.g., a downlink signal) associated with an engaged call. More about the operations performed by the controller 20 is described herein.\n\n[0051] The memory 26 may be any type of (e.g., non-transitory machine-readable) storage medium, such as random-access memory, CD-ROMS, DVDs, Magnetic tape, optical data storage devices, flash memory devices, and phase change memory. In one aspect, the memory may be a part of (e.g., integrated within) the local device. In another aspect, the memory may be a part of the controller 20. In some aspects, the memory may be a separate device, such as a data storage device. In which case, the memory may be communicatively coupled (e.g., via the network interface 21) with the controller 20 in order for the controller to perform one or more of the operations described herein.\n\n[0052] As shown, the memory has stored therein, an operating system (OS) 27, a media application 28, and a telephony application 29, which when executed by the controller cause the local device to perform one or more operations, as described herein. In one aspect, the memory may include more or less applications. The OS 27 is a software component that is responsible for management and coordination of activities and the sharing of resources (e.g., controller resources, memory, etc.) of the local device 2. In one aspect, the OS acts as a host for application programs (e.g., applications 28 and 29) that run on the device. In some aspects, the applications may run on top of the OS. In one aspect, the OS provides an interface to a hardware layer (not shown) of the local device, and may include one or more software drivers that communicate with the hardware layer. For example, the drivers can receive and process data packets received through the hardware layer from one or more other devices that are communicatively coupled to the device (e.g., user input devices, such as a display 25, which may be a touch-sensitive display screen, one or more of the sensors 10, etc.).\n\n[0053] In one aspect, the media application 28 may be an application which when executed by the local device streams media content to the local device (e.g., from the media content server 5). Specifically, the media application may be a music streaming application, which when executed streams music for playback by the speaker 22 (and/or speaker 83 of the audio output device). As another example, the media application 28 may be a multi-media (e.g., video and/or audio) streaming application, which streams multi-media content (e.g., movies, etc.) for playback at the local device (e.g., for video playback through the display screen 25 and/or audio playback through the speaker 22). In another aspect, the application may retrieve the media content from local memory (e.g., memory 26) and/or from a remote source, such as the media content server 5, as described herein. In one aspect, to stream media content, the media application may display a graphical user interface (GUI) on the display screen 25, through which a user may navigate the application in order to select one or more pieces of media content for streaming to the local device (and/or audio output device).\n\n[0054] In one aspect, the telephony application 29 may be an application which when executed by the local device allows the local device to initiate and conduct a telephone (telephony) call with one or more remote devices. For example, when initiated (e.g., when the user selects a UI item of the application displayed on the display screen 25), the application may display a GUI through which the local user may dial a telephone number. Once dialed, the local device may have connected to the remote device via a cellular network (e.g., a 4G Long Term Evolution (LTE) network) of the network 4, as described herein. In some aspects, the telephony application may be an audio-only (or voice-only) telephony application, which is capable of performing audio calls (e.g., where the local device and one or more remote devices exchange audio data captured by one or more microphones, which is used to drive one or more speakers of the respective devices). In another aspect, the telephony application may be a video call (or video conference) application, which allows the local device 2 to conduct a video call with one or more remote devices, as described herein. In another aspect, the local device may include a video call application, which is separate from the telephony application.\n\n[0055] In another aspect, the local device 2 may include one or more other applications, which when executed causes the local device (and/or audio output device) to playback (or output) audio and/or video (or image) content. For example, the memory may include a XR presentation application, which when executed by the (e.g., controller 20 of the) local device 2, allows the local user of the device to participate within a XR environment.\n\n[0056] The audio output device 6 includes a controller 80, a network interface 81, a speaker 83, a microphone 84, an accelerometer 85, and a volume control 82. In one aspect, the device may include more or less elements, such as having memory. In some aspects, the microphone may be an external or internal microphone, as described herein. In the case of an in-ear headphone, the internal microphone may sense inside the user's ear when the headphone is positioned on (or in) the user's ear. The accelerometer is arranged and configured to receive (detect or sense) speech vibrations that are produced while a user (e.g., who may be wearing the output device) is speaking, and produce an accelerometer signal that represents (or contains) the speech vibrations. Specifically, the accelerometer is configured to sense bone conduction vibrations that are transmitted from the vocal cords of the user to the user's ear (ear canal), while speaking and/or humming. For example, when the audio output device is a wireless headset, the accelerometer may be positioned anywhere on or within the headphone, which may touch a portion of the user's body in order to sense vibrations.\n\n[0057] In one aspect, controller 80 is configured to perform audio signal processing operations and/or networking operations, as described herein. For instance, the controller may be configured to obtain (or receive) an audio data (as an analog or digital audio signal) of media content or user-desired media content (e.g., music, etc.) for playback through the speaker 83. In some aspects, the controller may obtain audio data from local memory, or the controller may obtain audio data from the network interface 81, which may obtain the data from an external source such as the local device 2 (via its network interface 21). For instance, the output device may stream an audio signal from the local device (e.g., via the BLUETOOTH connection) for playback through the speaker 83. The audio signal may be a signal input audio channel (e.g., mono). In another aspect, the controller may obtain two or more input audio channel (e.g., stereo) for output through two or more speakers. In one aspect, in the case in which the output device includes two or more speakers, the controller may perform additional audio signal processing operations. For instance, the controller may spatially render the input audio channels (e.g., by applying spatial filters, such as head related transfer functions (HRTFs)) to produce binaural output audio signals for driving at least two speakers (e.g., a left speaker and a right speaker).\n\n[0058] In one aspect, the volume control 82 may perform similar operations as the control 12 of the local device. For instance, upon receiving user input, the control may adjust the (e.g., overall) volume of sound output by the (e.g., speaker 83 of the) audio output device 6. In some aspects, the volume control 82 may be used to adjust the volume at the local device 2. In which case, upon receiving user input, the audio output device 6 may transmit a control signal indicating a user-adjustment of the volume control 82 to the local device, which may use the signal to adjust the volume of one or more audio signals. More about adjusting the volume is described herein.\n\n[0059] As described herein, the controller 20 may be configured to perform (e.g., additional) audio signal processing operations based on elements that are coupled to the controller. For instance, when the local device includes two or more \u201cextra-aural\u201d speakers, which are arranged to output sound into the acoustic environment rather than speakers that are arranged to output sound into a user's ear (e.g., as speakers of an in-ear headphone), the controller may include a sound-output beamformer that is configured to produce speaker driver signals which when driving the two or more speakers produce spatially selective sound output. Thus, when used to drive the speakers, the local device may produce directional beam patterns that may be directed to locations within the environment.\n\n[0060] In some aspects, the controller 20 may include a sound-pickup beamformer that can be configured to process the audio (or microphone) signals produced two or more external microphones of the output device to form directional beam patterns (as one or more audio signals) for spatially selective sound pickup in certain directions, so as to be more sensitive to one or more sound source locations. In some aspects, the controller may perform audio processing operations upon the audio signals that contain the directional beam patterns (e.g., perform spectrally shaping).\n\n[0061] In another aspect, the controller 80 may perform one or more functions. For example, the controller 80 may be configured to perform an active noise cancellation (ANC) function to cause the speaker 83 to produce anti-noise in order to reduce ambient noise from the environment that is leaking into the user's ears. The ANC function may be implemented as one of a feedforward ANC, a feedback ANC, or a combination thereof. As a result, the controller may receive a reference microphone signal from a microphone that captures external ambient sound, such as microphone 84. In another aspect, the controller may perform any ANC method to produce the anti-noise. In another aspect, the controller 80 may perform a transparency function in which sound played back by the device is a reproduction of the ambient sound that is captured by the device's external microphone in a \u201ctransparent\u201d manner, e.g., as if the headphone was not being worn by the user. The controller processes at least one microphone signal captured by at least one external microphone 84 and filters the signal through a transparency filter, which may reduce acoustic occlusion due the audio output device being on, in, or over the user's ear, while also preserving the spatial filtering effect of the wear's anatomical features (e.g., head, pinna, shoulder, etc.). The filter also helps preserve the timbre and spatial cues associated with the actual ambient sound. In one aspect, the filter of the transparency function may be user specific according to specific measurements of the user's head. For instance, the controller may determine the transparency filter according to a head-related transfer function (HRTF) or, equivalently, head-related impulse response (HRIR) that is based on the user's anthropometrics.\n\n[0062] In one aspect, the (e.g., controller 20 of the) local device may perform (or control) at least some of the functions of the (e.g., controller 80 of the) audio output device 6. For instance, the controller 20 may perform the ANC function, whereby the anti-noise signal is produced from a reference microphone signal (e.g., from the audio output device and/or the local device). When produced, the local device may transmit the anti-noise signal to the audio output device for audio playback via the speaker 83.\n\n[0063] As described herein, both the local device and audio output device are configured to establish a wireless audio connection (e.g., BLUETOOTH connection) in order to exchange audio data. Thus, audio data and/or control signals may be exchanged between both devices via the wireless connection.\n\n[0064] In one aspect, operations performed by the controllers may be implemented in software (e.g., as instructions stored in memory and executed by either controller) and/or may be implemented by hardware logic structures as described herein.\n\n[0065] In another aspect, at least some of the operations performed by the audio system 1 as described herein may be performed by the local device 2 and/or by the audio output device 6. For instance, the local device may include two or more speakers and may be configured to perform sound-output beamformer operations (e.g., when the local device includes two or more speakers). In another aspect, at least some of the operations may be performed by a remote server that is communicatively coupled with either device, for example over the network (e.g., Internet).\n\n[0066] In one aspect, at least some elements of the local device 2 and/or the audio output device 6 may be integrated (or a part of) each the respective device. For example, when the audio output device is on-ear headphones, the microphone, speaker, and accelerometer may be a part of at least one earcup of the headphones that is placed on a user's ear. In another aspect, at least some of the elements may be separate electronic devices that are communicatively coupled to the device. For instance, the display screen 25 may be a separate device (e.g., being a display monitor or television) that is communicatively coupled (e.g., wired or wirelessly connected) with the local device to receive image data for display. As another example, the camera 24 may be a part of a separate electronic device (e.g., a webcam) that is coupled to the local device to provide captured image data.\n\n[0067] As described herein, the local device 2 and remote devices 3 of the audio system 1 may perform a joint media playback session while engaged in a call in order to allow users of the devices to communicate while experiencing simultaneous media content playback. In one aspect, the local device may initiate the joint media playback session, while already engaged in a call. FIG. 3 illustrates graphical examples of the local device and remote device participating in joint media playback, while engaged in a video conference call.\n\n[0068] FIG. 3 shows an example of the local device 2 and the remote device 3 engaged in a video call while performing a joint playback media session to synchronously playback video and audio content according to one aspect. Specifically, this figure shows a local user 30 and a remote user 31 (who may be at the same or different locations), who are engaged in a video call while simultaneously engaged in a joint media playback session. Displayed on the display 25 of the local device 2 is a video call user interface (UI) 44 (e.g., that is being displayed by a video call application (e.g., telephony application 29 shown in FIG. 2) that is being executed by the local device) that shows video (e.g., a video representation) of the local user 46 and video of remote user 45 (which is larger than the video of the local user) and positioned in the middle of the UI 44. Similarly, displayed on a display 32 of the remote device 3 is a video call UI 47, which is displayed by a video call application (which may be the same or different as the application executed on the local device), which shows the video of the remote and local users (with their positions transposed with respect to the positions shown on display 25 of the local device).\n\n[0069] In one aspect, the video representations may be produced using video data captured by one or more cameras of each device. For example, while the local user 30 is in the field of view of camera 24, the camera may capture video data of the local user, which is then displayed on the local device and transmitted (e.g., via network 4) to the remote device for displaying on display 32.\n\n[0070] Also shown on both devices is video of media content 49 of the joint media playback session of which both devices are engaged. In one aspect, both devices may stream the media content (e.g., using a video streaming application) to playback the media content (e.g., display the video on their displays and output audio content of the media content through one or more speakers) in sync, while both devices are engaged in the video call. As a result, both users may interact (e.g., have a conversation) with each other through the video call, while at the same time watch the video (and hear audio) of the media content 49.\n\n[0071] In one aspect, the video call UI 44 (and/or the video call UI 47) may include additional video representations based on a number of remote users who are participating within the video call. In this example, the local device's UI 44 includes one video representation for the remote user 45, since the local device is only engaged in a video call with the one remote user. As more remote users join the video call, the UI 44 may include additional video representations, one for each remote user. As an example, when engaged in a video call with three remote devices, the video call UI 44 may include three video representations, one for each remote device (and the video representation of the local user 46). In one aspect, each of the video representations may be positioned about the UI 44. Continuing with the previous example, the three video representations may be positioned in row of three. In another aspect, the video representations may be positioned differently.\n\n[0072] As described herein, the local device may engage in a joint media playback session while on a telephony (audio-only) call. In which case, both devices may display video of media content (and/or playback audio of the media content), while the users conduct a voice-only conversation. In one aspect, either of the devices may have initiated the telephony (or video) call, using any known method. For instance, the local user 30 may have initiated the telephony application, and dialed the remote device's telephone number.\n\n[0073] As shown in this example, audio and video content may be played back in a joint media playback session while devices are engaged in a video call. In some aspects, any type of media content may be played back during a joint media playback session while local and remote devices are engaged in either a telephony (or voice-only) call or a video call. For example, the media content may include an XR presentation of which users of the local device and remote device(s) may participate. In particular, the media content may include video (or visual) content of the XR environment as image data that may be displayed on the display screen 25 of the local device and on display screens of the remote device(s) 3. In addition, the media content may include audio data (or one or more audio signals) of sounds within the XR environment. For instance, the audio data may include sounds of objects (e.g., a dog barking) within the XR environment.\n\n[0074] As described herein, the users of the devices may participate within the XR environment. In particular, each of the devices may present different (or similar) perspectives of the XR environment. For instance, each device may present a first-person perspective of the environment (e.g., through a viewpoint of a virtual avatar associated with the device that is positioned within the XR environment). In which case, sounds (e.g., of objects) within the XR environment may be perceived differently by each of the participants. More about sounds of objects within XR environments is described herein.\n\n[0075] As described herein, the local device may engage in a joint media playback session with a remote device, while both devices are engaged in a call, such as an audio-only call. In which case, the local device may receive a downlink audio signal from the remote device that includes speech of a remote user of the remote device, while the local device may transmit an uplink audio signal that includes speech of the local user. In addition, the local device may receive media content, such as an audio signal of a musical composition associated with the playback session. As a result, the local device may drive one or more speakers with a mix of the downlink audio signal and the audio signal, thereby allowing the local device to engage in a conversation with the remote user, while experiencing the media content.\n\n[0076] While engaged in the joint media playback session, the local and remote devices may playback various types of media content, such as a musical composition, a movie, and an XR environment, as described herein. Audio data associated with each type of media content may be mastered differently. For instance, music may have a dynamic range of 96 dB, while audio of a movie may have a dynamic range of 144 dB. In addition, the media content may be mastered at different volume levels, where their signal levels may be higher (or greater) than a signal level of the downlink signal of the call. As result, when the signals of the media content and the downlink signal are mixed together, sound of the media content may be perceived louder by the local user than sound (e.g., speech) of the downlink signal and therefore may drown out the speech of the remote user contained within the downlink signal. In one aspect, to solve this problem, the local user may be provided with multiple volume controls, one for signal being output by the local device. This solution, however, has drawbacks. For example, to be effective, each audio signal that is being mixed and played back requires its own volume control. Thus, this solution may be unmanageable as the number of audio signals being played back increases. Therefore, there is a need for a single volume control (e.g., a master volume control for the local device) that adjusts the overall volume level of audio playback by applying different gains to the signals.\n\n[0077] To overcome these deficiencies, the present disclosure describes an audio system that includes a single volume control that is capable of applying different volume control behavior to different audio signals. Specifically, in response to receiving a user-adjustment at a single (e.g., master) volume control of the local device to adjust the overall volume level, the local device applies a first gain adjustment to the downlink signal and a second gain adjustment to the audio signal, and drives the speaker 22 of the local device with a mix of the signals at the adjusted volume level. For example, returning to the previous example, since the volume level of the audio signal of the media content may be mastered higher (or louder) than the downlink signal, as the local user turns down the volume at the volume control, the second gain adjustment may reduce the signal level of the audio signal more (or at a faster rate) than the first gain adjustment reduces the signal level of the downlink signal. Thus, as the overall volume is turned down, the level of the audio signal decreases more than the level of the downlink signal.\n\n[0078] FIG. 4 is a block diagram of the local device 2 that performs volume control operations according to one aspect. Specifically, this figure shows that the controller 20 has several operational blocks for performing audio signal processing operations for controlling the volume of audio output of the local device. As shown, the controller includes a call manager 52, a joint media playback session manager 51, a voice activity detector (VAD) 53, a volume-to-gain curve selector 54, an audio signal gain selector 55, a downlink signal gain selector 56, scalar gains 90, 57, and 58, and a (e.g., matrix) mixer 59. In one aspect, the controller may have more or less operational blocks. For example, the controller may include additional pairs of gain selectors and scalar gains, each pair for an additional audio signal that is to be mixed and played back during the call and playback session. In one aspect, at least some of the operational blocks may be optional, and therefore the operations of the optional blocks may be omitted or combined. For example, the scalar gain 90 may be optional, as described herein.\n\n[0079] The call manager 52 is configured to initiate (and conduct) a call between the local device 2 and one or more of the remote devices 3. In one aspect, the call manager may initiate the call in response to user input. For example, the call manager may be a part of (or receive instructions from) a telephony application that is being executed by the (e.g., controller 20 of the) local device. For instance, the telephony application may display a UI on the display screen 25 of the local device, which may provide the local user the ability to initiate the call, such as a keypad, a contacts list, etc. Once the UI receives user input (e.g., the local user dialing a remote user's telephone number using the keypad), the call manager may communicate with the network interface 21 of the local device 2 to establish the call, as describe herein. In one aspect, the telephony call may be over any network, such as over the PSTN and/or over the Internet (e.g., for a VoIP call). In some aspects, the call manager may initiate the call, as described herein, and/or using any method.\n\n[0080] Once initiated, the call manager may exchange call data between the remote device(s) with which the local device is engaged in the call. For example, the call manager may receive one or more downlink audio signals from each of the remote devices. In one aspect, the call manager may mix the downlink signals into (at least one) downlink audio signal (e.g., via matrix mixing operations). In some aspects, the call manager may receive audio and/or video (or image) data from the sensor(s) 10, and may transmit the data to each remote device with which the local device is engaged in the call. For example, the call manager may receive a microphone signal (which may include speech of the local user) from one or more microphones 23 and/or may receive image data captured by one or more cameras 24, and may transmit the microphone signal(s) and/or image data to each remote device. In some aspects, when the local device includes two or more microphones, the call manager may transmit a sound-pickup beamformer signal that includes sound of a directional beam pattern.\n\n[0081] The joint media playback session manager 51 is configured to initiate the joint media playback session between the local device and one or more remote devices (e.g., with which the local device is engaged in the call and) in which the devices may independently stream media content for (e.g., synchronous) playback. For instance, in response to receiving instructions to initiate the session, the playback session manager may transmit a request to the media content server 5 to initiate the session, as described herein. In particular, a media application executing within the local device may transmit instructions to the session manager in response to receiving user input (e.g., based on a user selecting a play button in UI of the media application, which may be displayed on the display 25 of the local device 2). In another aspect, the session manager may request user authorization before initiating the session. For instance, once a user initiates media playback in a media application, the session manager may provide a notification (e.g., a pop-up notification displayed on the display screen 25) requesting for user authorization to initiate a joint media playback session with (at least some of) the participants of the call. When user-authorization is received (e.g., by receiving a user-selection of a UI item within the pop-up notification), the session manager may process the request to initiate the session, as described herein.\n\n[0082] In one aspect, the joint media playback session manager 51 is configured to receive media content data (e.g., once the session has been initiated). In this case, the session manager is receiving at least one audio signal (or audio channel) associated with the media content. For example, the received audio signal may be associated with a musical composition of which the local user has required playback (e.g., via the UI of the media application). In one aspect, the session manager may receive two or more audio signals of a piece of media content. For instance, when streaming a musical composition from the media content server, the session manager may receive two or more audio channels (e.g., left and right channels of a stereophonic recording of the musical composition). In another aspect, the session may receive two or more channels, such as for example the entire audio soundtrack of a movie in 5.1-surround format.\n\n[0083] In another aspect, the media content data may include audio data of at least some of the audio channels of sound (or audios) objects within a sound space, such as a sound space of an XR environment of the media content. The audio data of a sound object may include 1) an audio signal that includes sound of a (e.g., virtual) object within a XR environment and 2) spatial data that spatially represents a sound source of the sound object. In one aspect, the sound objects may correspond to objects within an XR environment. For instance, a sound object may include sound of a virtual dog (e.g., a sound of barking) and the location of the sound within a XR environment that is to be displayed on the display screen 25. In one aspect, the spatial data may be an angular/parametric representation of the sound source within the XR environment (e.g., with respect to the local device). In some aspects, the spatial data may indicate a three-dimensional (3D) position of the sound source with respect to the device (e.g., located on a virtual sphere surrounding the device) as position data (e.g., elevation, azimuth, distance, etc.). In one aspect, any method may be performed to produce the angular/parametric representation of the sound source, such as a Higher Order Ambisonics (HOA) representation of the sound source by encoding the sound source into HOA B-Format by panning and/or upmixing the at least one of ambient signals. In another aspect, the sound objects may be in any audio format (e.g., that includes the object's audio signal and spatial information).\n\n[0084] The VAD 53 is configured to receive a microphone signal from microphone 23 of the sensors 10 and/or the one or more downlink audio signals from each of the remote devices with which the local device is engaged in the call, and is configured to perform voice activity detection (or speech detection) operations to detect a presence (or absence) of a user's voice (speech) contained therein. Specifically, the controller determines whether either of the signals include speech based on output from the VAD. For instance, the VAD may determine whether (at least a portion of) spectral content of a signal is associated with human speech. In another aspect, the VAD may determine a presence of speech based on whether a signal level of (e.g., the portion of spectral content of) the signal exceeds a threshold. In some aspects, the VAD may use any method to determine whether there is a presence of speech contained within the signal. The VAD is configured to generate an output based on the received signals. In particular, the VAD may generate a VAD signal which indicates whether or not speech is contained within a microphone signal of the local device, and/or may generate a VAD signal which indicates whether or not speech is contained within a downlink audio signal. For example, a VAD signal may have a high signal level (e.g., \u201c1\u201d) when the presence of speech is detected, and may have a low signal level (e.g., \u201c0\u201d) when speech is not detected (or at least not detected within a threshold level). In another aspect, the VAD signal need not be a binary decision (speech/not-speech); it could instead be a speech presence probability. In some aspects, the VAD signal may also indicate the signal level (e.g., sound pressure level (SPL)) of the detected speech.\n\n[0085] As illustrated, the VAD 53 may generate one VAD signal that indicates whether speech is contained within either one or more microphone signals and/or one or more downlink audio signals. In one aspect, the VAD may have multiple outputs. Specifically, the VAD may generate one VAD signal that indicates the presence (or absence) of speech contained within the microphone signal and generate another VAD signal for the downlink audio signal.\n\n[0086] In some aspects, the VAD may perform additional audio signal processing operations. For example, the VAD may perform speech digital signal processing (DSP) operations upon the downlink audio signal and/or microphone signal in order to reduce (or eliminate noise) contained therein (e.g., in order to produce a speech signal that mostly contains speech). In one aspect, to process a signal, the VAD may apply a high-pass filter due to most noise (or non-speech noise) having low-frequency content. In another aspect, the VAD may improve signal-to-noise ratio (SNR) of a signal by spectrally shaping the signal by applying one or more filters (e.g., a low-pass filter, a band-pass filter, a high-pass filter, etc.). In some aspects, the VAD may perform any operation to reduce noise within a signal.\n\n[0087] The scalar gain 90 is configured to receive the one or more audio signal of the media content from the session manager 51, and is configured to process the audio signal based on the VAD signal received from the VAD 53. Specifically, the scalar gain is configured to adjust (e.g., at least a portion of) the signal level of the audio signal by applying one or more scalar gain values (e.g., as gain adjustments) upon the audio signal to produce a gain-adjusted audio signal based on whether the VAD signal indicates that there is a presence of speech detected within the downlink audio signal (and/or the microphone signal). In particular, the gain adjustment may reduce a signal level of the audio signal of the media content associated (e.g., being streamed by) the joint media playback session. Thus, the scalar gain may apply a gain adjustment to the audio signal to reduce a signal level of the audio signal in response to (e.g., the controller 20) determining that the downlink signal includes speech. In one aspect, the applied scalar gain may be a predefined value. In some aspects, the application of this scalar gain may be performed prior to other audio signal processing operations, such as the application of other scalar gain 57, as described herein.\n\n[0088] In another aspect, the applied gain may be based on the VAD signal. For example, as described herein, the VAD signal may indicate a signal level of the downlink audio signal (or more specifically, a signal level of the speech contained therein). In which case, the scalar gain may be configured to adjust the applied scalar gain value based on the signal. For example, when the speech detected in the downlink audio signal is at a determined signal level, the scalar gain may apply the gain value to reduce the signal level of the audio signal to below that of the determined signal level of the downlink signal in order to ensure that the sound of the media content is lower than the speech within the call.\n\n[0089] The volume-to-gain curve selector 54 is configured to perform a contextual analysis of call and/or media content data to determine a prioritization of audio signals of the call and/or media playback session that indicates which audio signals are to be emphasized over other audio signals when played back by the (e.g., local device of the) audio system 1. Specifically, the curve selector determines whether the local user intends (or wishes) to emphasize or hear one or more sounds of the call and/or media content over other sounds during the call and playback session, and in response prioritizes those sounds that are to be emphasized over others. For instance, the curve selector determines whether to prioritize sounds of the call (e.g., the downlink audio signal) and/or one or more sounds of (e.g., one or more audio signal(s) of) the media content that is being played back during the media playback session based on one or more criteria. As described herein, upon determining which audio signals are to be prioritized over other audio signals (and therefore emphasized), the controller may apply different scalar gains (e.g., and/or vector gains) in response to receiving a user-adjustment at the volume control 12 (and/or the volume control 82 of the audio output device). As a result, when these gain-adjusted signals are used to drive the speaker 22, sounds of prioritized audio signals may have a higher volume level (e.g., a greater output sound level) than volume levels of audio signals that are less prioritized. More about applying scalar gains is described herein.\n\n[0090] In one aspect, a determination of audio signal priority may be based on whether the local user is speaking (e.g., to one or more remote users of the remote devices with which the local device is engaged in the call). Specifically, the curve selector determines whether the microphone signal produced by microphone 23 includes speech of the local user based on output from the VAD 53. For instance, the curve selector receives the VAD signal produced by the VAD 53 and determines whether the VAD signal indicates that the microphone signal includes speech (e.g., whether the signal has a high signal level). In response to determining that the output of the VAD indicates a detection of speech, the curve selector may prioritize the downlink audio signal over the audio signals of the media content.\n\n[0091] In one aspect, prioritization based on whether speech of the local user is detected for a period of time. In particular, the curve selector may prioritize the downlink audio signal of the call once the local user has spoken for the period of time. As an example, while engaged in the call, the devices may playback media such as a movie during the playback session. Occasionally, the local user may speak to one or more remote users. In which case, when the local user (and a remote user) are engaged in a (e.g., long) conversation, the VAD signal may detect speech for the period of time. In response, the curve selector may prioritize the downlink signal, since the local user may wish to hear the remote user over the media content. In other cases, however, the local user may wish to just make a comment or a short statement, without engaging in a conversation. In which case, the VAD signal may detect speech for less than the period of time. As a result, the curve selector may not prioritize the downlink audio signal. In one aspect, instead of prioritizing the downlink audio signal, the curve selector may prioritize the media content over the downlink signal, since the local user does not intend to talk at length to a remote user. In another aspect, the curve selector may not prioritize either of the signals. As described herein, when both signals are not prioritized (or may be prioritized the same), the controller may apply similar scalar gains upon the volume control receiving a user-adjustment. More about applying scalar gains is described herein.\n\n[0092] In another aspect, the curve selector may prioritize signals based on sensor data received from one or more of the sensors 10. For instance, the selector may prioritize the downlink audio signal(s) based on speech of the local (and/or remote) users. Specifically, the curve selector may perform speech recognition (e.g., through the use of a speech recognition algorithm) to analyze an audio signal (e.g., a microphone signal and/or the downlink audio signal) to find (or recognize) speech therein. In particular, the controller may analyze audio data of the signals according to the algorithm to identify a word or phrase contained therein. The curve selector may determine the prioritization based on the identified word or phrase. For instance, the selector may use the identified word or phrase to perform a table lookup into a data structure that associates prioritization values for the downlink signal with one or more words or phrases. Upon determining that the prioritization value is above a threshold, the curve selector may prioritize the downlink signal.\n\n[0093] In another aspect, the curve selector may prioritize the downlink audio signal(s) based on audio data contained therein. For example, similar to the prioritization based on the VAD signal of the microphone signal described above, the curve selector may prioritize the downlink signals based on whether a VAD signal detects speech (e.g., for the period of time) within the downlink audio signal. In one aspect, the curve selector may prioritize two or more downlink audio signals from two or more remote devices differently. For instance, upon detecting speech of one downlink signal, the curve selector may prioritize that downlink signal over other downlink signals and/or the audio signal of the media content. Thus, when one remote user is speaking, the speech from that user may be prioritized over the other remote users, who at the time may or may not be speaking.\n\n[0094] In some aspects, the curve selector may prioritize one or more downlink signals and/or one or more audio signals of the media content based on gestures performed by the local user. The curve selector may receive the sensor data that indicates whether the user is performing a gesture that is associated with the local user wishing to emphasize sound of one or more signals over others that are being played back. Specifically, the curve selector may use the sensor data to determine whether the user is gesturing towards an object that is displayed on the display screen 25 of the local device in order to emphasize sound associated with that object. As described herein, the local device may display a video representation of a remote user on the display screen of the local device. The curve selector may determine whether the local device wishes to emphasize sound of one or more remote users. For example, the curve selector may determine whether the user intends to emphasize sound of a remote user by determining whether the local user is looking at (or focusing on) the remote user's video representation. In particular, the curve selector may receive image data captured by camera 24, where the camera's field of view includes at least a portion of the local user (e.g., the local user's face). Using the image data, the curve selector may determine whether a gaze of at least one eye of the user is focused on the video representation of the remote user. If so, the curve selector may prioritize the downlink audio signal associated with that remote user's device.\n\n[0095] In another aspect, the curve selector may determine whether the user intends to emphasize sound of a remote user based on motion data (e.g., produced by the IMU 11). Specifically, the curve selector may determine, based on the motion data, whether the local user is moving at least a portion of the local device that indicates that the local user is gesturing towards the video representation of the remote user. For example, when the local device is an electronic device in which the display screen is integrated, the curve selector may determine whether the local user is tilting the display screen in a direction towards the video representation. For example, the selector determines whether the screen is tiling about a center axis that runs through the display screen in a direction towards the video representation displayed on the display screen with respect to the center axis. For example, referring to FIG. 3, the display screen 25 of the local device 2 is facing the local user 30, while conducting the call. The curve selector may determine that the local user wishes to prioritize speech of the remote user 31 in response to detecting motion data that indicates that the display screen 25 (or the local device 2) is tilting away from the local user about a lateral axis (e.g., an X-axis) that runs laterally through a center point of the display screen. In another aspect, the gesture may be a user-selection of the video representation. For instance, the user may perform a touch-selection on the display screen (which may be touch-sensitive) at a location that the video representation is displayed.\n\n[0096] In some aspects, the curve selector may determine whether the user intends to emphasize one or more sounds associated with the media content based on the sensor data. As described herein, the media content may include several audio signals, each associated with a sound source. For example, when the media content is a XR environment, the local device may display visual content of the XR environment on the display screen 25, and may drive the speaker 22 with one or more audio signals that include sounds of the XR environment. Specifically, the audio signals may each be associated with one or more objects that are displayed within the XR environment. In some aspects, the controller may spatial render (e.g., by applying HRTFs) these audio signals to provide the user with a 3D sound experience in which the sound sources are perceived at different locations within a sound space. In one aspect, the curve selector may determine whether the user wishes to emphasize a sound within the XR environment based on a user gesture towards a displayed object that is associated with the sound. In particular, the curve selector may perform this determination in a similar fashion as described with respect to the remote users. For example, the curve selector may determine that a gaze of the user is focused on the object and/or determine, based on motion data, that the user is tilting the displays screen in a direction towards the object. Based on this determination, the curve selector may prioritize this sound, over one or more other sounds within the XR environment. Although the prioritization is based on objects within an XR environment, this determination may be performed upon any type of media content that includes image data and one or more audio signals, such as a movie or a video game.\n\n[0097] In yet another aspect, the curve selector may determine which signals to prioritize based on particular operational functions that are being performed by the controller. As described herein, the controller may perform an ANC function to cause the speaker 22 (and/or speaker 83) to produce anti-noise. In addition to (or in lieu of) this function, the controller may perform a transparency function in which ambient sound is played back by the device. In one aspect, the prioritization of sounds may be determined based on which function is being performed by the controller. For example, if performing the ANC function in order to block out ambient sounds, the controller may prioritize sounds of the media content. In contrast, if the controller is performing the transparency function, the controller may prioritize the downlink audio signal since the local user may have activated the transparency function in order to hear the user's own voice while talking.\n\n[0098] As described thus far, the curve selector may prioritize sounds associated with the playback session and/or call based on one or more criteria. In another aspect, the curve selector may prioritize other sounds (or groups of sounds) that are being played back by the local device. For example, the curve selector may prioritize sounds produced by one or more software applications that are being executed by the (e.g., controller 20 of the) local device based on one or more criteria. The software applications may include a telephony application that is performing the call between the local device and the one or more remote devices, one or more media applications that are executing the media playback session, and/or other applications that may executing within the local device (e.g., a messaging application, an alarm application, etc.). In one aspect, the prioritization of audio signals associated with applications (e.g., that include sounds of the applications) may be based on an order at which the local device begins to execute the applications. For example, referring to FIG. 3, the curve selector may prioritize the video call application over the media application that is playing back the media content, since the local and remote devise are engaged in the video call before the local device initiates the media application and engages in the playback session. In one aspect, applications that are subsequently executed may be given a lower priority to applications that are already being executed by the local device.\n\n[0099] In one aspect, the curve selector may prioritize several sounds over a sliding scale based on the criteria mentioned herein. For example, the curve selector may prioritize some sounds as \u201chigh-priority\u201d, some sounds as \u201cmedium-priority\u201d, and other sounds as \u201clow-priority\u201d. In some aspects, the selector may numerically prioritize audio signals, where a high-priority audio signal has a high value (e.g., \u201c10\u201d), while lower priority audio signals have a lesser value (e.g., \u201c1\u201d). In other aspects, some audio signals may have the same priority. For instance, when engaged in a call with multiple (e.g., three or more) remote users, the curve selector may prioritize downlink audio signals of two remote devices the same (e.g., high) in response to determining that speech is detected in both signals.\n\n[0100] In one aspect, the curve selector 54 is configured to determine (or select) a volume-to-gain curve for at least one audio signal that is being (e.g., mixed and) played back by the local device during the call and/or playback session. Specifically, once the selector prioritizes the audio signals (e.g., determining which sounds the local user wants emphasized over other sounds), the selector may determine a curve for each audio signal, which indicates an amount of gain (or attenuation) to be applied to the signal based on a user-adjustment of the volume control.\n\n[0101] In one aspect, a volume-to-gain curve may associate each of several volume settings of the volume control to a (e.g., different) scalar gain value that may be applied to its respective audio signal (e.g., in response to the volume control receiving a user-adjustment). In other words, the curve is a function of gain with respect to volume settings. For example, the volume-to-gain curve may associate scalar gain values with volume settings of the volume control that indicate a position and/or orientation of the volume control, as described herein. In another aspect, the volume-to-gain curve may associate one or more vector gains with the volume settings, which may allow the control to select one or more frequency bands at which one or more selected gains are to be applied to an audio signal. In one aspect, the volume settings may be numerical values (e.g., 1-10), where each value is associated with an adjustment of the volume control, and each numerical value may be associated with a scalar gain value. In another aspect, the volume-to-gain curve may indicate a signal level (e.g., in dB) of an audio signal at a particular volume setting. Specifically, the curve may indicate a desired signal level (or gain) of the audio signal at each volume setting. This is illustrated in FIG. 5. From the desired signal level, the controller may determine a gain adjustment to be applied to each particular audio signal.\n\n[0102] In one aspect, the curve selector may select a curve for an audio signal based on the signal's priority. Specifically, a curve selected for a high-priority audio signal may have a rate of change that is lower than a curve selected for another audio signal of lower priority. For example, upon determining that the local user is speaking (e.g., based on the VAD signal), the curve selector may prioritize the downlink audio signal and select a first curve for the downlink signal and a second curve for the audio signal of the media content, where the first curve has a lower rate of change than the second curve. As described herein, upon receiving a user-adjustment of the volume control, such as turning down the overall volume level (thereby reducing the current volume setting by one, for example), the controller may use the first curve to determine a first gain adjustment based on a (first) gain of the first curve that is associated with the reduced volume setting, and may use the second curve to determine a second gain adjustment based on a (second) gain of the second curve that is associated with the reduced volume setting, where the first gain is lower than the second gain. Once determined, the controller may reduce (or attenuate) a signal level of the downlink signal according to the first gain from the first curve, and may reduce a signal level of the audio signal according to the second gain from the second curve. As a result, the signal level of the audio signal may be reduced more than the signal level of the downlink audio signal, which when both mixed and played back by the local device results in the sound of the downlink audio signal having a higher volume level than a volume level of the audio signal. This change in volume allows the downlink signal to be emphasized more over the audio signal (since the downlink signal will have a higher sound output level), thereby allowing the local user to engage in a conversation with the remote user without being distracted by the sound of the media content. More about applying scalar gain values is described herein.\n\n[0103] As described herein, the volume-to-gain curves may be functions of gains (e.g., scalar gain values) with respect to volume settings of the volume control 12. In one aspect, a volume-to-gain curve may be a linear function of gain with respect to volume settings, where different curves may have different slopes based on their associated audio signal's priority, as described herein. An example of a linear volume-to-gain curve is illustrated and described in FIG. 5, where a curve 61 for a downlink signal has a lesser slope than a curve 62 for an audio signal of the media content. More about these curves is described herein. In another aspect, the curve may be a non-linear function. In another aspect, the curves may be any type of function of gain with respect to volume settings.\n\n[0104] In one aspect, the curves may be stored in memory (e.g., of the controller 20) of the local device. Specifically, the curves may be stored in data structures (e.g., lookup tables), where each curve is stored in a particular lookup table. In some aspects, the curves may be predefined curves that are determined in a controlled environment (e.g., in a laboratory). In another aspect, at least some curves may be learned through machine learning operations or through user input. For example, the curves may be user-defined (e.g., by the local user of the local device). In some aspects, the curves may be stored in memo\n\n[0105] As described thus far, the curve selector 54 may select a volume-to-gain curve for at least some audio signals based on the signal's determined priority. In one aspect, the selector may select multiple (two or more) curves for at least one audio signal. As in the previous example, the curve selector may select a first curve with a lower rate of change for the downlink audio signal and select a second curve with a higher rate of change for the audio signal, when the downlink signal has a higher priority. In one aspect, these selections may correspond to a received user-adjustment from the volume control to reduce the overall volume level of the local device, since as the user turns down the volume, the output sound level of the downlink signal will be greater than the output sound level of the audio signal in order to emphasize the downlink signal. In some aspects, the curve selector may select different curves for audio signals that correspond to a user-adjustment of the volume control to increase the overall volume level. For example, the curve selector may select a third curve for the downlink signal and a fourth curve for the audio signal, where the third curve has a higher rate of change than the fourth curve. This will result in the output sound level of the downlink signal increasing more than the level of the audio signal, as the volume level of the local device is turned up.\n\n[0106] In another aspect, rather than selecting different curves, the selected curves may be used interchangeably between the signals. For example, the controller may use the first curve to determine the first gain for the downlink audio signal and use the second curve to determine the second gain for the audio signal, in response to the user-adjustment of the volume control reducing the overall volume level, as described herein. Alternatively, the controller may use the second curve to determine a gain for the downlink audio signal and the first curve to determine a gain for the audio signal, in response to the user-adjustment of the volume control increasing the overall volume level. More about using the curves to determine gains is described herein.\n\n[0107] In some aspects, the selection of curves may be based on one or more audio signal processing operations that have been (or going to be) performed upon one or more of the signals. For example, the curve selector 54 may adjust the selection of the volume-to-gain curve of the audio signal based on whether a scalar gain value has been applied to the audios signal in order to reduce the signal's level in response to the VAD 53 detecting speech (e.g., within the downlink audio signal and/or the microphone signal), as described herein. Specifically, the selected curve for the audio signal may have a lower rate of change than a selected curve for the audio signal when the scalar gain 90 does not apply a gain value (e.g., in response to the VAD signal having a low signal level). More about the curve selector selecting different curves based on audio processing operations is described herein.\n\n[0108] In one aspect, rather than (or in addition to) selecting volume-to-gain curves, the selector may determine different rates of change for gain values for audio signals based on prioritization. In particular, the selector may select a low rate of change for an audio signal with a high priority, and may select one or more high rates of change for audio signals of lower priority. For example, normally, the controller may adjust signal levels in a similar fashion when the volume level is increased or decreased (e.g., applying a gain when the volume is increased, such as 6 dB, and applying a similar reduction in gain or attenuation when the volume is decreased, such as \u22126 dB). When selecting different rates of change, however, the attenuation may change according to the rate. For example, if the high rate of change is twice that as the normal rate of change, signal levels of audio signals of lower priority may be attenuated by twice that of the particular attenuation (e.g., \u221212 dB).\n\n[0109] In some aspects, these selections may correspond to user-adjustments of the volume control that reduce the overall volume. As described herein, the selector may select different (e.g., inverse proportional) rates of change for audio signals for user-adjustments that increase the overall volume. As described herein, these rates of change may be used by the gain selectors for using these rates of change for determining new gains based on volume control user-adjustments.\n\n[0110] The audio signal gain selector 55 is configured to receive one or more volume-to-gain curves that were selected for one or more audio signals of the media content from the curve selector 54, and the downlink signal gain selector 56 is configured to receive one or more volume-to-gain curves that were selected for one or more downlink audio signals of the call from the curve selector. In one aspect, each of the gain selectors is configured to use their respective received gain curves to determine one or more scalar gain values based on user-adjustments of the volume control 12.\n\n[0111] In one aspect, each of the gain selectors is configured to receive a control signal generated by the volume control 12, which may be in response to the control receiving a user-adjustment. Specifically, the control signal may indicate a (current or) adjusted volume setting of the volume control. For instance, when the volume control is a (e.g., UI) slider at a starting (or muted) position, and the user-adjustment moves the slider to a half-way point along a slidable range, the control signal may indicate the volume setting at 50%. In another aspect, the volume setting may be a numerical value associated with the user-adjustment, as described herein. For instance, when the control signal is a rotatable knob that is currently at an orientation of 300\u00b0, the volume setting may be at 15 of 18 settings, which may correspond to an overall volume level of approximately 80%. Upon receiving a user adjustment that rotates the knob down to 280\u00b0, the control signal may indicate that the volume setting has went down by one, to 14.\n\n[0112] In one aspect, each of the gain selectors may select a gain-adjustment (e.g., which may be a gain to increase at least a portion of a signal level or an attenuation to reduce at least a portion of the signal level) as scalar gain values from their respective received volume-to-gain curves that are associated with the (adjusted) volume setting of the volume control. For example, when the curve is a linear function of gain with respect to volume settings, the gain selectors may select the gain along the curve that maps to the received volume setting. In another aspect, the gain selectors may select a difference in gain between the original volume setting and the new volume setting. For example, when reducing the overall volume level, the volume setting may be reduced by one, where the previous volume setting and the reduced volume setting both correspond to a different scalar gain value. As a result, when selecting the scalar gain value, the gain selectors may select the difference between the gain value associated with the previous volume setting and the gain value associated with the new (or reduced) volume setting. In some aspects, the gain selectors may indicate whether the selected scalar gain value is to be applied to increase the gain of a signal level or to be applied as an attenuation to reduce the signal level.\n\n[0113] In another aspect, the gain selectors may select the gain-adjustment based on a gain of the volume-to-gain curve that is associated with the volume setting. In another aspect, the selection of the scalar gain may be based on a desired signal level of the curve that is associated with the volume setting. As described herein, the volume-to-gain curve may indicate desired signal levels. In which case, upon determining a desired signal level (e.g., \u221230 dBFS) that is associated with the volume setting, the gain selectors may select a scalar gain that increases (or decreases) the signal level of the audio signal to that desired level. In this case, when the desired signal level is \u221230 dBFS and the audio signal is at \u221220 dBFS, the gain selector may apply \u221210 dB to reduce the signal level.\n\n[0114] In another aspect, the gain selectors may determine the scalar gain value based on the user-adjustment of the volume control in other ways. For example, as described herein, rather than (or in addition to) the curve selector determining curves, the selector may determine a rate of change for gain of one or more signals. In one aspect, the gain selectors may utilize the rate-of-change to determine a new gain value based on the adjusted volume control. For instance, when the volume control is adjusted to reduce the overall volume by reducing the volume setting by one, the gain selector may use the rate-of-change to adjust a current gain according to the changed volume setting. In another aspect, when the curve is in a lookup table, the gain selector may perform a table lookup into a data structure that stores the lookup table using the adjusted volume setting. Specifically, the gain selector 55 may perform a table lookup into a data structure that associates, for different user adjustments (e.g., volume settings), one or more gains for the audio signal of the streamed media content to select a gain associated with the (current) volume setting, and the gain selector 56 may perform a (e.g., similar) table lookup into the data structure to select another gain for the downlink audio signal, where both gains may be different (or the same). In some aspects, the gain selector may determine the appropriate scalar gain value for the signals of the call and/or playback session using any method.\n\n[0115] The scalar gain 57 and 58 receive the audio signal of the media content and the downlink audio signal associated with the call, respectively, and are configured to process their respective signals based on the selected scalar gain values. Specifically, each of the scalar gains is configured to apply a gain-adjustment to their respective signals to produce an adjusted signal, which may have a higher and/or lower signal level based on the adjustment. For example, the scalar gain 57 may apply a gain-adjustment according to a selected scalar gain value by the gain selector 55 to attenuate a signal level of the audio signal, and the scalar gain 58 may apply a gain-adjustment according to a selected scalar gain value by the gain selector 56 to attenuate a signal level of the downlink signal, differently than the gain-adjustment by the scalar gain 57 (e.g., in response to the user turning down the volume at the volume control). In some aspects, each of the scalar gains 90, 57, and/or 58 may perform similar operations to reduce or increase signal levels of associated signals.\n\n[0116] The mixer 59 is configured to receive the processed (e.g., gain-adjusted) signals from the scalar gains 55 and 58, and is configured to perform matrix mixing operations, for example, in order to produce a mix of the two signals. The controller may use the mixed signal to drive the speaker 22 to playback sound of the call, as well as the media content of the playback session, where sound of the signals have output audio levels that are at or below a current overall volume level of the local device. In another aspect, the mixer may receive one or more unprocessed signals (e.g., signals that are not gain-adjusted). For example, the mixer may receive the downlink audio signal from the call manager 52, rather than receiving the processed downlink audio signal from the scalar gain 58.\n\n[0117] In one aspect, the controller may optionally perform additional DSP operations. For example, the controller may perform spatially rendering operations upon one or more of the signals (and/or the mix), by applying spatial filters, such as head-related transfer functions (HRTFs to produce binaural audio signals for driving one or more speakers (e.g., a left speaker and a right speaker), as described herein. As another example, when the local device (and/or audio output device) includes one or more loudspeakers, the controller may render a HOA representation of the audio signal(s) and downlink signal to produce one or more loudspeaker driver signals (e.g., based on a predefined loudspeaker configuration). The controller 20 may then use the processed mix to drive the speaker 22, as described herein.\n\n[0118] As shown herein, the controller includes an audio signal gain selector 55 and scalar gain 57 to process an audio signal of the media content, and a downlink signal gain selector 56 and scalar gain 58 to process the downlink audio signal. In one aspect, the controller may include more or less gain selectors and/or scalar gains. For example, when the media content includes two or more audio signals, the controller may process each of those signals with a respective gain selector and scalar gain in order to adjust signal levels of each of the signals based on user-adjustments of the volume control, as described herein.\n\n[0119] As described thus far, the controller may prioritize signals of the call and/or playback session in order to determine gain-adjustments based on user-adjustments of the volume control. In one aspect, the volume-to-gain curve selector may not prioritize one or more of the signals. In which case, the controller may apply the same (or similar) gain-adjustments to signals that are not prioritized. In another aspect, signals that are prioritized the same, may be gain-adjusted similarly as well.\n\n[0120] In one aspect, the order of operations described herein may differ. For instance, the volume-to-gain curve selector 54 may be configured to determine curves (e.g., prioritizing signals and based on the prioritization determining curves), in response to the volume control receiving a user-adjustment. In which case,\n\n[0121] FIG. 5 shows examples of volume-to-gain curves according to one aspect. Specifically, this figure shows a volume-to-gain curve 61 for a downlink signal and a volume-to-gain curve 62 for an audio signal of media content. As shown, each of the curves is a linear function of gain (or gain-adjustment) with respect to volume settings, where each curve is a desired signal level for each respective signal. As shown, the curves are graphed in a graph 60, where the Y-axis represents gains (or gain adjustments) with respect to the dynamic range of the audio system 1 in the digital domain (e.g., decibels relative to full scale (dBFS)). Thus, the top of the Y-axis represents a maximum signal level, where each \u221210 dB step below 0 dB represents the gain (or attenuation) that is to be applied to the signal to attenuate the signal below a maximum level. In one aspect, the dynamic range of the audio system may change based on the bit depth of the digital audio data. The X-axis of the graph includes volume settings of the volume control, where the zeroth setting represents the lowest overall volume output level of the local device (e.g., mute), and the tenth setting represents the highest overall volume output level.\n\n[0122] As shown, both of the curves 61 and 62 have different slopes. For instance, the audio signal curve 62 has a slope of 1 that originates from \u2212100 dB at the zeroth volume setting and goes to 0 dB (maximum allowable signal level) at the tenth volume setting. Conversely, the curve 61 for the downlink signal has a lower slope of \u2156, where it starts at \u221240 dB at the zeroth volume setting and extends to 0 dB at the tenth setting. In one aspect, the differences in slopes may correspond to the determined priority of the signals, as described herein. For example, the downlink curve 61 may have a lower slope than the audio signal curve 62, due to having a higher priority than the audio signal. As a result, when the overall volume level is reduced, the signal level of the downlink signal is reduced less than the signal level of the audio signal.\n\n[0123] The graph 60 also illustrates the change in gain in response to a reduction of the volume setting by one. For example, the volume control may have a current volume setting of 7. In which case, the audio signal is at \u221230 dB, and the downlink signal is at \u221212 dB. In some aspects, the controller may apply one or more scalar gain values to the signals in order for both signals to have those desired levels. In response to the volume control receiving a user-adjustment, the volume setting of the control may be reduced by one, to six. As a result, the controller may determine gain adjustments to the signals using the curve(s) 61 and/or 62 according to the new volume setting. In this case, the controller may reduce the signal level of the downlink signal by 10 dB, since the gain has dropped from \u221230 dB to \u221240 dB, and may reduce the signal level of the downlink signal by 4 dB, since the gain has dropped from approximately \u221212 dB to \u221216 dB.\n\n[0124] Also shown, both curves 61 and 62 intersect at the maximum volume setting of ten. In which case, applied gains to both signals increases their respective signal levels to their respective maximum signal levels. In one aspect, at maximum signal levels, the signal level of the audio signal of the media content may be higher than the signal level of the downlink signal, since the audio signal may have originally been mastered higher than that of the downlink signal, as described herein.\n\n[0125] In one aspect, the curves may have different slopes based on whether the volume control receives a user-adjustment to increase or decrease the overall volume level, as described herein. In another aspect, either of the curves may be different types of functions. For instance, the downlink curve 61 may be a linear function of gain with respect to volume settings, whereas the audio signal curve 62 may be a non-linear function of gain with respect to volume settings.\n\n[0126] FIG. 6 is a flowchart of one aspect of a process 70 for using the (e.g., master) volume control 12 to adjust the overall volume level of the audio system 1. In one aspect, the process may be performed by (e.g., the controller 20 of) the local device 2 of the audio system 1. Specifically, at least some of the operations described herein may be performed by at least some of the operational blocks described in FIG. 4.\n\n[0127] The process 70 begins by the controller 20 initiating a call (e.g., a telephony call or a video call) between the local device 2 and one or more remote devices 3 (at block 71). As described herein, the call may be initiated by the call manager 52 in response to receiving a request by the local user. In one aspect, the initiation of the call may be in response to receiving an incoming call from one or more remote devices. In which case, the call may be initiated by the call manager in response to the user accepting the call (e.g., via a user selection of a UI item of a telephony application for picking up a call that is displayed on display screen 25 when an incoming call signal is received from a remote device).\n\n[0128] During the call, the controller 20 initiates, as the local device 2, a joint media playback session in which the local device and one or more remote devices independently stream media content for synchronous playback (at block 72). For example, the joint media playback session manager 47 may initiate the playback based on user input. In one aspect, the playback session may be between all of the devices that are conducting the call. In another aspect, the playback session may be initiated between the local device and at least some of the remote devices. In which case, when initiated the local user may define which remote devices are to participate. In some aspects, initiating the joint media playback session may be in response to the controller 20 receiving an initiation request from one or more of the remote devices and/or the media content server 5.\n\n[0129] The controller receives at least one downlink (audio) signal associated with the call and at least one audio signal associated with the media content (at block 73). For example, the local device receives a downlink audio signal from at least some of the remote devices with which the local device is engaged in the call. In addition, the local device may receive audio data and/or image (or video) data associated with the media content of the playback session. For example, the media content may include only audio data (as one or more audio signals), such as being a musical composition that is to be played back simultaneously by the local and remote device(s). As another example, the media content may include audio and image (or video) data, such as being a movie. As yet another example, the media content may include other types of content, such as (e.g., user-interactive content like) a XR presentation (e.g., a virtual reality environment) or a video game. In which case, initiating the joint media playback session may include independently streaming image data of the XR presentation for display on display screen of the local device.\n\n[0130] The controller 20 drives a speaker (e.g., speaker 22) with a mix of the downlink signal of the call and the audio signal of the media content at an overall volume level (at block 74). In one aspect, the controller may drive the speaker with the mix at a particular overall volume level. For instance, the speaker may be driven while the volume control is at a particular volume setting (e.g., at a volume setting of 7 out of 10). In some aspects, the controller may drive the speaker with the mix of signals prior to applying gain adjustments that are based on a determination of priority of signals, as described herein. As a result, the audio signal may have a greater signal level than a signal level of the downlink signal, as described herein.\n\n[0131] In one aspect, the controller may spatially render the signals by applying one or more spatial filters according to spatial characteristics (e.g., elevation, azimuth, distance, etc.), such that which when outputted through one or more speaker drivers a 3D sound is produced (e.g., giving the user the perception that sounds are being emitted from a particular location within an acoustic space). In another aspect, the controller may transmit the (e.g., mix of the) signals to the audio output device 6 in order to drive one or more speakers (e.g., speaker 83) of the output device.\n\n[0132] The controller receives a user-adjustment of a volume control (e.g., control 12) to adjust the overall volume level (at block 75). For example, the user could turn a control (e.g., digital crown or knob) on a device or make a certain gesture to reduce the volume. In response, the volume control may transmit a control signal to the controller 20 indicating that the volume setting of the volume control has been reduced.\n\n[0133] The controller 20 determines a first gain adjustment for the downlink signal and a second gain adjustment for the audio signal based on the user-adjustment of the volume control (at block 76). Specifically, the controller may determine scalar gain values to be applied to one or both of the signals, as described herein. In one aspect, the first gain adjustment may be different than the second gain adjustment. For example, the volume-to-gain curve selector determines a priority between the downlink signal and the audio signal based on one or more criteria. As an example, the curve selector determines whether the VAD 53 detects speech (e.g., for a period of time) within the downlink audio signal. In response, the curve selector prioritizes the downlink audio signal higher than the audio signal, and based on this prioritization selects (e.g., different) volume-to-gain curves for each signal. In another aspect, the gain adjustments may be determined based on the streamed media content of the joint media playback session. For example, when the audio signal is associated with an object that is displayed on the display screen 25 of the local device, the controller may determine whether the local user is focusing (e.g., looking at) the object on the display screen. If so, the curve selector may determine that the local user wishes to prioritize that sound over the sound of the downlink audio signal.\n\n[0134] Using the selected curves, the controller determines first and second gain adjustments for the signals that are associated with the adjusted (or changed) volume setting of the volume control. For example, referring to FIG. 5, when the volume setting is changed from 7 to 6, the controller may determine the first gain adjustment as a first attenuation of \u22124 dB and the second gain adjustment as a second attenuation of \u221210 dB. Thus, in this case, the second gain adjustment may be greater than the first gain adjustment, such that the signal level of the audio signal is reduced more than the signal level of the downlink signal, when applied.\n\n[0135] Thus, based on the user-adjustment of the volume control, the controller applies 1) the first gain adjustment to the downlink signal of the call and 2) the second gain adjustment to the audio signal associated with the media content (at block 77). In which case, the gain-adjusted audio signal may be reduced more than the gain-adjusted downlink signal. As a result, the signal level of the gain-adjusted downlink signal may be greater than the signal level of the gain-adjusted audio signal, such that when both signals are used to drive the speaker sound of the downlink signal is emphasized more than sound of the audio signal. In another aspect, the gain adjustments may cause the signal levels of both gain-adjusted signals to be lower than the highest signal level of the signals prior to the adjustment, since the volume adjustment is reducing the overall volume level. As a result, in this example, the signal levels of both signals may be lower than the original signal level of the audio signal.\n\n[0136] The controller then drives the speaker 22 with a mix of the (gain-adjusted) downlink signal and the (gain-adjusted) audio signal at the adjusted overall volume level (at block 78). In one aspect, the controller may perform spatial rendering of the signals by applying one or more spatial filters, as described herein. The spatial rendering of the signals may produce one or more driver signals, which the controller may use to drive one or more speakers of the local device (and/or the audio output device 6).\n\n[0137] Some aspects may perform variations to the process 70. For example, the specific operations of at least some of the processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different aspects. For example, at least some of the operations may be omitted. For example, the operations described at block 74 may be omitted, since controller may not drive the speaker prior to receiving the user-adjustment of the volume control. In which case, the controller may begin to drive the speaker after receiving user input.\n\n[0138] As described in this process, the controller may determine different gain adjustments for the downlink signal and the audio signal based on a reduction of the overall volume level. In some aspects, the controller may perform similar operations upon receiving an increase in the overall volume level. For example, the controller may receive a second user-adjustment of the volume control to increase the overall volume level, which was previously reduced (e.g., from a volume setting of 7 to 6). For instance, the volume control may receive user input to increase the volume setting back to 7. In response, the controller may apply gains to the downlink audio signal and the audio signal that are proportional to the attentions that were previously applied to the signals in order to return their respective signal levels back to levels from prior to the original gain reduction.\n\n[0139] In another aspect, the controller may perform at least some of the operations of process 70 in order to determine additional gain adjustments based on the increase of the overall volume level. In some aspects, the controller may determine different volume-to-gain curves when the overall volume level is increased, as opposed to when the overall volume level is decreased. As a result, the controller may determine a different gain adjustment for at least one signal (e.g., when the volume is increased back to the previous level). For instance, upon the user-adjustment increasing the volume level, the controller may apply a gain adjustment to the audio signal that increases the signal level of the audio signal more than the previous attenuation reduced a signal level of the audio signal when applied in response to the first user-adjustment.\n\n[0140] In one aspect, the process 70 determines gain adjustments for two signals, a downlink audio signal of the call and an audio signal of the media content. In some aspects, at least some operations may be performed for determining (and applying) gain adjustments for two or more signals. As described herein, the media content may include a XR presentation with multiple sounds, where each sound is contained within an audio signal. For instance, the controller may receive two audios signal, where a first audio signal contains ambient sounds of the XR presentation and a second audio signal is associated with (e.g., includes sounds of) an object within the XR presentation. In some aspects, the controller may determine gain adjustments based on the object within the XR presentation. For example, the controller may determine that the user wants sound of the object displayed within the display screen to be emphasized over other sounds (e.g., using sensor data). In response to receiving a user-adjustment of the volume control to reduce the overall volume level, the controller may determine three gain adjustments, a first gain adjustment for the downlink audio signal, a second gain adjustment for the ambient sound, and a third gain adjustment for the second audio signal of the object. In one aspect, the first and second gain adjustments may attenuate their respective signals more than the third gain adjustment attenuates the second audio signal, such that when used to drive the speaker, the local user can (primarily) hear the sound of the object.\n\n[0141] As described thus far, the controller may be configured to determine different gain adjustments for the downlink signal and the audio signal of the media content. In some aspects, the controller may determine the same gain adjustments for two or more signals that are to be used to drive the speaker. Specifically, upon determining that the local user does not want any sound emphasized other another, the controller may apply similar (or same) gain adjustments. For example, as described herein, the controller may determine different gain adjustments in response to the VAD detecting speech contained within the downlink signal. In response to determining that the microphone signal does not include speech (e.g., output of the VAD is in a low signal state), however, the controller may apply the same gain adjustments to the downlink signal and the audio signal of the media content. In some aspects, the same gain adjustments may be less (or reduce the signal levels of the signals less) than the gain adjustment that is applied to the audio signal when the controller determines that the downlink signal is to be emphasized. In some aspects, Once, however, speech is detected (or the controller determines that the user wishes the downlink signal to be emphasized), the controller may then determine and apply differing gain adjustments.\n\n[0142] As described in process 70, the operations may be performed to determine a gain adjustment for the downlink signal associated with a remote device and an audio signal of streamed media content. In one aspect, the operations may be performed to determine multiple (e.g., two or more) gain adjustments for multiple downlink audio signals received from two or more remote devices. In some aspects, the controller may apply the same gain adjustments to the downlink signals, upon determining that one or more downlink signals have high priority. In another aspect, the controller may apply different gain adjustments to one or more downlink signals.\n\n[0143] As previously described, the controller may prioritize sounds of some software applications based on an order at which they have been executed (or are currently being executed) by the local device, and therefore determine different gains to be applied to different sounds. For example, the audio signal may be associated with the media content from a media application that is currently being executed by the local device. Subsequently, the local user may execute another separate application (e.g., a messaging application). Upon receiving a user-adjustment of the volume control, the controller may attenuate sounds of the separate application more than an attenuation that is applied to sounds of the media application (e.g., by applying a higher gain value to an audio signal of the separate device and applying a gain value lower than the higher gain value to an audio signal of the media application). In another aspect, the controller may attenuate sounds of applications that have been executing longer (e.g., over a period of time), then sounds of applications that have been executing for lesser time (e.g., within the period of time).\n\n[0144] In another aspect, the master volume control is a physical control that is a part of the first electronic device. In some aspects, the master volume control is a user interface (UI) item that is displayed on a display screen of the first electronic device. In one aspect, the single volume control is an input including a gesture made by a user of the first electronic device.\n\n[0145] In one aspect, the single volume control includes several volume settings, each volume setting defining a different overall volume level of the first electronic device, the user-adjustment at the single volume control changes a current volume setting of the signal volume control to a new volume setting that is associated with the reduced overall volume level. In some aspects, the downlink signal is associated with a first volume-to-gain curve that associates the plurality of volume settings to a first plurality of gains and the audio signal of the media content is associated with a second volume-to-gain curve that associates the plurality of volume settings to a second plurality of gains, the method further includes, in response to receiving the user-adjustment, using the first volume-to-gain curve to determine the first gain adjustment based on a first gain that is associated with the new volume setting, and using the second volume-to-gain curve to determine the second gain adjustment based on a second gain that is associated with the new volume setting. In some aspects, the first and second volume-to-gain curves are linear functions of gain with respect to the plurality of volume settings of the single volume control, the first volume-to-gain curve has a greater slope than a slope of the second volume-to-gain curve such that at each volume setting a gain on the first volume-to-gain curve is lower than a gain on the second volume-to-gain curve. In one aspect, the first and second volume-to-gain curves are non-linear functions of gain with respect to volume settings of the single volume control.\n\n[0146] In another aspect, the user-adjustment is a first user-adjustment, the first gain adjustment is a first attenuation, and the second gain adjustment is a second attenuation, the method further includes receiving a second user-adjustment of the single volume control for the first electronic device to increase the reduced overall volume level back to the overall volume level; applying 1) a first gain to the gain-adjusted downlink signal and 2) a second gain to the gain-adjusted audio signal, the first and second gains increase signals levels of the gain-adjusted downlink signal and audio signal, respectively. In one aspect, the first gain is proportional to the first attenuation and the second gain is proportional to the second attenuation. In another aspect, the second gain increases a signal level of the gain-adjusted audio signal more than the second attenuation reduced a signal level of the audio signal when applied in response to receiving the first user-adjustment. In some aspects, when the second user-adjustment of the single volume control increases the overall volume level of the first electronic device to a maximum volume level, the applied second gain increases the signal level of the gain-adjusted audio signal higher than the applied first gain increase the signal level of the gain-adjusted downlink signal.\n\n[0147] In some aspects, determining the first and second gain adjustments includes using the user-adjustment at the single volume control to perform a table lookup into a data structure that associates, for different user-adjustments, a gain for the downlink signal and a gain for the audio signal of the streamed media content. In some aspects, the method further includes, in response to determining that the microphone signal does not include speech the first and second gain adjustments are the same.\n\n[0148] In one aspect, the application of the first and second gain adjustments reduce signal levels of the downlink signal and audio signal, respectively, the method further includes prior to receiving the user-adjustment, determining whether the downlink signal of the call includes speech based on output from a voice activity detector (VAD); and in response to determining that the downlink signal includes speech, applying a third gain adjustment to the audio signal to reduce a signal level of the audio signal. In another aspect, the second gain adjustment reduces the signal level of the audio signal more when the downlink signal includes speech than when the downlink signal does not include speech. In some aspects, the downlink signal is a first downlink signal, the method further includes while engaged in the call and the joint media playback session with the second electronic device and a third electronic device, receiving the first downlink signal from the second electronic device, a second downlink signal from the third electronic device, and the audio signal of the media content; in response to the user-adjustment, applying the first gain adjustment to the first downlink signal, the second gain adjustment to the audio signal, and a third gain adjustment to the second downlink signal, the third gain adjustment adjusts a signal level of the second downlink signal differential than the first gain adjustment adjusts a signal level of the first downlink signal.\n\n[0149] In one aspect, the call is initiated by a telephony application that is being executed by the first electronic device, the audio signal is a first audio signal from a media application that is being executed by the first electronic device that, the method further includes receiving a second audio signal from a separate application that is being executed by the first electronic device; determining the first gain adjustment, the second gain adjustment, and a third gain adjustment to be applied to the downlink signal, the first audio signal, and the second audio signal, respectively, based on an order of which the first electronic device begins to execute the telephony application, the media application, and the separate application. In another aspect, the second audio signal is attenuated less than at least one of the first audio signal and the downlink signal.\n\n[0150] It is well understood that the use of personally identifiable information should follow privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining the privacy of users. In particular, personally identifiable information data should be managed and handled so as to minimize risks of unintentional or unauthorized access or use, and the nature of authorized use should be clearly indicated to users.\n\n[0151] As previously explained, an aspect of the disclosure may be a non-transitory machine-readable medium (such as microelectronic memory) having stored thereon instructions, which program one or more data processing components (generically referred to here as a \u201cprocessor\u201d) to perform the network operations and audio signal processing operations, as described herein. In other aspects, some of these operations might be performed by specific hardware components that contain hardwired logic. Those operations might alternatively be performed by any combination of programmed data processing components and fixed hardwired circuit components.\n\n[0152] While certain aspects have been described and shown in the accompanying drawings, it is to be understood that such aspects are merely illustrative of and not restrictive on the broad disclosure, and that the disclosure is not limited to the specific constructions and arrangements shown and described, since various other modifications may occur to those of ordinary skill in the art. The description is thus to be regarded as illustrative instead of limiting.\n\n[0153] In some aspects, this disclosure may include the language, for example, \u201cat least one of [element A] and [element B].\u201d This language may refer to one or more of the elements. For example, \u201cat least one of A and B\u201d may refer to \u201cA,\u201d \u201cB,\u201d or \u201cA and B.\u201d Specifically, \u201cat least one of A and B\u201d may refer to \u201cat least one of A and at least one of B,\u201d or \u201cat least of either A or B.\u201d In some aspects, this disclosure may include the language, for example, \u201c[element A], [element B], and/or [element C].\u201d This language may refer to either of the elements or any combination thereof. For instance, \u201cA, B, and/or C\u201d may refer to \u201cA,\u201d \u201cB,\u201d \u201cC,\u201d \u201cA and B,\u201d \u201cA and C,\u201d \u201cB and C,\u201d or \u201cA, B, and C.\u201d"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 28,
      "claims_start": 27,
      "description_end": 27,
      "description_start": 8,
      "drawings_end": 7,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 28,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 27,
      "specification_start": 8,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 1000006480386,
    "guid": "US-20230008865-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0008/865",
    "intl_class_current_primary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "3/16",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H04R",
        "intl_subclass": "3/12",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06F3/16",
      "H04R3/12"
    ],
    "inventors": [
      {
        "city": "San Jose",
        "country": "US",
        "name": "Peard; Ross B.",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Milpitas",
        "country": "US",
        "name": "Shah; Pratik",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Peard; Ross B. et al.",
    "patent_title": "METHOD AND SYSTEM FOR VOLUME CONTROL",
    "publication_date": "2023-01-12",
    "publication_number": "20230008865",
    "related_apps": [
      {
        "country_code": "US",
        "filing_date": "2021-07-12",
        "number": "63220928"
      }
    ],
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2022-05-31",
    "appl_id": "17828585",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "San Francisco",
        "country": "US",
        "name": "Dignity Health",
        "state": "CA",
        "zip_code": "N/A"
      },
      {
        "authority_type": "assignee",
        "city": "Scottsdale",
        "country": "US",
        "name": "Arizona Board of Regents on Behalf of Arizona State University",
        "state": "AZ",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "54359196!PG-US-20230009372",
    "cpc_inventive": [
      {
        "cpc_class": "A61B",
        "cpc_subclass": "5/163",
        "version": "2017-08-01"
      },
      {
        "cpc_class": "A61B",
        "cpc_subclass": "5/1104",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61B",
        "cpc_subclass": "3/113",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61B",
        "cpc_subclass": "5/18",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61B",
        "cpc_subclass": "5/4845",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61B",
        "cpc_subclass": "5/746",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "Systems and methods for detecting onset, presence, and progression of particular states, including intoxication, include observing eye movements of a subject and correlating the observed movements to known baseline neurophysiological indicators of intoxication. A detection system may record eye movement data from a user, compare the eye movement data to a data model comprising threshold eye movement data samples, and from the comparison make a determination whether or not intoxication or impairment is present. The detection system may alert the user to take corrective action if onset or presence of a dangerous condition is detected. The eye movements detected include saccadic and intersaccadic parameters such as intersaccadic drift velocity. Measurements may be collected in situ with a field testing device. An interactive application may be provided on a user device to provoke the desired eye movements during recording.",
      "background": "CROSS-REFERENCE TO RELATED APPLICATION(S)\n[0001] This application is a continuation of U.S. application Ser. No. 16/428,791 filed May 31, 2019, which application is a continuation of U.S. application Ser. No. 15/306,892 filed Oct. 26, 2016, now U.S. Pat. No. 10,376,183, issued Aug. 13, 2019, which is a 371 application of PCT/US2015/027730 filed Apr. 27, 2015, which claims priority to U.S. Provisional Patent Application Ser. No. 62/010,600 filed on Jun. 11, 2014, and Provisional Patent Application Ser. No. 61/986,032 filed on Apr. 29, 2014, all of which are incorporated by reference herein for all purposes.",
      "brief": "BACKGROUND OF THE DISCLOSURE\n\n[0002] The present disclosure generally relates to systems and methods for acquiring data from a subject and, more particularly, to systems and methods for gathering and analyzing information about the subject's eye movements to detect a temporary neurologic abnormality or to predict a state of the subject, including the presence of conditions such as alcohol, marijuana, and other illicit or prescribed drug intoxication.\n\n[0003] Given the legalization of marijuana (Cannabis) for medicinal use in over twenty states, and the potential for complete legalization over time, there is a need to increase awareness and education of the general public regarding the safety risks of driving a vehicle or operating heavy equipment when impaired.\n\n[0004] Whether the use of marijuana is legal or illegal, all marijuana users should understand that being impaired places the driver, passengers, and the general public at significant risk. Though this issue also applies to all illicit (cocaine, non-medical opiates) and legal (codeine and other medications) mind-altering substances, it is especially true with marijuana, where the user may become impaired and not realize that the drug has clouded his judgment regarding the operation of a motor vehicle. There is a general misconception that marijuana poses little risk to the driving public, and that notion must be changed through appropriate public health messaging directed at medical marijuana card-holders.\n\n[0005] There are significant physiological differences between the use of alcohol and marijuana, and it is likely that users of marijuana may discount the potential deleterious effects of its active ingredient, tetrahydrocannabinol (THC). What may compound the problem further is the potential to use more than one substance, such as a combination of alcohol, marijuana, and/or another drug or a controlled medication, with synergistically intoxicating results.\n\n[0006] Marijuana usage has been widespread for decades, but its legal use for medical and recreational purposes is fairly new, and its precise and objective effect on driver impairment is largely unknown. A similar gap in knowledge existed for alcohol until a widespread system of standardized field sobriety testing was implemented. By combining field sobriety testing, breathalyzer testing, and blood-alcohol testing, legislatures were enabled to establish appropriate limits on the safe use of alcohol by drivers. However, there is no comparable \u201cBreathalyzer\u201d or a reproducible blood level test to detect marijuana use and related impairment.\n\n[0007] Early and objective detection of the physiological effects of marijuana and other psycho-active drug use can prevent impaired operation of motor vehicles, among other preventative measures. Considering the above, there continues to be a clear need for rapid, accurate, and non-invasive individualized systems and methods for detecting the presence or onset of drug intoxication.\n\nBRIEF SUMMARY\n\n[0008] The present invention overcomes drawbacks of previous technologies by providing systems and methods that afford a number of advantages and capabilities not contemplated by, recognized in, or possible in traditional system or known methodologies related to tracking or determining a subject's state, including the detection of intoxication by marijuana, opiates, codeine, alcohol, and other drugs.\n\n[0009] In one embodiment of the present invention, systems and methods are provided for monitoring, recording, and/or analyzing eye movements in situ to determine whether oculomotor dynamics are being affected by the onset or presence of intoxication. In one aspect, a sensor arrangement may include a camera and recording assembly for detecting and recording the eye movements.\n\n[0010] In some contemplated embodiments, systems and methods using in situ testing of eye movement dynamics may be employed to identify the onset or presence of states or physiological conditions, such as fatigue, hypoxia, stroke, intoxication, seizure, and other conditions. Eye saccades and the velocity of intersaccadic eye drift are detectably affected by the onset or presence of these conditions. A system and method may alert a user to the presence of these states or conditions in a testing environment. In particular, a system in accordance with the present invention may include devices and device assemblies that record baseline data of a subject and generate a data model representing the eye movement data of the subject, and further the system may include device and device assemblies that record eye movement data in situ and compare it to the data model to determine if the user is experiencing or about to experience any of the dangerous conditions.\n\n[0011] In a contemplated embodiment of the present invention, a system includes a sensing arrangement that collects eye movement data of a user, and a control unit in communication with the sensing arrangement. The control unit may be configured to compare the eye movement data to one or more baseline measurements of eye movement dynamics and, if the eye movement data diverges from one or more of the baseline measurements by a threshold amount, generate an alert for delivery to the user. Comparing the eye movement data to the baseline measurements may include calculating a current intersaccadic drift velocity of the user and comparing the current intersaccadic drift velocity to one or more threshold drift velocities of the baseline measurements. The eye movement data may include one or more saccade parameters, and comparing the eye movement data to the baseline measurements may include calculating a current intersaccadic drift velocity of the user from the saccade parameters and comparing the current intersaccadic drift velocity to one or more threshold drift velocities of the baseline measurements. The baseline measurements may include one or more bio-signatures of a substance.\n\n[0012] In another embodiment of the present invention, a method of determining a physiological state of a user includes recording from the user eye movement data of one or both of the user's eyes, comparing the eye movement data to one or more baseline measurements, and, if the eye movement data diverges from one or more of the baseline measurements by a threshold amount, delivering an alert to the user. The eye movement data may include one or both of saccade parameters and intersaccadic drift parameters.\n\n[0013] In another embodiment of the present invention, systems and methods of the present invention may be combined as a kit or apparatus, whose advantages and capabilities will be readily apparent from descriptions below.\n\n[0014] The foregoing and other advantages of the invention will appear from the following description. In the description, reference is made to the accompanying drawings which form a part hereof, and in which there is shown by way of illustration a preferred embodiment of the invention. Such embodiment does not necessarily represent the full scope of the invention, however, and reference is made therefore to the claims and herein for interpreting the scope of the invention.",
      "claims": "1-19. (canceled) \n\n20. A method of determining intoxication of a subject, the method comprising: receiving eye movement data relating to movement of one or both of the subject's eyes for a period of time; calculating corresponding values of one or more recorded eye movement dynamics from the eye movement data, the one or more recorded eye movement dynamics including at least one intersaccadic parameter; comparing the corresponding values of the one or more recorded eye movement dynamics to one or more baseline measurements to identify an intoxicated state of the subject; and responsive to a result of the comparison indicating the subject is intoxicated, causing a perceptible alert to be produced by an alerting arrangement.  \n\n21. The method of claim 20, wherein the one or more baseline measurements comprise one or more bio-signatures each associated with one of a plurality of substances. \n\n22. The method of claim 21, wherein each of the one or more bio-signatures comprises a pattern of eye movement dynamics from a user exposed to one of the plurality of sub stances. \n\n23. The method of claim 21, wherein each of the plurality of substances is a drug. \n\n24. The method of claim 23, wherein one of the plurality of substances associated with one of the bio-signatures is marijuana. \n\n25. The method of claim 24, wherein comparing the corresponding values of the one or more recorded eye movement dynamics to the one or more baseline measurements comprises determining whether one or more of the bio-signatures is present in the eye movement data. \n\n26. The method of claim 25, wherein comparing the corresponding values of the one or more recorded eye movement dynamics to the one or more baseline measurements further comprises, responsive to a determination that a first of the one or more bio-signatures is present in the eye movement data, identifying the drug associated with the first of the one or more bio-signatures and including an indication of the drug in the alert. \n\n27. The method of claim 20, wherein the intersaccadic parameter is intersaccadic drift velocity. \n\n28. The method of claim 27, further comprising providing the result of the comparison indicating the subject is intoxicated when the intersaccadic drift velocity is slower than the one or more baseline measurements. \n\n29. A method of determining intoxication of a subject, the method comprising: receiving eye movement data relating to movement of one or both of the subject's eyes for a period of time; comparing, with a control unit, the eye movement data to one or more baseline measurements corresponding to an intoxicated state, the comparing comprising: extracting one or more current eye movement dynamics, including one or more intersaccadic parameters, from user eye movement data; and comparing the one or more intersaccadic parameters to one or more thresholds of the one or more baseline measurements; and  when the one or more intersaccadic parameters diverge from one or more of the one or more thresholds by a threshold amount, delivering, by the control unit, an alert indicating the subject is in the intoxicated state.  \n\n30. The method of claim 29, wherein the one or more intersaccadic parameters includes an intersaccadic drift velocity. \n\n31. The method of claim 30, wherein, when the one or more intersaccadic parameters diverge from one or more of the one or more thresholds by a threshold amount, includes when the intersaccadic drift velocity is slower than one or more of the one or more thresholds by the threshold amount. \n\n32. The method of claim 30, further comprising calculating the intersaccadic drift velocity by identifying, in the eye movement data, a drift period comprising a duration and a distance; and determining the intersaccadic drift velocity from the duration and the distance. \n\n33. The method of claim 29, wherein the one or more current eye movement dynamics further includes at least one saccadic parameter. \n\n34. The method of claim 29, wherein the one or more baseline measurements include saccadic and microsaccadic eye movement parameters. \n\n35. The method of claim 29, wherein the intoxicated state is a marijuana-induced intoxicated state. \n\n36. A method of determining intoxication of a subject, the method comprising: receiving a calibration set of eye movement data relating to movement of one or both of the subject's eyes for a first period of time; extracting one or more calibration eye movement dynamics, including one or more intersaccadic parameters, from the calibration set of eye movement data; calculating a threshold value from the one or more calibration eye movement dynamics; receiving eye movement data relating to movement of one or both of the subject's eyes for a second period of time; extracting one or more eye movement dynamics, including one or more intersaccadic parameters, from the eye movement data; comparing the one or more eye movement dynamics to the threshold value; and if the one or more eye movement dynamics diverge from the threshold value, delivering an alert indicating the subject is intoxicated.  \n\n37. The method of claim 36, wherein the one or more intersaccadic parameters relate to intersaccadic drift. \n\n38. The method of claim 36, wherein the one or more intersaccadic parameters comprise one or more intersaccadic drift velocities. \n\n39. The method of claim 36, further comprising providing to the subject a device having an interactive application, and recording the eye movement data while the subject engages with the interactive application. \n\n40. The method of claim 36, wherein calculating the threshold value includes calculating a plurality of threshold values reflecting a progression from non-intoxication conditions to intoxicated conditions.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0015] The present invention will hereafter be described with reference to the accompanying drawings, wherein like reference numerals denote like elements.\n\n[0016] FIG. 1 is a diagram of a detection system in accordance with the present invention.\n\n[0017] FIG. 2 is a flowchart illustrating a method for detecting intoxication in accordance with the present invention.\n\nDETAILED DESCRIPTION\n\n[0018] Systems and methods for detecting onset, presence, and progression of particular states, including intoxication, through observation of eye movements are described herein. Acute intoxication is shown by the inventors to affect oculomotor dynamics, including saccadic metrics and intersaccadic drift metrics, with increasing severity as the intoxication progresses. In particular, intersaccadic drift velocity increases as acute intoxication develops and progresses, and select oculomotor dynamics can be tracked against a baseline to alert a subject before the effects of intoxication impair the subject's ability to perform certain actions, such as operating a motor vehicle.\n\n[0019] The systems and methods described herein are offered for illustrative purposes only, and are not intended to limit the scope of the present invention in any way. Indeed, various modifications of the invention in addition to those shown and described herein will become apparent to those skilled in the art from the foregoing description and the following examples and fall within the scope of the appended claims. For example, specific disclosure related to the detection of impairment by marijuana is provided, although it will be appreciated that the systems and methods may be applied for detection of codeine, alcohol, or other drug use and for any subject without undue experimentation.\n\n[0020] Using the approach of the present invention, a detection system may record eye movement data from a user, compare the eye movement data to a data model comprising threshold eye movement data samples, and from the comparison make a determination whether or not the user's brain function is suffering from or is subject to impairment by drug intoxication. The detection system may alert the user or another party to take corrective action if onset or presence of a dangerous impaired condition is detected.\n\n[0021] Referring to FIG. 1, an embodiment of the detection system 10 may include a sensing arrangement 12 configured to detect and record eye movement dynamics of the user. The sensing arrangement 12 may include one or more sensors suitable for collecting the eye movement data. Such sensors may include a camera or other imaging or motion tracking device capable of recording at a suitably high speed and level of detail so that the user's eye movement dynamics, including saccades and intersaccadic drift, are captured. A monocular arrangement of one or more sensors for one of the user's eyes may be used, or one or more sensors may be included for each eye to obtain binocular data. In some embodiments, the sensors may be miniaturized or otherwise compact, portable, and non-invasive. The sensors may further be vehicle-independent, and may be wireless, to facilitate integration of the sensors into any deployment of the detection system 10. For example, the sensing arrangement 12 may include sensors that are integrated into eyewear, such as on the frame or within the lenses of a pair of glasses. This allows for eye movement data collected even as the user turns his head, and allows the sensors to be positioned close to the eyes. In another example, the sensors may be integrated into a heads-up display for a vehicle. In yet another example, the sensors may be integrated into existing personal devices, such as mobile phones and tablet computers. That is, the system 10 may use the camera of the personal device in the sensing arrangement 12, and may use other native or add-on devices as well.\n\n[0022] The sensing arrangement 12 may further include integrated or discrete devices for processing, storing, arid transmitting collected data. Such devices may include a processor, volatile and/or permanent memory, a wired or wireless transmitter, and associated power circuits and power supply for operating the devices. Software modules may define and execute instructions for operating the sensors, configuring databases, registers, or other data stores, and controlling transmission of the data. The collected data may be shared via transmission to a control unit 14 that may be integrated with or disposed physically remotely from the sensing arrangement 12. The eye movement data, or a subset thereof, may be transmitted in real-time as it is captured by the sensors, or it may be stored for later transmission.\n\n[0023] The control unit 14 may use the processing hardware (i.e., processor, memory, and the like) of the sensing arrangement 12, or may include its own processing hardware for analyzing the eye movement data and generating an alert to the user if needed. The control unit 14 may include a plurality of modules that cooperate to process the eye movement data in a particular fashion, such as according to the methods described below. Each module may include software (or firmware) that, when executed, configures the control unit 14 to perform a desired function. A data analysis module 16 may extract information from the eye movement data for comparison to the data model. The data analysis module 16 may include one or more data filters, such as a Butterworth or other suitable bandpass filter, that retain only desired signal elements of the eye movement data. The data analysis module 16 may include program instructions for calculating, from the eye movement data, one or more eye movement dynamics, such as saccades and/or intersaccadic drift velocities, of the user's eyes. The calculation may be performed substantially in real-time, such that a calculated intersaccadic drift velocity may be considered the current drift velocity of the user's eyes.\n\n[0024] A comparison module 18 may receive the processed eye movement data from the data analysis module 16 and may compare it to the data model as described in detail below. The control unit 14 may include or have access to a model data store 20 that stores the data model. The model data store 20 may be a database, data record, register, or other suitable arrangement for storing data. In some embodiments, the data model may simply be a threshold drift velocity, and may thus be stored as a single data record in memory accessible by the comparison module 18. In other embodiments, the data model may be a lookup table, linked list, array, or other suitable data type depending on the data samples for eye movement dynamics or bio-signatures needed to be stored in the data model.\n\n[0025] In some embodiments, the control unit 14 may include a data model generator 22. The data model generator 22 is a module that receives eye movement data collected by the sensing arrangement 12 during a modeling step as described below. The data model generator 22 may extract, or cause the data analysis module 16 to extract, information from the collected eye movement data that will constitute the threshold eye movement data samples in the data model. The data model generator 22 may then create the data model from the threshold eye movement data samples, and may store the data model in the data model store 20. In other embodiments, the data model may be generated and stored in the data model store 20 by a separate modeling unit (not shown) of the system 10. The modeling unit may include its own sensing arrangement, processing hardware, and program modules. One suitable modeling unit may be the Eyelink 1000 by SR Research Ltd. of Mississauga, Ontario, Canada.\n\n[0026] The control unit 14 may include or communicate with an alerting arrangement 24 configured to produce an alert to the user according to the results of the data comparison in the comparison module 18. The alerting arrangement may be any suitable indicator and associated hardware and software for driving the indicator. Suitable indicators include, without limitation: a visual display such as one or more light-emitting diodes, a liquid crystal display, a projector, and the like; a bell, buzzer, or other audible signaling means; and a piezoelectric or other vibrating device.\n\n[0027] The detection system 10 may be used to execute any suitable method of detecting dangerous conditions that are indicated by eye movement data. Referring to FIG. 2, the detection system 10 may execute a method of detecting onset or presence of intoxication in the user. At step 100, the system may record baseline measurements of the eye movement dynamics for the data model. The baseline measurements are taken of a subject which may or may not be the user. It may be advantageous that the data model use baseline measurements of the user himself in order to individualize the operation of the system, but the baseline measurements may be taken from a non-user subject, or taken from a plurality of subjects and averaged if desired. The conditions in which the baseline measurements are recorded may depend on the desired specificity of the data model. In some embodiments, the baseline measurements may be taken in normal (i.e., sea-level or other typical atmospheric oxygen supply) conditions. In other embodiments, the baseline measurements may be taken in known intoxicated conditions. In still other embodiments, the baseline measurements may be taken continuously or at predetermined intervals as the subject is exposed to a progression from normal to intoxicated conditions. The baseline measurements may include eye movement parameters, including saccadic and microsaccadic movement, pupillary response, and eye response to light stimuli. The baseline measurements may also include eye measurements not directly related to movements, such as pupil size.\n\n[0028] At step 105, the system may calculate one or more threshold drift velocities from the recorded baseline measurements. The threshold drift velocities may depend on the format of the collected baseline measurements. For example, where only normal-condition or only intoxicated-condition baseline measurements were taken, a single threshold drift velocity (i.e., threshold-normal or threshold-intoxicated drift velocity) may be calculated. Where progressive baseline measurements were obtained, one or more threshold drift velocities reflecting the subject's progression into, and degree of, intoxication may be calculated. Calculating the threshold drift velocities may include averaging calculated velocities from all or a portion of the individuals measured for baseline measurements. Similarly to calculation of drift velocities, any other measured parameter (e.g. pupil size or papillary response) may be calculated by averaging or normalizing the recorded baseline measurements from multiple individuals. At step 110, the system may generate the data model for the baseline-tested subject(s). The data model may represent the progression of the intersaccadic drift velocity of the subject from normal conditions to intoxicated conditions, and further beyond an intoxicated threshold into increasingly severe intoxication. The data model may be generated and stored in any suitable format that allows the system to subsequently compare eye movement data collected in situ from the user against the data model to determine the user's current impairment.\n\n[0029] The data model may include one or more bio-signatures of neurological impairment. A bio-signature is a characteristic pattern that can be identified in measurements recorded from individuals that are exposed to particular substances. The pattern may be evident by comparing the baselines measurements of exposed individuals to those of non-exposed individuals. In some embodiments, the bio-signatures may be synthesized from the baseline measurements. The bio-signatures may be general (i.e., standardized across a population of patients, such as by demographic) or patient-specific. Bio-signatures may be unique to a particular substance, or may signify the effects of a particular group of substances. Using bio-signature identification for the data model, the system may identify which drugs and drug categories affect and/or impair brain and neurological function. The bio-signatures may correlate with drug dose and effect of the drug on key receptor sites of the central nervous system. The bio-signatures may identify any substance that can be considered impairing, such as stimulants, depressants, and hallucinogens. Non-limiting examples of drug categories that may produce an identifiable biosignature include: hallucinogens, narcotics, stimulants, depressants, cabbinoids, dissocisative anesthetics, and inhalants. Non-limiting examples of drugs that may produce an identifiable bio-signature include: ecstasy, speed, base, ice, methamphetamine, amphetamine, dexamphetamine, crystal methamphetamine, paramethoxyamphetamine, cocaine, crack cocaine, marijuana (cannabis), GHB, inhalants, heroin, morphine, codiene, methadone, buprenorphine, pethidine, barbiturates, dilaudid, kapanol, MS contin, OxyCotin, lysergic acid diethylamide (LSD), psylocibin (aka magic mushrooms), phencyclidine (PCP), ketamine, and mescaline.\n\n[0030] The steps 100, 105, 110 for obtaining the data model may be performed at any suitable time before testing the user in situ for signs of intoxication. In one embodiment, the steps f00-110 may be performed far in advance and remotely from the test environment. In another embodiment, the steps 100-110 may be performed in the test environment, immediately preceding testing the user. For example, the user may activate the system 10, such as by donning and activating eyewear housing the sensing assembly 12, which initiates step 100 of recording the baseline measurements in the present conditions. This may be in normal conditions, such as when the user is about to drive his vehicle in the morning, and only the normal eye movement data would be collected as baseline measurements. In still other embodiments, the data model may be created by the system 10 or another system using a different method than described above.\n\n[0031] At step 115, optionally the system may calibrate itself to the user if the data model or comparison method require it. For example, the data model may be a standardized model generated from baseline measurements of (a) non-user subject(s), or the comparison method may determine the presence of intoxication from a percentage deviation from the user's threshold-normal drift velocity value(s). See below. In such an embodiment, the system calibrates (step 115) by recording a calibration set, such as ten seconds or less but preferably five seconds or less, of eye movement data of the user when the system is activated in the test environment under normal conditions. The system may compare the calibration data to the data model. In one embodiment, this involves determining a deviation of the user's threshold-normal drift velocity from the threshold-normal drift velocity of the model. The system can then adapt the data model to the user.\n\n[0032] At step 120, the system may record in situ eye movement data from the user continuously or at predetermined intervals while the system is activated. At step 125, the system may calculate, in real-time or at predetermined intervals, the user's current drift velocity. At step 130, the system may compare the current drift velocity and other recorded user parameters to the data model to determine the user's progression (or lack thereof) toward intoxication. Such progression may be calculated within any suitable paradigm. Examples include, without limitation: ratio or percentage by which the current drift velocity exceeds the user's or the data model's threshold-normal drift velocity; ratio or percentage by which the current drift velocity is below or above the threshold-intoxicated drift velocity; comparison of current drift velocity to points on a curve between threshold-normal and threshold-intoxicated values in the data model; and the like. Additionally or alternatively, the parameters of the data model that are compared to the recorded user parameters may be taken from one or more bio-signatures within the data model, as described above. The user-to-data model comparison (step 130) may include determining whether the recorded user parameters are a match to one of the bio-signatures. If the user is neither intoxicated nor within a predetermined proximity to the threshold-intoxicated value of the data model, the system returns to step 120 and continues recording current data. If the user's condition warrants (i.e., the current drift velocity is above or within a certain range of the threshold-intoxicated value), at step 135 the system may alert the user to take corrective action. If there is a match between the recorded user parameters and one or more bio-signatures in the data model, the alert (step 135) may include an identification of the substance(s) to which the user has been exposed and which may be impairing the user.\n\n[0033] Embodiments of the system and methods may be used by law enforcement for field sobriety testing as well as for drug education and drug recognition evaluation programs. With respect to field sobriety testing, the system may be implemented as a handheld or otherwise transportable device configured to record the eye movement dynamics of a vehicle driver or heavy equipment operator. The device may perform the analysis and matching steps locally using it native processing power and memory, or the device may transmit the recorded user parameters to a remote processing unit to perform those steps. In an example of a drug evaluation program, such as a drug recognition evaluation (DRE) program, the eye movement dynamics and analysis results may be used to supplement a series of predefined DRE tasks that are routinely performed by trained law enforcement experts, such as in addition to drawing blood to determine the present of elevated levels of alcohol or other substances. During the DRE process the eye movement dynamics may be analyzed for the presence of horizontal gaze nystagmus, pupillary size, pupillary reflex, and response to stimuli, among other parameters. For law enforcement use, security of the recorded and transmitted information may be performed under a data security framework that meets or exceeds the tenets of \u201cchain of custody,\u201d and CJIS and/or HIPAA compliance, as required by the legal system.\n\n[0034] Currently, there are very few principled educational systems to prepare marijuana users\u2014even for those who are responsible medical users\u2014to evaluate and understand the effects of individual impairment from the drug. A pilot project in accordance with this disclosure is underway to respond rapidly to meet the need. In Phase One, the collaborating parties will develop and pilot impaired driver simulation software that correlates driver impairment as a function of marijuana dosage. Due to the pressing concerns about marijuana legalization and its widespread use, the collaborators will rapidly develop and release an interactive educational tool to be made available to medical and recreational marijuana users, family members, and teachers, to explain and demonstrate the dose-related effects of marijuana on driving. This tool may be in the form of an application that operates on a smart phone (iPhone/Android/MS), an iPad or other tablet, or a computer (OS/MS Windows/Linux). The application will enable the user to estimate impairment, driving ability, and the level of impairment as a function of reported equivalent marijuana usage/dosage in terms of comparable equivalent blood-THC level. This goal of this application is to assist in reinforcing education among the users and general public of the very real dangers of driving when impaired by marijuana.\n\n[0035] In Phase Two, the collaborators will develop an affordable eye movement recording device that is based on existing laboratory technology, to assess eye movement dynamics. The eye movements may be captured by a sensing arrangement while the subject user engages in an interactive version of the impairment application similar to the one described in Phase One of this proposal. The device will also capture pupil size and measure response to a bright light stimulus to detect neurophysiologic findings characteristic of marijuana use. The eye movement capture and analysis technology has been used in medicine to diagnose certain neurologic conditions. This technology provides a reliable and objective indicator of physiological brain impairment and will show the user of marijuana that the drug impacts brain function and causes impairment. The data obtained from the device would be used to provide educational programs.\n\n[0036] The described system and methods may be implemented in any environment and during any task that may subject the user to dangerous conditions that affect eye movements. The various configurations presented above are merely examples and are in no way meant to limit the scope of this disclosure. Variations of the configurations described herein will be apparent to persons of ordinary skill in the art, such variations being within the intended scope of the present application. In particular, features from one or more of the above-described configurations may be selected to create alternative configurations comprised of a sub-combination of features that may not be explicitly described above. In addition, features from one or more of the above-described configurations may be selected and combined to create alternative configurations comprised of a combination of features which may not be explicitly described above. Features suitable for such combinations and sub-combinations would be readily apparent to persons skilled in the art upon review of the present application as a whole. The subject matter described herein and in the recited claims intends to cover and embrace all suitable changes in technology."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 9,
      "claims_start": 8,
      "description_end": 8,
      "description_start": 4,
      "drawings_end": 3,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 9,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 8,
      "specification_start": 4,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 54359196,
    "guid": "US-20230009372-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0009/372",
    "intl_class_current_primary": [
      {
        "intl_class": "A61B",
        "intl_subclass": "5/11",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "A61B",
        "intl_subclass": "3/113",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61B",
        "intl_subclass": "5/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61B",
        "intl_subclass": "5/16",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61B",
        "intl_subclass": "5/18",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "A61B5/11",
      "A61B3/113"
    ],
    "inventors": [
      {
        "city": "Anthem",
        "country": "US",
        "name": "Macknik; Stephen L.",
        "postal_code": "N/A",
        "state": "AZ"
      },
      {
        "city": "Anthem",
        "country": "US",
        "name": "Martinez-Conde; Susana",
        "postal_code": "N/A",
        "state": "AZ"
      },
      {
        "city": "Scottsdale",
        "country": "US",
        "name": "Dale; Richard E.",
        "postal_code": "N/A",
        "state": "AZ"
      },
      {
        "city": "Phoenix",
        "country": "US",
        "name": "Besserman; Richard",
        "postal_code": "N/A",
        "state": "AZ"
      },
      {
        "city": "Chandler",
        "country": "US",
        "name": "McDaniel; Troy Lee",
        "postal_code": "N/A",
        "state": "AZ"
      }
    ],
    "inventors_short": "Macknik; Stephen L. et al.",
    "patent_title": "SYSTEMS AND METHODS FOR NON-INTRUSIVE DRUG IMPAIRMENT DETECTION",
    "publication_date": "2023-01-12",
    "publication_number": "20230009372",
    "related_apps": [
      {
        "country_code": "US",
        "filing_date": "2019-05-31",
        "number": "continuation 16428791",
        "patent_number": "11344226"
      },
      {
        "country_code": "US",
        "filing_date": "2016-10-26",
        "number": "17828585",
        "patent_number": "10376183"
      },
      {
        "country_code": "US",
        "filing_date": "2015-04-27",
        "number": "continuation 15306892"
      },
      {
        "country_code": "US",
        "filing_date": "2014-06-11",
        "number": "continuation PCT/US2015/027730"
      },
      {
        "country_code": "US",
        "filing_date": "2014-04-29",
        "number": "16428791"
      },
      {
        "country_code": "WO",
        "number": "62010600"
      },
      {
        "country_code": "US",
        "number": "61986032"
      },
      {
        "country_code": "US"
      },
      {
        "country_code": "US"
      }
    ],
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2019-12-20",
    "appl_id": "17785126",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "G\u00f6teborg",
        "country": "SE",
        "name": "Volvo Truck Corporation",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "69147667!PG-US-20230009869",
    "cpc_inventive": [
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "30/0601",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/063",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "30/08",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/047",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "50/30",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06Q",
        "cpc_subclass": "10/20",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "The present disclosure generally relates to a computer implemented method for operating an autonomous vehicle, specifically in relation to efficient planning of interactions with service providers. The present disclosure also relates to a corresponding control system and computer program product.",
      "brief": "TECHNICAL FIELD\n\n[0001] The present disclosure generally relates to a computer implemented method for operating an autonomous vehicle, specifically in relation to efficient planning of interactions with service providers. The present disclosure also relates to a corresponding control system and computer program product.\n\nBACKGROUND\n\n[0002] Recently there have been great advances in the semi and fully autonomous operation of a vehicle, effectively providing driver assistance and safety functions, such as adaptive cruise control, pedestrian detection, front and rear collision warning, lane departure warning and general obstacle detection. Such an autonomous vehicle typically makes use of a plurality of sensors that are configured to detect information about an environment surrounding the vehicle. The sensor may for example implement camera vision and radar or LiDAR technologies, possibly fusing the outputs form the sensor for forming an understanding of the vehicle environment.\n\n[0003] To ensure that such a vehicle is functioning in a proper manner, it has to be serviced at e.g. regular intervals and/or when a component of the vehicle indicates that it may be malfunctioning. However, since autonomous vehicles inherently lacks continuous human interaction, such as for filling up gas and also assisted in other maintenance issues such as changing a bulb or fuse, checking oil level, checking air pressure in the tires, cleaning windows, etc., the autonomous vehicle must in some implementations by itself ensure that it is properly maintained.\n\n[0004] An example of such an implementation is presented in SE1650341A1, disclosing a method for providing service to an autonomous vehicle. Specifically, data is collected from the vehicle and a service need is determined based on the collected data. In case a service need is present, an inquiry is transmitted to a set of service providers and as a response an offer is received from at least one of the service providers. The vehicle may then select the offer best matching e.g. the service needs while at the same time ensuring that a predefined arrival time (such as for goods delivery) is sufficiently maintained.\n\n[0005] Even though the solution proposed in SE1650341A1 provides improvements in relation to automated maintenance for an autonomous vehicle, SE1650341A1 is completely silent in regard to how the autonomous vehicle should prioritize its operation to optimize a general operation of the vehicle. As such, there appears to be room for further improvements, where also the mentioned optimization focus is taken into account.\n\nSUMMARY\n\n[0006] In accordance to an aspect of the present disclosure, the above is at least partly alleviated by means of a computer implemented method for operating an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the method comprises the steps of identifying, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration (such as a prolonged operation of the vehicle), establishing, using the wireless transceiver, a networked communication with the service provider, receiving, at the electronic control unit, an offer from the service provider to provide the service, estimating, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and deciding, using the electronic control unit, if to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay.\n\n[0007] The present disclosure presents a scheme for matching a \u201cneed\u201d for a vehicle (such as e.g. need to be charged, washed, paused or parked e.g. to not arrive too early at the destination, generally maintained or serviced, etc.) with an offer from a service provider, where the service provider is adapted to fulfill the vehicle needs. The service provider may for example be a charging station, a washing station, etc. In line with the present disclosure, the vehicle may preferably be adapted to itself be responsible to ensure that it doesn't end up in a situation where e.g. the battery is (almost) discharged and the charging station is occupied by another vehicle.\n\n[0008] In line with the present disclosure, this is achieved by putting specific focus on the step of deciding if to accept the offer or not. Generally, the decision is made based on at least present operational duration (e.g. \u201chow long may the vehicle operate without receiving the service\u201d), a previously established time for arrival (e.g. to not arrive too late) and the estimated delay in case the service is accepted. For example, a transport mission (also below defined as a predefined operational mission) may have a time range defined for when to arrive at the destination. Accordingly, in case the estimated delay will not push the time of arrival outside the time range, then it may be clear-cut to accept the offer. However, the time range may have been selected to ensure that other delays, such as general traffic delays, are accounted for. Thus, the decision making must also take such considerations into account.\n\n[0009] Accordingly, in a preferred embodiment of the present disclosure the functionality behind the decision making may include a machine learning component, where the machine learning component has been previously trained on data from a large population of vehicles, for example including historical data relating to the operation of such vehicles. In a specifically preferred embodiment, the machine learning component may also be trained/targeted towards penalizing a situation where the vehicle fails to complete the predefined operational mission, for example within the predefined time range. Accordingly, the machine-earning component is preferably targeted towards balancing a short-term service cost (such as the delay involved with e.g. charging) with the long-term risk of not accepting the offer.\n\n[0010] An example of a long-term cost involving electrical vehicles, may be in the case where the state of charge of a battery of the vehicle reaches below a defined lowest charging level. When the battery reaches such a low state of charge, battery stress may occur severely limiting the lifetime of the battery, resulting in that the battery must be changed earlier that what would normally be necessary resulting in an increased cost for the owner of the vehicle. Another example of a long-term cost involving a sensor may be that if the sensor is not being cleaned \u201cin time\u201d, it may be that the sensor no longer may be cleaned to again function in a normal manner, also in this case having to be changed before what would normally be necessary resulting in an increased cost for the owner of the vehicle. Failing the predefined operational mission may for example be to not charge the vehicle at an early stage, and then in the end having to wait an undesirable long time to charge at a later stage. Similar examples may for example be illustrated in relation to washing of the vehicle, where a failure may be defined as to not washing the vehicle when needed and thus not being able to continue to drive due to sensors being e.g. covered with dirt (thus not functioning).\n\n[0011] Within the context of the present disclosure it should be understood that the expressions \u201cservice provider\u201d and \u201cservice\u201d should be interpreted broadly. That is, a service provider may in the broadest sense be a function to extend the operational duration for the vehicle, as well as for component(s) and/or cargo comprised with the vehicle.\n\n[0012] The method according to the present disclosure may in some embodiments be performed on-board the vehicle, e.g. using an electronic control unit (ECU) comprised with the vehicle. However, at least one portion of the method may in some alternative embodiments be performed using a remote server such as a cloud server, where the cloud server being network connected to an electronic control unit (ECU) comprised with the vehicle.\n\n[0013] In an embodiment of the present disclosure the method further comprises the step of operating the vehicle according to the amended route if the offer is accepted. For example, the vehicle is preferable equipped with a wireless transceiver for providing information to the service provider that the offer is accepted, possibly comprising an estimated time of arrival at the service provider. According to such an implementation it may for example be possible for the service provider to \u201creserve a spot\u201d, such as for charging the vehicle. The information may also be used by the service provider for planning the service of the vehicle.\n\n[0014] Possibly, the adjusted time for arrival at the destination may also be provided to a server arranged remotely from the vehicle. The server may, as will be further elaborated below, be part of a mission control arrangement possibly adapted to coordinate a vehicle fleet comprising a plurality of autonomous vehicles, for example used for planning handling of the vehicle (and the other vehicles) at the destination. As will be apparent, in case the vehicle will arrive late it may be desirable to allow the mission control arrangement to reschedule e.g. cargo handling at the destination, such that the vehicle is handled in the most desirable manner when arriving at the destination. The server may as such be adapted to perform a real time traffic coordination of the vehicle fleet, at least partly based on the decisions made in relation to the present disclosure.\n\n[0015] It should be understood that the server in some embodiments may be allowed to override the decision taken by the electronic control unit comprised with the vehicle, meaning that the server, a function implemented at the server, or an operator in communication with the server, could possibly deny the vehicle the service. Such a decision could for example be taken based on knowledge not available at the vehicle, where the server e.g. in comparison could have more information available.\n\n[0016] It is of course advantageous if the decision of if to accept the offer takes into account an estimated schedule of availability of the service, possibly defining a waiting time for receiving the service. Accordingly, since the waiting time may be fluctuating over time, it may be desirable to arrange the functionality behind the decision making to e.g. take into account historical data for receiving a service at different service providers. Thus, it is preferred to balance the time it could take to receive the service now as compared to the time it could take to receive the service at a later stage.\n\n[0017] Preferably, the step of deciding if to accept the offer advantageously comprises optimizing the operational duration and/or optimizing an arrival time at the destination. Again, the decision scheme should in line with the present disclosure balance when to arrive with an overall operational duration for the vehicle. For example, it may as such be advantageous to receive the service now to prolong the operational duration, even if the vehicle is arriving late at the destination. That is, it may in some embodiments be desirable to allow the decision making to be dependent not only on a single mission (start to destination) but on a plurality of e.g. consecutive missions. Thus, even if the first mission is slightly late due to the reception of a service, a set of consecutive missions could in total possibly be performed in a better way (as compared to if the service was not received at that specific point in time).\n\n[0018] Still further, and in relation to coordinating a vehicles fleet, by applying the scheme according to the present disclosure it may also be possible to maximize long term fleet operational income with respect to operation failure and/or latest arrival time. Operating income may for example be defined as revenue of goods transportation minus operating costs such as energy consumption.\n\n[0019] According to another aspect of the present disclosure, there is provided a control system adapted to operate an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the control system is adapted to identify, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration, establish, using the wireless transceiver, a networked communication with the service provider, receive, at the electronic control unit, an offer from the service provider to provide the service, estimate, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and decide, using the electronic control unit, if to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay. This aspect of the present disclosure provides similar advantages as discussed above in relation to the previous aspect of the present disclosure.\n\n[0020] In a preferred embodiment of the present disclosure, the control system is provided as an on-board component of the vehicle, typically further comprising the above-mentioned sensors. The vehicle may in turn be e.g. one of a bus, a truck, a car, or any form of construction equipment or working machine. Since the vehicle may be one of e.g. a construction equipment or a working machine, the expression road as used above should be interpreted broadly, including any dedicated areas where the vehicle is operating. The vehicle may furthermore be at least one of a pure electrical vehicle (PEV) and a hybrid electric vehicle (HEV). It should however be understood that the control system possibly may be implemented with e.g. a server arranged remotely from the vehicle. Such a server may for example form part of a so called \u201ccontrol tower\u201d arranged to operate/control a plurality of different vehicles.\n\n[0021] According to a further aspect of the present disclosure there is provided a computer program product comprising a non-transitory computer readable medium having stored thereon computer program means for operating a control system adapted to operate an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the computer program product comprises code for identifying, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration, code for establishing, using the wireless transceiver, a networked communication with the service provider, code for receiving, at the electronic control unit, an offer from the service provider to provide the service, code for estimating, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and code for deciding, using the electronic control unit, if to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay. Also this aspect of the present disclosure provides similar advantages as discussed above in relation to the previous aspects of the present disclosure.\n\n[0022] A software executed by the server for operation in accordance to the present disclosure may be stored on a computer readable medium, being any type of memory device, including one of a removable nonvolatile random access memory, a hard disk drive, a floppy disk, a CD-ROM, a DVD-ROM, a USB memory, an SD memory card, or a similar computer readable medium known in the art.\n\n[0023] Further advantages and advantageous features of the present disclosure are disclosed in the following description and in the dependent claims.",
      "claims": "1. A computer implemented method for operating an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the method comprises the steps of: identifying, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration, establishing, using the wireless transceiver, a networked communication with the service provider, receiving, at the electronic control unit, an offer from the service provider to provide the service, estimating, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and deciding, using the electronic control unit, whether to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay.  \n\n2. The method of claim 1, further comprising: operating the vehicle according to the amended route if the offer is accepted.  \n\n3. The method f claim 2, further comprising: providing, using the wireless transceiver, information to the service provider that the offer is accepted.  \n\n4. The method of claim 3, wherein the information provided to the service provider comprises an estimated time of arrival at the service provider. \n\n5. The method of claim 2, further comprising: providing, using the wireless transceiver, a remotely located server an adjusted time for arrival at the destination.  \n\n6. The method Of claim 1, wherein the step of deciding whether to accept the offer is further based on an estimated schedule of availability of the service. \n\n7. The method of claim 6, wherein the estimated schedule of availability of the service defines a waiting time for receiving the service. \n\n8. The method of claim 7, wherein the waiting time is fluctuating over time. \n\n9. The method of claim 1, wherein the step of deciding whether to accept the offer comprises optimizing the operational duration and/or optimizing an arrival time at the destination. \n\n10. (canceled) \n\n11. The method of claim 1, wherein the service provider is at least one of a charging station and a washing station for the vehicle. \n\n12. A control system adapted to operate an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the control system is adapted to: identify, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration, establish, using the wireless transceiver, a networked communication with the service provider, receive, at the electronic control unit, an offer from the service provider to provide the service, estimate, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and decide, using the electronic control unit, whether to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay.  \n\n13. The control system of claim 12, wherein the control system is further adapted to: operate the vehicle according to the amended route if the offer is accepted.  \n\n14. The control system of claim 13, wherein the control system is further adapted to: provide, using the wireless transceiver, information to the service provider that the offer is accepted.  \n\n15. The control system of claim 14, wherein the information provided to the service provider comprises an estimated time of arrival at the service provider. \n\n16. The control system of claim 13, wherein the control system is further adapted to: provide, using the wireless transceiver, a remotely located server an adjusted time for arrival at the destination.  \n\n17. The control system of claim 12, wherein deciding whether to accept the offer is further based on an estimated schedule of availability of the service. \n\n18. The control system of claim 17, wherein the estimated schedule of availability of the service defines a waiting time for receiving the service. \n\n19. The control system of claim 18, wherein the waiting time is fluctuating over time. \n\n20-22. (canceled) \n\n23. An autonomous vehicle, comprising the control system of claim 12. \n\n24. (canceled) \n\n25. A computer program product comprising a non-transitory computer readable medium having stored thereon computer program means for operating a control system adapted to operate an autonomous vehicle travelling along a predetermined route using a control system, the predetermined route including a predetermined destination and a previously established time for arrival at the destination, the control system comprising a wireless transceiver and an electronic control unit, wherein the computer program product comprises: code for identifying, using the electronic control unit, a service provider arranged within a predetermined distance from the vehicle, the service provider arranged to provide a service to the vehicle for extending a present operational duration, code for establishing, using the wireless transceiver, a networked communication with the service provider, code for receiving, at the electronic control unit, an offer from the service provider to provide the service, code for estimating, using the electronic control unit, a delay for amending the route to accept the offer from the service provider, and code for deciding, using the electronic control unit, whether to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0024] With reference to the appended drawings, below follows a more detailed description of embodiments of the present disclosure cited as examples.\n\n[0025] In the drawings:\n\n[0026] FIGS. 1A illustrates a truck, 1B a bus and 1C a wheel loader in which the control system according to the present disclosure may be incorporated;\n\n[0027] FIG. 2 illustrates a conceptual control system in accordance to a currently preferred embodiment of the present disclosure;\n\n[0028] FIG. 3 provides an illustration of the operation of the control system for directing the vehicle to a service provider; and\n\n[0029] FIG. 4 illustrates the processing steps for performing the method according to the present disclosure.\n\nDETAILED DESCRIPTION\n\n[0030] The present disclosure will now be described more fully hereinafter with reference to the accompanying drawings, in which currently preferred embodiments of the present disclosure are shown. This disclosure may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided for thoroughness and completeness, and fully convey the scope of the disclosure to the skilled addressee. Like reference characters refer to like elements throughout.\n\n[0031] Referring now to the drawings and to FIG. 1A in particular, there is depicted an exemplary vehicle, here illustrated as a truck 100, in which a control system 200 (as shown in FIG. 2) according to the present disclosure may be incorporated. The control system 200 may of course be implemented, possibly in a slightly different way, in a bus 102 as shown in FIG. 1B, wheel loader 104 as shown in FIG. 10, a car, etc.\n\n[0032] The vehicle may for example be one of an electric or hybrid vehicle, or possibly a gas, gasoline or diesel vehicle. The vehicle comprises an electric machine (in case of being an electric or hybrid vehicle) or an engine (such as an internal combustion engine in case of being a gas, gasoline or diesel vehicle). The vehicle may further be manually operated, fully or semi-autonomous.\n\n[0033] FIG. 2 shows a conceptual and exemplary implementation of the control system 200 according to the present disclosure, typically provided as an onboard component of the vehicle 100, 102, 104. The control system 200 comprises an electronic control unit (ECU) 202 and a transceiver 204, the transceiver connected to the ECU 202.\n\n[0034] For reference, the ECU 202 may be manifested as a general-purpose processor, an application specific processor, a circuit containing processing components, a group of distributed processing components, a group of distributed computers configured for processing, a field programmable gate array (FPGA), etc. The processor may be or include any number of hardware components for conducting data or signal processing or for executing computer code stored in memory. The memory may be one or more devices for storing data and/or computer code for completing or facilitating the various methods described in the present description. The memory may include volatile memory or non-volatile memory. The memory may include database components, object code components, script components, or any other type of information structure for supporting the various activities of the present description. According to an exemplary embodiment, any distributed or local memory device may be utilized with the systems and methods of this description. According to an exemplary embodiment the memory is communicably connected to the processor (e.g., via a circuit or any other wired, wireless, or network connection) and includes computer code for executing one or more processes described herein.\n\n[0035] The ECU 202 is further provided with an interface for allowing communication with e.g. a first 206 and a second 208 sensor generating data relating to e.g. a current operation of the vehicle 100, 102, 104. Such sensors 206, 206 may include anything from fuel level sensors, tire pressure sensors, oil level sensors, etc., generating data that directly may be used for determining if a service (e.g. such as adjusting the tire pressure or the oil level) is needed. Other sensors are of course possible and within the scope of the present disclosure. The determination of if a service is needed may in some embodiments be made by comparing the data from each of the sensors with at least one predetermined threshold. For example, if the tire pressure is below a predetermined threshold it may be necessary to increase the pressure in that tire (or change the tire). A service is thus needed for performing such an action.\n\n[0036] The ECU 202 may also be connected, e.g. using a CAN bus (or similar present or future communication means) to e.g. a further ECU 210, where the further ECU 2in turn may be connected to other sensors or by itself adapted to determine if a service is needed for the vehicle.\n\n[0037] The ECU 202 may further use the transceiver 204 to communicate with e.g. a service provider 215 (or a plurality of service providers) and/or a server 202. The server 220 may as mentioned above be a mission control server managing e.g. a logistical operation relating to a plurality of vehicles.\n\n[0038] The server 102 may in some embodiment be seen as a central \u201ccontrol hub\u201d, providing an entity such as a transport provider with means for communicating instructions to the vehicles 100, 102, 106 for transporting a predefined cargo from a pic-up location (start) to a destination, possibly travelling along a route that has been determined by the server 220 to a drop-off location (end).\n\n[0039] The communication between the transceiver 204 and the service provider 215 and/or the server 202 may be by means of a network connection, such as using the Internet. In some embodiments the transceiver 204 may for example be adapted for allowing communication using WLAN, CDMA, GSM, GPRS, 3/4/5G mobile communications, or similar. Other present of future wireless communication protocols are possible and within the scope of the present disclosure.\n\n[0040] As mentioned above, the control system 200 is preferably provided as an onboard component of the vehicle 100, 102, 104. However, the computation performed in line with the present disclosure may as an alternative be distributed between e.g. computational means comprised with the vehicle 100, 102, 104 and the server 220. Accordingly, the vehicle 100, 102, 104 may be arranged to take independent decision, or make the decisions in conjunction with the server 220.\n\n[0041] With further reference to FIG. 3, when performing the scheme according to the present disclosure e.g. the vehicle 100 is travelling along a first portion 304 of a route 302. The vehicle 100 has identified that e.g. the first sensor 206 comprised with the vehicle 100 is malfunctioning due to undesirable dirt at a sensing surface of the sensor 206. For example, the sensor 206 may be a camera and the sensor surface may be a lens comprised with the camera. The ECU 202 could possibly have identified that images acquired by the camera have an undesirable quality, being below a normal quality level, and that thus the lens has to be cleaned to again be able to produce images of a normal (and expected) image quality. Accordingly, the ECU 202 has determined that the vehicle is in need of a service, the service in this embodiment being a cleaning service.\n\n[0042] As the vehicle 100 travels along the route 302, it gets within a predetermined distance from a service provider 215 and establishes a networked communication with the service provider 215. The predetermined distance may in some embodiments be implemented as a zone 320 surrounding the service provider 215. The vehicle 100 will in turn receive an offer from the service provider 215 to provide the service. The service provider 215 is located such that the vehicle 100 must take a detour to the service provider 215, i.e. the vehicle 100 will be at least slightly delayed if visiting the service provider 215 to receive the service. In FIG. 3 the service provider is a washing station suitable for providing desired cleaning service to the specific type of the vehicle 100.\n\n[0043] The vehicle 100 must now decide on if it should accept the offer from the service provider 215. This decision must be taken by the vehicle 100 before the detour to the service provider 215 must be made, in FIG. 3 shown as a second portion 306 of the route 306.\n\n[0044] In line with the present disclosure, the decision making is preferably implemented as a machine learning component at the ECU 202. As discussed above, it is advantageous to adapt the machine learning component in such a way that it balances the need of being able to continue to have the vehicle 100 operating while at the same at the same time not arriving too late at the destination (or completing the mission within a predefined time frame).\n\n[0045] Generally, the decision is made based on at least present operational duration (e.g. \u201chow long may the vehicle operate without receiving the service\u201d), a previously established time for arrival (e.g. to not arrive too late) and the estimated delay in case the service is accepted.\n\n[0046] Accordingly, the machine learning based decision making may for example be implemented such that the decision to not to accept the service shall not introduce an immediate negative reward, i.e. at time t.sub.i+1. Rather, it may have a more long-term effect. For example, within the scope of the present disclosure it is possible to foresee a situation where a service was not decided for the vehicle 100 even though it was in relative need of this service.\n\n[0047] However, when the vehicle 100 again is considered for the service it is in urgent need, but at this point in time it cannot readily receive the service because e.g. another later appearing along the route 302 service provider is blocked by another vehicle. In the context of machine learning, this may be implemented by as assigning a delayed penalty reward for a, in the long run, fault action. In line with such an embodiment, historical data relating to the service provider 215 and the specific road 302 may be used in training the machine learning implemented decision component. It should of course be understood that it also may be possible to include a reward (being the opposite to a penalty) in case the vehicle is arriving in time or at least within the predefined time range. In some implementations of the present disclosure, it may be preferred to train the machine learning component in e.g. a \u201cvirtual environment\u201d, for allowing sufficient testing and/or simulation before being applied to the vehicle and/or a fleet of vehicles.\n\n[0048] Accordingly, in line with the present disclosure it is preferred to provide the machine learning component with adequate training data to learn to balance the short-term service cost with the long-term risk of not accepting the service. Furthermore, it is also preferred to adapt the machine learning component such that it \u201cunderstands\u201d that a specific service is less favorable, from a fleet perspective, when it risks blocking an after coming vehicle and/or a service is too much risks to put the mission progress at stake.\n\n[0049] Based on the above, the ECU 202 makes a decision on if to accept the offer from the service provider 215 and controls the vehicle 100 to selects a detour 308 portion of the route 302. If not accepting the offer, the ECU 202 will control the vehicle 100 to continue along a third portion 310 of the route 302. Independently of if accepting or not accepting the offer from the service provider 215, the vehicle 100 will take the final portion 312 of the route 302 to the predetermined destination.\n\n[0050] In summary and with further reference to FIG. 4, the present disclosure relates to a computer implemented method for operating an autonomous vehicle 100, 102, 104, travelling along a predetermined route 302 using a control system 200, the predetermined route 302 including a predetermined destination and a previously established time for arrival at the destination, the control system 200 comprising a wireless transceiver 204 and an electronic control unit 202, wherein the method comprises the steps of identifying, S1, using the electronic control unit 202, a service provider 215 arranged within a predetermined distance from the vehicle 100, 102, 104, the service provider 215 arranged to provide a service to the vehicle 100, 102, 104 for extending a present operational duration, establishing, S2, using the wireless transceiver 204, a networked communication with the service provider 215, receiving, S3, at the electronic control unit 202, an offer from the service provider 215 to provide the service, estimating, S4, using the electronic control unit 215, a delay for amending the route to accept the offer from the service provider 215, and deciding, S5, using the electronic control unit 202, if to accept the offer based on a combination of the present operational duration, the previously established time for arrival and the estimated delay. The present disclosure also relates to a corresponding control system and computer program product.\n\n[0051] The present disclosure presents a scheme for matching a \u201cneed\u201d for the vehicle 100, 102, 104 (such as e.g. need to be charged, washed, etc.) with an offer from the service provider 215, where the service provider 215 is adapted to fulfill the vehicle needs. The service provider 215 may for example be a charging station, a washing station, a parking spot for the vehicle 100, 102, 104 to pause/park e.g. to not arrive too early at the destination, a station providing general maintenance or service, etc. In line with the present disclosure, the vehicle 100, 102, 104 may preferably be adapted to itself be responsible to ensure that it doesn't end up in a situation where e.g. the battery is (almost) discharged and the charging station is occupied by another vehicle.\n\n[0052] The present disclosure contemplates methods, devices and program products on any machine-readable media for accomplishing various operations. The embodiments of the present disclosure may be implemented using existing computer processors, or by a special purpose computer processor for an appropriate system, incorporated for this or another purpose, or by a hardwired system. Embodiments within the scope of the present disclosure include program products comprising machine-readable media for carrying or having machine-executable instructions or data structures stored thereon. Such machine-readable media can be any available media that can be accessed by a general purpose or special purpose computer or other machine with a processor.\n\n[0053] By way of example, such machine-readable media can comprise RAM, ROM, EPROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to carry or store desired program code in the form of machine-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer or other machine with a processor. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or a combination of hardwired or wireless) to a machine, the machine properly views the connection as a machine-readable medium. Thus, any such connection is properly termed a machine-readable medium. Combinations of the above are also included within the scope of machine-readable media. Machine-executable instructions include, for example, instructions and data that cause a general-purpose computer, special purpose computer, or special purpose processing machines to perform a certain function or group of functions.\n\n[0054] Although the figures may show a specific order of method steps, the order of the steps may differ from what is depicted. In addition, two or more steps may be performed concurrently or with partial concurrence. Such variation will depend on the software and hardware systems chosen and on designer choice. All such variations are within the scope of the disclosure. Likewise, software implementations could be accomplished with standard programming techniques with rule-based logic and other logic to accomplish the various connection steps, processing steps, comparison steps and decision steps.\n\n[0055] Additionally, even though the disclosure has been described with reference to specific exemplifying embodiments thereof, many different alterations, modifications and the like will become apparent for those skilled in the art.\n\n[0056] Variations to the disclosed embodiments can be understood and effected by the skilled addressee in practicing the claimed disclosure, from a study of the drawings, the disclosure, and the appended claims. Furthermore, in the claims, the word \u201ccomprising\u201d does not exclude other elements or steps, and the indefinite article \u201ca\u201d or \u201can\u201d does not exclude a plurality."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 11,
      "claims_start": 10,
      "description_end": 10,
      "description_start": 6,
      "drawings_end": 5,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 11,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 10,
      "specification_start": 6,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 69147667,
    "guid": "US-20230009869-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0009/869",
    "intl_class_current_primary": [
      {
        "intl_class": "G06Q",
        "intl_subclass": "10/04",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06Q",
        "intl_subclass": "10/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G06Q",
        "intl_subclass": "10/06",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06Q10/04",
      "G06Q10/00"
    ],
    "inventors": [
      {
        "city": "G\u00f6teborg",
        "country": "SE",
        "name": "Hellgren; Jonas",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "G\u00f6teborg",
        "country": "SE",
        "name": "Hernvall; Christina",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Sk\u00f6vde",
        "country": "SE",
        "name": "Biller; Claus",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "G\u00f6teborg",
        "country": "SE",
        "name": "M\u00f6rk; Tomas",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "G\u00f6teborg",
        "country": "SE",
        "name": "Johansson; Helena",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "Hellgren; Jonas et al.",
    "patent_title": "METHOD FOR OPERATING AN AUTONOMOUS VEHICLE",
    "publication_date": "2023-01-12",
    "publication_number": "20230009869",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2020-12-02",
    "appl_id": "17782711",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Carouge",
        "country": "CH",
        "name": "ID Quantique SA",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "assignees": [
      {
        "city": "Carouge",
        "country": "CH",
        "name": "ID Quantique SA",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "composite_id": "68808135!PG-US-20230007979",
    "cpc_inventive": [
      {
        "cpc_class": "G01S",
        "cpc_subclass": "7/4863",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G01S",
        "cpc_subclass": "7/4873",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G01S",
        "cpc_subclass": "17/894",
        "version": "2020-01-01"
      },
      {
        "cpc_class": "G01S",
        "cpc_subclass": "17/931",
        "version": "2020-01-01"
      },
      {
        "cpc_class": "G01S",
        "cpc_subclass": "17/18",
        "version": "2020-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "The present invention relates to a lidar (1000) comprising an emitter (1100) and a receiver (1200), wherein the receiver (1200) comprises a discrete amplification photon detector (1210), wherein the receiver (1200) comprises a discriminator (1220), wherein the discriminator (1220) has an input connected to an output signal of the discrete amplification photon detector (1210), and wherein the discriminator (1220) is configured to output a signal indicating that the output signal of the discrete amplification photon detector (1210) is higher than a predetermined threshold.",
      "brief": "[0001] The present invention generally relates to a lidar employing a photon-number-resolving detector. In particular, the invention relates to a lidar using a discrete amplification photon detector as detector.\n\nBACKGROUND OF THE INVENTION\n\n[0002] The basic principle of a lidar is the measurement of the round-trip time of a light pulse, for instance a laser pulse, which is scattered from a target back into the receiver, in order to determine the distance between the lidar and the target. Lidars are finding a common use, for instance, in autonomous vehicles. This use requires the lidar to emit as little light as possible, for instance due to safety requirements, power consumptions, energy requirements, etc. Moreover, the light is transmitted in various, unexpected, weather and environmental conditions. This may result in very little light being scattered back to the receiver. It is thus necessary to employ a receiver capable of detecting very low amount of light. Additionally, in applications such as gas sensing, a lidar may be forced to work with a very small reflected signal, due to the nature of the reflection object, namely the molecules of gas.\n\n[0003] Single-photon detectors enable lidar measurements with very low intensities as compared with traditional photodetectors. In principle, a single-photon detector outputs an electrical pulse when a photon arrives at the detector, with a probability that is dependent upon the properties of the detector. This will be referred in the following as \u201cclicking\u201d, meaning that when the single-photon detector \u201cclicks\u201d it outputs a signal indicating one or more photons have reached the detector.\n\n[0004] However, single-photon detectors may suffer from false positive signals due to noise. This noise, in lidar systems, can particularly comprise light not generated form the lidar itself and/or thermal noise in the detector. In the first case light reaching the detector may be, for instance, solar radiation, light emitted from another lidar in the proximity, or more generally light from a source with emission characteristics covering the functional range of the detector. In the second case, thermal noise may result in the single-photon detector clicking even in the absence of any light being received. For a typical single-photon detector, such as a single-photon avalanche diode (SPAD), a click due to noise is indistinguishable from a click due to a lidar event. This makes it difficult to trust the signal outputted by the SPAD. Moreover, typical SPADs have a dead time after a detection event, which prevents consecutive detection events within a predetermined time window. This limits the time resolution of the lidar, since no measurement can occur during the dead time.\n\n[0005] Those problems severely limits the performance of SPAD-based single-photon lidar systems.\n\n[0006] Lidars according to the prior art are known, for instance, from: [0007] \u201cSmart three-dimensional imaging lidar using two Geiger-mode avalanche photodiodes\u201d by Hong Jin Kong, Tae Hoon Kim, Sung Eun Jo, and Min Seok Oh, Optics Express Vol. 19, Issue 20, pp. 19323-19329 (2011), [0008] \u201cA real-time noise filtering strategy for photon counting 3D imaging lidar\u201d, by Zijing Zhang, Yuan Zhao, Yong Zhang, Long Wu, and Jianzhong Su, Optics Express Vol. 21, Issue 8, pp. 9247-9254 (2013), [0009] \u201cLow Intensity LiDAR using Compressed Sensing and a Photon Number Resolving Detector\u201d, Yoni Shera, Lior Cohen, Daniel Istrati, Hagai S. Eisenberg, Proc. SPIE 10546, Emerging Digital Micromirror Device Based Systems and Applications X, 105460J (22 Feb. 2018), [0010] \u201cThresholded Quantum LIDAR\u2014Exploiting Photon-Number-Resolving Detection\u201d, Lior Cohen, Elisha S. Matekole, Yoni Sher, Daniel Istrati, Hagai S. Eisenberg, Jonathan P. Dowling, arXiv:1906.09615 [quant-ph], [0011] US 6,200,818 B1, US 2018/259625 A1. WO 2018/129410 A1, US 2018/113202 A1.\n\n[0012] Most of the prior art devices rely on a combination of the signals from a plurality of SPADs. While this allows increasing the signal to noise ratio, it still suffers from the issues related to the dead time of the various SPADs. Moreover, most the prior art devices do not operate in the range of 950nm to 1700nm.\n\nSUMMARY OF THE INVENTION\n\n[0013] The inventors have generally realized that some or all of the above problems can be solved by using a discrete amplification photon detector, in the following DAPD.\n\n[0014] Generally speaking, a DAPD outputs an electrical pulse whose amplitude is dependent upon the number of photons incident upon the detector. By applying a predetermined threshold it is possible to achieve an increase in the signal to noise ratio of the detector thus solving problems related to the noise. Moreover, DAPDs have effectively no dead time, or a very short dead time compared to SPADs, thus also mitigating the problem related to the length of the dead time window.\n\n[0015] An embodiment of the invention can relate to a lidar comprising an emitter and a receiver, wherein the receiver can comprise a discrete amplification photon detector, wherein the receiver can comprise a discriminator, wherein the discriminator can have an input connected to an output signal of the discrete amplification photon detector, and wherein the discriminator can be configured to output a signal indicating that the output signal of the discrete amplification photon detector is higher than a predetermined threshold.\n\n[0016] Thanks to this implementation, owing to the possibility of the DAPD to output a signal which is a function of the number of photons detected, and by filtering those outputs which are lower than the predetermined threshold, it is advantageously possible to increase the signal-to-noise ratio of the lidar.\n\n[0017] In some embodiments, the predetermined threshold can be dynamic, as a function of an input, wherein the input can comprise a moving speed of the lidar, and/or, wherein the input can comprise a value of the output signal of the discrete amplification photon detector, averaged across a predetermined time.\n\n[0018] Thanks to this approach it is advantageously possible to adapt the threshold to the detecting conditions, which may allow an increase in precision and/or in signal-to-noise ratio and/or in the detection range, depending on the specific configuration.\n\n[0019] In some embodiments, the discriminator can be configured to determine a time corresponding to a maximum value of the output signal.\n\n[0020] Thanks to this approach, in the presence of an output signal having a variable amplitude, the detection of the maximum amplitude can be advantageously used to obtain a more precise measurement and/or better define a measurement window.\n\n[0021] In some embodiments, the lidar can further comprise a controller for controlling at least an emission of the emitter, wherein the controller can control the emitter so as to emit a random, or pseudo-random, or predetermined pulse sequence.\n\n[0022] Thanks to this implementation it is advantageously possible to recognize pulses from one another which may be useful to distinguish from successive pulses, so as to increase the detection range of the lidar, and/or to distinguish pulses among different lidar equipment operating in the same environment.\n\n[0023] In some embodiments, the receiver can be configured to detect light having a wavelength in a range from 950 nm to 1700 nm, preferably at 1550 nm.\n\n[0024] Thanks for this approach it is advantageously possible to implement a system which is safe to the human eye, in addition to being compatible with a wide array of commercially available components.\n\n[0025] In some embodiments, the receiver can comprise optics, wherein the optics can comprise a filter for filtering light having a frequency lower than 950 nm and/or higher than 1700 nm, preferably having a frequency lower than 1500 nm and/or higher than 1600 nm.\n\n[0026] Thanks for this approach it is advantageously possible to filter out light which may cause unwanted detection at the receiver.",
      "claims": "1. A lidar (1000, 2000, 3000) comprising an emitter (1100, 5100) and a receiver (1200, 3200), wherein the receiver (1200, 3200) comprises a discrete amplification photon detector (1210, 4210), wherein the receiver (1200, 3200) comprises a discriminator (1220), wherein the discriminator (1220) has an input connected to an output signal of the discrete amplification photon detector (1210, 4210), and wherein the discriminator (1220) is configured to output a signal indicating that the output signal of the discrete amplification photon detector (1210, 4210) is higher than a predetermined threshold.  \n\n2. The lidar (1000, 2000, 3000) according to claim 1, wherein the predetermined threshold is dynamic, as a function of an input, wherein the input comprises a moving speed of the lidar, and/or, wherein the input comprises a value of the output signal of the discrete amplification photon detector (1210, 4210), averaged across a predetermined time.  \n\n3. The lidar (1000, 2000, 3000) according to claim 1, wherein the discriminator (1220) is configured to determine a time corresponding to a maximum value of the output signal.  \n\n4. The lidar (1000, 2000, 3000) according to claim 1, further comprising a controller (2300) for controlling at least an emission of the emitter (1100, 5100), wherein the controller (2300) controls the emitter (1100) so as to emit a random, or pseudo-random, or predetermined pulse sequence.  \n\n5. The lidar (1000, 2000, 3000) according to claim 1 wherein the receiver (1200, 3200) is configured to detect light having a wavelength in a range from 950 nm to 1700 nm, preferably at 1550 nm.  \n\n6. The lidar (3000) according to claim 1 wherein the receiver (3200) comprises optics (3230), wherein the optics (3230) comprise a filter for filtering light having a frequency lower than 950 nm and/or higher than 1700 nm, preferably having a frequency lower than 1500 nm and/or higher than 1600 nm.",
      "description": "BRIEF DESCRIPTION OF THE FIGURES\n\n[0027] FIG. 1 schematically illustrates a lidar 1000;\n\n[0028] FIG. 2 schematically illustrates a lidar 2000 comprising a controller 2300;\n\n[0029] FIG. 3 schematically illustrates a lidar 3000 comprising optics 3230;\n\n[0030] FIG. 4A schematically illustrates a physical implementation of a discrete amplification photon detector 4210;\n\n[0031] FIG. 4B schematically illustrates an electrical schematic of the discrete amplification photon detector 4210;\n\n[0032] FIG. 4C schematically illustrates a preferred positioning of the discrete amplification photon detector 4210 in use;\n\n[0033] FIG. 5 schematically illustrates an emitter 5100 comprising optics 5110.\n\nDETAILED DESCRIPTION OF PREFERRED EMBODIMENTS\n\n[0034] FIG. 1 schematically illustrates a lidar 1000. Lidar 1000 comprises an emitter 1100 and a receiver 1200.\n\n[0035] The emitter 1100 could be, for instance, a laser emitter, potentially coupled with optics, as will be discussed in the following in particular with reference to emitter 5100. An exemplary embodiment of the laser could be an Erbium-doped fibre laser, for instance as commercially available from NuPhoton, model EDFL-Nano-1550-0.9-0.8uJ-800mW-FCA-02. In some embodiments, the laser can generate optical pulses with a wavelength comprised between 950 nm and 1700 nm, preferably of 1550 nm. This value is particularly advantageous because it is safe to the human eye. Moreover, this wavelength is generally used in telecom devices so that commercial off-the-shelf components are readily available. Additionally, solar spectral irradiance is comparatively low in this wavelength range than at visible wavelengths, which reduces the interference caused by sunrays on the operation of the lidar. Still further, atmospheric absorption is comparatively low at this wavelength, so that the lidar can reach longer distances with a given emission power. In some embodiments, the laser can emit light with a peak optical power comprised between 0.5 kW and 2 kW, preferably 1 kW.\n\n[0036] The frequency of the emitted pulses is preferably chosen so as to avoid range ambiguity. In particular, return signal from a reflection will appear to be arriving from a distance less than the true range of the reflection, when arriving from a distance D\n\nD >[c/(2*PRF)]wherein: [0037] cis the speed of light; [0038] PRF is the pulse repetition frequency.\n\n[0039] So, for instance, for a lidar with a nominal range of 300 m, a maximum pulse repetition frequency of approximately 500 kHz must be used. Once the pulse repetition frequency is fixed, the maximum pulses duration is also fixed, although it will still be possible to use shorter pulses. Generally, the shorter the pulse length, the higher the precision of the lidar. In some practical embodiments, the pulses can have a width comprised between 100 ps and 2 ns, preferably between 200 ps and 1 ns, even more preferably 500 ps.\n\n[0040] Using TCSPC (Time-Correlated Single Photon Counting), preferably jointly with other known technics, it is advantageously possible to increase the repetition rate into the\n\n[0041] MHz range whilst still avoiding range ambiguity. The use of higher frequencies advantageously allows to increase signal-to-noise ratio for a given integration time, since the signal increases.\n\n[0042] The receiver 1200 comprises a discrete amplification photon detector 1210. Thanks to this configuration, light emitted from emitter 1100 and reflected by the object OBJ is received by the discrete amplification photon detector 1210. Thanks to the high sensitivity of the discrete amplification photon detector 1210, which is capable of detecting as little as a single photon, it is possible to reliably operate the lidar with low amount of light being emitted by the emitter 1100 and/or to detect objects OBJ at greater distances with a given power for the emitter 1100. The object OBJ can be varied, comprising for instance objects along a road, for a vehicle application, or gas, for gas sensing applications.\n\n[0043] In some embodiments, the discrete amplification photon detector 1210 can be implemented by the commercially available DAPD NIR 5\u00d75 Array from Amplification Technologies.\n\n[0044] The receiver 1200 further comprises a discriminator 1220, having an input connected to an output signal of the discrete amplification photon detector 1210. The discriminator 1220 is configured to output a signal indicating that the output signal of the discrete amplification photon detector 1210 is higher than a predetermined threshold.\n\n[0045] It is firstly noted that the connection of the output of the discrete amplification photon detector 1210 and the input of the discriminator 1220 does not need to be direct. In some embodiments, for instance, additional electronic elements could be placed along the connection line, such as, for instance, amplifiers. For the implementation of the embodiment it is sufficient that the input of the discriminator 1220 receives a signal corresponding to, and/or based on, the output of the discrete amplification photon detector 1210.\n\n[0046] In some embodiments, the discriminator 1220 operates by detecting an amplitude of the signal on its input and by providing an output with a first value if said amplitude is bigger than a predetermined amplitude threshold, or an output with a second value if said amplitude is smaller than a predetermined amplitude threshold. The discriminator 1220 can be implemented, for instance, by a differential amplifier having a positive input connected to the output of the discrete amplification photon detector 1210 and a negative input connected to the predetermined amplitude threshold. It will however be clear to those skilled in the art that numerous manners exist for implementing a circuit operating as described.\n\n[0047] Thanks to this approach, when the output of the discrete amplification photon detector 1210 is below the predetermined amplitude threshold the output of the discriminator 1220 will not switch. This advantageously allows filtering out signals from the discrete amplification photon detector 1210 indicating a number of photons below a predetermined photon threshold. Such low signals are mostly resulting from noise, from ambient light or dark counts, or haze or fog, which scatters the laser light back to the detector. By filtering them through the discriminator 1220, the signal to noise ratio of the receiver 1200 can be advantageously increased such that, for instance, a strong lidar signal from a solid target can be seen on a background of fog-scattered laser light, thus improving performance in poor visibility scattering environments. This can also be particularly advantageous in gas sensing applications, in which the reflected signal can be very low, so that an increase in signal-to-noise ratio is highly beneficial.\n\n[0048] In some embodiments, the predetermined amplitude threshold value can be selected to correspond to a predetermined photon threshold value. That is, amplitude of the output of the discrete amplification photon detector 1210 is a function of the number of photons impinging on the discrete amplification photon detector 1210, so that an output amplitude value is a function of the input number of photons. The predetermined amplitude threshold value can then be selected to correspond to a predetermined photon threshold value of at least 1.5 photons, preferably at least 3.5. In some embodiments, if it is only possible to evaluate the amplitude of the receiver 1200 based on an integer number of input photons, a predetermined amplitude such as 1.5 can be computed based on a linear interpretation of the amplitude output at the lower and higher integer number of photons.\n\n[0049] On the other hand, when the output of the discrete amplification photon detector 1210 rises above the predetermined amplitude threshold, the output of the discriminator 1220 can switch from the first value to the second value, indicating the change in the output of the discrete amplification photon detector 1210.\n\n[0050] In some other embodiments, the discriminator 1220 can operate by integrating an amplitude of the signal on its input over a predetermined time. The discriminator can then provide an output with a first value if the result of the integration is higher than a predetermined amplitude threshold or an output with a second value if the result of the integration is lower than a predetermined amplitude threshold. This is particularly advantageous when photons reflected from the same object arrive at slightly different times, even though they belong to the same pulse. This may happen, for instance, if a detection pulse of the receiver 1200 is shorter than an emission pulse of the emitter 1100. In this case, the instantaneous amplitude outputted by the receiver 1200 can be lower than the predetermined amplitude threshold, and thus be discarded as noise. On the other hand, by integrating the output of the receiver 1200 it is possible to correctly detect that the photons belong to the same emission pulse. In some embodiments, the predetermined integration time can thus have a minimum value corresponding to the length of the emission pulse of the emitted 1100. A maximum value of the predetermined integration time can be selected depending on the precision requirements of the lidar. In some practical embodiments, the predetermined integration time can be at least 5 ns, preferably 10 ns.\n\n[0051] As it is apparent, the invention allows not only filtering of low-strength signals, so as to increase the signal-to-noise ratio, but it also allows the amplitude of the output signal to be evaluated. This is particularly advantageous in that it allows the lidar not only to gather information on the presence or absence of a reflection, but also on the amount of light reflected, that is, on the characteristics of the reflection. This is particularly advantageous in that algorithms for obstacle detections can benefit from the additional given information on reflectivity and can improve their accuracy.\n\n[0052] In addition, in some embodiments, the predetermined amplitude threshold applied by the discriminator 1220 can be dynamic, that is variable as a function of an input, as opposed to the previously described static values.\n\n[0053] The input can be, in some embodiments, the moving speed of the lidar. In particular by decreasing the amplitude threshold as the speed increases and/or by increasing the amplitude threshold as the speed decreases. At higher speed, typical of a non-urban use, this can allow a better detection at longer distances, which generally result in a lower reflection, while the potential increase of the signal-to-noise ratio can be compensated by a reduced need for precision in this environment. Conversely, at lower speed, the increase in the threshold can allow an increased precision, needed for an urban environment, at the expense of some long-range visibility. In some embodiments, a minimum amplitude threshold can be set to be an equivalent predetermined photon threshold value of at least 1.5 photons.\n\n[0054] Alternatively, or in addition, the input can be the value of the output signal of the discrete amplification photon detector 1210, averaged across a predetermined time. In particular by increasing the amplitude threshold as the value of the output signal increases and/or by decreasing the amplitude threshold as the value of the output signal decreases. This allows making advantageous use of a better reflection, resulting in an increased value of the output signal, by increasing the threshold so as to increase the signal-to-noise ratio. Conversely, in case of a reduced value of the output signal, for instance in case of fog or other environmental causes, a lower signal-to-noise ratio may be accepted in order to obtain a signal from the lidar.\n\n[0055] In some embodiments, the discriminator 1220 is further configured to determine a time corresponding to a maximum value of the output signal. That is, the signal outputted by the receiver 1200 is likely to have a non-constant shape, with at least one maximum value. The discriminator 1220 can therefore be used not only to determine if the value is above the predetermined threshold, but also the time at which the maximum value occurs.\n\n[0056] This information can be particularly useful for several reasons. In some embodiments, the maximum value could be used, instead of other timing values of the pulse outputted by the receiver 1200, in order to compute the distance of the object which caused the reflection. Alternatively, or in addition, the maximum value could be used as central value of the integration interval described above. This is particularly advantageous in that it improves the position of the integration window so that the outcome of the integration operation can be more reliable. Moreover, depending on signal's amplitude, given a fixed threshold, the detection time will shift. This shift can be compensated by knowing the maximum of the peak. Therefore, the detection of the maximum and the detection time allows for the shift correction.\n\n[0057] It will be clear that, although the function above has been described as being carried out by the discriminator, it could also be possible to implement another, separate, element for the purpose of determining the timing of the maximum value of the output signal.\n\n[0058] FIG. 2 schematically illustrates a lidar 2000 comprising a controller 2300. The controller controls at least the emission of the emitter 1100 and the output of the receiver 1200, so as to compute a time of flight of the pulse emitted by the emitter 1100 and received by the receiver 1200 in order to compute the distance to the object OBJ which reflected the pulse.\n\n[0059] In some embodiments, the controller 2300 is configured to evaluate the output of the receiver 1200 only in a predetermined time window with respect to the emission time of the emitter. That is, defining t0 as the emission time of the emitter 1100, the controller is configured to evaluate as reflection an output of the receiver 1200 received between t1 and t2. This approach is particularly advantageous, for instance, in order to avoid detecting as reflection to the pulse at t0 a reflection to a previous or subsequent pulse. This is generally referred to as \u201ctime gating\u201d of the receiver 1200.\n\n[0060] This is particularly advantageous in foggy, dusty, or similar conditions, where a significant part of the reflected signal is backscatter from particles. This backscatter signal may result in a dead-time of the receiver 1200 which, even if contained, due to the high value of the backscatter, may reduce the sensitivity of the receiver 1200 at a time at which the reflection of an object is receiver. In order to avoid this, the controller 2300 can configured to evaluate as reflection an output of the receiver 1200 received between t1 and t2. In some embodiments, t1 may be t0, namely the time of emission by the emitter, plus a predetermined time. The predetermined time can be at least 50 ns, preferably at least 100 ns. In some embodiments, t2 may be t0, namely the time of emission by the emitter, plus a predetermined time. The predetermined time can be at most 400 ns, preferably at most 300 ns.\n\n[0061] Those values have been found to be particularly advantageous in practical embodiments. In particular, the maximum value of t2 limits the distance at which the lidar can detect objects to a value sufficient for most applications. On the other hand, the minimum value of t1, though removing the possibility of detection in the immediate proximity of the lidar, is not particularly limiting since this range is likely to have been imaged by the lidar, moving forward, in a previous scan. At the same time, this is the range which is likely to cause the highest level of scattering, so that removing this scattering by imposing a minimum value of t1, may be particularly advantageous.\n\n[0062] In some embodiments, alternatively or in addition to the controller 2300 being configured to evaluate as reflection an output of the receiver 1200 received between t1 and t2, it may be possible to enable the receiver only between t1 and t2. This is particularly advantageous, since it avoids that a reflection due to backscattering may cause a dead-time at the receiver 1200. This can be done, in some embodiments, by varying a bias voltage of the receiver 1200. For instance, with reference to the embodiment illustrated in FIG. 4B, this can be achieved by varying voltage V1 In particular, the bias voltage can be set to a first predetermined value, at which the receiver 1200 does not detect photons, before t1 and after t2. Conversely, the bias voltage can be set to a second predetermined value, at which the receiver 1200 does detect photons, between t1 and t2.\n\n[0063] Moreover, in some embodiments, the use of a discrete amplification photon detector as receiver 1200 allows a particularly reduced dead time in the detection operation. This, in turn, allows the controller 2300 to use a fast, preferably arbitrary, pulse train to encode the laser pulses emitted by the emitter 1100. In particular, in some embodiments, the controller 2300 can control the emitter 1100 so as to emit a random, or pseudo-random, or predetermined pulse sequence. The use of such pulse sequence instead of a single pulse can further be specific to a given lidar. This can mitigate cross-talk/interference between vehicles or gas sensing equipment comprising the lidar. The low dead-time of the discrete amplification photon detector as receiver 1200 allows this to be done effectively. In some embodiments, the pulse sequence may have the characteristics previously described for the single pulse, in particular the complete pulse sequence may have a duration between 100 ps and 2 ns, preferably between 200 ps and 1 ns, even more preferably 500 ps. The random or pseudo-random variation of the pulse sequence can be expressed by the number and/or duration of the pulses within the pulse sequence.\n\n[0064] Moreover, the low dead time allows resolving objects, or different parts of the same object, that are close together in range. This is particularly advantageous for applications where range profiling, that is making a 3D picture of one or more objects, is required. This is, for instance, particularly advantageous in the aerospace and defence sector, where the lidar may be used to identify potential enemy aircraft. In some embodiments, the receiver 1200 can be configured to detect light having a frequency in a range from 950 nm to 1700 nm, preferably at 1550 nm. In general, the receiver 1200 preferably has a detection range which is at least partially overlapping, preferably completely overlapping, the emission range of the emitter 1100. This is advantageously possible thanks to the implementation of the receiver 1200 by means of a discrete amplification photon detector.\n\n[0065] FIG. 3 schematically illustrates a lidar 3000 comprising optics 3230. The optics 3230 can generically operate as a filter and/or as a lens.\n\n[0066] In some embodiments, the optics 3230 comprise a filter for filtering light having a frequency lower than 950 nm and/or higher than 1700 nm, preferably having a frequency lower than 1500 nm and/or higher than 1600 nm. This can be achieved, for instance, by the combination of a low-pass and a high-pass filter, or by a band-pass filter. In some embodiments, the bandpass filter preferably has a bandwidth of 1 nm to 10nm. In some embodiments, the filter may have a bandwidth corresponding the emission bandwidth of the emitter 1100.\n\n[0067] In some embodiment, the optics 3230 comprise a lens for converging light from a predetermined angle onto the discrete amplification photon detector 1210. It will be clear to those skilled in the art how to select an appropriate converging lens, given the dimensions of the discrete amplification photon detector 1210 and the distance from the lens. The angle is preferably measured along the vertical direction. The angle bisector preferably corresponds to the horizontal direction.\n\n[0068] FIG. 4A schematically illustrates a physical implementation of a discrete amplification photon detector 4210, which can be used to implement one or more discrete amplification photon detector 1210.\n\n[0069] The discrete amplification photon detector 4210 comprises a plurality of pixels 4211 arranged in a matrix of N rows by M columns, wherein N is comprised between 1 and 1064, preferably between 1 and 64, and wherein M is comprised between 1 and 1064, preferably between 1 and 64. Each pixel 4211 can implement a discrete amplification photon detector 1210. The use of several pixels at the same time provides a faster data acquisition than scanning the field with a single pixel. In the illustrated embodiment, the discrete amplification photon detector 4210 comprises 5 rows and 5 columns for a total of 25 pixels 4211. The pixels have been numbered for ease of description, as will become clearer with respect to FIG. 4C. It will be clear that this is for explanation purposes only and the invention is not limited to this specific configuration.\n\n[0070] Each of the pixels 4211 is capable of detecting light from a single photon and from a plurality of photons, as described above. Moreover, each of the pixels 4211 is capable of outputting an output signal which is a function of the number of photons detected by the pixel 4211. In this respect, each of the pixels 4211 acts a single discrete amplification photon detector, thus having the capability of implementing the functionality described for discrete amplification photon detector 1211.\n\n[0071] The plurality of pixels allows imaging a broader image. That is, thanks to the optics 3230 using a plurality of pixels 4211 can obtain a broader detection angle. This advantageously allows for lidar architectures such as flash lidar, where the whole scene is illuminated in one flash and a static receiver observes everything, or hybrid flash/scanning.\n\n[0072] Moreover, the output of at least two pixels 4211 can be combined in a particularly advantageous manner in order to increase the signal quality of the lidar. In particular, it is possible to combine the output of several discriminators 1220, each connected to its own pixel 4211, that is, to its own receiver 1200. In this manner, at the level of the single pixel 4211 and the respective discriminator 1220, the same effects in increase in signal-to-noise ratio as previously described can be obtained. Moreover, the output of those discriminators 1220 can be combined to further increase the signal-to-noise ratio of the lidar.\n\n[0073] In some embodiments, the combination of the plurality of outputs of the discriminators 1220 can be, for instance, a majority function. That is, the combined output of the discriminators 1220 can correspond to the value having the highest appearance among the various output of the various discriminators 1220.\n\n[0074] Alternatively, or in addition, in some embodiments, the combination of the plurality of outputs of the discriminators 1220 can be, for instance, an average function. That is, the combined output of the discriminators 1220 can correspond an average of the various output of the various discriminators 1220\n\n[0075] FIG. 4B schematically illustrates an electrical schematic of the discrete amplification photon detector 4210. In particular, FIG. 4B schematically illustrates three pixels, one of which, pixel 4211 is indicated. Each pixel is provided with an output signal OUT which can be connected to a respective discriminator. In some embodiments, practical values of the R, R2, C1 and C2 components can be selected to be between 1k\u03a9 and 100 k\u03a9 for R1 and/or, and between 1 nF and 100 nF for C1 and/or C2.\n\n[0076] FIG. 4C schematically illustrates a preferred positioning of the discrete amplification photon detector 4210 in use. In particular, as it can be seen comparing FIGS. 4A and 4C, the discrete amplification photon detector 4210 can be positioned with its diagonal 4212 substantially corresponding to a direction in which the emitter 1100 has a diverging emission. Alternatively, or in addition, in case the emitter 1100 has a diverging emission along more than one direction, the discrete amplification photon detector 4210 is positioned with its diagonal 4212 substantially corresponding to the direction in which the emitter 1100 has the most diverging emission. In some embodiments, for instance in the embodiment described with reference to FIG. 5 below, the emitter 5100 has a diverging emission substantially along the vertical axis. In such embodiments, as illustrated in FIG. 4C, the diagonal 4212 of the discrete amplification photon detector 4210 is placed vertically, along vertical axis Y.\n\n[0077] In some embodiments, the direction of the diverging emission of the emitter 1100 can be changed by optics, such as lenses and/or mirrors. For instance, the emitter 1100 can emit light with a single diverging direction along the horizontal axis. This can then be rotated, for instance by 90 degrees, at the emitter, by optics, so as to emit light with a single diverging direction along the vertical axis. This emitted light, reflected by the object OBJ, can then be rotated again at the receiver, for instance by 90 degrees, so that the diverging direction of the light received at receiver is again substantially corresponding to the horizontal axis.\n\n[0078] Therefore, as a generalization of the concept described above, in such embodiments, it will be understood that the matrix of N rows by M columns has a diagonal 4212 and the emitter 1100 is configured to emit light with a diverging direction, or with a maximum diverging direction, along a first predetermined direction. The diagonal 4212 then corresponds to a projection, on the discrete amplification photon detector 4210, of light emitted by the emitter 1100 along the first predetermined direction. In other words, the discrete amplification photon detector 4210 can be positioned so that its diagonal corresponds to the diverging direction of the light from the emitter 1100 once such light is reflected and reaches the discrete amplification photon detector 4210.\n\n[0079] FIG. 5 schematically illustrates an emitter 5100 comprising optics 5110 in addition to emitter 1100. In particular, the optics 5110 are positioned such that the signal emitted by the emitter 1100 passes through the optics 5110.\n\n[0080] In some embodiments, the optics 5110 may comprise lenses configured to provide an output optical signal substantially collimated in the horizontal axis and diverging in the vertical axis, preferably with an angle of 1 degree to 2.6 degrees, preferably 1.8 degrees. The angle is preferably measured along the vertical direction. The angle bisector preferably corresponds to the horizontal direction. The advantage of this approach is that it allows to directly scanning a vertical area of a predetermined size. It has been found that those specific values further advantageously allow scanning an area which provides significant and sufficient information for autonomous driving applications, while avoiding a too large area being scanned. The scanning in the horizontal direction can then be achieved by changing the output direction of the output optical signal, for instance by means of a mirror, preferably a rotating mirror, in a known manner.\n\n[0081] Although various embodiments have been described and/or illustrated independently, each comprising one or more features, it will be clear that the invention is not limited to the specifically described and/or illustrated embodiments. In addition, it will be possible to combine one or more features, from one or more embodiments, without necessarily comprising all features from the respective embodiments, so as to obtain alternative embodiments of the invention.\n\nLIST OF REFERENCE NUMERALS\n\n[0082] 1000: lidar [0083] 1100: emitter [0084] 1200: receiver [0085] 1210: discrete amplification photon detector [0086] 1220: discriminator [0087] OBJ: object [0088] 2000: lidar [0089] 2300: controller [0090] 3000: lidar [0091] 3200: receiver [0092] 3230: optics [0093] 4210: discrete amplification photon detector [0094] 4211, 1-25: pixel [0095] 4212: diagonal [0096] A1: amplifier [0097] C1, C2: capacitor [0098] D1: diode [0099] OUT: output [0100] R1, R2: resistance [0101] V1, V2: voltage bias [0102] 5100: emitter [0103] 5110: optics"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 10,
      "claims_start": 10,
      "description_end": 10,
      "description_start": 5,
      "drawings_end": 4,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 10,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 10,
      "specification_start": 5,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 68808135,
    "foreign_priority": [
      {
        "app_filing_date": "2019-12-06",
        "app_number": "19214065.5",
        "country": "EP"
      }
    ],
    "guid": "US-20230007979-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0007/979",
    "intl_class_current_primary": [
      {
        "intl_class": "G01S",
        "intl_subclass": "17/18",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G01S",
        "intl_subclass": "17/931",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G01S",
        "intl_subclass": "7/487",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G01S17/18",
      "G01S17/931"
    ],
    "inventors": [
      {
        "city": "Keynsham, Bristol",
        "country": "GB",
        "name": "DUNNING; Alexander",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Carouge (GE)",
        "country": "CH",
        "name": "BOSO; Gianluca",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Vessy",
        "country": "CH",
        "name": "BUSSIERES; Felix",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "DUNNING; Alexander et al.",
    "patent_title": "LIDAR WITH PHOTON-RESOLVING DETECTOR",
    "publication_date": "2023-01-12",
    "publication_number": "20230007979",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2020-12-07",
    "appl_id": "17783363",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "PALAISEAU",
        "country": "FR",
        "name": "OFFICE NATIONAL D\u2019ETUDES ET DE RECHERCHES A\u00c9ROSPATIALES",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "70154538!PG-US-20230011501",
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "18/2415",
        "version": "2023-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "7/584",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "20/10",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "G01C",
        "cpc_subclass": "21/165",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "17/18",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "Disclosed is a box-regularized particle filtering process which includes an Epanechnikov kernel smoothing step. For this purpose, the process uses a special method for generating random numbers that follow an Epanechnikov probability density function. The process can be performed autonomously in a navigation system using correlation measurement, in particular on board an aircraft such as an aircraft, a flying drone or any self-propelled aerial carrier.",
      "background": "CROSS-REFERENCE RELATED TO PRIORITY APPLICATIONS\n[0001] This application is the U.S. national phase of International Application No. PCT/EP2020/084829 filed Dec. 7, 2020, which designated the U.S. and claims priority to FR Patent Application No. 191439 filed Dec. 13, 2019, the entire contents of each of which are hereby incorporated by reference.",
      "brief": "BACKGROUND OF THE INVENTION\n\nField of the Invention\n\n[0002] The present invention relates to a particle filtering method, as well as a computing unit and a navigation system using measurement correlation that implement such a method. More particularly, the particle filtering method is of the box regularized type.\n\nDescription of the Related Art\n\n[0003] The navigation function of an aircraft includes estimation of its instant position, its instant speed and its instant orientation, referred to as attitude, in the navigation reference frame, also referred to as the local geographical trihedron. All the instant values of the position coordinates, i.e. the latitude, the longitude and the attitude of the aircraft, of speed, comprising a speed coordinate in the direction of North, a speed coordinate in the direction of East and a downward speed coordinate, and of attitude angles of the aircraft, comprising a roll angle, a pitch angle and a yaw angle, constitute the instant state of the system formed by the aircraft. The aircraft may be an airplane, a drone or any self-propelled aerial carrier, without limitation.\n\n[0004] The acceleration and the angular speed of the aircraft are measured repetitively and at high rate, each along three axes, for example at a repetition frequency of approximately 1000 Hz (hertz), by means of accelerometers and gyrometers of an inertial unit that is on board the aircraft. The navigation system then delivers estimates of position and speed coordinates, and attitude angles of the aircraft, by integrating the results of the accelerometric and gyrometric measurements over time. However, each measurement of acceleration and angular velocity is affected by an error, which is mainly composed of a bias, a scale factor and a random process, and the accumulation of the measurement errors results in an error that affects the estimation of the instant state of the aircraft. This error on the instant state that is estimated increases as a function of time, and is commonly referred to as inertial drift. It is then necessary to associate the inertial unit with at least one additional sensor, in order to correct or reduce the inertial drift.\n\n[0005] Usual methods for correcting or reducing inertial drift consist in using geolocation signals, such as satellite navigation signals, for example of the GNSS type, standing for \u201cglobal navigation satellite system\u201d, or using signals that are produced by beacons located on the ground, for example radionavigation signals or signals of the GBAS type, standing for \u201cground-based augmentation system\u201d, or receiving by radio a location of the aircraft produced by means of radar. However, such methods are sensitive to interference or to decoys, to the availability of coverage of the area where the aircraft is located in positioning or communication signals, etc. It is then desirable in some circumstances to have on board the aircraft a method that is autonomous for correcting and/or reducing inertial drift. For this purpose, it is usual to associate with the inertial unit a telemetry sensor that measures a distance between the aircraft and the ground. This telemetry sensor may be a radio-altimeter, a laser telemeter, etc., without limitation. It measures the distance between the aircraft and the ground along a direction that may be fixed or not with respect to the aircraft. When this distance measurement direction may be variable, its orientation with respect to the aircraft is known. The inertial unit is then also associated with a computing unit that correlates results of successive measurements that are made by the telemetry sensor with the instant state of the aircraft as estimated by the inertial unit. More precisely, a characterization of the zone over which the aircraft is flying is stored on board the aircraft, for example in the form of a relief map that associates a relief height value with each pair of latitude and longitude values. Such a relief map record may be in the form of a table, with the latitude and longitude constituting the inputs of the table, and the relief height values constituting the reading responses in the table. Alternatively, the characterization of the zone being flown over may be stored in the form of an analytical function that makes it possible to compute the relief height values as a function of the latitude or longitude values, or in any other suitable form. Then, at each new estimation of the instant position of the aircraft that is produced by the navigation system, a value of the distance that should exist between the aircraft and the ground is obtained by interrogating the characterization of the zone being flown over as stored on board the aircraft, in accordance with the estimated position of the aircraft. Optionally, the aircraft-ground distance value may result from a computation that combines the estimated position of the aircraft with the result or results of one or more interrogations of the characterization of the zone being flown over that is stored on board the aircraft, in particular when the measurement direction of the telemetry sensor is oblique with respect to the altitude axis. Such a computation is known to persons skilled in the art, so that it is not necessary to repeat it here. The aircraft-ground distance value that is thus estimated is next compared with the measurement result that is delivered by the telemetry sensor. Such navigation method with measurement correlation is commonly referred to as navigation with terrain correlation. Several variants of it have been proposed, but some of them have great sensitivity to the existence of non-linearities of the terrain profiles. In other words, they have the drawback of lack of robustness in their efficiency of convergence towards the true state of the aircraft, according to the possible terrain profiles.\n\n[0006] The navigation methods using terrain correlation that are based on a box-regularized particle filter, or BRPF, make it possible to correct the inertial drift with greater robustness, compatible with the existence of non-linearities and terrain ambiguities. The article by Merlinge, N., Dahia, K., Piet-Lahanier, H., Brusey, J., & Horri, N., which is entitled \u201cA Box Regularized Particle Filter for state estimation with severely ambiguous and non-linear measurements\u201d, Automatica (2019), Vol. 104, pp. 102-110, describes such method. Each of these methods then consists in iteratively computing an instant state of the aircraft from a last state determined previously, and correlating the distance measurement result that is obtained by the telemetry sensor with a distance value that is moreover reconstituted from the characterization of the terrestrial relief loaded on board the aircraft and position and attitude values. However, a box-regularized particle filter proceeds by manipulating state intervals, of nine-dimension when each state of the aircraft is formed by three position coordinates, three speed coordinates and three attitude angles as described above. In addition, a weight value with probabilistic signification is associated with each state interval: the weight of each state interval corresponds to the probability of the true state of the aircraft being located in this state interval. However, these navigation methods with terrain correlation that are based on particle filters have not yet been implemented for real aircraft, because of the very great computing resources that are necessary. This is why it is required, for numerous aeronautical applications, for the navigation method using terrain correlation that is used to be able to be implemented by a computing unit of the field-programable gate array type, or FPGA. However, the circuits of this type have capacities that are still too limited.\n\n[0007] However, it is known that such a box-regularized particle filtering method provides better statistical characterization of the true state of the aircraft if random jamming of the state intervals is added, to reduce correlations that exist between at least some of the state intervals as these state intervals result directly from the particle filtering. This random jamming consists in modifications of the bounds that determine each state interval, or in an equivalent manner modifications that are applied to the center values and lengths of intervals that determine each state interval on all the state coordinates. It is also known that the random jamming that is thus added is best adapted to such particle filtering method when it corresponds to a probability density function of the type f(x) = 3.square-solid.(1 - x.sup.2)/4, referred to as Epanechnikov kernel, where x is a random variable comprised between -1 and 1, the values -1 and 1 being permitted. However, generating random variables according to such Epanechnikov kernel by limiting the computing resources that are necessary is difficult.\n\nSUMMARY OF THE INVENTION\n\n[0008] From this situation, one object of the present invention is to combine a particle filtering with random jamming processes that are each in accordance with an Epanechnikov kernel, while limiting the computing resources necessary.\n\n[0009] A supplementary object of the invention is to provide such a combination that can be used autonomously on board an aircraft. Beyond this, an object of the invention is to contribute to a box-regularized particle filtering method being able to be implemented by a circuit of the FPGA type.\n\n[0010] In order to achieve at least one of these objects or another, a first aspect of the invention proposes a novel box particle filtering method for predicting a state of a system by means of a set of state intervals with weights that are associated with these state intervals, so as to form a probability distribution that characterizes the state of the system. The system concerned may be a terrestrial, airborne, maritime or space vehicle that is provided with a navigation system using measurement correlation. The method of the invention comprises repetitively applying a sequence of steps to the set of state intervals with their associated weights for updating these state intervals and weights. This sequence of steps comprises a so-called smoothing step, which consists in modifying at least one of the state intervals by applying random modifications to a set of interval bounds, or center values and interval lengths, which determine this state interval according to state coordinates of the system. According to the invention, the random modifications relating to each state interval to be modified, which is identified by an integer index i, are determined by executing the following steps: [0011] generating a first random value, denoted \u03b2.sub.i and comprised between 0 and 1, the values 0 and 1 being permitted, according to a statistical-distribution beta law with a first parameter equal to d and a second parameter equal to 2, where d is a number of state coordinates of the system; [0012] generating 2.square-solid.d second random values, denoted v.sub.k,i, each according to a normal statistical distribution law with zero mean value and standard deviation equal to unity, where k is another integer index that varies from 1 to 2.square-solid.d and identifies the interval bounds, or center values and interval lengths, for each state interval; [0013] calculating a first number, denoted \u03be.sub.i, according to the first formula: \u03be.sub.i = [\u03a3.sub.k=1 to 2.square-solid.d (V.sub.k,i).sup.2].sup.\u00bd; [0014] calculating a second number, denoted \u03b1.sub.i, according to the second formula: \u03b1.sub.i = \u03b2.sub.i.sup.\u00bd/\u03be.sub.i; and [0015] calculating 2.square-solid.d third numbers, denoted \u03b5.sub.k,i, according to the third formula: \u03b5.sub.k,i = v.sub.k,i.square-solid.\u03b1.sub.i. Then the random modifications that are applied to the state interval i are proportional one-to-one to the third numbers \u03b5.sub.k,i, with a proportionality coefficient that is non-zero and common to these random modifications.\n\n[0016] The third numbers \u03b5.sub.k,i that are thus generated follow a probability density function of the Epanechnikov kernel type. Furthermore, the aforementioned steps can easily be implemented autonomously by a computing unit, in particular of the FPGA type, which is installed on board a vehicle, without communication with external computing means.\n\n[0017] Preferably, all the interval bounds, or center values and interval lengths, which determine the state interval i, may be modified by implementing the following steps: [0018] combining the random modifications that relate to this state interval i using a square matrix of dimension 2.square-solid.d, so as to obtain 2.square-solid.d combinations of random modifications; then [0019] adding the combinations of random modifications that are thus obtained one-to-one to the interval bounds, or center values and interval lengths, of the state interval i.\n\n[0020] Furthermore, and preferably, the matrix that is used for combining the random modifications may be such that the product of this matrix multiplied by its transpose is equal to a mean product matrix, where the mean product matrix is square of dimension 2.square-solid.d, and has as coefficients mean values that are calculated over all the state intervals, of products of the interval bounds, or center values or interval lengths, taken in pairs separately for each state interval.\n\n[0021] In preferred embodiments of the invention, each first random value \u03b2.sub.i may be generated using an algorithm that combines: [0022] a generation of two random numbers each according to a uniform statistical distribution law; and [0023] at least one acceptance criterion that is based on the two random numbers, such that, if the at least one acceptance criterion is satisfied, a first of the two random numbers is used for calculating the first value \u03b2.sub.i, otherwise the generation of the two random numbers is recommenced.\n\n[0024] Advantageously, each of the two random numbers may be generated using a method of the shift register type with linear feedback.\n\n[0025] Such embodiments allow implementations of the method of the invention by a computing unit that is autonomous, while reducing even further the computing resources of this unit. Furthermore, the first values \u03b2.sub.i that are thus generated ensure that the random interference that is applied to the state intervals is in accordance with an Epanechnikov kernel. In particular, Cheng\u2019s algorithm, known to persons skilled in the art, may be used for generating each first value \u03b2.sub.i.\n\n[0026] Independently, each second random value v.sub.k,i may be calculated as a sum of several initial random values, each of these initial random values being generated according to a uniform statistical distribution law. In this case, each initial random value can likewise be generated using a method of the linear-feedback shift register type. Such methods for generating second random values v.sub.k,i also facilitate implementations of the method of the invention by a computing unit that is autonomous, while further reducing the computing resources of this unit. Furthermore, the second values v.sub.k,i that are thus generated also ensure that the random jamming that is applied to the state intervals is in accordance with an Epanechnikov kernel.\n\n[0027] Particularly advantageously, respective estimations of each first number \u03be.sub.i and/or of each second number \u03b1.sub.i may be obtained using the following steps at least once, where X is a positive or zero variable number and \u03b1 is an exponent value equal to 2 or \u00bd: [0028] /a/ writing the number X in the form X = (1+m).Math.2.sup.ex, where ex is a negative, positive or zero integer, and m is a mantissa comprised between 0 and 1, the value 0 and being permitted, so that a binary representation of the number X is: [0029] I(X) = L.Math.(m + ex + B), where L=2.sup.n with n being a number of bits of a binary writing of the mantissa m, and B being a positive or zero constant number, referred to as bias; /b/ calculating a binary representation of X.sup.\u03b1 in the form: [0030] I(X.sup.\u03b1) = \u03b1.Math.I(X) + L.Math.(1 - a).Math.(B - \u03c3), where \u03c3 is a constant number the value of which is recorded ; and [0031] /c/ obtaining the estimation of the value of X.sup.\u03b1 from the binary representation I(X.sup.\u03b1). Steps /a/-/c/ are then applied to X = \u03a3.sub.k=1 .sub.\u00e0 .sub.2.Math.d (v.sub.k,i).sup.2 with \u03b1=\u00bd, to obtain an estimation of the first number \u03be.sub.i . [0032] Steps /a/-/c/ may optionally be applied previously to an absolute value of each second random value, according to X = |v.sub.k,i| , with \u03b1=2. [0033] Steps /a/-/c/ may also be applied to X = \u03b2.sub.i with \u03b1=\u00bd, to obtain an estimation of the second number \u03b1.sub.i as a result of a division of the estimation of \u03b2.sub.i.sup.\u00bd that is thus obtained by the estimation of the first number \u03be.sub.i.\n\n[0034] Such calculations, which on each occasion replace the estimation of the function of X.sup.\u03b1 by a calculation based on the binary representation of the number X, are particularly economical in computer resources, and short in terms of computing time. Furthermore, they can also be performed by a circuit of the FPGA type.\n\n[0035] In these advantageous implementations of the invention that use binary representations of the numbers, at least one of the following additional features may be reproduced, alone or in combination of a plurality of them: [0036] the number n of the bits of the binary writing of the mantissa m may be equal to 23, and the bias B may be equal to 127; [0037] the constant number \u03c3 may be between 0 and 1, preferably between 0 and 0.5; and [0038] the obtaining of the estimation of the binary value of X.sup.\u03b1 may be completed by at least once executing the following additional step, after the step /c/: [0039] /d/ calculating a new estimation of the value of X.sup.\u03b1 from a prior estimation of the value of X.sup.\u03b1, by applying an approximate equation resolution recursive algorithm to the equation Y.sup.\u215f\u03b1 - X =0 of unknown Y, the estimation of the value of X.sup.\u03b1 that was obtained at the step /c/ being used as prior estimation for a first application of the algorithm, and the new estimation of the value of X.sup.\u03b1 that is produced by a q.sup.th application of the algorithm forming the prior estimation of the value of X.sup.\u03b1 for the (q+1).sup.th application of the algorithm, if such a (q+1).sup.th application of the algorithm is carried out, q being an integer greater than or equal to 1. For example, the approximate equation resolution recursive algorithm that is used at the step /d/ may be Newton\u2019s method.\n\n[0040] Generally for the invention, the sequence of steps that is applied repetitively for updating the state intervals with the weights that are associated therewith may comprise the following steps /1/ to /5/:  [0041] /1/ a prediction step, comprising predicting subsequent state intervals, each subsequent state interval being obtained by applying at least one propagation rule to one of a plurality of prior state intervals; [0042] /2/ a step of measuring a true state of the system; [0043] /3/ a step of contracting at least one of the subsequent state intervals, according to at least one result of measuring the true state that was obtained at step /2/; [0044] /4/ a weight updating step, comprising allocating a weight to each subsequent state interval according to a size of this subsequent state interval as resulting from step /3/, a size of the subsequent state interval as resulting from step /1/ before step /3/, and a weight of the prior state interval from which the subsequent state interval resulted during step /1/; and [0045] /5/ a step of redistributing the state intervals, comprising replacing at least one of the subsequent state intervals by a plurality of sub-intervals that result from a division of the subsequent state interval, each sub-interval forming a new state interval, this redistribution step comprising applying the smoothing step to at least each new state interval. Then the state intervals as resulting from an implementation of the sequence of steps /1//5/, comprising the new state intervals and subsequent state intervals that were kept without being replaced by a plurality of new state intervals, including optionally certain state intervals to which the smoothing step cannot have been applied, constituting the prior state intervals for a following implementation of the sequence of steps /1/ to /5/.\n\n[0046] A second aspect of the invention proposes a computing unit, this computing unit comprising at least one first input that is adapted for receiving results of repeated measurements of acceleration and angular velocity of a system, and a second input that is adapted for receiving results of repeated measurements of a true state of the system, supplementary with respect to the acceleration and angular velocity measurements. The computing unit is then arranged for implementing a box-regularized particle filtering method that is in accordance with the first aspect of the invention. In this way, the computing unit outputs a series of state intervals with respective weights, the weight that is associated with each of the state intervals corresponding to a probability of the true state of the system being in this state interval.\n\n[0047] Such a computing unit may be one of the following types: [0048] field programmable gate array, or FPGA, [0049] fixed-gate network circuit, known by the acronym DSP, standing for \u201cDigital Signal Processor\u201d, and [0050] central processing unit, known by the acronym CPU standing for \u201cComputer Processing Unit\u201d or RISC standing for \u201cReduced Instruction Set Computer\u201d.\n\n[0051] A third aspect of the invention relates to a navigation system using measurement correlation, which is adapted for being installed on board a vehicle and which comprises:  [0052] an inertial unit, which is adapted for iteratively measuring accelerations and angular velocities of the vehicle, and for deducing, by using results of measurements of accelerations and angular velocities, subsequent state intervals respectively from a plurality of prior state intervals, each state of the vehicle comprising position, speed and attitude coordinates of this vehicle; [0053] a measurement system, which is adapted for iteratively measuring at least one feature of a true state of the vehicle; and [0054] a computing unit that is in accordance with the second aspect of the invention, and which is adapted for reducing at least one position, speed and/or attitude drift of the inertial unit, using the results of measurement of the feature of the true state of the vehicle that are delivered by the measurement system.\n\n[0055] Finally, a fourth aspect of the invention proposes a vehicle that comprises a navigation system using measurement correlation in accordance with the third aspect of the invention. Such a vehicle may be an aircraft, in particular an airplane, a flying drone or any self-propelled air carrier, or a vehicle capable of moving on the ground, in particular a drone able to move on the ground, or a ship, a submarine, a spacecraft, in particular a space probe, a satellite, etc., without limitation. According to the circumstances, the measurement system may be a telemetry sensor that is intended to measure a distance between the vehicle and a point on the ground, a system for locating seamarks or stars, a sonar measuring a height of water under the ship or the submarine, etc.",
      "claims": "1. A box-regularized particle filtering method, for predicting a state of a system by a set of state intervals with weights associated with said state intervals, so as to form a probability distribution that characterizes the state of the system, said system being a land, air, sea or space vehicle that is provided with a navigation system using measurement correlation, the method comprising repetitively applying a sequence of steps to the set of state intervals with the associated weights for updating the state intervals and said associated weights, the sequence of steps comprising a so-called smoothing step that consists in modifying at least one of the state intervals by applying random modifications to a set of interval bounds, or center values and interval lengths, that determine the state interval according to state coordinates of the system, wherein the random modifications relating to each state interval to be modified, which is identified by an integer index i, are determined by executing the following steps: generating a first random value, denoted \u03b2.sub.i and comprised between 0 and 1, the values 0 and 1 being permitted, according to a statistical-distribution beta law with a first parameter equal to d and a second parameter equal to 2, where d is a number of state coordinates of the system; generating 2.Math.d second random values, denoted v.sub.k,i, each according to a normal statistical distribution law with zero mean value and standard deviation equal to unity, where k is another integer index that varies from 1 to 2.Math.d and identifies the interval bounds, or center values and interval lengths, for each state interval; calculating a first number, denoted \u03be.sub.i, according to the first formula: \u03be.sub.i = [\u03a3.sub.k=1 .sub.to .sub.2.Math..sub.d (V.sub.k,i).sup.2].sup.\u00bd., calculating a second number, denoted \u03b1.sub.i, according to the second formula: \u03b1.sub.i = \u03b2.sub.i.sup.\u00bd/\u03be.sub.i; and calculating 2.Math.d third numbers, denoted \u03b5.sub.k,.sub.i, according to the third formula: \u03b5.sub.k,.sub.i = V.sub.k,i\u2022\u03b1.sub.i, and wherein the random modifications that are applied to the state interval i are proportional one-to-one to the third numbers \u03b5.sub.k,i, with a proportionality coefficient that is non-zero and common to said random modifications.  \n\n2. The method according to claim 1, wherein all the interval bounds, or center values and interval lengths, which determine the state interval i, are modified by implementing the following steps: combining the random modifications that relate to said state interval i using a square matrix of dimension 2.Math.d, so as to obtain 2.Math.d combinations of random modifications; then adding said combinations of random modifications that are thus obtained one-to-one to the interval bounds, or center values and interval lengths, of the state interval i.  \n\n3. The method according to claim 2, wherein the matrix that is used for combining the random modifications is such that the product of said matrix multiplied by a transpose of said matrix is equal to a mean product matrix, said mean product matrix being square of dimension 2.Math.d, and having as coefficients mean values that are calculated over all the state intervals, of products of the interval bounds, or center values or interval lengths, taken in pairs separately for each state interval. \n\n4. The method according to claim 1, wherein each first random value \u03b2.sub.i is generated using an algorithm that combines: a generation of two random numbers each according to a uniform statistical distribution law; and at least one acceptance criterion that is based on the two random numbers, such that, if said at least one acceptance criterion is satisfied, a first of the two random numbers is used for calculating the first value \u03b2.sub.i, otherwise the generation of the two random numbers is recommenced.  \n\n5. The method according to claim 4, wherein each of the two random numbers is generated using a method of the shift register type with linear feedback. \n\n6. The method according to claim 1, wherein each second random value V.sub.k,i is calculated as a sum of several initial random values, each of said initial random values being generated according to a uniform statistical distribution law. \n\n7. The method according to claim 1, wherein respective estimations of each first number \u03be.sub.i and of each second number \u03b1.sub.i are obtained using the following steps at least once, where X is a positive or zero variable number and \u03b1 is an exponent value equal to 2 or \u00bd: /a/ writing the number X in the form X = (1+m).2.sup.ex, where ex is a negative, positive or zero integer, and m is a mantissa comprised between 0 and 1, the value 0 and being permitted, so that a binary representation of the number X is: I(X) = L.Math.(m + ex + B), where L=2.sup.n with n being a number of bits of a binary writing of the mantissa m, and B being a positive or zero constant number, referred to as bias; /b/ calculating a binary representation of X.sup.\u03b1 in the form: I(X.sup.\u03b1) = \u03b1.Math.I(X) + L.Math.(1 - \u03b1).Math.(B - \u03c3), where \u03c3 is a constant number the value of which is recorded ; and /c/ obtaining the estimation of the value of X.sup.\u03b1 from the binary representation I(X.sup.\u03b1), steps /a/-/c/ being applied to X = E.sub.k=1 \u00e0 .sub.2.Math.d (V.sub.k,.sub.i).sup.2 with \u03b1=\u00bd, to obtain an estimation of the first number \u03be.sub.i , and steps /a/-/c/ being applied to X = \u03b2.sub.i with \u03b1=\u00bd, to obtain an estimation of the second number (\u03b1.sub.i as a result of a division of the estimation of (\u03b2.sub.i.sup.\u00bd by the estimation of the first number \u03be.sub.i.  \n\n8. The method according to claim 7, wherein the obtaining of the estimation of the value of X.sup.\u03b1 is completed by at least once executing the following additional step, after the step /c/: /d/ calculating a new estimation of the value of X.sup.\u03b1 from a prior estimation of the value of X.sup.\u03b1, by applying an approximate equation resolution recursive algorithm to the equation Y.sup.\u215f\u03b1 - X =0 of unknown Y, the estimation of the value of X.sup.\u03b1 that was obtained at the step /c/ being used as prior estimation for a first application of said algorithm, and the new estimation of the value of X.sup.\u03b1 that is produced by a q.sup.th application of the algorithm forming the prior estimation of the value of X.sup.\u03b1 for the (q+1).sup.th application of said algorithm, if such (q+1).sup.th application of the algorithm is carried out, q being an integer greater than or equal to 1.  \n\n9. The method according to claim 1, wherein the sequence of steps that is applied repetitively for updating the state intervals with the weights that are associated with said state intervals comprises the following steps /1/ to /5/: /1/ a prediction step, comprising predicting subsequent state intervals, each subsequent state interval being obtained by applying at least one propagation rule to one of a plurality of prior state intervals; /2/ a step of measuring a true state of the system; /3/ a step of contracting at least one of the subsequent state intervals, according to at least one result of measuring the true state that was obtained at step /2/; /4/ a weight updating step, comprising allocating a weight to each subsequent state interval according to a size of said subsequent state interval as resulting from step /3/, a size of said subsequent state interval as resulting from step /1/ before step /3/, and a weight of the prior state interval from which the subsequent state interval resulted during step /1/; and /5/ a step of redistributing the state intervals, comprising replacing at least one of the subsequent state intervals by a plurality of sub-intervals that result from a division of the subsequent state interval, each sub-interval forming a new state interval, said redistribution step comprising applying the smoothing step to at least each new state interval, the state intervals as resulting from an implementation of the sequence of steps /1/-/5/, comprising the new state intervals and subsequent state intervals that were kept without being replaced by a plurality of new state intervals, being intended to constitute the prior state intervals for a following implementation of the sequence of steps /1/ to /5/.  \n\n10. A computing unit, comprising at least one first input adapted for receiving results of repeated measurements of acceleration and angular velocity of a system, and a second input adapted for receiving results of repeated measurements of a true state of the system, supplementary with respect to the acceleration and angular velocity measurements, and the computing unit being arranged for implementing a box-regularized particle filtering method that is in accordance with claim 1, so as to output a series of state intervals with respective weights, the weight that is associated with each of the state intervals corresponding to a probability of the true state of the system being in said state interval. \n\n11. The computing unit according to claim 10, of a type consisting of a circuit with an array of programmable ports, a circuit with an array of fixed ports, or a central processor unit. \n\n12. A navigation system using measurement correlation, adapted for being installed on board a vehicle, comprising: an inertial unit, adapted for iteratively measuring accelerations and angular velocities of the vehicle, and for deducing, by using results of measurements of accelerations and angular velocities, subsequent state intervals respectively from a plurality of prior state intervals, each state of the vehicle comprising position, speed and attitude coordinates of said vehicle; a measurement system, adapted for iteratively measuring at least one feature of a true state of the vehicle vehicle and a computing unit in accordance with claim 10, and adapted for reducing at least one drift of the inertial unit, from a position drift, a speed drift and an attitude drift, using the results of measurement of the feature of the true state of the vehicle that are delivered by the measurement system.  \n\n13. A vehicle, comprising a navigation system using measurement correlation that is in accordance with claim 12. \n\n14. The method of claim 7, wherein steps /a/-/c/ are applied previously to an absolute value of each second random value, according to X = |V.sub.k,i|, with \u03b1=2. \n\n15. The method according to claim 2, wherein each first random value \u03b2i is generated using an algorithm that combines: a generation of two random numbers each according to a uniform statistical distribution law; and at least one acceptance criterion that is based on the two random numbers, such that, if said at least one acceptance criterion is satisfied, a first of the two random numbers is used for calculating the first value \u03b2i, otherwise the generation of the two random numbers is recommenced.  \n\n16. The method according to claim 3, wherein each first random value \u03b2i is generated using an algorithm that combines: a generation of two random numbers each according to a uniform statistical distribution law; and at least one acceptance criterion that is based on the two random numbers, such that, if said at least one acceptance criterion is satisfied, a first of the two random numbers is used for calculating the first value \u03b2i, otherwise the generation of the two random numbers is recommenced.  \n\n17. The method according to claim 2, wherein each second random value \u03bd.sub.k,.sub.i is calculated as a sum of several initial random values, each of said initial random values being generated according to a uniform statistical distribution law. \n\n18. The method according to claim 3, wherein each second random value \u03bd.sub.k,.sub.i is calculated as a sum of several initial random values, each of said initial random values being generated according to a uniform statistical distribution law. \n\n19. The method according to claim 4, wherein each second random value \u03bd.sub.k,.sub.i is calculated as a sum of several initial random values, each of said initial random values being generated according to a uniform statistical distribution law. \n\n20. The method according to claim 5, wherein each second random value \u03bd.sub.k,.sub.i is calculated as a sum of several initial random values, each of said initial random values being generated according to a uniform statistical distribution law.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0056] The features and advantages of the present invention will emerge more clearly in the following detailed description of non-limitative example embodiments, with reference to the accompanying figures, among which:\n\n[0057] FIG. 1 shows an aircraft that is equipped with an inertial unit using terrain correlation according to the invention;\n\n[0058] FIG. 2 is a diagram that shows the concatenation of steps of a method according to the invention;\n\n[0059] FIG. 3 is a diagram that details the implementation of a smoothing step according to the invention; and\n\n[0060] FIG. 4is a flow diagram of an algorithm that can be used for implementing the invention.\n\nDESCRIPTION OF THE PREFERRED EMBODIMENTS\n\n[0061] For clarity sake, the dimensions of the elements that are shown symbolically in FIG. 1 correspond neither to real dimensions nor to ratios of real dimensions. Furthermore, the invention is described by way of non-limitative example for a case of application to an aircraft, but it is understood that it can be applied to any vehicle that is provided with a navigation system using measurement correlation, whether this vehicle be terrestrial, airborne, maritime, space, etc.\n\n[0062] A method for calculating an estimation of the value of X.sup.\u03b1 that can be used in the invention is described first. X is a number of variable value, positive or zero, and \u03b1 designates an exponent that can be equal to 2 or \u00bd.\n\n[0063] In a known manner, the number X can be written uniquely in the following form, in accordance with IEEE 754:\n\nX = (1+m)\u22c52ex,\n\nwhere ex is a positive or zero integer, and m is a mantissa comprised between 0 and 1, the value zero also being permitted. The number ex and the mantissa m thus depend on the value of the number X.\n\n[0064] This thus gives\n\nlog2X = ex + m + \u03c3,\n\nwhere \u03c3 is a fixed real number making it possible to minimize an error of the value of log.sub.2X, in particular when a numeric interval thereof is known a priori for the number X. For example, the value of the number \u03c3 may be taken to be equal to 0.043036. And therefore:\n\nex + m = log2X - \u03c3\u2009\u2009.\n\n[0065] Moreover, the number X can be represented in binary form by I(X) defined by:\n\nI(X) = L\u22c5(m + ex + B),\n\nwhere L=2.sup.n, with n being a number of bits set for writing the mantissa m in binary form, and B is a positive or zero constant number, which is referred to as bias. In the representation I(X) of the number X, L, m, ex and B are expressed in binary form. For example, n may be equal to 23, and B may be equal to 127. By transferring into the binary representation I(X) the expression of ex + m as coming from log.sub.2X, this gives:\n\nI(X) = L\u22c5log2X - \u03c3+B,\u2004i.e.: log2X = I(X)/L + \u03c3 - B\u2009\u2009.\n\n[0066] However, in the same way as I(X) in the previous line, the binary representation of X.sup.\u03b1 is:\n\nI(X\u03b1)=L\u22c5log2X\u03b1\u2212\u03c3+B\n\nHowever, log.sub.2(X.sup.\u03b1) = \u03b1.Math.log.sub.2(X), therefore: I(X.sup.\u03b1) = L.Math.[\u03b1.Math.log.sub.2(X) - \u03c3 + B], and, by replacing log.sub.2(X) with its expression as a function of the binary representation I(X), this gives:\n\nIXa=L\u22c5a\u22c5IX/L + \u03c3 - B - \u03c3 + B, i.e.: IXa=a\u22c5IX + L\u22c51\u2004-\u2004a\u22c5B - \u03c3\n\nAn approximate value of X.sup.\u03b1, denoted Y.sub.0, can then be reconstructed from the binary representation of X.sup.\u03b1 that was thus obtained, using a method that is the inverse of the one that supplies the binary representation of a number from this number. The difference between this approximate value Y.sub.0 and the true value of X.sup.\u03b1 depends on the value that was adopted for the number \u03c3. For many applications, the approximate value Y.sub.0 is suitable in a satisfactory manner for replacing X.sup.\u03b1, considering the simplicity of the method for obtaining this approximate value Y.sub.0, as has just been described.\n\n[0067] For the particular case where the exponent \u03b1 is equal to 2:\n\nIX2=2\u22c5IX - L\u22c5B - \u03c3\n\n[0068] For the particular case where the exponent \u03b1 is equal to \u00bd:\n\nIX1/2=IX/2 + 0,5\u22c5 L\u22c5B - \u03c3\n\n[0069] For applications where the approximate value Y.sub.0 does not constitute a sufficiently precise evaluation of X.sup.\u03b1, it is possible to improve this evaluation by using one of the methods for refining approximate values that are known to a person skilled in the art. Newton\u2018s algorithm, also referred to as Newton\u2019s method, may be used in particular, by applying it to the function f(Y) = Y.sup.\u215f\u03b1 - X and to the equation f(Y) = 0. Successive approximate values Y.sub.q, q being an integer index for numbering these values, can thus be obtained according to the formula: Y.sub.q+1 = Y.sub.q - f(Y.sub.q)/f\u2019(Y.sub.q), where f\u2018(Y.sub.q) is the value of the function derived from f, estimated for the value Y.sub.q. That is, when calculating the expression of f\u2019(Y) from the expression of f(Y):\n\nYq+1=1 - a\u22c5Yq+a\u22c5X\u22c5Yqa-1/a, for q = 0, 1, 2,\n\n[0070] For the particular case where the exponent \u03b1 is equal to 2, this gives:\n\nYq+1 = -Yq+ 2\u22c5X\u22c5Yq\u2004\u20091/2\n\nIn particular, the first-order approximate value of X.sup.2 is:\n\nY1=\u2212Y0+2\u22c5X\u22c5Y01/2\u2009\u2009.\n\nThe value of Y.sub.q.sup.\u00bd can be estimated each time using the formula I(X.sup.\u03b1) = \u03b1.Math.I(X) + L.Math.(1 - \u03b1).Math.(B - \u03c3), and by replacing in this formula X by Y.sub.q and \u03b1 by \u00bd.\n\n[0071] For the particular case where the exponent \u03b1 is equal to \u00bd, this gives:\n\nYq+1=0,5\u22c5Yq+0,5\u22c5X/Yq\u2009\u2009.\n\nIn particular, the first-order approximate value of X.sup.\u00bd is:\n\nY1=0,5\u22c5Y0+0,5\u22c5X/Y0\u2009\u2009.\n\n[0072] The approximate value Y.sub.0, as obtained using the binary representations of numbers, and the values Y.sub.q, .sub.q\u22651, as obtained using one of the methods for refining approximate values such as Newton\u2019s method, does not require extensive computing resources. They can therefore be calculated easily by a computing unit of the FPGA, DSP, CPU or RISC type.\n\n[0073] In accordance with FIG. 1, an aircraft 20 is equipped with a navigation unit using terrain correlation, designated by the reference 10. The navigation unit 10 comprises an inertial unit 1, a telemetry sensor 2 and a computing unit 3. In a known manner, the inertial unit 1 repetitively makes measurements of three acceleration coordinates and of three angular velocity coordinates of the aircraft 20, by means of accelerometers and gyrometers, not shown. Moreover, the telemetry sensor 2 repetitively makes measurements of the distance H that exists between the ground 100 and the aircraft 20. This distance H is measured in a direction that may be fixed with respect to the aircraft 20, for example perpendicular to a reference plane of the aircraft. Optionally, the telemetry measurement direction may be variable with respect to the aircraft 20, but in such case this measurement direction is controlled and taken into account in a suitable manner, and known moreover. The distance H therefore varies depending on the terrestrial relief over which the aircraft 20 is flying, and also depending on its altitude and attitude. Hereinafter, the present description is limited solely to the case where the measurements that are supplementary with respect to those of the inertial unit 1 consist of the measurements of the distance H. This case corresponds to a navigation with terrain correlation. However, it is understood that other supplementary measurements may be used alternatively to those of the distance H, or in addition to them. Finally, the invention is compatible with inertial unit and telemetry sensor models as commercially available. In particular, the inertial unit may be of an MEMS type, standing for \u201cMicro-Electro-Mechanical System\u201d, of the quadrason type, of the gyrolaser type, etc., and the telemetry sensor may be a radio altimeter, a laser telemeter, etc.\n\n[0074] Each possible state for the aircraft 20 may be composed of three spatial coordinate values that identify a position for the aircraft, for example in the terrestrial reference frame, three velocity values, each according to one of the spatial coordinates, and three angular values for identifying an orientation of the aircraft, for example three Euler angle values, for a total of nine state coordinates. Under these conditions, a state interval for the aircraft 20 is formed by a combination of nine unidimensional intervals that each relate separately to one of the state coordinates. Such a state interval is referred to as a box in the jargon of a person skilled in the art.\n\n[0075] The box-regularized particle filtering method is initialized by the supply of several initial state intervals, each associated with a weight that indicates a probability of the true state of the aircraft 20 being located initially in this initial state interval. Thus, N initial state intervals are supplied, N being an integer, for example equal to 16 or 32, preferably less than or equal to 128. Each initial state interval is individually associated with a weight value, which may be equal to 1/N.\n\n[0076] The method next consists in successive iterations of a sequence of steps, each new implementation of the sequence of steps producing an updating of state intervals, with updated values of weights that are associated one-to-one with the updated state intervals. Furthermore, each new implementation of the sequence of steps is executed from the state intervals and their associated weight values as supplied by the just prior implementation of the sequence of steps.\n\n[0077] Each sequence of steps comprises a prediction step denoted /1/ in FIG. 2, a measurement step denoted /2/, a contraction step denoted /3/, a step /4/ of updating the weight of each state interval, and a step /5/ of redistribution of the state intervals. Steps /1/ and /3/ to /5/ are implemented for each state interval. Since step /5/ produces a redistribution of the state intervals as resulting from step /3/, it is preferably implemented so as to keep a constant number of state intervals. Then the computing unit 3 can be designed and sized to process N state intervals at each implementation of the sequence of steps /1/ to /5/. i is an integer index from 1 to N, which counts the state intervals that are processed at each iteration of this sequence of steps /1/ to /5/.\n\n[0078] For each state interval i, denoted box_i, the prediction step /1/ consists in collecting the results of last measurements of acceleration and angular velocity, as delivered by the inertial unit 1. Optionally, the results of several measurements that were made by the inertial unit 1 since the previous implementation of the sequence of steps /1/ to /5/ can be collected. For each of the box_i state intervals, this step /1/ also comprises calculating a change in this state interval during the period of time that has elapsed between the two implementations of the step /1/, relating to the box_i state interval, from the results of the measurements of acceleration and angular velocity, is assumed to be known to a person skilled in the art. In this regard reference can be made to the article by Merlinge, N., Dahia, K., Piet-Lahanier, H., Brusey, J., & Horri, N., which is entitled \u201cA Box Regularized Particle Filter for state estimation with severely ambiguous and non-linear measurements\u201d, Automatica (2019), Vol. 104, pp. 102-110. This step /1/ results in a translation, usually accompanied by a change in length, of each unidimensional interval that relates to one of the state coordinates of the aircraft 20. In the general part of the present description, each box_i state interval as existing at the moment of starting the execution of step /1/ has been called prior state interval, and this box_i state interval as modified by the step /1/ has also been called subsequent state interval.\n\n[0079] The measurement step /2/ consists in collecting the result of the last distance measurement H as delivered by the telemetry sensor 2. Optionally, the results of a plurality of last measurements that were made by the telemetry sensor 2 since the previous execution of step /2/, at the time of the previous iteration of the sequence of steps /1/ to /5/, may be collected.\n\n[0080] The contraction step /3/ consists in reducing the size of each box_i state interval according to any incompatibilities that might exist between parts of this state interval and the last result or results of distance measurement H collected at step /2/. Typically, the six position and attitude coordinates of the aircraft 20, such as possibly varying within each box_i state interval resulting from step /1/, are combined with relief height values read in a terrain map that is stored, in order to obtain an estimation of the distance H associated with each state. Such calculation may be carried out in the known manner that has been reminded at the start of this description. The distance H thus estimated is compared with the result of the measurement of step /2/. Generally, an implicit equation can be used for converting each state of the aircraft 20 into a distance value H, using the terrain map stored. However, such equation, referred to as an observation equation by a person skilled in the art, may be difficult to invert locally, so that modeling its inverse function by an analytical function part and a tabulated function part may be advantageous. A person skilled in the art will also be able to refer to the article by Merlinge et al., Automatica 104(2019), cited above, with regard to step /3/, to which the improvement to the method procured by the invention does not directly relate.\n\n[0081] The updating step /4/ consists in updating the value of the weight of each state interval as resulting from step /3/. For example, a new value of the weight of the box_i state interval may be equal to the value of the weight of this box_i state interval as existing before implementing this update, multiplied by the size of the box_i state interval as resulting from the contraction step /3/, and divided by the size of the box_i state interval as resulting from the prediction step /1/ before applying the contraction step /3/. Other formulae for updating the weight values may be adopted alternatively. Optionally, the weight values as resulting from one of these formulae may also be corrected, for example by multiplying them by a non-zero common factor, to ensure that the sum thereof is equal to unity.\n\n[0082] The purpose of step /5/ is to redistribute the state intervals as resulting from step /3/, to obtain a better statistical representativeness of the states that are possible for the aircraft 20. The implementation of this step /5/ may be subject to the result of an optional test designated by CR in FIG. 2. This test consists in determining whether a representativity criterion is satisfied for the state intervals with their respective weight values. It relates to all the values of the weights W.sub.i as updated at step /4/, where W.sub.i is the weight that is associated with the box_i state interval. A first representativeness criterion that may be used is the one known by the term N-effective criterion. This criterion is satisfied if (\u03a3.sub.i=1....sub.N w.sub.i.sup.2).sup.-1 < \u03b8.sub.eff.Math.N, where \u03b8.sub.eff is an adjustment parameter of the N-effective criterion, comprised between 0 and 1. Another representativity criterion that is also possible is the one that is known by the term entropic criterion: it is satisfied if log(N) + \u03a3.sub.i=1...N w.sub.i.Math.log(w.sub.i) > \u03b8.sub.ent.Math.N, where \u03b8.sub.ent is an adjustment parameter of the entropic criterion, comprised between 0 and 1. However, other representativity criteria that are also known to a person skilled in the art may also be used alternatively. Step /5/ of redistribution of the state intervals is then applied if the representativeness criterion is not satisfied.\n\n[0083] Step /5/ first comprises determining a division number n.sub.i that is attributed to the box_i state interval, with the index i also numbering the state intervals from 1 to N. This sub-step is denoted /5-1/ in FIG. 2. In a known manner, it can be executed using a multinomial drawing method. Such a method consists in drawing points randomly and repetitively within a unidimensional segment, to determine the number n.sub.i of sub-intervals that are intended to replace the box_i state interval, according to the values of the weights w.sub.i of all the state intervals as updated at step /4/. The drawing segment is formed by a juxtaposition of base segments, the individual lengths of which correspond one-to-one to the values of the weights of the state intervals. The number n.sub.i of sub-intervals that will replace the box_i state interval is then proportional to the number of randomly drawn points that belong to the base segment whose the length is equal to the weight value of the box_i state interval. If the value of n.sub.i that is determined for one of the state intervals is zero, this state interval is suppressed, otherwise the box_i state interval is divided into n.sub.i sub-intervals, the value obtained for n.sub.i being statistically all the greater, the higher the weight of the box_i state interval as resulting from step /4/. Optionally, the values of n.sub.i that are thus obtained may be multiplied by a constant factor and rounded, in order to limit the total number of sub-intervals used, and/or to ensure that the sum of all the values of n.sub.i is equal to N. However, this way of implementing sub-step /5-1/ is provided only by way of example, and different methods may be used alternatively in variant embodiments of the invention.\n\n[0084] Each box_i state interval is then intended to be divided into n.sub.i sub-intervals, at sub-step /5-3/, for example in one of the ways that are described in the following articles: \u201cAn introduction to box particle filtering [lecture notes]\u201d by Gning, A., Ristic, B., Mihaylova, L., & Abdallah, F., IEEE Signal Processing Magazine (2013), Vol. 30(4), pp. 166-171; and \u201cA Box Regularized Particle Filter for state estimation with severely ambiguous and non-linear measurements\u201d by Merlinge, N., Dahia, K., Piet-Lahanier, H., Brusey, J., & Horri, N., Automatica (2019), Vol. 104, pp. 102-110, already cited above.\n\n[0085] These division methods consist in first determining which one of the state coordinates of the system is that where the box_i state interval is the most extended. This determination is the object of sub-step /5-2/, which is now described.\n\n[0086] Each of the state intervals box_i relates to a plurality of state coordinates of at least two different types. For example, in case of the navigation unit 10 that was described above for the aircraft 20, the three position coordinates, denoted x.sub.1, x.sub.2 and x.sub.3, are state coordinates of a first type, the three speeds, denoted v.sub.1, v.sub.2 and v.sub.3, are state coordinates of second type, and the three attitude angles, denoted \u03b8.sub.1, \u03b8.sub.2 and \u03b8.sub.3, of the aircraft 20 are state coordinates of a third type. Thus normalized dimensionless values of the lengths of the unidimensional intervals of the box_i state interval, according to each of the state coordinates, are:\n\n\u0394xj_n=\u0394Xj\u22c5\u0394X12+\u0394X22+\u0394X32\u22121/2,\n\n\u0394vj_n=\u0394Vj\u22c5\u0394V12+\u0394V22+\u0394V32\u22121/2,\n\nand\n\n\u0394\u03b8j_n=\u0394\u03b8j\u22c5\u0394\u03b812+\u0394\u03b822+\u03b832\u22121/2,\n\nfor j=1, 2 and 3 in each case, where \u2206x.sub.j, \u2206v.sub.j and \u2206\u03b8.sub.j are lengths of the respective unidimensional intervals of the nine state coordinates for the box_i state interval. Each of the values \u2206x.sub.j.sup.2, \u2206v.sub.j.sup.2 and \u2206\u03b8.sub.j.sup.2 can be calculated using the method that was presented above for X.sup.\u03b1, with \u03b1 equal to 2. Then each of the factors (\u2206x.sub.1.sup.2+\u2206x.sub.2.sup.2+\u2206x.sub.3.sup.2).sup.-\u00bd, (\u2206v.sub.1.sup.2+\u2206v.sub.2.sup.2+\u2206v.sub.3.sup.2).sup.-\u00bd and \u2206\u03b8.sub.1.sup.2+\u2206\u03b8.sub.2.sup.2+\u2206.sub.3.sup.2).sup.-\u00bd can also be calculated using the same method, but with \u03b1 then being equal to -\u00bd. The normalized values \u2206x.sub.j_n, \u2206v.sub.j_n and V\u03b8.sub.j_n of the lengths of the new unidimensional intervals of the box_i state interval are then obtained in accordance with previous formulae, by calculation of products.\n\n[0087] These normalized values \u2206x.sub.j_n, \u2206v.sub.j_n and \u2206\u03b8.sub.j_n can be compared with each other, and the state coordinate where the box_i state interval is the most extended is the one that corresponds to the highest of the normalized values \u2206x.sub.j_n, \u2206v.sub.j_n and \u2206\u03b8.sub.j_n, considering their absolute values. For example, the box_i state interval is the most extended according to the position coordinate x.sub.1 if \u2206x.sub.1_n is the highest of the normalized values \u2206x.sub.j_n, \u2206v.sub.j_n and \u2206\u03b8.sub.j_n, or it is the most extended according to the speed coordinate v.sub.2 if \u2206v.sub.2\u00ad_n is the highest of the nine normalized values, etc.\n\n[0088] At sub-step /5-3/, the box_i state interval is divided into n.sub.i contiguous sub-intervals, by dividing, into n.sub.i segments with the same lengths, that one of the unidimensional intervals of box_i that is the most extended, within the meaning of the normalized values \u2206x.sub.j_n, \u2206v.sub.j_n and \u2206\u03b8.sub.j_n. Each sub-interval therefore has one of these segments as a unidimensional interval along the state coordinate that corresponds to the maximum extension of the box_i state interval, and the same unidimensional intervals as this box_i state interval along the other state coordinates. The n.sub.i sub-intervals that are thus constructed therefore constitute a partitioning of the box_i state interval. They form new state intervals from which the box-regularized particle filtering is continued. A weight value is attributed to each of them, which is equal to that of the box_i state interval divided by the division number n.sub.i. These new state intervals, with those of the state intervals that have not been divided, are then re-numbered by the index i, advantageously from 1 to N, to continue the filtering method.\n\n[0089] Finally, sub-step /5-4/, which was referred to as smoothing step in the general part of the present description, consists in correcting the new state intervals as resulting from sub-step /5-3/, so that they produce, with their associated weight values, an even better statistical representation of the state of the system, i.e. of the state of the aircraft 20 for the example in question. Such a modification of the state intervals is also commonly referred to as regularization of the statistical representation of the state of the system, in the jargon of a person skilled in the art. It can be applied not only to the new state intervals that each resulted from a division of one of the subsequent state intervals, but also to all the state intervals, including those that have not been divided. Corrections that are applied for this to the state intervals may consist of random shifts of the bounds of the unidimensional intervals that constitute the edges of each box_i state interval. Preferably, the respective weight values that are associated with the state intervals are not modified in this sub-step /5-4/. For the invention, a smoothing method by Epanechnikov kernel is applied, in particular as described in the article by Merlinge et al., Automatica 104(2019), already cited.\n\n[0090] In a known manner, the Epanechnikov kernel is defined by the probability density function f(x) = 3.square-solid.(1 - x.sup.2)/4, where x is the random variable comprised between -1 and 1, the values -1 and 1 being permitted. Its average value is zero, and its variance is equal to \u2155.\n\n[0091] Sub-step /5-4/ therefore initially comprises generating random corrections to be applied to each unidimensional state-coordinate interval that each determines box_i state intervals, and then applying these corrections. A detailed implementation of sub-step /5-4/ is shown in FIG. 3.\n\n[0092] The number of state coordinates of the system in question is denoted d, and the number of bounds that determine each state interval of this system, i.e. each box used in the regularized particle filtering method, is denoted d\u2018=2.Math.d. In the case of the aircraft 20, d=9 and d\u2019=18.\n\n[0093] N first random values are first generated, denoted \u03b2.sub.i and comprised between 0 and 1, the values 0 and 1 being permitted, i being the integer index of numbering of the state intervals as used previously, each according to a statistical distribution beta law, with first parameter equal to d and second parameter equal to 2. In a known manner, the beta law the parameters of which are d and 2 is defined by the probability density function x.sup.d/2-1.square-solid.(1-x).square-solid.\u0393(d/2+2)/[\u0393(d/2).square-solid.\u0393(2)], where x designates the random variable, and \u0393 designates the gamma function. One possible way of generating the values \u03b2.sub.i in accordance with the beta law uses Cheng\u2019s algorithm, which will be reminded below, with reference to FIG. 4.\n\n[0094] Next N.Math.d\u2018 second random values are generated, denoted v.sub.k,i, i also being the same index previously and k being another integer index that varies from 1 to d\u2019, each according to a normal statistical distribution law with zero mean value and standard deviation equal to unity, commonly referred to as reduced normal law. The index k counts the degrees of freedom in the definition of each state interval. It identifies two unidimensional interval bounds for each state coordinate or, in an equivalent manner, a center value and an interval length for each unidimensional state-coordinate interval. In a known manner, the normal law with zero mean value and standard deviation equal to unity is defined by the probability density function (\u215f\u03a0.sup.\u00bd).square-solid.exp[-x.sup.2/2], where x also designates the random variable, but in this case positive, zero or negative. It can be simulated by a sum of initial random values that are each generated according to a uniform statistical distribution law, and such a uniform statistical distribution law can be produced by a method of the LFSR type, standing for \u201clinear feedback shift register\u201d, for example.\n\n[0095] N first numbers, denoted \u03be.sub.i, are then calculated from the random values v.sub.k,i in the following manner: \u03be.sub.i = [\u03a3.sub.k=1 .sub.\u00e0 .sub.d\u2032 (v.sub.k,i).sup.2].sup.\u00bd. Advantageously, each of these numbers \u03be.sub.i can be calculated by applying the method for calculating an estimation of x.sup.\u03b1 that was described above, to X = v.sub.k,i with \u03b1=2, then to X = \u03a3.sub.k=1 .sub.\u00e0 .sub.d\u2032 (v.sub.k,i).sup.2 with \u03b1=\u00bd.\n\n[0096] N second numbers, denoted \u03b1.sub.i, are next calculated according to the formula: \u03b1.sub.i = (\u03b2.sub.i.sup.\u00bd/\u03be.sub.i. Advantageously, each of these numbers \u03b1.sub.i can be calculated by once again applying the method for calculating an estimation of x.sup.\u03b1 that was described above, to X = \u03b2.sub.i with \u03b1=\u00bd.\n\n[0097] Under these conditions, N.square-solid.d\u2032 third numbers that are calculated in the following manner: \u03b5.sub.k,i = v.sub.k,i.square-solid.\u03b1.sub.i, each satisfy the Epanechnikov statistical distribution law of zero average value and of variance equal to \u2155.\n\n[0098] Moreover, a noise amplitude, denoted h, is calculated in the following manner:\n\nh\u2004=\u2004\u03bc\u22c5A\u22c5N\u22121/(d\u2032+4),\n\nwhere A = [8.square-solid.c.sub.d.sup.,-1.square-solid.(d\u2018+4).square-solid.(2.square-solid.\u03a0.sup.\u00bd).sup.d\u2032].sup.\u215f(d\u2032+4). In this expression of A, c.sub.d\u2032 designates the volume of the hypersphere of dimension d\u2019 and of unitary radius, and .Math. is an adjustment parameter that is comprised between 0 et 1. In a known manner, c.sub.d\u2032 = \u03a0.sup.d\u2032/2/\u0393(d\u2032/2 + 1), where \u0393 again designates the gamma function. The value of c.sub.d\u2032 can either be precalculated and stored so as to be available for the computing unit 3, or be calculated by the latter, for example using a prerecorded table of values of the gamma function. The adjustment parameter .Math. makes it possible to control a trade-off between the efficiency of the random smoothing and efficacy of the particle filter. This is because the efficiency of the particle filter results from a continuity of the possible trajectories for the system, described by the trajectories of the state intervals that are maintained during a plurality of successive implementations of the sequence of steps /1/ to /5/. On the other hand, random smoothing produces jamming of these trajectories. The value of the parameter .Math. can be fixed initially for the method to be executed by the computing unit 3. It depends in particular on the numbers N and d. For example, the adjustment parameter .Math. may be taken equal to 0.1. The use of the adjustment parameter .Math. is in particular described in the thesis of Merlinge, N., entitled \u201cState estimation and trajectory planning using box particle kernels\u201d, Paris-Saclay University, 2018.\n\n[0099] The random modifications to be applied to the box_i state interval are then h.square-solid.\u03b5.sub.k,i, where k identifies the state coordinate bounds, or the center values or lengths of state coordinate intervals. These modifications can be arranged in the form of a vector E.sub.i, such that E.sub.i = [h.square-solid.\u03b5.sub.k,i].sub.k=1,...,d\u2032.\n\n[0100] A possible method for applying the random modifications to the state intervals that result from sub-step /5-3/ is now described. For this purpose, it is possible to represent each box_i state interval by a vector of height d\u2018, the coordinates of which contain the bounds of all the unidimensional state coordinate intervals for this state interval, or the center values and the lengths of its unidimensional intervals. Then the randomly modified values of the vector can be obtained by replacing this vector by + D x E.sub.i, where D is a square matrix of dimension d\u2019 such that the product of D multiplied by the transpose of D is equal to .Math..sub.i=1 .sub.\u00e0 .sub.N \n\n\u039ei\u22c5wi\u22c5t\u039ei\n\n: D x .sup.tD = .Math..sub.i=1 .sub.\u00e0 .sub.N \n\n\u039ei\u22c5wi\u22c5t\u039ei,\n\nwhere w.sub.i is the weight value of the box_i state interval. For example, the matrix D can be determined by Cholesky\u2019s method, which is well-known to a person skilled in the art so that it is not necessary to describe it again here. Under these circumstances, the vector \n\n\u039ei\n\nis to be replaced by the vector \n\n\u039ei\n\n+ D x E.sub.i, for applying the random variations according to the Epanechnikov kernel to the unidimensional intervals of state coordinates of the box_i state interval. All the vectors + D x E.sub.i determine all the new state intervals that result from the modification by smoothing. In the general part of the present description, the square matrix .Math..sub.i=1 .sub.to .sub.N \n\n\u039ei\u22c5wi\u22c5t\u039ei,\n\nof dimension d\u2032 has been called the mean product matrix. Its coefficients are the mean values calculated over all the state intervals in question, of products of interval bounds, or center values or interval lengths, taken in pairs separately for each state interval.\n\n[0101] The method that has just been described for applying, at sub-step /5-4/, a smoothing by Epanechnikov kernel to the sub-intervals as resulting from sub-step /5-3/, is implemented by the computer unit 3 of the navigation system using measurement correlation 10.\n\n[0102] The updated state intervals, as resulting from the sub-step /5-3/ or from sub-steps /5-4/, with their associated weight values, constitute the result of the box-regularized particle filtering method for an implementation of the sequence of steps /1/ to /5/. A plurality of iterations are chained recurrently, each new iteration from the results of the previous iteration. The iteration that is implemented last supplies an updated probability distribution that characterizes the true state of the aircraft 20. Furthermore, the updated state intervals that result therefrom, associated with their respective weight values, are to be used as prior state intervals for a new implementation of the sequence of steps /1/ to /5/.\n\n[0103] A description is now given, with reference to FIG. 4, of a method for generating each random value \u03b2.sub.i, so as to satisfy the Beta(d, 2) statistical distribution law. This method corresponds to Cheng\u2019s algorithm, published in the article entitled \u201cGenerating beta variates with non-integral shape parameters\u201d, Communications of the ACM, 21(4), pp. 317-322, 1978.\n\n[0104] For this purpose, the following numbers a and b are first determined at step /i/: [0105] a, which is equal to the minimum value between d and 2: a = min(d, 2), and [0106] b which is equal to the maximum value between d and 2: b = max(d, 2).\n\n[0107] For most of the applications of the invention, d is greater than 2, so that a=2 and b=d. Next, at step /ii/, the three numbers \u03b1, \u03b2 and \u03b3 are calculated such that:\n\n\u03b1=a + b, which is equal to d+2, \n\n\u03b2=\u03b1\u22122/2\u22c5a\u22c5b - \u03b11/2,which is equal to d/3\u22c5d - 21/2;\n\nand\n\n\u03b3=a + 1/\u03b2\n\n[0108] The numbers \u03b1, \u03b2 and \u03b3 may have been precalculated and stored in order to be directly accessible to the computing unit 3.\n\n[0109] At the step /iii/, two numbers u.sub.1 and u.sub.2 are generated randomly each according to the uniform statistical distribution law, between 0 and 1, for example using a method of the LFSR type. Then the numbers V, W, Z, R and S are calculated at step /iv/, according to the following formulae: [0110] V = \u03b2.square-solid.log[u.sub.1/(1-u.sub.1)], where log designates the base-e logarithmic function, [0111] W = a.square-solid.exp(V), where exp designates the base-e exponential function, [0112] Z = u.sub.1.sup.2.square-solid.u.sub.2, [0113] R = \u03b3.square-solid.V - log(4), and [0114] S = a + R - W.\n\n[0115] The value of the number Z can advantageously be calculated each time by applying the calculation method of an estimation of x.sup.\u03b1 that was described above, to X = u.sub.1 with \u03b1=2. Furthermore, the values of the logarithm and exponential functions can be obtained from tables of values prerecorded for these functions.\n\n[0116] Steps /v/ to /viii/ are then implemented successively, forming a sequence of tests that are applied in series: [0117] step /v/: if S + 1 + log(5) is greater than or equal to 5.square-solid.Z, go directly to step /viii/; [0118] step /vi/: if S is greater than or equal to log(Z), go directly to step /viii/; [0119] step /vii/: if R + \u03b1.square-solid.[log[\u03b1/(b+W)] is less than log(Z), go back to step /iii/; [0120] step /viii/: if a is equal to d, then the value \u03b2.sub.i that is generated randomly according to the beta law of parameters d and 2, is equal to W/(b+W), and if a is different from d, it is equal to b/(b+W). For most of the applications of the invention, where d is greater than 2, the random value \u03b2.sub.i is equal to d/(d+W).\n\n[0121] One of the advantages of this algorithm is that the number of implementations of the sequence of steps /iii/ to /viii/ that is necessary for obtaining the N random values \u03b2.sub.i is predictable.\n\n[0122] It is clear that the invention can be reproduced by modifying secondary aspects of the implementations that have been described in detail above, while keeping at least some of the advantages mentioned. Among such possible modifications, the following are cited non-limitedly: [0123] alternative algorithms may be used for some steps or sub-steps; [0124] the inertial unit may be used as a source for measuring the complete state of the vehicle. The state of the system as considered for the invention then comprises three acceleration values and three angular velocity values, according to the three spatial coordinates, in addition to the three values of spatial coordinates that identify a position for the vehicle, three speed values and three angular attitude values. In this case, each state of the system comprises values for fifteen state coordinates; [0125] in step /\u215f, a dynamic state-evolution model may be used for the system, this model being able to take account, when it is a vehicle, of commands that are applied to one or more engines and to an attitude control system of the vehicle; and [0126] the computing unit may be a processor with an ARM structure, a processor with several computing cores, one or more graphics processors, etc., in place of a chip of the FPGA, DSP, CPU or RISC type.\n\n[0127] Finally, the invention may be applied to fields other than aeronautics. For example, a box-regularized particle filtering method that is in accordance with the invention may also be implemented for a vehicle on the ground, a surface seagoing ship, a submarine, a satellite or a space probe, using in each case a reference frame and true state measurements that are suitable."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 17,
      "claims_start": 15,
      "description_end": 15,
      "description_start": 6,
      "drawings_end": 5,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 17,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 15,
      "specification_start": 6,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 70154538,
    "foreign_priority": [
      {
        "app_filing_date": "2019-12-13",
        "app_number": "1914349",
        "country": "FR"
      }
    ],
    "guid": "US-20230011501-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0011/501",
    "intl_class_current_primary": [
      {
        "intl_class": "G06N",
        "intl_subclass": "20/10",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "17/18",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G06F",
        "intl_subclass": "7/58",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06N20/10",
      "G06F17/18"
    ],
    "inventors": [
      {
        "city": "PALAISEAU",
        "country": "FR",
        "name": "MERLINGE; Nicolas",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "ASNIERES",
        "country": "FR",
        "name": "AUDEBERT; Cl\u00e9ment",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "PALAISEAU",
        "country": "FR",
        "name": "DAHIA; Karim",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "PALAISEAU",
        "country": "FR",
        "name": "HERISSE; Bruno",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "PALAISEAU",
        "country": "FR",
        "name": "ALLARD; Jean-Michel",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "MERLINGE; Nicolas et al.",
    "patent_title": "PARTICLE FILTERING AND NAVIGATION SYSTEM USING MEASUREMENT CORRELATION",
    "publication_date": "2023-01-12",
    "publication_number": "20230011501",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2020-12-15",
    "appl_id": "17785374",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Brentford",
        "country": "GB",
        "name": "Galvani Bioelectronics Limited",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "74130256!PG-US-20230010306",
    "cpc_inventive": [
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/3787",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/05",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/37518",
        "version": "2017-08-01"
      },
      {
        "cpc_class": "H02J",
        "cpc_subclass": "50/20",
        "version": "2016-02-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/37516",
        "version": "2017-08-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/0558",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/37205",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A stent for intravascular stimulation comprises a scaffold comprising first and second scaffold structures, each scaffold structure comprising at least one substantially annular portion. The stent further comprises one or more anodal electrodes formed from or electrically coupled to at least a substantially annular portion of the first scaffold structure and one or more cathodal electrodes electrically formed from or coupled to at least a substantially annular portion of the second scaffold structure. The stent further comprises an anodal lead electrically coupled to the first scaffold structure to form a conductive path from the one or more anodal electrodes to a generator and a cathodal lead electrically coupled to the second scaffold structure to form a conductive path from the one or more cathodal electrodes to the generator. The stent further comprises a sleeve of insulating material, wherein the scaffold structures are attached to or formed on the sleeve of insulating material and are separated from each other by a distance such that the first and second scaffold structures are electrically insulated from each other.",
      "background": "PRIORITY CLAIM\n[0001] The present application is a National Phase entry of PCT Application No. PCT/GB2020/053223, filed Dec. 15, 2020, which claims priority from U.S. Provisional Application No. 62/948,593, filed Dec. 16, 2019, each of which is hereby fully incorporated herein by reference.",
      "brief": "TECHNICAL FIELD\n\n[0002] This disclosure relates to neuro-modulation devices and methods for non-destructively stimulating a nerve.\n\nBACKGROUND\n\n[0003] Electrical devices of various shapes and sizes including one or more electrode have been used for neuro-stimulation of target anatomy. An intravascular device which can effectively stimulate a target nerve is desirable, as such intravascular device may be less invasive than other forms of devices with electrodes.\n\nSUMMARY\n\n[0004] In one aspect, the invention provides a neuro-modulation device (or a neuro-stimulation device) in the form of a stent for intravascular stimulation, the stent comprising: a scaffold comprising first and second scaffold structures, each scaffold structure comprising at least one substantially annular portion; one or more anodal electrodes formed from or electrically coupled to at least a substantially annular portion of the first scaffold structure and one or more cathodal electrodes electrically formed from or coupled to at least a substantially annular portion of the second scaffold structure; an anodal lead electrically coupled to the first scaffold structure to form a conductive path from the one or more anodal electrodes to a generator and a cathodal lead electrically coupled to the second scaffold structure to form a conductive path from the one or more cathodal electrodes to the generator; a sleeve of insulating material, wherein the scaffold structures are attached to or formed on the sleeve of insulating material and are separated from each other by a distance such that the first and second scaffold structures are electrically insulated from each other.\n\n[0005] In the context of a stent, a scaffold is a well-understood structure which is capable of supporting the external wall of the vessel into which the stent is placed. A scaffold may be a continuous or substantially continuous piece of material or a framework of interconnected members. It would be understood that the at least one substantially annular portion of each scaffold will have a dimension along the longitudinal axis of the stent (i.e. it will be three-dimensional) and that there is, in practice, no limit to the length along this axis. In other words, the scaffold may be substantially annular, including ring-shaped, toroidal and/or cylindrical.\n\n[0006] The sleeve of insulating material may cover at least a part of both an inner surface and an outer surface of the scaffold structures. In other words, the scaffold structure may be embedded at least partly within the sleeve with only contact surfaces of the electrodes being exposed.\n\n[0007] A further effect of including insulating material on the inner surface of the stent (i.e. between the central portion of the inside of the blood vessel and the electrode) is to promote outward injection of charge rather than inward. This reduces unwanted conduction of signal via the blood, which could lead to short-circuiting, and facilitates a more effective targeting of nerves, which are located outside of the blood vessel. In some embodiments where the scaffold functions as or provides an electrically conductive path, the insulating material may be provided to provide electrical insulation (e.g. to preventshort-circuiting) between scaffold structures.\n\n[0008] The anodal electrode may also be referred to as a return electrode, and the cathodal electrode may also be referred to as a stimulating electrode.\n\n[0009] The cathodal electrode (or the stimulating electrode) may have a surface area of between 0.1 cm.sup.2 and 0.01 cm.sup.2, optionally between 0.04 cm.sup.2 and 0.08 cm.sup.2, further optionally between 0.05 cm.sup.2 and 0.075 cm.sup.2, still further optionally between 0.06 cm.sup.2 and 0.07 cm.sup.2, and further optionally 0.067 cm.sup.2.\n\n[0010] In a further aspect, the invention provides a stent for intravascular stimulation, the stent comprising: a scaffold formed from an electrically insulating material and comprising at least a first substantially annular portion and a second substantially annular portion spaced apart from the first by a distance; one or more anodal electrodes attached to the first annular portion of the scaffold and one or more cathodal electrodes attached to the second annular portion of the scaffold; wherein each anodal electrode is electrically coupled to an anodal lead to form a conductive path from the respective anodal electrode to a generator and each cathodal electrode is electrically coupled to a cathodal lead to form a conductive path from the respective cathodal electrode to the generator; a sleeve of insulating material, wherein the first and second substantially annular portions are attached to or formed on the sleeve of insulating material.\n\n[0011] In a further aspect, the invention provides a stent for intravascular stimulation, the stent comprising: a scaffold formed from a material and comprising at least a first substantially annular portion and a second substantially annular portion spaced apart from the first by a distance; a pulse generator configured to generate electrical signals for delivery to a nerve for intravascular stimulation; one or more anodal electrodes formed from or attached to either the generator or the first substantially annular portion of the scaffold and one or more cathodal electrodes formed from or attached to either the generator or the second substantially annular portion of the scaffold, wherein each anodal electrode and each cathodal electrode is electrically coupled to the pulse generator;\n\n[0012] optionally a sleeve of insulating material, wherein the first and second substantially annular portions are attached to or formed on the sleeve of insulating material; and a transducer coupled to the pulse generator and configured to receive energy for delivery of power and/or communications to the pulse generator.\n\n[0013] An example of a suitable transducer is an antenna configured to receive EM energy such as RF energy, and convert it into DC power. An alternative suitable transducer is an ultrasonic transducer configured to receive electrical energy and convert it into mechanical energy.\n\n[0014] The system may further comprise a battery-operated energizer or charger which may be used to wirelessly power the IPG stimulator via the transducer (e.g. antenna) described above. The powering modality between the charger and the implanted device can be near-field, mid-field, RF or ultrasound. Optionally the battery of the energizer is rechargeable with an external near-field charger.\n\n[0015] In some embodiments, the stent comprises an energy storage circuit or a battery (for example connected to the pulse generator or the transducer) which can be charged. In other embodiments, the stent may be formed without a battery.\n\n[0016] In a further aspect, the invention provides a system for delivery of intravascular stimulation comprising a stent according to any one of above paragraphs and a radio frequency (RF) transmitter configured to transmit RF energy which, when received by the RF antenna of the stent, delivers power and/or communications to the pulse generator of the stent.\n\n[0017] In a further aspect, the invention provides a stent structure comprising a scaffold having at least one substantially annular portion comprising one or more hooks or projections, each hook or projection being connected at one end to the substantially annular portion and being unconnected at an opposing end to enable attachment of an electrode to the hook or projection.\n\n[0018] In a further aspect, the invention provides a stent for intravascular stimulation, the stent comprising: a scaffold formed from a material and comprising at least a first substantially annular portion and a second substantially annular portion spaced apart from the first by a distance; a pulse generator configured to generate electrical signals for delivery to a nerve for intravascular stimulation; one or more anodal electrodes formed from or attached to either the generator or the first substantially annular portion of the scaffold and one or more cathodal electrodes formed from or attached to either the generator or the second substantially annular portion of the scaffold, wherein each anodal electrode and each cathodal electrode is electrically coupled to the pulse generator;\n\n[0019] Optionally, a sleeve of insulating material, optionally wherein the first and second substantially annular portions are attached to or formed on the sleeve of insulating material; and/or wherein the pulse generator is configured to deliver a signal for intravascular stimulation via the anodal and cathodal electrodes for duration of between 60 seconds and 300 seconds, wherein the signal is formed of a train of pulses, and/or wherein: the pulses have a current amplitudes of between 10 mA and 50 mA, optionally between 20 mA and 40 mA; and/or the pulses have a pulse width of between 1 ms to 4 ms, optionally between 2 ms and 3 ms; and/or wherein either: A: the pulses have a frequency of between 5 Hz and 15 Hz, optionally between 8 Hz and 12 Hz, optionally 10 Hz and the pulse train is delivered according to an ON/OFF cycle having a duty cycle of between 10% and 30%, optionally between 15% and 25%, optionally 20%.; or B: the pulses have a frequency of between 0.5 Hz and 1.5 Hz, optionally 1 Hz and the pulse train is delivered continuously.",
      "claims": "1-48. (canceled) \n\n49. A stent for intravascular stimulation, the stent comprising: a scaffold comprising a first and a second scaffold structure, each scaffold structure comprising at least one substantially annular portion; one or more anodal electrodes formed from or electrically coupled to at least a substantially annular portion of the first scaffold structure and one or more cathodal electrodes electrically formed from or coupled to at least a substantially annular portion of the second scaffold structure; an anodal lead electrically coupled to the first scaffold structure to form a conductive path from the one or more anodal electrodes to a generator and a cathodal lead electrically coupled to the second scaffold structure to form a conductive path from the one or more cathodal electrodes to the generator; a sleeve of insulating material, wherein the scaffold structures are attached to or formed on the sleeve of insulating material and are separated from each other by a distance such that the first and second scaffold structures are electrically insulated from each other.  \n\n50. The stent of claim 49, wherein the one or more anodal electrodes are formed from the entirety of the substantially annular portion of the first scaffold structure and the one or more cathodal electrodes are formed from the entirety of the substantially annular portion of the second scaffold structure. \n\n51. The stent of claim 49, wherein the one or more anodal electrodes are crimped or welded to the first scaffold structure and the one or more cathodal electrodes are crimped or welded to the second scaffold structure. \n\n52. The stent of claim 49, wherein the anodal lead is crimped or welded to the first scaffold structure and the cathodal lead is crimped or welded to the second scaffold structure. \n\n53. The stent of claim 49, wherein the at least one substantially annular portion comprises one or more hooks or projections, each hook or projection being connected at one end to the substantially annular portion and being unconnected at an opposing end to enable attachment of an electrode to the hook or projection. \n\n54. The stent of claim 49, wherein the anodal and cathodal electrodes comprise platinum, optionally formed from an alloy of platinum and iridium, optionally in a ratio of 9:1 by weight. \n\n55. The stent of claim 49, wherein the surfaces of the anodal and cathodal electrodes are coated with one of: PEDOT, TiNi, IrOx, PtBlack, or treated using a process of laser roughening. \n\n56. The stent of claim 49, wherein the sleeve of insulating material has a first end and a second end, wherein one of the substantially annular portions comprising the anodal and cathodal electrodes is closer to the first end than the second end. \n\n57. The stent of claim 56, wherein the distance between the substantially annular portion comprising the anodal electrodes and the proximal end is one of (i) the same as, (ii) greater than or (iii) less than the distance between the substantially annular portion comprising the cathodal electrodes and the distal end. \n\n58. A system for delivery of intravascular stimulation comprising: a stent according to claim 49; and a radio frequency (RF) transmitter configured to transmit RF energy which, when received by the RF antenna of the stent, delivers power and/or communications to the pulse generator of the stent. \n\n59. The system of claim 58, wherein the RF transmitter is removably attachable to a patient, and optionally includes attachment means such as a strap for attaching the RF transmitter to the patient. \n\n60. The system of claim 58, wherein the RF energy is near-field, mid-field or ultrasound. \n\n61. The system of claim 58, wherein the stent further comprises a capacitor for storing charge received from the delivery of RF energy to the antenna. \n\n62. The system of claim 61, further comprising a control system configured such that: during a first period of time, the RF transmitter is caused to transmit RF energy such that the RF antenna receives power at a first level of current to deliver charge to the capacitor; and  during a second period of time after the first period of time has elapsed, the stent is caused to deliver intravascular stimulation continuously using charge from the capacitor, and the RF transmitter is caused not to transmit RF energy such that the RF antenna does not receive power.  \n\n63. The system of claim 61, further comprising a control system configured such that: during a first period of time, the RF transmitter is caused to transmit RF energy such that the RF antenna receives power at a first level of current to deliver charge to the capacitor; and  during a second period of time after the first period of time has elapsed, the stent is caused to deliver intravascular stimulation in a burst pattern using charge from the capacitor, and the RF transmitter is caused to transmit RF energy such that the RF antenna receives power at a second level of current, greater than the first.  \n\n64. A stent for intravascular stimulation, the stent comprising: a scaffold formed from an electrically insulating material and comprising at least a first substantially annular portion and a second substantially annular portion spaced apart from the first by a distance; one or more anodal electrodes attached to the first annular portion of the scaffold and one or more cathodal electrodes attached to the second annular portion of the scaffold; wherein each anodal electrode is electrically coupled to an anodal lead to form a conductive path from the respective anodal electrode to a generator and each cathodal electrode is electrically coupled to a cathodal lead to form a conductive path from the respective cathodal electrode to the generator; a sleeve of insulating material, wherein the first and second substantially annular portions are attached to or formed on the sleeve of insulating material.  \n\n65. A stent for intravascular stimulation, the stent comprising: a scaffold formed from a material and comprising at least a first substantially annular portion and a second substantially annular portion spaced apart from the first by a distance; a pulse generator configured to generate electrical signals for delivery to a nerve for intravascular stimulation; one or more anodal electrodes formed from or attached to either the generator or the first substantially annular portion of the scaffold and one or more cathodal electrodes formed from or attached to either the generator or the second substantially annular portion of the scaffold, wherein each anodal electrode and each cathodal electrode is electrically coupled to the pulse generator; optionally a sleeve of insulating material, wherein the first and second substantially annular portions are attached to or formed on the sleeve of insulating material; and a transducer coupled to the pulse generator and configured to receive energy for delivery of power and/or communications to the pulse generator.  \n\n66. The stent of claim 65, wherein the scaffold is formed from an electrically insulating material, each anodal electrode is electrically coupled to the pulse generator via a respective anodal lead and each cathodal electrode is electrically coupled to the pulse generator via a respective cathodal lead. \n\n67. The stent of claim 66, wherein the scaffold comprises first and second scaffold structures, the first scaffold structure comprising the first substantially annular portion and the second scaffold structure comprising the second substantially annular portion, wherein each anodal electrode is electrically coupled to the pulse generator via the first scaffold structure, and each cathodal electrode is electrically coupled to the pulse generator via the second scaffold structure, wherein the scaffold structures are attached to or formed on the sleeve of insulating material and are separated from each other by a distance such that the first and second scaffold structures are electrically insulated from each other. \n\n68. The stent of claim 65, comprising a first RF antenna formed from a conductor attached to, optionally weaved through, the first scaffold structure, and a second RF antenna formed from a conductor attached to, optionally weaved through, the second scaffold structure.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0020] Embodiments of the invention will be described, by way of example, with reference to the following drawings, in which:\n\n[0021] FIG. 1 is an embodiment of an exemplary split-stent-electrode according to the invention;\n\n[0022] FIG. 2 is a cross-section of an exemplary stent-electrode according to the invention showing an arrangement of contact electrodes;\n\n[0023] FIG. 3 is a cross-section of three further exemplary stent-electrodes according to the invention showing arrangements of contact electrodes comprising 4, 6 and 8 contacts per anode/cathode;\n\n[0024] FIGS. 4-6 show embodiments of split-stent-electrodes according to the invention each comprising an IPG stimulator and an antenna for receiving power;\n\n[0025] FIG. 7 is a diagram of an exemplary stent-electrode according to the invention implanted into a patient and in electrical communication with a charger;\n\n[0026] FIG. 8 is a diagram of a system for providing power to an exemplary stent-electrode according to the invention;\n\n[0027] FIGS. 9 and 10 are time plots showing relationships between current delivered to an exemplary stent-electrode according to the invention and voltage out during setup, stimulation and standby periods;\n\n[0028] FIG. 11 is a plot showing eCAP data for an exemplary stent-electrode according to the invention;\n\n[0029] FIG. 12 is a communications and control system according to the invention;\n\n[0030] FIGS. 13 and 14 are diagram of an exemplary stent-electrode according to the invention;\n\n[0031] FIGS. 15A and 15B are modelling of weighting functions and activation functions of potentials at the nerve (at a 1.6 mm electrode-fiber distance) of the exemplary stent-electrodes of FIGS. 16 to 22.\n\n[0032] FIG. 16 shows a two ring electrode stent assuming the scaffold provides a perfect insulator, according to the invention;\n\n[0033] FIG. 17 shows an alternative two ring electrode stent according to the invention;\n\n[0034] FIG. 18 shows an alternative two ring electrode stent with 25 .Math.m PTFE coating according to the invention;\n\n[0035] FIG. 19 shows a one ring stent anode according to the invention;\n\n[0036] FIGS. 20 and 21 show two asymmetric split stents according to the invention;\n\n[0037] FIG. 22 shows a symmetric split stent according to the invention; and\n\n[0038] FIG. 23 is a graph illustrating results of porcine ex-vivo comparator studies.\n\nDETAILED DESCRIPTION OF THE DRAWINGS\n\n[0039] FIG. 1 shows an exemplary split-stent-electrode chronic intravascular stimulator according to an aspect of the invention and comprising of an anode and cathode built on a scaffold made of 316L stainless steel. As shown, the scaffold is responsible for providing sufficient radial compliance for the stent to withstand the environment within a blood vessel. Accordingly, the scaffold provides sufficient, hoop force, mechanical strength and robustness to achieve that function, as well as to allow for crimping and expansion to ensure chronic lifetime survivability. Whilst the scaffold of the embodiment of FIG. 1 is made from 316L stainless steel, other forms of stainless steel may be used, as could Nitinol, Cobolt Alloy or other stent material optimized for the purpose described.\n\n[0040] The stimulator comprises a plurality of contact electrodes (or pads), which are crimped to the stent scaffold such that a conductive path is formed between the contacts and the stent scaffold. Although the contacts are crimped to the scaffold in the embodiment of FIG. 1, other means of attachment could be used, such as welding for example the contacts may be laser welded or resistance welded to the scaffold. Other processes for providing mechanical attachment may also be used. In the event that the scaffold is electrically conductive the attachment process is such that electrical continuity is provided between the contacts and the stent scaffold. In the event that the scaffold is non-conductive a separate conductor such as a wire provides electrical continuity between the contacts and the stent scaffold. Alternatively, the contacts could be formed as part of (or all of) the stent scaffold. The contact electrodes comprise an anode and cathode, though this nomenclature may not always be appropriate in all use-cases for the invention - for instance it will be appreciated that in symmetric biphasic stimulation, each electrode will act as both a cathode and an anode. In this situation, either contact electrode will satisfy the requirements for being an anode and a cathode. In some embodiments, monopolar stimulation may be achieved if an implantable pulse generator is provided, wherein a case of the implantable pulse generator is used as one of the electrode or electrodes. In other embodiments where an implantable pulse generator is provided, the electrode (or a plurality of the electrodes) may be provided (or located) on the implantable pulse generator itself. In some embodiments, the scaffold may have an electrode, although there may also be cases in which the scaffold acts only as a structural support with any electrodes being provided (or located) on the IPG.\n\n[0041] The contact electrodes of the embodiment of FIG. 1 are made from platinum though it will be appreciated that other materials may be used in order to achieve the necessary electrical charge injection properties and/or suitability to coating processes that enhance charge storage capacity. For example, the contact electrodes may comprise platinum, or be formed from an alloy of platinum and iridium, such as an alloy made from 90% platinum and 10% iridium. Alternatively or additionally, the surfaces of the contact electrodes may be coated possibly with PEDOT, PEDOT:PTS, TiNi, IrOx, PtBlack or treated using a process of laser roughening.\n\n[0042] The scaffold, including the anode and cathode, of the stimulator is attached to a flexible, conforming, insulating material in the form of a sleeve. The sleeve of the embodiment of FIG. 1 is made from high durometer polyurethane, but other materials may be suitable, such as nylon, polyester, or Pebax. The thickness of the polyurethane may be between 25 .Math.m and 50 .Math.m.\n\n[0043] The attachment or formation between the scaffold structures and the sleeve can be achieved in various ways. For example an insulating sleeve may be provided, and the scaffold may be deposited on the outer surface of the insulating sleeve using a conventional material deposition process. Instead of deposition, the scaffold may be formed separately and attached to the sleeve by any conventional process such as adhesive or welding. Alternatively, the scaffold and the sleeve may be manufactured together using, for example, an additive manufacturing process. Alternatively it is possible to manufacture the scaffold structure and then form the sleeve on the outer surface of the scaffold structure, for example by depositing, overmolding, overlaying or otherwise placing the sleeve on the scaffold structure. In this case, it may be preferable to remove material from the sleeve to reveal parts of the scaffold structure such as the electrodes. Removing material in this way could be achieved from an etching or laser machining process, for example.\n\n[0044] As shown in FIG. 1, the scaffold is formed in two parts \u2013 or structures \u2013 which are spaced apart from each other and thus electrically insulated from each other by virtue of the insulating sleeve. The or each cathode contact electrode is provided on one scaffold structure, whilst the or each anode contact electrode is provided on the other scaffold structure. Thus, providing each scaffold structure is formed from an electrically conductive material, it will be appreciated that this aspect of the invention may be performed by electrically connecting each scaffold structure to an implantable pulse generator (discussed elsewhere herein) at a single point, and forming the electrical path from the implantable pulse generator to each contact electrode via the scaffold structure. In other words, electrical contact to each exposed electrode can be achieved by welding or crimping a conductive insulated wire assembly directly onto the stent scaffold and not to each individual electrodes, thereby minimizing the number of welding points to improve the overall robustness of the stent-electrode design. In other embodiments, a plurality of implantable pulse generators may be provided to electrically connect the scaffold structures at a single or multiple points.\n\n[0045] The arrangement of the scaffold structures and insulating sleeve is such that the anodal and cathodal contact electrodes do not short circuit each other. In other words, it will be appreciated that a portion of the insulating sleeve defines the inter-electrode separation between the anode and cathode.\n\n[0046] In various embodiments of the invention, the insulating material of the sleeve may encapsulate the entirety of the stent scaffold, leaving only the outwardly facing electrode surfaces exposed towards the endothelial tissue to stimulate the nerves around the artery (or other vessel wall). In such embodiments, it will be appreciated that the total electrode area is determined by the sum of the total exposed electrode material and not the totality of the stent scaffolding material of the corresponding anode or cathode. This allows a degree of freedom and control in the design of the stimulator that allows somewhat independent optimization of its mechanical and electrical functions.\n\n[0047] Other embodiments of split-stent-electrode chronic intravascular stimulator operating on the principles described above are shown in FIGS. 4 to 6 and 20 to 22.\n\n[0048] Whilst it will be appreciated that an advantage of the split-stent type is that the scaffold itself is conductive, the scaffold may also be non-conductive. In the event that the scaffold is non-conductive the IPG may be used to bridge the split parts of the stent, both mechanically and electrically.\n\n[0049] Exemplary embodiments of stent-electrode chronic intravascular stimulators according to the invention which are not of the split-stent type are shown in FIGS. 13 and 16 to 19. Such embodiments also comprise an anode and cathode, but are instead built on a non-conductive scaffold. In these embodiments, the scaffold is made of biocompatible stable polymer, but other non-conductive scaffold material may be used instead so long as the scaffold provides sufficient, hoop force, mechanical strength and robustness to achieve its function, as well as to allow for crimping and expansion to ensure chronic lifetime survivability.\n\n[0050] It will be appreciated that the embodiments of stimulators shown in FIGS. 13 and 16 to 19, the scaffolds of which are not conductive, have electrodes that are crimped rather than welded onto the scaffold, though other means of attachment such as bonding could be used instead. Moreover, it will be appreciated that each electrode contacts is electrically contacted by welding or crimping to an insulated wire assembly connectable to an implantable signal generator (discussed elsewhere herein). In some embodiments the IPG may be passive; that is, for example, not powered by its own power source such as a battery. In such cases, a transducer may be provided on the implantable device for receiving signals from an external pulse generator, which are then communicated to the electrodes to deliver the signals. As described elsewhere herein, the transducers may be configured to receive whichever form of energy is preferred, such as EM, including RF, or ultrasonic, and converting such signals to electrical signals for delivery to a nerve.\n\n[0051] As shown in FIGS. 1 to 3 and elsewhere, the stent-electrode chronic intravascular stimulators (both the split-stent and non-conductive types) may comprise multiple electrode contacts per anode and cathode. In FIG. 2, six electrode contacts per anode and cathode are shown, but the number of contacts can vary between 4 and 12 depending of the target vessel size. FIG. 3 shows alternative embodiments where 4, 6 and 8 electrodes are used. Optionally, 4, 6, 8, 10 or 12 electrode contacts per anode and cathode are used, but 5, 7, 9 or 11 electrode contacts per anode and cathode may be used instead. It is possible for different numbers of electrode contacts to be used for the anode and the cathode, but it is preferred to use the same number for both the anode and the cathode. Referring to FIGS. 16 to 22, embodiments according to the invention may be implemented using a symmetric or an asymmetric scaffolding configuration. A symmetric design is typically more compact, but asymmetry may be helpful to facilitate placement and to robustly anchor the device onto a target site. FIG. 22 is an example of a symmetric configuration wherein the sets of electrodes are the same distance away from the respective ends of the scaffold and sleeve of insulating material. FIGS. 16 to 21 are examples of an asymmetric configuration wherein one of the sets of electrode contacts (in this case the anode, with blue indication corresponding to the more negative of the two values indicated on the accompanying measurement scale) is closer to one end of the scaffold and sleeve of insulating material than the other of the sets (in this case the cathode, with red indication corresponding to the more positive of the two values indicated on the accompanying measurement scale) is to the other end. Symmetric and asymmetric embodiments may also exist with a unitary body. The symmetric scaffolding configuration described above may exist as an asymmetric embodiment, and the asymmetric scaffolding configuration described above may exist as a symmetric embodiment.\n\n[0052] FIGS. 4 to 8 show an embodiment of a split-stent-electrode chronic intravascular stimulator comprising a miniature implantable pulse generator (IPG) with wireless antenna for receiving power and communication from a transmitter (described elsewhere herein). FIGS. 4 to 8 show a split-stent, though the embodiment would be just as applicable to a non-conductive type.\n\n[0053] As shown in FIGS. 4 to 6, the embodiments comprises two scaffold structures each including contact electrode(s) as described elsewhere herein. One scaffold structure provides an anode and the other provides a cathode. Each contact electrode is coupled to a miniature IPG in any manner as described elsewhere herein.\n\n[0054] FIG. 4 show two versions of a wireless antenna for receiving power and communication from a transmitter. In FIG. 4a, the antenna is made of electrically insulated multi-conductor weaved in the stent scaffold and attached to the IPG through hermetic feedthroughs. In FIG. 4b the antenna is encapsulated within the IPG stimulator itself. FIGS. 5 and 6 show an alternative arrangement wherein the antenna is made of electrically insulated multi-conductor weaved with an anchor (described below). The embodiment illustrated in FIG. 4b may use ultrasound as the powering modality in addition to, or alternative to those that require an antenna (such as RF). For example, the IPG stimulator may comprise a transducer for delivering electrical energy to the electrodes using energy source such as ultrasound. In other words, some embodiments do not require an antenna.\n\n[0055] It will be appreciated that the stimulator may be delivered into the target location of the splenic artery by a balloon expandable catheter. Alternatively or in addition, the stimulator may be made of a self-expanding Nitinol scaffold and comprise an anchor, and be delivered to the target location of the splenic artery by a selective release mechanism. In the examples shown in FIGS. 5 and 6, the electrodes can be partially deployed and retracted using a wire pully mechanism (\u201c1\u201d) that may comprise of one or two wires controlled proximally by the operator. Partial deployment of the electrodes may be used to determine the optimal placement of the device intraoperatively. A second deployment mechanism may be used to anchor the device in the vessel using a NiTi anchor released by retracting a wire holding the anchor in place in its collapsed state. This mechanism allows for a controlled release.\n\n[0056] FIG. 6a shows a self-expandable device with the same controlled release mechanism described above, wherein the antenna is embedded with the IPG but retaining a retractable NiTi anchor as described above. FIG. 6b shows a further embodiment wherein the antenna is embedded with the IPG stimulator and anchoring to the artery is accomplished using the NiTi stent scaffold itself. In this case, the scaffold can be made of an expandable insulative polymer matrix. The embodiment illustrated in FIG. 6b may use ultrasound as the powering modality in addition to, or alternative to those that require an antenna (such as RF). For example, the IPG stimulator may comprise a transducer for delivering electrical energy to the electrodes using energy source such as ultrasound.\n\n[0057] FIG. 7 illustrates a stent as described elsewhere herein as part of a system according to an aspect of the invention. The system comprises the stent-electrode stimulator, which is shown in FIG. 7 as deployed into the splenic artery through femoral access using a 7Fr to 9Fr delivery system. The system further comprises battery-operated energizer or charger which is used to wirelessly power the IPG stimulator via the antenna described above. The powering modality between the charger and the stent-electrode IPG stimulator can be near-field, mid-field or ultrasound. Optionally the battery of the energizer is rechargeable with an external near-field charger.\n\n[0058] The energizer (and optionally its charger) may be a wearable device, or may be implanted in a subcutaneous pocket of a patient. A wearable device may be advantageous for ad-hoc stimulation and/or where the charger requires frequent recharging. Conversely, an implantable device may be advantageous to deliver continuous stimulation, or deliver a scheduled therapy on a program, wherein the powering modality between the implanted charger and the IPG stimulator can be near-field, mid-field or ultrasound.\n\n[0059] FIG. 8 shows a power system for the devices described in connection with FIG. 7. As described in more detail below, energy supplied by the charger is stored and accumulated within the IPG in storage elements such as super-capacitors, and subsequently used to apply therapy.\n\n[0060] FIG. 9 shows an energy transfer schedule applicable to the system of FIGS. 7 and 8. In this case, energy supplied from a wireless charger is stored in capacitors until the total therapeutic dose is accumulated, at which time it is used to apply therapy. Compared with the system of FIG. 10, this system requires larger capacitors and a longer time to accumulate the charge for stimulation. On the other hand this system requires a lower input energy to \u2018trickle-charge\u2019 the capacitor or other storage elements to the desired output voltage level. Moreover, it only requires energy to be supplied continuously until the required voltage is reached, at which point the charger can cease delivery of energy.\n\n[0061] FIG. 10 shows an alternative energy transfer schedule applicable to the system of FIGS. 7 and 8. Again, in this case energy supplied from a wireless charger is stored in capacitors until the charge required to deliver a micro-burst is accumulated. In particular, this is the charge required to deliver an \u2018active period\u2019 dose comprising of a burst of pulses in one active period. Once the required charge is accumulated, it is used to apply therapy. Compared with the system of FIG. 9, this system requires smaller capacitors, and less time to accumulate the charge for stimulation. On the other hand, the system requires slightly higher input energy to charge the capacitor or other storage elements to the desired output voltage level. Moreover, it requires energy supplied continuously through the duration of the therapy.\n\n[0062] FIG. 11 shows eCAP amplitude (in percentage of first responses) over number N of pulses for different signal parameters, specifically 1 Hz, 10 Hz, 30 Hz continuous, and a burst pattern of 10 Hz comprising 5 pulses every 5 seconds. In one embodiment, preferred parameters of stent-electrode stimulation systems described elsewhere herein are 10 Hz pulses with 0.5 sec ON time and 5 sec OFF time at current amplitudes ranging from 10 mA to 40 mA and pulse widths ranging from 1 ms to 4 ms, for a total duration of 60 sec up to 300 sec. In another embodiment, preferred parameters of stent-electrode stimulation systems described elsewhere herein are 1 Hz pulses delivered continuously at current amplitudes ranging from 10 mA to 40 mA and pulse widths ranging from 1 ms to 4 ms, for a total duration of 60 sec up to 300 sec. This will achieve the same therapeutic effect as 10 Hz, 0.5 sec/5 sec ON/OFF parameters but with less peak input energy demands.\n\n[0063] FIG. 12A shows a communications and control system according to the invention. The system comprises the stent-IPG stimulator and charger described elsewhere herein, and a deployment catheter. It also comprises a patient remote (PR) and clinician programmer (CP).\n\n[0064] In a preferred embodiment, the CP connects to the charger via BLE to program therapy parameters in non-volatile memory. Therapy parameters may also be programmed into the stent IPG non-volatile memory. The CP & PR are used to monitor therapy by communicatively coupling with the charger while the charger energizes the IPG stent implant to deliver therapy.\n\n[0065] In the illustrated embodiment, the CP/PR software applications serve as the gateway to cloud connectivity and are used to download therapy parameters, track compliance and send patient reminders. Additionally the CP/PR can connect directly to the stent implant via NFC link for diagnostics or monitoring purposes.\n\n[0066] In the illustrated embodiment, the charger energizes the stent-IPG through wireless powering scheme based on 6.78 MHz or 13.56 MHz ISM bands, ultrasound or mid-field powering. It supports BLE with the CP/PR and NFC with the stent IPG. It will be appreciated that other wireless powering schemes may be used, and other communication protocols may be used for communication.\n\n[0067] In some embodiments, the stent-IPG stimulator is energized by the charger while therapy is being delivered. It may also link to external devices such as the charger and CP/PR over the NFC protocol. The stent-IPG is a single-fault safe device since it does not contain a battery and is not intended to be explanted in the event of failure.\n\n[0068] In summary, the charger of the system of FIG. 12A energizes the stent-IPG stimulator via NFC to deliver therapy and links to the CP/PR via BLE. The CP/PR of FIG. 12 provides patient app gateway to the cloud in order to download/upload therapy parameters, track compliance and sends reminders. The CP/PR also links to the charger via BLE to monitor progress during therapy and links to the stent-IPG stimulator via NFC for diagnostics and monitoring. Finally, the stent-IPG stimulator is energized by charger while therapy is delivered and links to Charger or CP/PR via NFC. It is a single-fault safe device, by which it is meant that it does not comprise a battery.\n\n[0069] FIGS. 13 and 14 are diagram of an exemplary stent-electrode 1300 according to the invention. As shown, in this particular example of a stent-electrode 1300, the inter-electrode distance (IED) is 1.5 mm, the electrode length (Le) is 1.5 mm, the thickness of the PTFE material (see FIG. 14) is between 25 and 50 .Math.m and the length of insulation (in this case the PTFE insulation film) over-hang (L.sub.IOH). Of course, it will be appreciated that these dimensions are merely exemplary, and other dimensions will be suitable depending on the particular application. In the example shown, six electrodes 1305 are equally spaced circumferentially around the perimeter of each substantially annular portion of the stent-electrode 1300, though more or fewer electrodes can be provided, and the spacing can be adjusted depending on the particular application. In the illustrated case, two substantially annular portions 1308a, 1308b are provided, resulting in a total of twelve electrodes.\n\n[0070] The stent-electrode 1300 of FIGS. 13 and 14 has features and concepts in common with the devices described above in connection with FIGS. 1 to 12. In addition, the stent-electrode 1300 of FIGS. 13 and 14 is provided with hooks or projections 1310 that, in the illustrated embodiment, extend from the or each substantially annular portion 1308 of the scaffold 1320, and extend along the longitudinal axis of the stent. These hooks or projections 1310 can be any suitable shape, and are for the purpose of attaching or forming the electrodes 1305. As shown in FIG. 13, each hook or projection 1310 of the relevant substantially annular portion 1308a, 1308b carries a respective electrode 1305, though it is not necessary for every hook or projection 1310 of the scaffold 1320 to carry an electrode.\n\n[0071] FIG. 14 shows the cross section of the stent-electrode in FIG. 13. The scaffold 1320 of the stent-electrode has a circular cross section, and is made from Nitinol (i.e. an alloy of nickel and titanium). Each hook or projection 1310 is has an annular cross-section, though the feature need not be hollow, and could take other cross-sectional shapes. Each hook or projection 1310  may be formed in the same material as the rest of the scaffold or be made from polytetrafluoroethylene (PTFE), though other insulating materials could be used instead. Such insulating materials can be provided at least partly over the hook to provide an increased mechanical attachment between the scaffold hook and the electrode layer or electrode coating, for example the platinum layer 1330. In an embodiment where the scaffold provides an electrically conductive path, at least a part of the electrode layer 1330 is in mechanical and electrical contact with the scaffold hook. In some embodiments, insulating material may not be provided between the scaffold hook 1310 and the platinum layer 1330. Surrounding each hook or projection 1310 is a platinum layer 1330, which forms the electrode 1305 itself. It will be appreciated that in the illustrated embodiment where the stent-electrode scaffold 1320 is conductive, an electrical signal may pass from the scaffold 1320 to the electrode 1305, specifically to the platinum layer 1330. Any suitable electrical connection between these two components will facilitate this electrical coupling. In cases where the scaffold is not conductive, a separate electrical connection, for example a wire, may couple directly to the electrode, for example to a platinum layer similar to the one illustrated.\n\n[0072] To prevent an electrical signal passing from the platinum layer toward the inside of the vessel rather than toward the vessel wall as intended, a PTFE coating 1340 is provided over the portion of the platinum layer that is positioned internal to the scaffold 1320; i.e. the portion of the platinum layer that faces radially inwardly. It will be noted that there is no such PTFE coating 1340 provided over the portion of the platinum layer that is positioned external to the scaffold 1320; i.e. the portion of the platinum layer that faces radially outwardly. It will be appreciated that this arrangement promotes outward injection of charge rather than inward, which reduces unwanted conduction of signal via the blood, as described above.\n\n[0073] FIGS. 15A and 15B show modelling of weighting functions and activation functions of potentials at the nerve for different electrode designs and configurations (for example those illustrated in FIGS. 16-23). The activation function is the second spatial derivative of the electric field, and it allows determination of the location(s) on the electrode where the system will cause greatest activation of, for example, the nerve. This enables determination of factors such as the number, positioning, spatial separation, and size of electrodes. The activation function aids selection of the ideal electrode design and configuration.\n\n[0074] FIGS. 16 to 23 show various examples of six-electrode stents of one-ring, two-ring and split-stent design employing aspects of the invention described above.\n\n[0075] The stent may be positioned inside the splenic artery for stimulating the splenic nerve or branches of the splenic nerve.\n\n[0076] The stent of this application could be used in conjunction with any suitable blood vessel in order to apply an electrical signal to any corresponding nerve. Other examples include: the carotid artery and the vagus nerve and the cervical sympathetic ganglion; the aorta and the phrenic nerve, the vagus nerve, the superior mesenteric ganglion, and the inferior mesenteric ganglion; the renal artery and the renal nerves; and the subclavian artery and the brachial plexus; and common hepatic artery and its associated nerves; and gastroduodenal artery and its associated nerves; iliac artery and splanchnic nerves.\n\n[0077] The invention may be useful for treating subjects who are suffering from, or who are at risk of developing, diseases, disorders or conditions associated with inflammation, e.g. inflammatory disorders, e.g., autoimmune disorders. The invention may treat or ameliorate the effects of such diseases, disorders or conditions by reducing inflammation. This may be achieved by decreasing the production and release of pro-inflammatory cytokines, and/or by increasing the production and release of anti-inflammatory cytokines and pro-resolving molecules, from the spleen, by electrically stimulating the splenic arterial nerve as described herein.\n\n[0078] Inflammatory disorders include autoimmune disorders, such as arthritis (e.g. rheumatoid arthritis, osteoarthritis, psoriatic arthritis), Grave\u2019s disease, myasthenia gravis, thryoiditis, systemic lupus erythematosus, Goodpasture\u2019s syndrome, Behcets\u2019s syndrome, allograft rejection, graft-versus-host disease, ankylosing spondylitis, Berger\u2019s disease, diabetes including Type I diabetes, Reitier\u2019s syndrome, spondyloarthropathy psoriasis, multiple sclerosis, Inflammatory Bowel Disease, Crohn\u2019s disease, Addison\u2019s disease, autoimmune mediated hair loss (e.g., alopecia areata) and ulcerative colitis.\n\n[0079] Certain examples of inflammatory disorders include diseases involving the gastrointestinal tract and associated tissues, such as appendicitis, peptic, gastric and duodenal ulcers, peritonitis, pancreatitis, ulcerative, pseudomembranous, acute and ischemic colitis, inflammatory bowel disease, diverticulitis, cholangitis, cholecystitis, Crohn\u2019s disease, Whipple\u2019s disease, hepatitis, abdominal obstruction, volvulus, post-operative ileus, ileus, celiac disease, periodontal disease, pernicious anemia, amebiasis and enteritis.\n\n[0080] Examples of inflammatory disease, disorders or conditions affecting the bones, joints, muscles and connective tissues include the various arthritides and arthralgias, osteomyelitis, gout, periodontal disease, rheumatoid arthritis, spondyloarthropathy, ankylosing spondylitis and synovitis.\n\n[0081] Further examples include systemic or local inflammatory diseases and conditions, such as asthma, allergy, anaphylactic shock, immune complex disease, sepsis, septicemia, endotoxic shock, eosinophilic granuloma, granulomatosis, organ ischemia, reperfusion injury, organ necrosis, hay fever, cachexia, hyperexia, septic abortion, HIV infection, herpes infection, organ transplant rejection, disseminated bacteremia, Dengue fever, malaria and sarcoidosis.\n\n[0082] Other examples include diseases involving the urogential system and associated tissues, such as diseases that include epididymitis, vaginitis, orchitis, urinary tract infection, kidney stone, prostatitis, urethritis, pelvic inflammatory bowel disease, contrast induced nephropathy, reperfusion kidney injury, acute kidney injury, infected kidney stone, herpes infection, and candidiasis.\n\n[0083] Other examples include involving the respiratory system and associated tissues, such as bronchitis, asthma, hay fever, ventilator associated lung injury, cystic fibrosis, adult respiratory distress syndrome, pneumonitis, alvealitis, epiglottitis, rhinitis, achlasia, respiratory syncytial virus, pharyngitis, sinusitis, pneumonitis, alvealitis, influenza, pulmonary embolism, hyatid cysts and/or bronchiolitis.\n\n[0084] Further examples are dermatological diseases and conditions of the skin (such as bums, dermatitis, dermatomyositis, burns, cellulitis, abscess, contact dermatitis, dermatomyositis, , warts, wheal, sunburn, urticaria warts, and wheals); diseases involving the cardiovascular system and associated tissues, (such as myocardial infarction, cardiac tamponade, vasulitis, aortic dissection, coronary artery disease, peripheral vascular disease, aortic abdominal aneurysm, angiitis, endocarditis, arteritis, atherosclerosis, thrombophlebitis, pericarditis, myocarditis, myocardial ischemia, congestive heart failure, periarteritis nodosa, and rheumatic fever, filariasis thrombophlebitis, deep vein thrombosis); as well as various cancers, tumors and proliferative disorders (such as Hodgkin\u2019s disease), nosocomial infection; and, in any case the inflammatory or immune host response to any primary disease.\n\n[0085] Other examples of inflammatory disorders include diseases involving the central or peripheral nervous system and associated tissues, such as Alzheimer\u2019s disease, depression, multiple sclerosis, cerebral infarction, cerebral embolism, carotid artery disease, concussion, subdural hematoma, epidural hematoma, transient ischemic attack, temporal arteritis, spinal cord injury without radiological finding (SCIWORA), cord compression, meningitis, encephalitis, cardiac arrest, Guillain-Barre, spinal cord injury, cerebral venous thrombosis and paralysis.\n\n[0086] Inflammatory disorders also include conditions associated with immune or inflammatory response (i.e. acute inflammatory episodes) include injury to nerves or other tissue and pain associated with nerve or other tissue. Injury may be due to a physical, chemical or mechanical trauma. Non- limiting examples of injury include acute trauma, burn, whiplash, musculoskeletal strains, and post-operative surgery complications, such as DVT, cardiac dysrhythmia, ventilator associated lung injury, and post-operative ileus.\n\n[0087] Conditions associated with a particular organ such as eye or ear may also include an immune or inflammatory response such as conjunctivitis, iritis, glaucoma, episcleritis, acute retinal occlusion, rupture globe, otitis media, otitis externa, uveitis and Meniere\u2019s disease.\n\n[0088] Another example of an inflammatory disorder is post-operative ileus (POI). POI is experienced by the vast majority of patients undergoing abdominal surgery. POI is characterized by transient impairment of gastro-intestinal (GI) function along the GI tract as well pain and discomfort to the patient and increased hospitalization costs.\n\n[0089] The impairment of GI function is not limited to the site of surgery, for example, patients undergoing laparotomy can experience colonic or ruminal dysfunction. POI is at least in part mediated by enhanced levels of pro-inflammatory cytokines and infiltration of leukocytes at the surgical site. Neural inhibitory pathways activated in response to inflammation contribute to the paralysis of secondary GI organs distal to the site of surgery. Stimulation of neural activity as taught herein may thus be effective in the treatment or prevention of POI.\n\n[0090] The invention is particularly useful in treating autoimmune disorders (e.g. rheumatoid arthritis, osteoarthritis, psoriatic arthritis, spondyloarthropathy, ankylosing spondylitis, psoriasis, systemic lupus erythematosus (SLE), multiple sclerosis, Inflammatory Bowel Disease, Crohn\u2019s disease, and ulcerative colitis) and sepsis.\n\n[0091] This invention is particularly useful for treating B cell mediated autoimmune disorders (e.g. systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA)).\n\n[0092] The invention is particularly useful for treating inflammatory conditions associated with bacterial infections. For example, the invention is particularly useful for treating inflammatory conditions caused or exacerbated by Escherichia coli, Staphylococcus aureus, Pneumococcus, Haemophilus influenza, Neisseria meningitides, Streptococcus pneumonia, Methicillin-resistant Staphylococcus aureus (MRSA), Klebsiella or Enterobacter infection.\n\n[0093] Treatment of the inflammatory disorder can be assessed in various ways, but typically involves determining an improvement in one or more physiological parameters of the subject.\n\n[0094] Useful physiological parameters may be one or more of the group consisting of: the level of a pro-inflammatory cytokine, the level of an anti-inflammatory cytokine, the level of a catecholamine, the level of an immune cell population, the level of an immune cell surface co-stimulatory molecule, the level of a factor involved in the inflammation cascade, the level of an immune response mediator, and the rate of splenic blood flow.\n\n[0095] Improvement in a determined physiological parameter in the context of the invention may be one or more of the group consisting of: a reduction in a pro-inflammatory cytokine, an increase in an anti-inflammatory cytokine, an increase in a catecholamine, a change in an immune cell population, a change in an immune cell surface co-stimulatory molecule, a reduction in a factor involved in the inflammation cascade, a change in the level of an immune response mediator and a decrease in splenic blood flow. The invention might not lead to a change in all of these parameters.\n\n[0096] By stimulating a splenic arterial nerve at a site where the splenic artery is not in direct contact with the pancreas, the spleen may: (a) decrease the secretion of a pro-inflammatory cytokine compared to baseline secretion; and/or (b) increase the secretion of an anti-inflammatory cytokine compared to baseline secretion. For example, the decrease in a pro-inflammatory cytokine secretion may be by: \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490% or \u226495%. The increase in an anti-inflammatory cytokine secretion may be by: \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, \u226495%, \u2264100%, \u2264150% or \u2264200%.\n\n[0097] Once the cytokine is secreted into the circulation, its concentration in the circulation is diluted. Stimulation of the splenic arterial nerve may result in: (a) a decrease in the level of a pro-inflammatory cytokine in the plasma or serum by \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u2264 35%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, or \u226495%; and/or (b) an increase in the level of an anti-inflammatory cytokine in the plasma or serum by \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, \u226495%, \u2264100%, \u2264 150% or \u2264200%. Preferably the level in the serum is measured.\n\n[0098] By stimulating the splenic arterial nerve, the level of catecholamine (e.g. norepinephrine or epinephrine), e.g. its level in the spleen, may increase, for example, by: \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, \u226495%, \u2264 100%, \u2264150% or \u2264200%.\n\n[0099] For example, the inventors found that stimulating a splenic arterial nerve can decrease the level of a pro-inflammatory cytokine (e.g. TNF\u03b1) in the serum by 30%-60%.\n\n[0100] Pro-inflammatory cytokines are known in the art. Examples of these include tumor necrosis factor (TNF; also known as TNF\u03b1 or cachectin), interleukin (IL)-1\u03b1, IL-1\u03b2, IL-2; IL-5, IL-6, IL-8, IL-15, IL 18, interferon \u03b3 (IFN-\u03b3); platelet-activating factor (PAF), thromboxane; soluble adhesion molecules; vasoactive neuropeptides; phospholipase A2; plasminogen activator inhibitor (PAI-1); free radical generation; neopterin; CD14; prostacyclin; neutrophil elastase; protein kinase; monocyte chemotactic proteins 1 and 2 (MCP-1, MCP-2); macrophage migration inhibitory factor (MIF), high mobility group box protein 1 (HMGB-1), and other known factors.\n\n[0101] Anti-inflammatory cytokines are also known in the art. Examples of these include IL-4, IL-10, IL-17, IL-13, IL-1\u03b1, and TNF\u03b1 receptor.\n\n[0102] It will be recognized that some of the pro-inflammatory cytokines may act as anti-inflammatory cytokines in certain circumstances, and vice-versa. Such cytokines are typically referred to as pleiotropic cytokines.\n\n[0103] In some embodiments, stimulation of the splenic arterial nerve may result in: (a) a decrease in the level of an anti-inflammatory cytokine in the plasma or serum by \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, or \u226495%; and/or (b) an increase in the level of a pro-inflammatory cytokine in the plasma or serum by \u22645%, \u226410%, \u226415%, \u226420%, \u226425%, \u226430%, \u226435%, \u226440%, \u226445%, \u226450%, \u226460%, \u226470%, \u226480%, \u226490%, \u226495%, \u2264100%, \u2264150% or \u2264200%.\n\n[0104] In this context the invention may be useful for increasing an immune response in a subject. For example, increasing an immune response or a pro-inflammatory response may be beneficial in a subject who is immunocompromised and thus in need of increasing pro-inflammatory cytokines for inducing beneficial pro-inflammatory responses. This may be particularly beneficial in immunocompromised subjects who are particularly vulnerable to infections. Examples of immunocompromised subjects in which this embodiment of the invention may be useful include, but are not limited to, subjects undergoing chemotherapy, subjects with HIV or AIDS, subjects taking a course of steroids, and subjects with immunosenescence, for example, subjects with age-associated immunodeficiency. In some embodiments, the invention may be used to increase a pro-inflammatory response in a subject wherein that subject is undergoing or is about to undergo a therapy in which immunocompromisation is an undesired side effect of that therapy. In other embodiments, the invention is useful for inducing a pro-inflammatory response to boost the acquisition of resistance provided by a vaccine. In other words, the neuro-stimulation device of the invention may be used in a method of vaccination, e.g. to boost the efficacy of a vaccine.\n\n[0105] Factors involved in immune responses may be useful measurable parameters in the context of the invention, for example, TGF, PDGF, VEGF, EGF, FGF, I-CAM, nitric oxide.\n\n[0106] Chemokines may also be useful measurable parameters in the context of the invention, such as 6cKine and MIP3beta, and chemokine receptors, including CCR7 receptor.\n\n[0107] Changes in immune cell population (Langerhans cells, dendritic cells, lymphocytes, monocytes, macrophages), or immune cell surface co-stimulatory molecules (Major Histocompatibility, CD80, CD86, CD28, CD40) may also be useful measurable parameters in the context of the invention. Applying a signal to the nerves according to the invention can cause a reduction in the total counts of circulating or tissue-specific (e.g. joint-specific in the case of rheumatoid arthritis) leukocytes (including monocytes and macrophages, lymphocytes, neutrophils, etc.).\n\n[0108] Factors involved in the inflammatory cascade may also be useful measurable parameters in the context of the invention. For example, the signal transduction cascades include factors such as NF\u03ba-B, Egr-1, Smads, toll-like receptors, and MAP kinases.\n\n[0109] Methods of assessing these physiological parameters are known in the art. Detection of any of the measurable parameters may be done before, during and/or after modulation of neural activity in the nerve.\n\n[0110] For example, a cytokine, chemokine, or a catecholamine (e.g. norepinephrine or epinephrine) may be directly detected, e.g. by ELISA. Alternatively, the presence or amount of a nucleic acid, such as a polyribonucleotide, encoding a polypeptide described herein may serve as a measure of the presence or amount of the polypeptide. Thus, it will be understood that detecting the presence or amount of a polypeptide will include detecting the presence or amount of a polynucleotide encoding the polypeptide.\n\n[0111] Quantitative changes of the biological molecules (e.g. cytokines) can be measured in a living body sample such as urine or plasma. Detection of the biological molecules may be performed directly on a sample taken from a subject, or the sample may be treated between being taken from a subject and being analyzed. For example, a blood sample may be treated by adding anti-coagulants (e.g. EDTA), followed by removing cells and cellular debris, leaving plasma containing the relevant molecules (e.g. cytokines) for analysis. Alternatively, a blood sample may be allowed to coagulate, followed by removing cells and various clotting factors, leaving serum containing the relevant molecules (e.g. cytokines) for analysis.\n\n[0112] In the embodiments where the signal is applied whilst the subject is asleep, the invention may involve determining the subject\u2019s circadian rhythm phase markers, such as the level of cortisol (or its metabolites thereof), the level of melatonin (or its metabolites thereof) or core body temperature. Cortisol or melatonin levels can be measured in the blood (e.g. plasma or serum), saliva or urine. Methods of determining the levels of these markers are known in the art, e.g. by enzyme-linked immunosorbent assay (ELISA) or radioimmunoassay. If measurements of the subject\u2019s circadian rhythm phase markers indicate circadian oscillations of inflammatory markers which may beneficially be regulated by application of a signal with a neuro-stimulation device or system of the invention, then application of the signal at night at a suitable periodicity according to the subject\u2019s circadian rhythm may be appropriate.\n\n[0113] As used herein, a physiological parameter is not affected by the modulation (e.g. stimulation) of the splenic neural activity if the parameter does not change (in response to nerve modulation) from the normal value or normal range for that value of that parameter exhibited by the subject or subject when no intervention has been performed, i.e. it does not depart from the baseline value for that parameter. Such a physiological parameter may be arterial pressure, heart rate or glucose metabolism. Suitable methods for determining changes in any these physiological parameters would be appreciated by the skilled person.\n\n[0114] The skilled person will appreciate that the baseline for any neural activity in a subject need not be a fixed or specific value, but rather can fluctuate within a normal range or may be an average value with associated error and confidence intervals. Suitable methods for determining baseline values are well known to the skilled person.\n\n[0115] As described herein, a physiological parameter is determined in a subject when the value for that parameter exhibited by the subject at the time of detection is determined. A detector (e.g. a physiological sensor subsystem, a physiological data processing module, a physiological sensor, etc.) is any element able to make such a determination.\n\n[0116] Thus, in certain embodiments, the method according to this aspect of the invention further comprises a step of determining one or more physiological parameters of the subject, wherein the signal is applied only when the determined physiological parameter meets or exceeds a predefined threshold value. In such embodiments wherein more than one physiological parameter of the subject is determined, the signal may be applied when any one of the determined physiological parameters meets or exceeds its threshold value, alternatively only when all of the determined physiological parameters meet or exceed their threshold values. In certain embodiments, the signal is applied by a system of the invention, which in addition to the neuro-stimulation device comprises at least one detector configured to determine the one or more physiological parameters of the subject.\n\n[0117] In certain embodiments, the physiological parameter is an action potential or pattern of action potentials in a nerve of the subject, wherein the action potential or pattern of action potentials is associated with the disease, disorder or condition to be treated.\n\n[0118] A predefined threshold value for a physiological parameter is defined elsewhere herein.\n\n[0119] A subject of the invention may, in addition to being treated with a neuro-stimulation device or system according to the invention, receive medicine for their disease, disorder or condition, as discussed elsewhere herein. In the methods of the invention, anticoagulant therapy, e.g. with heparin, may be administered to the subject prior to, following, and/or simultaneously with the application of the neuro-stimulation device of the invention.\n\nSuitable Forms of an Electrical Signal\n\n[0120] The neuro-stimulation device according to the invention applies an electrical signal via at least one electrode which is placed in proximity to, i.e. in a signaling relationship with, a splenic arterial nerve when the distal end of the catheter or stent of the neuro-stimulation device is inserted into a blood vessel, for example a splenic artery. The electrode may be said to be placed in signaling contact with the splenic arterial nerve. As used herein, \u201csignaling contact\u201d is where at least part of the electrical signal applied via the at least one electrode is received at the nerve.\n\n[0121] Electrical signals applied according to the invention (in any medical setting described herein) may be non-destructive. As used herein, a \u201cnon-destructive signal\u201d is a signal that, when applied, does not irreversibly damage the underlying neural signal conduction ability of the nerve. That is, application of a non-destructive signal maintains the ability of the nerve or fibers thereof, or other nerve tissue to which the signal is applied, to conduct action potentials when application of the signal ceases, even if that conduction is in practice artificially stimulated as a result of application of the non-destructive signal.\n\n[0122] Electrical signals applied according to the invention may be a voltage or a current waveform (e.g. constant voltage or a constant current waveform).\n\n[0123] The electrical signal may be characterized by one or more electrical signal parameters. The electrical signal parameters include waveform, frequency, and amplitude.\n\n[0124] Alternatively or additionally, the electrical signal may be characterized by the pattern of application of the electrical signal to the nerve. The pattern of application refers to the timing of the application of the electrical signal to the nerve. The pattern of application may be continuous application or periodic application.\n\n[0125] Continuous application refers to a situation in which the electrical signal is applied to the nerve in a continuous manner. In embodiments where the electrical signal is a series of pulses, the gaps between those pulses (i.e. between the pulse width and the phase duration) do not mean that the signal is not continuously applied.\n\n[0126] Periodic application refers to where the electrical signal is applied to the nerve in a repeating pattern (e.g. an on-off pattern).\n\n[0127] In the context of the treatment of a disease, disorder or condition associated with inflammation, e.g. an inflammatory disorder, e.g. an autoimmune disorder, the pattern of application of the electrical signal may be continuous application, periodic application and/or episodic application. Episodic application refers to where the electrical signal is applied to the nerve for a discrete number of episodes throughout a day. Each episode may be defined by a set duration or a set number of iterations of the electrical signal. Where the electrical signal is applied periodically and episodically, it means that the signal is applied in a periodic manner for each episode of application. Where the electrical signal is applied continuously and episodically, it means that the signal is applied in a continuous manner for each episode of application.\n\n[0128] The inventors have found preferred electrical signal parameters and patterns of signal application for stimulating neural activity in a splenic arterial nerve by applying the signal to the application site for use in accordance with the invention, which parameters and/or patterns lead to increased immunosuppressive effects while reducing possible systemic effects when stimulating neural activity in said nerve. The preferred signal parameters and patterns of application are discussed in detail below.\n\n[0129] The inventors have also found improved waveforms of the electrical signal which decrease the pulse height required in order to stimulate neural activity in a human nerve supplying the spleen, whilst reducing the burden on the stimulator. The improved waveforms are discussed in detail below.\n\nWaveform\n\n[0130] Modulation (e.g. stimulation) of a nerve e.g. supplying the spleen can be achieved using electrical signals applied by the neuro-stimulation device (stent) of the invention.\n\n[0131] A pulse train comprises a plurality of sequential pulses, where each pulse may be characterized by pulse width, pulse height and/or interphase delay. Pulse width refers to a width (or time duration) of a primary phase of the waveform. In some cases where a pulse comprises a first phase that is the primary phase and a second phase which is the recovery phase, for example an anodic and/or a cathodic phase, the pulse width refers to a width (or duration) of the first phase. A pulse duration refers to the time duration during which the pulse is applied or delivered for. This may also be referred to as a stimulation time.\n\n[0132] Interphase delay refers to the time period from the end of a pulse to the start of the next pulse. Pulse height, which is also referred to as pulse amplitude, refers to the amplitude of current of the pulse, typically measured in amps.\n\n[0133] Pulse width and pulse height are preferably constant for all of the pulses in the pulse train. Likewise, interphase delay is preferably constant between all of the pulses in the pulse train.\n\n[0134] Through experimental studies, the inventors have found improved waveforms of the electrical signal, which decrease the pulse height required in order to stimulate neural activity in a human nerve supplying the spleen, thereby optimizing the biological efficacy and reproducibility of stimulation parameters of the electrical signal for use in humans whilst reducing the burden on the stimulator. Thus, the electrical signal may comprise a pulse train having a pulse width >0.1 ms, optionally \u22650.4 ms, optionally \u22651 ms, optionally >1 ms. Additionally or alternatively, the pulse width may be \u22645 ms, optionally \u22643 ms, optionally \u22642 ms. Optionally, the pulse width may be between 0.1 and 5 ms, optionally between 0.4 and 4 ms, optionally between 1 and 3 ms, optionally between 1.5 and 2.5 ms, optionally between 1.75 ms and 2.25 ms, optionally between 1.9 ms and 2.1 ms, optionally 2 ms. Moreover, the pulse train may have an interphase delay of \u22640.3 ms, more optionally \u22640.25 ms. Additionally or alternatively, the interphase delay may be \u22650 ms,\u22650.1 ms, optionally \u22650.2 ms, more optionally 0.2 ms.\n\n[0135] With an increased pulse width, a decrease in the pulse height required to stimulate neural activity in a human splenic nerve is observed. The pulse height required to stimulate neural activity in a nerve is also referred to herein as the \u2018stimulation threshold\u2019 and the \u2018pulse height threshold\u2019.\n\n[0136] The inclusion of an interphase delay may reduce the threshold of pulse height required to stimulate neural activity in a human splenic nerve. Therefore, in some examples, the pulse train may have an interphase delay.\n\n[0137] Longer interphase delays may produce greater reductions in pulse height threshold. Accordingly, the interphase delay may have a lower limit of \u22650 ms, \u22650.1 ms, optionally \u22650.15 ms, optionally \u22650.19 ms, optionally \u22650.2 ms. At interphase delays greater than 0.3 ms it was found that there is no further reduction in pulse height threshold. Accordingly, the upper limit of interphase delay of the pulse train may be \u22640.3 ms, more optionally \u22640.25 ms. Any combination of the upper and lower limits of interphase delay is possible. Preferred ranges of interphase delay include between 0.1 ms and 0.3 ms, and between 0.2 ms and 0.25 ms. The pulses are optionally square pulses. However, other pulse waveforms such as sawtooth, sinusoidal, triangular, trapezoidal, quasitrapezodial or complex waveforms may also be used with the invention.\n\n[0138] The pulses may be biphasic in nature. The term \u201cbiphasic\u201d refers to a pulse, which applies to the nerve over time both a positive and negative charge (anodic and cathodic phases). For biphasic pulses, the pulse width includes the time duration of a primary phase of the waveform, for example the anodic phase or the cathodic phase. The primary phase may also be referred to herein as the stimulation phase.\n\n[0139] The pulses may be charge-balanced. A charge-balanced pulse refers to a pulse which, over the period of the pulse, applies equal amounts (or thereabouts) of positive and negative charge to the nerve. The biphasic pulses are preferably charge-balanced.\n\n[0140] The pulses may be symmetric or asymmetric. A symmetric pulse is a pulse where the waveform when applying a positive charge to the nerve is symmetrical to the waveform when applying a negative charge to the nerve. An asymmetric pulse is a pulse where the waveform when applying a positive charge to the nerve is not symmetrical with the waveform when applying a negative charge to the nerve.\n\n[0141] If the biphasic pulse is asymmetric, but remains charged balanced, then the areas of the opposing phases must equal. Amplitude (see below) can be reduced, but the pulse width would need to be extended to ensure the area under the curve is matched.\n\n[0142] In an exemplary embodiment, the waveform is a pulse train with biphasic, asymmetric, charge balanced square pulses.\n\nAmplitude\n\n[0143] For the purpose of the invention, the amplitude is referred to herein in terms of charge density per phase. Charge density per phase applied to the nerve by the electrical signal is defined as the integral of the current over one phase (e.g. over one phase of the biphasic pulse in the case of a charge-balanced biphasic pulse) over a stimulating electrode surface area. Thus, charge density per phase applied to the nerve by the electrical signal is the charge per phase per unit of surface area of the at least one electrode intravascularly, and also the integral of the current density over one phase of the signal waveform. Put another way, the charge density per phase applied to the nerve by the electrical signal is the charge per phase applied to the nerve by the electrical signal divided by the surface area of the at least one electrode (generally the cathode) intravascularly.\n\n[0144] The charge density per phase that is useful for the invention represents the amount of energy required to stimulate neural activity in a nerve supplying the spleen to increase immunosuppressive effects.\n\n[0145] The inventors found the current that is useful to stimulate neural activity in a splenic arterial nerve to be between 1 mA and 400 mA, 1 mA and 100 mA; preferably between 5 mA and 50 mA, optionally between 5 mA and 100 mA, preferably between 10 mA and 40 mA, preferably between 20 mA and 30 mA.\n\n[0146] The charge density per phase required to input to stimulate neural activity in a human splenic arterial nerve may be \u22644000 .Math.C per cm.sup.2 per phase, optionally between 20 .Math.C to 3500 .Math.C per cm.sup.2 per phase, optionally between 50 .Math.C to 3000 .Math.C per cm.sup.2 per phase, optionally between 200 .Math.C to 2000 .Math.C per cm.sup.2 per phase, optionally between 300 .Math.C to 1800 .Math.C per cm.sup.2 per phase, optionally between 400 .Math.C to 1500 .Math.C per cm.sup.2 per phase, optionally between 500 .Math.C to 1500 .Math.C per cm.sup.2 per phase.\n\n[0147] For example, the charge density per phase applied by the electrical signal may be \u2264100 .Math.C per cm.sup.2 per phase, \u2264150 .Math.C per cm.sup.2 per phase, \u2264200 .Math.C per cm.sup.2 per phase, \u2264250 .Math.C per cm.sup.2 per phase, \u2264300 .Math.C per cm.sup.2 per phase, \u2264400 .Math.C per cm.sup.2 per phase, \u2264500 .Math.C per cm.sup.2 per phase, \u2264750 .Math.C per cm.sup.2 per phase, \u22641000 .Math.C per cm.sup.2 per phase, \u22641250 .Math.C per cm.sup.2 per phase, or \u2264 1500 .Math.C per cm.sup.2 per phase. Additionally or alternatively, the charge density per phase applied by the electrical signal may be \u226550 .Math.C per cm.sup.2 per phase, \u2265100 .Math.C per cm.sup.2 per phase, \u2265150 .Math.C per cm.sup.2 per phase, \u2265200 .Math.C per cm.sup.2 per phase, \u2265250 .Math.C per cm.sup.2 per phase, \u2265300 .Math.C per cm.sup.2 per phase, \u2265400 .Math.C per cm.sup.2 per phase, \u2265500 .Math.C per cm.sup.2 per phase, \u2265750 .Math.C per cm.sup.2 per phase, \u2265 1000 .Math.C per cm.sup.2 per phase, or \u22651250 .Math.C per cm.sup.2.\n\n[0148] The total charge applied to the nerve by the electrical signal in any given time period is a result of the charge density per phase of the signal, in addition to the frequency of the signal, the pattern of application of the signal and the surface area of at least one electrode intravascularly. The frequency of the signal, the pattern of application of the signal and the surface area of at least one electrode intravascularly are discussed further herein.\n\n[0149] It will be appreciated by the skilled person that the amplitude of an applied electrical signal necessary to achieve the intended stimulation of the neural activity will depend upon the positioning of the electrode and the associated electrophysiological characteristics (e.g. impedance). It is within the ability of the skilled person to determine the appropriate current amplitude for achieving the intended modulation of the neural activity in a given subject.\n\n[0150] It would be of course understood in the art that the electrical signal applied to the nerve would be within clinical safety margins (e.g. suitable for maintaining nerve signaling function, suitable for maintaining nerve integrity, and suitable for maintaining the safety of the subject). The electrical parameters within the clinical safety margin would typically be determined by pre-clinical studies.\n\n[0151] The table below demonstrates example electrical signal parameters for each corresponding recruitment level of human splenic nerve using computational models. These are example values only, where a different current amplitude or pulse width may be used depending on the electrode surface area or electrode configuration of a device to achieve a corresponding charge density. In the examples given below, the electrode area is assumed to be 0.067 cm.sup.2. A range around the example values provided may also be used.\n\nTABLE-US-00001 Recruitment (10%) Recruitment (50%) Recruitment (100%) Pulse width Stimulation amplitude Charge densities Stimulation Amplitude Charge densities Stimulation Amplitude Charge densities 200 .Math.s ~84.2 mA ~250 .Math.C/cm.sup.2 ~139.7 mA ~417 .Math.C/cm.sup.2 ~385.2 mA ~1149 .Math.C/cm.sup.2 400 .Math.s ~43 mA ~250 .Math.C/cm.sup.2 ~75.25 mA ~449 .Math.C/cm.sup.2 ~209.6 mA ~1251 .Math.C/cm.sup.2 600 .Math.s \u223c28.67 mA ~250 .Math.C/cm.sup.2 ~55.55 mA ~497 .Math.C/cm.sup.2 ~152.3 mA ~1363 .Math.C/cm.sup.2 800 .Math.s ~21.5 mA ~250 .Math.C/cm.sup.2 ~44.79 mA ~534 .Math.C/cm.sup.2 ~130.8 mA ~1561 .Math.C/cm.sup.2 1000 .Math.s ~17.92 mA ~267.3 .Math.C/cm.sup.2 ~39.41 mA ~590 .Math.C/cm.sup.2 \u223c112.9 mA ~1685 .Math.C/cm.sup.2 2000 .Math.s ~16.12 mA ~481 .Math.C/cm.sup.2 ~37.62 mA ~1123 .Math.C/cm.sup.2 ~107.5 mA ~3208 .Math.C/cm.sup.2\n\nPeriodic Application\n\n[0152] Periodic application refers to where the electrical signal is applied to the nerve in a repeating pattern. The preferred repeating pattern is an on-off pattern, where the signal is applied in a sequence of pulse trains for a first duration, referred to herein as an \u2018on\u2019 duration, then stopped for a second duration, referred to herein as an \u2018off\u2019 duration, then applied again for the first duration, then stopped again for the second duration, etc.\n\n[0153] The periodic on-off pattern may have an on duration of between 0.1 and 10 s and an off duration of between 0.5 and 30 s. For example, the on duration may be \u22640.2 s, \u22640.5 s, \u22641 s, \u22642 s, \u22645 s, or \u226410 s. Alternatively or additionally, the on duration may be \u22650.1 s, \u22650.2 s, \u22650.5 s, \u2265 1 s, \u22652 s, or \u22655 s. Any combination of the upper and lower limits above for the on duration is also possible. For example, the off duration may be \u22641 s, \u22643 s, \u22645 s, \u226410 s, \u226415 s, \u226420 s, \u226425 s, or \u226430 s. Alternatively or additionally, the off duration may be \u22650.5 s, \u22651 s, \u22652 s, \u22655 s, \u226510 s, \u2265 15 s, \u226520 s, or \u226425 s. Any combination of the upper and lower limits above for the off duration is also possible.\n\n[0154] In an exemplary embodiment, the periodic on-off pattern has an on duration of 0.5 s on, and an off duration of 4.5 sec off.\n\n[0155] Periodic application may also be referred to as a duty cycled application. A duty cycle represents the percentage of time that the signal is applied to the nerve for a cycle of the periodic pattern. For example, a duty cycle of 20% may represent a periodic pattern having an on duration of 2 s, and an off duration of 10 s. Alternatively, a duty cycle of 20% may represent a periodic pattern having a on duration of 1 s, and an off duration of 5 s.\n\n[0156] Duty cycles suitable for the present invention are between 0.1% and 100%. For example, the duty cycle may be 10%.\n\nEpisodic Application\n\n[0157] Episodic application refers to where the electrical signal is applied to the nerve for a discrete number of episodes throughout a day. The electrical signal according to the invention may be applied for up to a maximum of twenty-six episodes per day. For example, the number of episodes of signal application per day may be one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve or another number up to twenty-six.\n\n[0158] The electrical signal may be applied episodically every 2 to 3 hours. For example, the electrical signal may be applied episodically once every 2 hours, 2 hour 15 min, 2 hour 30 min, 2 hour 45 min, or 3 hours.\n\n[0159] Each episode may be defined by a set duration or a set number of iterations of the electrical signal. In some embodiments, each episode comprises applying to the nerve between 10 and 2400 pulses of the electrical signal, optionally between 100 and 2400 pulses of the electrical signal, further optionally between 50 and 2400 pulses, e.g. between 200 and 1200 pulses of the electrical signal, between 400 and 600 pulses of the electrical signal, etc. For example, each episode may comprise applying \u226410, \u226450, \u226460, \u2264100, \u2264400, \u2264600, \u2264800, \u22641200, \u22641600, \u22642000, or \u22642400 pulses of the electrical signal. In another example, each episode may comprise applying \u2264200, \u2264 400, \u2264600, \u2264800, \u22641000, or \u22641200 pulses of the electrical signal. In a further example, each episode may comprise applying \u2264400, \u2264425, \u2264450, \u2264475, \u2264500, \u2264525, \u2264550, \u2264575, or \u2264600 pulses of the electrical signal.\n\n[0160] In other embodiments, each episode comprises between 20 and 40 iterations of the periodic pattern. For example, each episode comprises applying 20, 25, 30, 35, or 40 iterations of the periodic pattern, or any number therebetween. The higher the frequency, the lower the number of iterations.\n\n[0161] As mentioned previously, in some embodiments, the episodes may be based on the subject\u2019s sleep-wake cycle, in particular the episodes may be whilst the subject is asleep. In some such embodiments, the episodes may be applied between 10 pm and 6 am. This may also be influenced by the surgery times. The sleep-wake cycle may be measured via known methods by detecting the subject\u2019s circadian rhythm phase markers (e.g. cortisol level, melatonin level or core body temperature), and/or a detector for detecting the subject\u2019s movements.\n\nFrequency\n\n[0162] Frequency is defined as the reciprocal of the phase duration of the electrical waveform (i.e. \u215fphase), or put another way the inter-pulse timing (pulse-to-pulse timing).\n\n[0163] The inventors have found preferred frequencies for stimulating a splenic arterial nerve when using the device of the invention. In particular, the inventors have found preferred frequencies for embodiments where the electrical signal is applied periodically and for embodiments where the electrical signal is applied continuously.\n\n[0164] In embodiments where the electrical signal is applied periodically, the electrical signal has a frequency of \u2264300 Hz, preferably \u226450 Hz, more preferably \u226410 Hz. For example, the frequency of the electrical signal may be \u226450 Hz, \u2264100 Hz, \u2264150 Hz, \u2264200 Hz, \u2264250 Hz or \u2264300 Hz. In other examples, the frequency of the electrical signal may be \u226410 Hz, \u226415 Hz, \u226420 Hz, \u226425 Hz, \u226430 Hz, \u226435 Hz, \u226440 Hz, \u226445 Hz, or \u226450 Hz. In further examples, the frequency may be \u22641 Hz, \u22642 Hz, \u22645 Hz, or \u226410 Hz. Additionally or alternatively, the frequency of the electrical signal may be \u226510 Hz, \u226515 Hz, \u226520 Hz, \u226525 Hz, \u226530 Hz, \u226535 Hz \u226540 Hz, \u226545 Hz, or \u226550 Hz. In other examples, the frequency of the electrical signal may be \u22650.1 Hz, \u22650.2 Hz, \u22650.5 Hz, \u22651 Hz, \u22652 Hz, or \u22655 Hz. Any combination of the upper and lower limits above is also possible.\n\n[0165] In embodiments where the electrical signal is applied continuously, the electrical signal has a frequency of \u226450 Hz, preferably \u226410 Hz, more preferably \u22642 Hz, even more preferably \u2264 1 Hz. For example, the frequency may be \u22641 Hz, \u22642 Hz, \u22645 Hz, or \u226410 Hz. In other examples the frequency may be \u22640.1 Hz, \u22640.2 Hz, \u22640.3 Hz, \u22640.4 Hz \u22640.5 Hz, \u22640.6 Hz \u22640.7 Hz, \u22640.8 Hz, or \u22640.9 Hz. Additionally or alternatively, the frequency of the electrical signal may be \u22650.1 Hz, \u22650.2 Hz, \u22650.5 Hz, \u22651 Hz, \u22652 Hz, or \u22655 Hz. Any combination of the upper and lower limits above is also possible.\n\n[0166] Where the signal waveform comprises a pulse train, the pulses are applied to the nerve at intervals according to the above-mentioned frequencies. For example, a frequency of 50 Hz results in 50 pulses being applied to the nerve per second.\n\nStudy 1: Porcine Ex-Vivo Comparator Studies\n\n[0167] FIG. 23 shows results of a study in which an ex-vivo porcine splenic artery was utilized to compare nerve activation via extravascular (filled) vs intravascular (open) interfaces. Similar symbols/colors represent paired measurements. The results of three varied stent designs are shown. Current response curves have been converted to charge density (which may also be referred to as charge density per phase) to illustrate the shift in charge density requirements when comparing extravascular to intravascular interfaces. As can be seen, x-axis refers to log[C/ph/cm.sup.2]. Thus, the scale on the x-axis which spans from -6.0 to -1.5 refers to 10.sup.-6.0 to 10.sup.-1.5C/ph/cm.sup.2. The y-axis refers to area under the curve (AUC) for eCAP measurements (response curve from each experiment has been normalised). Intravascular stimulation requirements show a dextral displacement of the charge density curve by an average factor of 5.45x, ranging from 2.18 to 11.8x. Extravascular stimulation was delivered with 0.5-2 msec biphasic pulses up to 50 mA. Intravascular stimulation was delivered with 1-4 msec biphasic pulses up to 50 mA. Internal circumferential coverage of the intravascular electrodes ranged from 100% (D1/D2) to 50% (D3) of the arterial wall.\n\nGeneral Definitions\n\n[0168] The term \u201ccomprising\u201d encompasses \u201cincluding\u201d as well as \u201cconsisting\u201d e.g. a composition \u201ccomprising\u201d X may consist exclusively of X or may include something additional e.g. X + Y. The term \u201caround\u201d or \u201cabout\u201d in relation to a numerical value is optional and means, for example, x\u00b110%. Unless otherwise indicated each embodiment as described herein may be combined with another embodiment as described herein.\n\n[0169] Any range or device value given herein may be extended or altered without losing the effect sought, as will be apparent to the skilled person.\n\n[0170] It will be understood that the benefits and advantages described above may relate to one embodiment or may relate to several embodiments. The embodiments are not limited to those that solve any or all of the stated problems or those that have any or all of the stated benefits and advantages.\n\n[0171] Any reference to \u2018an\u2019 item refers to one or more of those items. The term \u2018comprising\u2019 is used herein to mean including the method blocks or elements identified, but that such blocks or elements do not comprise an exclusive list and a method or apparatus may contain additional blocks or elements.\n\n[0172] The steps of the methods described herein may be carried out in any suitable order, or simultaneously where appropriate. Additionally, individual blocks may be deleted from any of the methods without departing from the spirit and scope of the subject matter described herein. Aspects of any of the examples described above may be combined with aspects of any of the other examples described to form further examples without losing the effect sought.\n\n[0173] It will be understood that the above description of a preferred embodiment is given by way of example only and that various modifications may be made by those skilled in the art. Although various embodiments have been described above with a certain degree of particularity, or with reference to one or more individual embodiments, those skilled in the art could make numerous alterations to the disclosed embodiments without departing from the scope of this invention."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 39,
      "claims_start": 37,
      "description_end": 37,
      "description_start": 26,
      "drawings_end": 25,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 39,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 37,
      "specification_start": 26,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 74130256,
    "guid": "US-20230010306-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0010/306",
    "intl_class_current_primary": [
      {
        "intl_class": "A61N",
        "intl_subclass": "1/375",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "A61N",
        "intl_subclass": "1/05",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61N",
        "intl_subclass": "1/378",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H02J",
        "intl_subclass": "50/20",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "A61N1/375",
      "A61N1/05"
    ],
    "inventors": [
      {
        "city": "San Francisco",
        "country": "US",
        "name": "Bashirullah; Rizwan",
        "postal_code": "N/A",
        "state": "CA"
      },
      {
        "city": "Collegeville",
        "country": "US",
        "name": "Hunsberger; Gerald Edwin",
        "postal_code": "N/A",
        "state": "PA"
      }
    ],
    "inventors_short": "Bashirullah; Rizwan et al.",
    "patent_title": "STENT-ELECTRODE INTRAVASCULAR NEUROMODULATOR AND ASSOCIATED METHODS FOR ACTIVATION OF A NERVE",
    "publication_date": "2023-01-12",
    "publication_number": "20230010306",
    "related_apps": [
      {
        "country_code": "US",
        "filing_date": "2019-12-16",
        "number": "62948593"
      }
    ],
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2019-12-13",
    "appl_id": "17756995",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Mountain View",
        "country": "US",
        "name": "Google LLC",
        "state": "CA",
        "zip_code": "N/A"
      }
    ],
    "assignees": [
      {
        "city": "Mountain View",
        "country": "US",
        "name": "Google LLC",
        "postal_code": "N/A",
        "state": "CA",
        "type_code": "02"
      }
    ],
    "composite_id": "69173409!PG-US-20230009613",
    "cpc_inventive": [
      {
        "cpc_class": "G10L",
        "cpc_subclass": "13/04",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/047",
        "version": "2023-01-01"
      },
      {
        "cpc_class": "G10L",
        "cpc_subclass": "25/30",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G10L",
        "cpc_subclass": "13/047",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/044",
        "version": "2023-01-01"
      },
      {
        "cpc_class": "G10L",
        "cpc_subclass": "15/063",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G10L",
        "cpc_subclass": "13/086",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/082",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/045",
        "version": "2023-01-01"
      },
      {
        "cpc_class": "G10L",
        "cpc_subclass": "15/16",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A method (800) of training a text-to-speech (TTS) model (108) includes obtaining training data (150) including reference input text (104) that includes a sequence of characters, a sequence of reference audio features (402) representative of the sequence of characters, and a sequence of reference phone labels (502) representative of distinct speech sounds of the reference audio features. For each of a plurality of time steps, the method includes generating a corresponding predicted audio feature (120) based on a respective portion of the reference input text for the time step and generating, using a phone label mapping network (510), a corresponding predicted phone label (520) associated with the predicted audio feature. The method also includes aligning the predicted phone label with the reference phone label to determine a corresponding predicted phone label loss (622) and updating the TTS model based on the corresponding predicted phone label loss.",
      "brief": "TECHNICAL FIELD\n\n[0001] This disclosure relates to training speech synthesis to generate distinct speech sounds.\n\nBACKGROUND\n\n[0002] Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. For instance, neural networks may convert input text to output speech. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.\n\n[0003] Speech synthesis systems (i.e., neural networks that convert input text to output speech) are trained to generate natural and intelligible speech. These systems are typically trained to minimize the distance between the hypothesized representation of a speech signal (i.e., the output of the network) and a reference (i.e., training) speech signal.\n\nSUMMARY\n\n[0004] One aspect of the disclosure provides a method of training a text-to-speech (TTS) model. The method includes obtaining, at data processing hardware, training data that includes reference input text that includes a sequence of characters in a particular language, a sequence of reference audio features representative of the sequence of characters, and a sequence of reference phone labels representative of distinct speech sounds for the sequence of reference audio features. For each of a plurality of time steps, the method includes generating, by the data processing hardware, a corresponding predicted audio feature based on a respective portion of the reference input text for the time step and generating, by the data processing hardware, using a phone label mapping network, a corresponding predicted phone label associated with the predicted audio feature for the time step. The method also includes aligning, by the data processing hardware, the predicted phone label for the time step with the reference phone label for the time step to determine a corresponding predicted phone label loss and updating, by the data processing hardware, the TTS model based on the corresponding predicted phone label loss determined for each of the plurality of time steps.\n\n[0005] Implementations of the disclosure may include one or more of the following optional features. In some implementations, the method further includes, for each of the plurality of time steps, determining, by the data processing hardware, a corresponding predicted audio feature loss based on the corresponding predicted audio feature for the time step and the reference audio feature of the sequence of reference audio features for the time step. Updating the TTS model may be further based on the corresponding predicted audio feature loss determined for each of the plurality of time steps.\n\n[0006] In some examples, the method further includes, executing, by the data processing hardware, using a speech recognition model, a forced-alignment process to time-align the sequence of reference audio features with the sequence of reference phone labels. Optionally, generating, for each of the plurality of time steps, the corresponding predicted audio feature may include processing, using an encoder neural network, the sequence of characters to generate a feature representation of the sequence of characters and processing, using an attention network, the feature representation to generate a fixed-length context vector for the time step. Generating the corresponding predicted audio feature may also include processing, using a decoder neural network, the fixed-length context vector for the time step to generate the corresponding predicted audio feature for the time step.\n\n[0007] In some implementations, the encoder neural network includes a convolutional layer and a bidirectional long short-term memory (LSTM) layer and the feature representation includes a sequential feature representation that represents a local structure of the sequence of characters around a particular character in the sequence of characters. In some examples, the decoder neural network includes an autoregressive neural network that includes a convolutional subnetwork and an output layer. The corresponding predicted audio feature generated for each of the plurality of time steps may be further based on the reference audio feature of the sequence of reference audio features for a previous time step and the reference phone label of the sequence of reference audio features for the previous time step.\n\n[0008] In some implementations, generating, for each of the plurality of time steps, using the phone label mapping network, the corresponding predicted phone label includes receiving, at the phone label mapping network, the predicted audio feature generated for the corresponding time step as a feature input, processing, by the phone label mapping network, the received predicted audio feature generated for the corresponding time step to generate the corresponding predicted phone label for the corresponding time step, and mapping, by the phone label mapping network, the corresponding predicted phone label generated for the corresponding time step to the predicted audio feature generated for the corresponding time step.\n\n[0009] In some examples, the reference audio features include reference mel-frequency spectrograms and the predicted audio features include predicted mel-frequency spectrograms. The sequence of reference audio features may correspond to a reference time-domain audio waveform, each of the plurality of time steps may correspond to a respective time in a predicted audio waveform, and the corresponding predicted audio feature generated for each of the plurality of time steps may include an amplitude value of the predicted audio waveform at the corresponding time step.\n\n[0010] In some implementations, the TTS model includes an encoder neural network, an attention network, and a decoder neural network. In other implementations, the TTS model includes a parametric synthesizer model having a single frame synchronous neural network.\n\n[0011] Another aspect of the disclosure provides a system for training a text-to-speech model. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware cause the data processing hardware to perform operations. The operations include obtaining training data that includes reference input text that includes a sequence of characters in a particular language, a sequence of reference audio features representative of the sequence of characters, and a sequence of reference phone labels representative of distinct speech sounds for the sequence of reference audio features. For each of a plurality of time steps, the operations include generating a corresponding predicted audio feature based on a respective portion of the reference input text for the time step and generating using a phone label mapping network, a corresponding predicted phone label associated with the predicted audio feature for the time step. The operations also include aligning the predicted phone label for the time step with the reference phone label for the time step to determine a corresponding predicted phone label loss and updating the TTS model based on the corresponding predicted phone label loss determined for each of the plurality of time steps.\n\n[0012] This aspect may include one or more of the following optional features. In some implementations, the operations further include, for each of the plurality of time steps, determining a corresponding predicted audio feature loss based on the corresponding predicted audio feature for the time step and the reference audio feature of the sequence of reference audio features for the time step. Updating the TTS model may be further based on the corresponding predicted audio feature loss determined for each of the plurality of time steps.\n\n[0013] In some examples, the operations further include executing, using a speech recognition model, a forced-alignment process to time-align the sequence of reference audio features with the sequence of reference phone labels. Optionally, generating, for each of the plurality of time steps, the corresponding predicted audio feature may include processing, using an encoder neural network, the sequence of characters to generate a feature representation of the sequence of characters and processing, using an attention network, the feature representation to generate a fixed-length context vector for the time step. Generating the corresponding predicted audio feature may also include processing, using a decoder neural network, the fixed-length context vector for the time step to generate the corresponding predicted audio feature for the time step.\n\n[0014] In some implementations, the encoder neural network includes a convolutional layer and a bidirectional long short-term memory (LSTM) layer and the feature representation includes a sequential feature representation that represents a local structure of the sequence of characters around a particular character in the sequence of characters. In some examples, the decoder neural network includes an autoregressive neural network that includes a convolutional subnetwork and an output layer. The corresponding predicted audio feature generated for each of the plurality of time steps may be further based on the reference audio feature of the sequence of reference audio features for a previous time step and the reference phone label of the sequence of reference audio features for the previous time step.\n\n[0015] In some implementations, generating, for each of the plurality of time steps, using the phone label mapping network, the corresponding predicted phone label includes receiving, at the phone label mapping network, the predicted audio feature generated for the corresponding time step as a feature input, processing, by the phone label mapping network, the received predicted audio feature generated for the corresponding time step to generate the corresponding predicted phone label for the corresponding time step, and mapping, by the phone label mapping network, the corresponding predicted phone label generated for the corresponding time step to the predicted audio feature generated for the corresponding time step.\n\n[0016] In some examples, the reference audio features include reference mel-frequency spectrograms and the predicted audio features include predicted mel-frequency spectrograms. The sequence of reference audio features may correspond to a reference time-domain audio waveform, each of the plurality of time steps may correspond to a respective time in a predicted audio waveform, and the corresponding predicted audio feature generated for each of the plurality of time steps may include an amplitude value of the predicted audio waveform at the corresponding time step.\n\n[0017] In some implementations, the TTS model includes an encoder neural network, an attention network, and a decoder neural network. In other implementations, the TTS model includes a parametric synthesizer model having a single frame synchronous neural network.\n\n[0018] The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.",
      "claims": "1. A method of training a text-to-speech (TTS) model, the method comprising: obtaining, at data processing hardware, training data comprising: reference input text, the reference input text comprising a sequence of characters in a particular language; a sequence of reference audio features representative of the sequence of characters; and a sequence of reference phone labels representative of distinct speech sounds for the sequence of reference audio features;  for each of a plurality of time steps: generating, by the data processing hardware, corresponding predicted audio feature based on a respective portion of the reference input text for the time step; generating, by the data processing hardware, using a phone label mapping network, a corresponding predicted phone label associated with the predicted audio feature for the time step; and aligning, by the data processing hardware, the predicted phone label for the time step with the reference phone label for the time step to determine a corresponding predicted phone label loss; and  updating, by the data processing hardware, the TTS model based on the corresponding predicted phone label loss determined for each of the plurality of time steps.  \n\n2. The method of claim 1, further comprising: for each of the plurality of time steps, determining, by the data processing hardware, a corresponding predicted audio feature loss based on the corresponding predicted audio feature for the time step and the reference audio feature of the sequence of reference audio features for the time step, wherein updating the TTS model is further based on the corresponding predicted audio feature loss determined for each of the plurality of time steps.  \n\n3. The method of claim 1, further comprising, executing, by the data processing hardware, using a speech recognition model, a forced-alignment process to time-align the sequence of reference audio features with the sequence of reference phone labels. \n\n4. The method of claim 1, wherein generating, for each of the plurality of time steps, the corresponding predicted audio feature comprises: processing, using an encoder neural network, the sequence of characters to generate a feature representation of the sequence of characters; processing, using an attention network, the feature representation to generate a fixed-length context vector for the time step; and processing, using a decoder neural network, the fixed-length context vector for the time step to generate the corresponding predicted audio feature for the time step.  \n\n5. The method of claim 4, wherein: the encoder neural network comprises a convolutional layer and a bidirectional long short-term memory (LSTM) layer; and the feature representation comprises a sequential feature representation that represents a local structure of the sequence of characters around a particular character in the sequence of characters.  \n\n6. The method of claim 4, wherein the decoder neural network comprises an autoregressive neural network comprising a convolutional subnetwork and an output layer. \n\n7. The method of claim 1, wherein the corresponding predicted audio feature generated for each of the plurality of time steps is further based on the reference audio feature of the sequence of reference audio features for a previous time step and the reference phone label of the sequence of reference audio features for the previous time step. \n\n8. The method of claim 1, wherein generating, for each of the plurality of time steps, using the phone label mapping network, the corresponding predicted phone label comprises: receiving, at the phone label mapping network, the predicted audio feature generated for the corresponding time step as a feature input; processing, by the phone label mapping network, the received predicted audio feature generated for the corresponding time step to generate the corresponding predicted phone label for the corresponding time step; and mapping, by the phone label mapping network, the corresponding predicted phone label generated for the corresponding time step to the predicted audio feature generated for the corresponding time step.  \n\n9. The method of claim 1, wherein the reference audio features comprise reference mel-frequency spectrograms and the predicted audio features comprise predicted mel-frequency spectrograms. \n\n10. The method of claim 1, wherein: the sequence of reference audio features corresponds to a reference time-domain audio waveform; each of the plurality of time steps corresponds to a respective time in a predicted audio waveform; and the corresponding predicted audio feature generated for each of the plurality of time steps comprises an amplitude value of the predicted audio waveform at the corresponding time step.  \n\n11. The method of claim 1, wherein the TTS model comprises an encoder neural network, an attention network, and a decoder neural network. \n\n12. The method of claim 1, wherein the TTS model comprises a parametric synthesizer model having a single frame synchronous neural network. \n\n13. A system for training a text-to-speech model, the system comprising: data processing hardware; and memory hardware in communication with the data processing hardware, the memory hardware storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising: obtaining training data comprising: reference input text, the reference input text comprising a sequence of characters in a particular language; a sequence of reference audio features representative of the sequence of characters; and a sequence of reference phone labels representative of distinct speech sounds for the sequence of reference audio features; for each of a plurality of time steps: generating, corresponding predicted audio feature based on a respective portion of the reference input text for the time step; generating, using a phone label mapping network, a corresponding predicted phone label associated with the predicted audio feature for the time step; and aligning the predicted phone label for the time step with the reference phone label for the time step to determine a corresponding predicted phone label loss; and updating the TTS model based on the corresponding predicted phone label loss determined for each of the plurality of time steps.   \n\n14. The system of claim 13, wherein the operations further comprise: for each of the plurality of time steps, determining a corresponding predicted audio feature loss based on the corresponding predicted audio feature for the time step and the reference audio feature of the sequence of reference audio features for the time step, wherein updating the TTS model is further based on the corresponding predicted audio feature loss determined for each of the plurality of time steps.  \n\n15. The system of claim 13, wherein the operations further comprise, executing using a speech recognition model, a forced-alignment process to time-align the sequence of reference audio features with the sequence of reference phone labels. \n\n16. The system of claim 13, wherein generating, for each of the plurality of time steps, the corresponding predicted audio feature comprises: processing, using an encoder neural network, the sequence of characters to generate a feature representation of the sequence of characters; processing, using an attention network, the feature representation to generate a fixed-length context vector for the time step; and processing, using a decoder neural network, the fixed-length context vector for the time step to generate the corresponding predicted audio feature for the time step.  \n\n17. The system of claim 16, wherein: the encoder neural network comprises a convolutional subnetwork and a bidirectional long short-term memory (LSTM) layer; and the feature representation comprises a sequential feature representation that represents a local structure of the sequence of characters around a particular character in the sequence of characters.  \n\n18. The system of claim 16, wherein the decoder neural network comprises an autoregressive neural network comprising a convolutional subnetwork and an output layer. \n\n19. The system of claim 13, wherein the corresponding predicted audio feature generated for each of the plurality of time steps is further based on the reference audio feature of the sequence of reference audio features for a previous time step and the reference phone label of the sequence of reference audio features for the previous time step. \n\n20. The system of claim 13, wherein generating, for each of the plurality of time steps, using the phone label mapping network, the corresponding predicted phone label comprises: receiving, at the phone label mapping network, the predicted audio feature generated for the corresponding time step as a feature input; processing, by the phone label mapping network, the received predicted audio feature generated for the corresponding time step to generate the corresponding predicted phone label for the corresponding time step; and mapping, by the phone label mapping network, the corresponding predicted phone label generated for the corresponding time step to the predicted audio feature generated for the corresponding time step.  \n\n21. The system of claim 13, wherein the reference audio features comprise reference mel-frequency spectrograms and the predicted audio features comprise predicted mel-frequency spectrograms. \n\n22. The system of claim 13, wherein: the sequence of reference audio features corresponds to a reference time-domain audio waveform; each of the plurality of time steps corresponds to a respective time in a predicted audio waveform; and the corresponding predicted audio feature generated for each of the plurality of time steps comprises an amplitude value of the predicted audio waveform at the corresponding time step.  \n\n23. The system of claim 13, wherein the TTS model comprises an encoder neural network, an attention network, and a decoder neural network. \n\n24. The system of claim 13, wherein the TTS model comprises a parametric synthesizer model having a single frame synchronous neural network.",
      "description": "DESCRIPTION OF DRAWINGS\n\n[0019] FIG. 1 is a schematic view of an example system for training speech synthesis.\n\n[0020] FIG. 2 is a schematic view of an example architecture for a decoder neural network.\n\n[0021] FIG. 3 is a schematic view of an example architecture of an autoregressive neural network.\n\n[0022] FIG. 4 is a schematic view of training the decoder neural network with multiple time steps.\n\n[0023] FIG. 5 is a schematic view of training the decoder neural network with a phone label mapping network and multiple time steps.\n\n[0024] FIG. 6 is a schematic view of loss functions for the decoder neural network.\n\n[0025] FIG. 7 is a schematic view of the decoder neural network during inference with multiple time steps.\n\n[0026] FIG. 8 is a flowchart of an example arrangement of operations for a method of training speech synthesis to generate distinct speech sounds.\n\n[0027] FIG. 9 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.\n\n[0028] Like reference symbols in the various drawings indicate like elements.\n\nDETAILED DESCRIPTION\n\n[0029] Speech synthesis systems (e.g., traditional parametric and end-to-end neural network learning models) are trained to generate natural and intelligible speech. These systems are trained to minimize the distance between a hypothesized representation of a speech signal and a reference (i.e., training) signal. These representations may be a variety of formats (e.g., vocoder parameters, filterbank coefficients, direct waveform representations, etc.) and are directly convertible to audible speech. The distance between the hypothesized representation generated by the system and the reference signal is measured using a loss function. For example, loss functions such as mean absolute error, mean square error, or mean bias error may be used to indicate the accuracy of the prediction (i.e., the hypothesis) and may be fed back to the model to optimize or train parameters (e.g., weights) of the model.\n\n[0030] However, the resultant hypothesized signal is interpreted as categorical speech sounds (i.e., phones) by listeners to understand the speech. This relationship sound is not used in the training of speech synthesis. This leads to less intelligible synthetic speech because some speech sounds (e.g., the fricative consonants /\u222b/ and /f/ or the vowels /\u00e6/ and /a/) are perceptually distinct to listeners despite being relatively close in the signal space. That is, the loss function measuring the accuracy of these speech sounds may indicate an accurate prediction despite being unintelligible to a listener.\n\n[0031] Thus, implementations herein are directed toward a system for training speech synthesis models (i.e., parametric or end-to-end text-to-speech (TTS) models) to generate distinct speech sounds by using audio data that includes frame-aligned phone labels. Here, the speech synthesis model includes a phone label mapping network configured to predict a phone label for every frame of a speech representation (e.g., predicted audio feature) and a cross-entropy loss term that measures the distance between the speech representation and a reference audio feature (e.g., a ground truth acoustic signal) and a reference phone label.\n\n[0032] Referring now to FIG. 1, in some implementations, an example system 100 includes a computing device 10 executing a text-to-speech (TTS) model 108. The computing device 10 may correspond to any computing device, such as a desktop workstation, a laptop workstation, or a mobile device (i.e., a smart phone). The computing device may also be a single computer, multiple computers, or a distributed system (e.g., a cloud environment) having scalable / elastic computing resources 18 (e.g., data processing hardware) and/or storage resources 16 (e.g., memory hardware). A data store 50 is overlain on the storage resources 16 to allow scalable use of the storage resources 16 by the computing resources 18.\n\n[0033] The TTS model 108 receives input text 104 and processes that input text 104 (i.e., a sequence of characters) through one or more neural networks to generate speech 106 (i.e., audio data representative of the sequence of characters 104) that is a verbalization or a narration of the input text 104. For example, when the TTS model 108 receives as input the text of a page of a book, the model 108 processes the text to generate a verbal narration of the page.\n\n[0034] The computing device 10 also executes subsystems 102 including an input subsystem 102, 102A, configured to receive the input text 104 as input, and an output subsystem 102, 102B, configured to provide speech 106 as output. That is, the input subsystem 102A may receive the text input 104 while the output subsystem 102B may output the audio data 106 representative of the text input 104. The input text 104 includes a sequence of characters in a particular natural language, e.g., English, Spanish, or French. The sequence of characters can include letters, numbers, punctuation marks, and/or other special characters. The speech 106 that the TTS model 108 generates approximates human speech, i.e., how a human would verbalize the sequence of characters that make up the input text 104.\n\n[0035] The input subsystem 102A may include an optical character recognition (OCR) unit to convert images of typed, handwritten, or printed text into machine-encoded text. The output subsystem 102B, in some examples, includes an audio output device to convert time-domain audio waveforms into audio. In some implementations, the input subsystem 102A is configured to convert each character in the sequence of characters 104 (i.e., the input text) into a one-hot vector and embed each one-hot vector in a continuous vector. That is, the input subsystem 102A may represent each character in the sequence as a one-hot vector and then generate an embedding 103, i.e., a vector or other ordered collection of numeric values, of the character.\n\n[0036] The TTS model 108 may be configured to receive character embeddings 103 from the input subsystem 102A and process those character embeddings 103 to generate time-domain audio waveforms 119. The time-domain audio waveforms 119 are audio waveforms that define an amplitude of an audio signal over time. In particular, an encoder neural network 110 of the TTS model 108 is configured to receive the character embeddings 103 from the input subsystem 102A and generate a fixed-length context vector 115 for each mel-frequency spectrogram 118 that a decoder neural network 114 will later generate. The fixed-length context vectors 115 define features that appear in particular positions in the sequence of characters 104. The features defined by the context vectors 115 model the context in which each character in the sequence of characters 104 appears.\n\n[0037] In some implementations, the encoder neural network 110 includes one or more convolutional layers 111 followed by a bi-directional long short-term memory (\u201cLSTM\u201d) layer 112. The neurons in each convolutional layer may receive input from only a small subset of neurons in a previous layer. This neuron connectivity allows the convolutional layers to learn filters that activate when particular hidden features appear in particular positions in a sequence of characters 104. In some examples, each filter in each convolutional layer spans four, five, or six characters. Each convolutional layer may be followed by batch normalization and rectified linear units (\u201cReLUs\u201d). In implementations where the encoder neural network 110 includes one or more convolutional layers, a bi-directional LSTM layer 112 may optionally follow those convolutional layers. The bi-directional LSTM layer 112 may be configured to process the hidden features generated by the final convolutional layer 111 to generate a sequential feature representation of the sequence of characters 104. As used herein, a sequential feature representation represents a local structure of the sequence of characters 104 around a particular character. In some examples, the sequential feature representation includes a sequence of feature vectors.\n\n[0038] In some implementations, the encoder neural network 110 also includes an attention network 113. The attention network 113 may be configured to receive a sequential feature representation from another component of the encoder neural network 110, e.g., a bi-directional LSTM layer 112, and process the sequential feature representation to generate a fixed-length context vector 115 for each decoder output step. That is, the attention network 113 may generate a fixed-length context vector 115 for each frame of a mel-frequency spectrogram 118 that the decoder neural network 114 will later generate. A frame is a unit of the mel-frequency spectrogram 118 that is based on a small portion of the input signal, e.g., a 10 millisecond sample of the input signal. The attention network 113 may determine a weight for each element of the encoder output and generates the fixed-length context vector 115 by determining a weighted sum of each element. The attention weights may change for each decoder time step.\n\n[0039] The decoder neural network 114 is configured to receive as input the fixed-length context vectors 115 and generate as output a corresponding frame of a mel-frequency spectrogram 118. The mel-frequency spectrogram 118 is a frequency-domain representation of sound. Mel-frequency spectrograms emphasize lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. In some implementations, the decoder neural network 114  generates mel-frequency spectrograms 118 having a frame length of 12.5 milliseconds. In other implementations, the decoder neural network 114 is configured to generate mel-frequency spectrograms 118 having frame lengths less than or greater than 12.5 milliseconds. The architecture of the decoder neural network 114 will be described in more detail below with reference to FIG. 2.\n\n[0040] In some examples, the decoder 114 includes an autoregressive neural network 116 configured to receive mel-frequency spectrograms 118 and generate corresponding audio output samples 119 based on the mel-frequency spectrograms 118. The decoder 114 may include a vocoder network 117. For example, the vocoder network 117 may be based on a parallel feed-forward neural network.\n\n[0041] The autoregressive neural network 116 may receive the mel-frequency spectrograms 118 from the decoder neural network 114 and operate in an auto-regressive manner over multiple time steps. At a given time step, the autoregressive neural network 116 generates a probability distribution over a plurality of possible audio output samples 119 for the time step conditioned on the following: (i) a current output sequence of audio data 119, including respective audio output samples 119 for one or more preceding time steps in the output sequence of audio data, and (ii) the mel-frequency spectrogram 118 for the time step. The output subsystem 102B, in some examples, selects the audio sample 119 for the current step using the probability distribution for the time step. An exemplary architecture of the autoregressive neural network 116 is described in more detail below with reference to FIG. 3. The output subsystem 102B, after selecting the audio output sample 106 for the time step from the probability distribution, may output/play the audio output sample 106 through an audio output device (e.g., a speaker).\n\n[0042] Thus, the TTS model 108 may generate speech from text using neural networks trained on sample input texts 104 and corresponding mel-frequency spectrograms 118 of human speech alone. That is, the model 108 does not receive complex linguistic and acoustic features that require significant domain expertise to produce. Instead, the system may convert input character sequences 104 to mel-frequency spectrograms 118 using an end-to-end neural network. In some examples, the autoregressive neural network 116 of the decoder 114 is separately-trained on mel-frequency spectrograms 118 and may generate time-domain audio waveforms 119.\n\n[0043] Referring now to FIG. 2, an example decoder architecture 200 for the decoder neural network 114 includes a pre-net 210 through which a mel-frequency spectrogram prediction for a previous time step passes. The pre-net 210 may include two fully-connected layers of hidden ReLUs. The pre-net 210 acts as an information bottleneck for learning attention to increase convergence speed and to improve generalization capability of the speech synthesis system during training. In order to introduce output variation at inference time, dropout with probability 0.5 may be applied to layers in the pre-net.\n\n[0044] The decoder architecture 200, in some implementations, also includes an LSTM subnetwork 220 with two or more LSTM layers. At each time step, the LSTM subnetwork 220 receives a concatenation of the output of the pre-net 210 and a fixed-length context vector 202 for the time step. The LSTM layers may be regularized using zoneout with probability of, for example, 0.1. A linear projection 230 receives as input the output of the LSTM subnetwork 220 and produces a prediction of the mel-frequency spectrogram 118P.\n\n[0045] In some examples, a convolutional post-net 240 with one or more convolutional layers processes the predicted mel-frequency spectrogram 118P for the time step to predict a residual 242 to add to the predicted mel-frequency spectrogram 118P at adder 244. This improves the overall reconstruction. Each convolutional layer except for the final convolutional layer may be followed by batch normalization and hyperbolic tangent (TanH) activations. The convolutional layers are regularized using dropout with a probability of, for example, 0.5. The residual 242 is added to the predicted mel-frequency spectrogram 118P generated by the linear projection 230, and the sum (i.e., the mel-frequency spectrogram 118) may be provided to the autoregressive network 116 (FIG. 1).\n\n[0046] In some examples, in parallel to the decoder neural network 114 predicting mel-frequency spectrograms 118 for each time step, a concatenation of the output of the LSTM subnetwork 220 and the fixed-length context vector 115 is projected to a scalar and passed through a sigmoid activation to predict the probability that the output sequence of mel frequency spectrograms 118 has completed. This \u201cstop token\u201d prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. When the stop token indicates that generation has terminated, i.e., when the stop token probability exceeds a threshold value, the decoder neural network 114 stops predicting mel-frequency spectrograms 118P and returns the mel-frequency spectrograms predicted up to that point. Alternatively, the decoder neural network 114 may always generate mel-frequency spectrograms 118 of the same length (e.g., 10 seconds).\n\n[0047] Referring now to FIG. 3, an exemplary autoregressive network architecture 300 generates the sequences of audio data 119 each including a respective audio output sample at each of multiple time steps. Generally, each time step in a given audio output sequence corresponds to a respective time in an audio waveform. The audio output sample at the time step characterizes the waveform at the corresponding time. In some implementations, the audio output sample 119 at each time step in the audio output sequence is the amplitude of the audio waveform at the corresponding time, i.e., the audio output sequence 119 generated by the architecture 300 is a raw audio waveform. In some implementations, the audio output sample 119 at each time step in the sequence is a compressed representation of the waveform at the corresponding time. For example, the audio output sample is a .Math.-law transformed representation of the waveform.\n\n[0048] In some implementations, the architecture 300 receives as input, at each of multiple time steps, a mel-frequency spectrogram 118 and generates a corresponding audio output sample 119 at each of the multiple time steps by conditioning the architecture 300 on both the mel-frequency spectrogram 118 and at least a portion of current audio output sequence (i.e., one or more audio output samples 118 from preceding time steps). Together, the audio output samples 119 form an audio output sequence 106.\n\n[0049] Specifically, at each time step during the generation of an audio output sequence 119, a convolutional subnetwork 310 receives a current audio output sequence 340 that includes audio output samples 119 that the architecture 300 has already generated previous to the current time step. The convolutional subnetwork 310 processes the current audio output sequence 340, conditioned on a mel-frequency spectrogram 118 for the time step, to generate an alternative representation 344 for the time step. The alternative representation 344 may be a numeric representation, i.e., an ordered collection of numeric values, in which the current audio output sequence 340 has been encoded by the convolutional subnetwork, for example by encoding features of the current audio output sequence 340.\n\n[0050] In some examples, the output layer 320 is configured to, at each of the time steps, receive the alternative representation 344 and generate a probability distribution over possible audio output samples 119 for the time step. In some implementations, the output layer 320 is a logistic output layer that generates parameters, e.g., mean, log scale, or mixture weight, of a logistic mixture distribution over possible audio output samples 119 for the time step. The logistic output layer may include ReLUs followed by a linear projection. In other implementations, the output layer 320 is a softmax output layer that generates a discrete probability distribution over possible audio samples 119 for the time step. That is, each possible audio sample 119 is associated with a discrete score.\n\n[0051] Once the output layer 320 has generated the score distribution for a given time step, the output subsystem 102B (FIG. 1), or some other external subsystem, may select an audio output sample 119 to be included in the audio output sequence 106 in accordance with the score distribution for the given time step. In some examples, the convolutional subnetwork 310 generally includes multiple audio-processing convolutional neural network layers. More specifically, the audio-processing convolutional neural network layers may include multiple causal convolutional layers.\n\n[0052] A causal convolutional layer is a convolutional layer that operates on an input sequence 104 that has a respective input at each of multiple time steps by, for each time step, generating an output that depends only on the inputs at the time step and at the time steps before the time step in the input sequence and not on any inputs at any time steps after the time step in the input sequence. In some cases, the causal convolutional layers are implemented by applying a normal convolution and then shifting each output of the normal convolution by a few time steps, i.e., shifting each output forward by (filter length - 1) time steps, prior to applying the activation function for the convolutional layer, where \u201cfilter length\u201d is the length of the filter of the convolution that is being applied.\n\n[0053] Referring now to FIG. 4, a schematic view 400 shows an example maximum likelihood training procedure typically used for training decoder neural networks 114. During the maximum likelihood training procedure, the decoder neural network 114 receives as input, at each time step, an output from the encoder 110 (i.e., the fixed-length context vector 115) and a reference audio feature 402, 402a-n (i.e., a \u201cground truth\u201d acoustic target) from the previous time step and generates a corresponding predicted audio feature 120 (e.g., a hypothesis). This is known as teacher-forcing. For example, at \u201cTime Step 1\u201d, the decoder 114 receives the reference audio feature 402 associated with \u201cTime Step 0\u201d (not shown). Similarly, at \u201cTime Step 2\u201d, the decoder 114 receives the reference audio feature 402 associated with \u201cTime Step 1\u201d, so on and so forth. That is, instead of providing the decoder 114 with the hypothesis/predicted audio feature 120, 120a-n (e.g., output mel-frequency spectrogram 118, 118a-n, output time-domain audio waveform 119, output filter coefficients, or vocoder parameters)) from the previous time step as it done during inference, the hypothesis/predicted audio feature 120 is replaced with the reference audio feature 402 during training. The training data 150 (FIG. 1) may include sample input reference texts 104T including a sequence of characters in a particular language and a sequence of reference audio features 402 representative of the sequence of characters. The reference audio features 402 may include reference filter coefficients, reference mel-frequency spectrograms, reference audio samples of a time-domain waveform, or reference vocoder parameters.\n\n[0054] The distance between the resulting audio output sequence 106 (FIG. 1) and the sequence of reference audio samples 402 is determined via a loss function (e.g., using mean absolute error). This distance may be small for certain speech sounds (e.g., some fricative consonants) due to small spectral differences, but despite the small differences, these certain speech sounds may be distinct to listeners. Inaccuracy of these speech sounds may significantly increase the unintelligibility of the resulting audio.\n\n[0055] FIG. 5 shows a schematic view 500 of the TTS model 108 incorporating a phone label mapping network 510 during training to increase the accuracy of the predicted audio features 120 (e.g., mel-frequency spectrograms 118 or predicted audio samples of a time-domain audio waveform) of the decoder 114 for distinct sounds with similar spectral properties. In the example shown, the phone label mapping network 510 receives, for each of a plurality of time steps during training (i.e., each frame), the predicted audio feature 120 for the time step output from the decoder 114 and maps the predicted audio feature 120 to a corresponding phone label 520, 520a-n for the time step representative of a distinct speech sound. That is, the phone label mapping network 510  receives the predicted audio feature 120 generated for the corresponding time step as a feature input, processes the received predicted audio feature 120 to generate the predicted phone label 520 for the corresponding time step, and maps the corresponding predicted phone label 520 generated for the corresponding time step to the predicted audio feature 120 generated for the corresponding time step. As used herein, a phone label 520 refers to any distinct speech sound regardless of whether the exact sound is critical to the meaning of words (as contrasted with phonemes). To put another way, a phone label, or simply phone, is a speech segment that possesses distinct spectral properties to a human listener and serves as a basic unit of phonetic speech analysis. The decoder 114 receives an output from the encoder 110 (i.e., the fixed-length context vector 115) and reference audio features 402, 402a-n with frame aligned phone labels 502, 502a-n of the previous time step. That is, the decoder 114 receives similar inputs as in the maximum likelihood training procedure of FIG. 4, but each reference audio feature 402 of the training data 150 additionally includes a corresponding frame-aligned phone label 502. Here, the frame-aligned phone labels 502 are used during the teacher-forcing training to increase the accuracy of the phone label mapping network 510 and the decoder network 114 such that the predicted audio features (e.g., mel-frequency spectrograms 118) predicted by the decoder 410 learn to match the reference audio feature 402 and the corresponding predicted phone labels 520 learn to match the frame-aligned reference phone labels 502.\n\n[0056] Referring now to FIG. 6, in some implementations, an example decoder training procedure 600 trains the phone label mapping network 510 and the decoder 114 simultaneously. In this case, the training data 150 includes reference input text 104 including a sequence of characters in a particular language; a sequence of reference audio features 402 representative of the sequence of characters, and a sequence of reference phone labels 502 represented of distinct speech sounds for the sequence of reference audio features 402. In some examples, the training procedure 600 executes, using a speech recognition model, a forced-alignment process to time-align the sequence of reference audio features 402 with the sequence of reference phone labels 502. These reference phone labels 502 correspond to \u201cground truth\u201d phone label targets 502 for determining the accuracy of the predicted phone label 520 output by the phone label mapping network 510 using a loss function. In some implementations, the predicted phone label 520 for each time step is aligned with the reference phone label to determine a corresponding phone label loss 622. The phone label loss 622 may update the TTS model 108. In some implementations, for each of the plurality of time steps, a decoder loss function 610 determines a corresponding predicted audio feature loss 612 based on the corresponding predicted audio feature 120 for the time step and the reference audio feature 402 of the sequence of reference audio features 402 for the time step. In some examples, the TTS model 108 is updated based on the corresponding predicted audio feature loss 612 determined for each of the plurality of time steps\n\n[0057] In the example shown, during each time step, a cross-entropy loss function 630 combines a decoder loss 612 (i.e., a predicted audio feature loss) associated with the predicted audio feature 120 output from the decoder 114 and a phone label loss 622 associated with the corresponding predicted phone label 520 output from the phone mapper (e.g., phone label mapping network) 510. Put another way, the cross-entropy loss function 630 adds or otherwise combines the loss (e.g., decoder loss 612) of the time step frame of the spectrogram 118 generated by the decoder network 114 (i.e., a measure of the inaccuracy of the hypothesis output) with the loss (e.g., phone label loss 622) of the corresponding mapped phone label 520 generated by the phone label mapping network 510 (i.e., a measure of the inaccuracy of the phone label mapping network hypothesis). For example, a decoder loss function 610 may be represented by L.sub.D(x, x\u02b9) to produce the decoder loss 612 measuring an accuracy of the predicted audio feature 120 (e.g., mel-frequency spectrogram 118) and a phone label loss function may be represented as L.sub.PH(ph, ph\u02b9) to produce the phone label loss 622 measuring an accuracy of the predicted phone label 520. In this example, the cross entropy loss function 630 may combine the loss functions 610, 620 and be represented as follows.\n\nLCE(x,ph,x\u2032,ph\u2032)=LD(x,x\u2032)+c\u2217LPH(ph,ph\u2032)\n\n[0058] Here, L.sub.CE represents a cross-entropy loss result 632, x represents the reference audio feature 402, ph represents the frame-aligned reference phone label 502, x\u02b9 represents the predicted audio feature 120, and ph\u02b9 represents the predicted phone label 520. In some examples, the TTS model 108 is updated based on the corresponding predicted phone label loss 622 and/or the decoder loss 612 determined for each time step.\n\n[0059] The phone label loss function 620, in some examples, is multiplied by a constant (i.e., c) to apply a weight to the mapped predicted phone labels 520. Here, the constant weights the contribution of the predicted phone label 520 in training the decoder 114 to predict speech sounds. For example, increasing a magnitude of the constant increases the magnitude of weight or contribution of the predicted phone label 520 mapping for updating the TTS model, while decreasing the magnitude of the constant decreases the magnitude of the weight or contribution of the predicted phone label 520 mapping for updating the TTS model. The weight may be predetermined or adjusted dynamically based on, for example, the training data 150. Implementing the phone label loss function 620, in some implementations, includes comparing the phone label hypothesis 520 predicted by the phone mapper 510 to the ground truth phone label targets 502 of the training data 150 using a separate forced alignment procedure. Importantly, if the text input 104 is phone based, there is no requirement that the ground truth phone label targets 502 generated from the forced alignment procedure match or be identical to the phones of the input text 104 (though they may be).\n\n[0060] The cross-entropy loss result 632 of the combined loss function 630, in some implementations, is returned back to the decoder network 114 (and, in some examples, the encoder 110) to train the decoder network 114. That is, the loss result 632 is used to optimize the parameters (e.g., the weights) of the decoder network 114. In other words, the cross-entropy loss result 632, based on the decoder loss 612 (via the decoder loss function 610) of the decoder network 114 and the phone label loss 622 (via the phone label loss function 620) of the phone label mapping network 510, is used during the decoder training procedure 600 to adjust both the parameters of the decoder network 114 and the phone label mapping network 510. Thus, the decoder 114 will be optimized in a way that directs the decoder 114 to generate predicted audio features 120 (e.g., spectrograms 118) that the phone label mapping network 510 may use to more accurately map predicted phone labels 520. The corresponding predicted audio feature 120 generated for each of the plurality of time steps may include an amplitude value of the predicted waveform 119 at the corresponding time step.\n\n[0061] In some implementations, the loss result 632 is fed back to the decoder 114 after every time step or frame to update the TTS model (e.g., change the parameters of the decoder 114 from the loss result 632) by accumulating the loss result 632 over every frame from the sequence of characters 104, and, in some examples, over multiple sequence of characters 104 in a batch. Batch processing allows for training on multiple sequence of characters simultaneously for efficient math processing on advanced computing devices 10 (e.g., graphical processing units (GPU) and tensor processing units (TPU)). Additionally, the updates from the loss result 632 may be averaged over many observations to increase stability of the training. In some examples, the phone label mapping network 510 is trained on just the resulting phone label loss 622 from the phone label loss function 620. That is, parameters of the phone label mapping network 510 are not altered or influenced by the decoder loss 612 from the decoder loss function 610.\n\n[0062] Referring now to FIG. 7, in some examples, a schematic view 700 shows the decoder network 114 of the TTS model 108 discarding the uses of the mapped phone labels 520 during inference. Inference refers to the process of using the trained TTS model 108 to receive input text 104 and predict output audio samples 119 for producing synthesized speech 106 representative of the input text 104. That is, during inference (i.e., after the TTS model 108 is trained and is in use), predicted phone labels 420 output from the phone label mapping network 510 are not applied. During each time step at inference, the decoder 114 receives the predicted audio feature 120 (e.g., mel-frequency spectrogram 118) from decoder 114 during the previous time step as opposed to the reference audio feature 402 and associated frame-aligned reference phone labels 502 for the previous time step used during training.\n\n[0063] The phone label mapping network 510 may include many different types of neural networks. For example, the phone label mapping network 510 may include a recurrent neural network (RNN), an LTSM network, or a deep feed forward (DFF) neural network. Training the TTS model 108 and the phone label mapping network 510 together advantageously decreases the word error rate (WER) of the resulting audio data (e.g., mel-frequency spectrograms 118), thus indicating that the synthesized speech 106 produced therefrom is more intelligible to human listeners.\n\n[0064] Examples herein illustrate a TTS model 108 that includes the encoder 110 and the decoder 114, however the phone label mapping network 510 is similarly applicable to other types of TTS models or systems to achieve similar improvements in WER. For example, the TTS model 108 may include a parametric synthesizer model that uses the same or similar loss modification, but instead of a decoupled encoder and decoder (e.g., with an attention network), the parametric synthesizer model may include a single frame synchronous neural network. Here, the input textual representation (e.g., sequence of characters of input text 104) may be mapped to the acoustic frames by an external duration model in order for the input and output to be frame synchronous. The phone label mapping network 510 may extend this model in the same manner as the end-to-end speech synthesis model described above.\n\n[0065] FIG. 8 is a flowchart of an exemplary arrangement of operations for a method 800 of training a TTS model. The method 800 starts at operation 802 with obtaining, at data processing hardware 18, training data 150 that includes reference input text 104 that includes a sequence of characters in a particular language, a sequence of reference audio features 402 representative of the sequence of characters, and a sequence of reference phone labels 502 representative of distinct speech sounds for the sequence of reference audio features 402. For each of a plurality of time steps, the method 800 includes, at step 804, generating, by the data processing hardware 18, a corresponding predicted audio feature 120 based on a respective portion of the reference input text 104 for the time step. The method 800, at step 806, includes generating, by the data processing hardware 18, using a phone label mapping network 510, a corresponding predicted phone label 520 associated with the predicted audio feature 120 for the time step. The method 800, at step 808, includes aligning, by the data processing hardware 18, the predicted phone label 520 for the time step with the reference phone label 502 for the time step to determine a corresponding predicted phone label loss 622. The method 800, at step 810, also includes updating, by the data processing hardware 18, the TTS model 108 based on the corresponding predicted phone label loss 622 determined for each of the plurality of time steps.\n\n[0066] FIG. 9 is schematic view of an example computing device 900 that may be used to implement the systems and methods described in this document. The computing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.\n\n[0067] The computing device 900 includes a processor 910, memory 920, a storage device 930, a high-speed interface/controller 940 connecting to the memory 920 and high-speed expansion ports 950, and a low speed interface/controller 960 connecting to a low speed bus 970 and a storage device 930. Each of the components 910, 920, 930, 940, 950, and 960, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 910 can process instructions for execution within the computing device 900, including instructions stored in the memory 920 or on the storage device 930 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 980 coupled to high speed interface 940. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).\n\n[0068] The memory 920 stores information non-transitorily within the computing device 900. The memory 920 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). The non-transitory memory 920 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 900. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.\n\n[0069] The storage device 930 is capable of providing mass storage for the computing device 900. In some implementations, the storage device 930 is a computer-readable medium. In various different implementations, the storage device 930 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory 920, the storage device 930, or memory on processor 910.\n\n[0070] The high speed controller 940 manages bandwidth-intensive operations for the computing device 900, while the low speed controller 960 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 940 is coupled to the memory 920, the display 980 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 950, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 960 is coupled to the storage device 930 and a low-speed expansion port 990. The low-speed expansion port 990, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.\n\n[0071] The computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 900a or multiple times in a group of such servers 900a, as a laptop computer 900b, or as part of a rack server system 900c.\n\n[0072] Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.\n\n[0073] A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an \u201capplication,\u201d an \u201capp,\u201d or a \u201cprogram.\u201d Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.\n\n[0074] These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms \u201cmachine-readable medium\u201d and \u201ccomputer-readable medium\u201d refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term \u201cmachine-readable signal\u201d refers to any signal used to provide machine instructions and/or data to a programmable processor.\n\n[0075] The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.\n\n[0076] To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user\u2019s client device in response to requests received from the web browser.\n\n[0077] A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 20,
      "claims_start": 19,
      "description_end": 19,
      "description_start": 11,
      "drawings_end": 10,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 20,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 19,
      "specification_start": 11,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 69173409,
    "guid": "US-20230009613-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0009/613",
    "intl_class_current_primary": [
      {
        "intl_class": "G10L",
        "intl_subclass": "13/047",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G10L",
        "intl_subclass": "13/08",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G10L",
        "intl_subclass": "15/06",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G10L",
        "intl_subclass": "15/16",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G10L13/047",
      "G10L13/08"
    ],
    "inventors": [
      {
        "city": "Brooklyn",
        "country": "US",
        "name": "Rosenberg; Andrew",
        "postal_code": "N/A",
        "state": "NY"
      },
      {
        "city": "Mt. Kisco",
        "country": "US",
        "name": "Ramabhadran; Bhuvana",
        "postal_code": "N/A",
        "state": "NY"
      },
      {
        "city": "Sandyston",
        "country": "US",
        "name": "Biadsy; Fadi",
        "postal_code": "N/A",
        "state": "NY"
      },
      {
        "city": "Mountain View",
        "country": "US",
        "name": "Zhang; Yu",
        "postal_code": "N/A",
        "state": "CA"
      }
    ],
    "inventors_short": "Rosenberg; Andrew et al.",
    "patent_title": "Training Speech Synthesis to Generate Distinct Speech Sounds",
    "publication_date": "2023-01-12",
    "publication_number": "20230009613",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2021-01-15",
    "appl_id": "17785065",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Tokyo",
        "country": "JP",
        "name": "Sony Group Corporation",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "assignees": [
      {
        "city": "Tokyo",
        "country": "JP",
        "name": "Sony Group Corporation",
        "postal_code": "N/A",
        "state": "N/A",
        "type_code": "03"
      }
    ],
    "composite_id": "69185434!PG-US-20230010512",
    "cpc_inventive": [
      {
        "cpc_class": "H04L",
        "cpc_subclass": "47/25",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "1/0002",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/044",
        "version": "2023-01-01"
      },
      {
        "cpc_class": "G06N",
        "cpc_subclass": "3/084",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04W",
        "cpc_subclass": "28/02",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "41/147",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04W",
        "cpc_subclass": "28/22",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A network entity for a mobile telecommunications system, including circuitry configured to perform transmission rate control of data transmissions according to a transmission control protocol, wherein the transmission rate control is performed based on an output of a machine learning algorithm including a prediction of a congestion of the data transmissions.",
      "brief": "TECHNICAL FIELD\n\n[0001] The present disclosure generally pertains to a network entity and a user equipment of a mobile telecommunications system.\n\nTECHNICAL BACKGROUND\n\n[0002] Several generations of mobile telecommunications systems are known, e.g. the third generation (\u201c3G\u201d), which is based on the International Mobile Telecommunications-2000 (IMT-2000) specifications, the fourth generation (\u201c4G\u201d), which provides capabilities as defined in the International Mobile Telecommunications-Advanced Standard (IMT-Advanced Standard), and the current fifth generation (\u201c5G\u201d), which is under development and which might be put into practice in the year 2020.\n\n[0003] A candidate for providing the requirements of 5G is the so-called Long Term Evolution (\u201cLTE\u201d), which is a wireless communications technology allowing high-speed data communications for mobile phones and data terminals and which is already used for 4G mobile telecommunications systems. Other candidates for meeting the 5G requirements are termed New Radio (NR) Access Technology Systems. An NR can be based on ME technology, just as some aspect of LTE was based on previous generations of mobile communications technology.\n\n[0004] LTE is based on the GSM/EDGE (\u201cGlobal System for Mobile Communications\u201d/\u201cEnhanced Data rates for GSM Evolution\u201d also called EGPRS) of the second generation (\u201c2G\u201d) and UMTS/HSPA (\u201cUniversal Mobile Telecommunications System\u201d/\u201cHigh Speed Packet Access\u201d) of the third generation (\u201c3G\u201d) network technologies.\n\n[0005] LTE is standardized under the control of 3GPP (\u201c3rd Generation Partnership Project\u201d) and there exists a successor LTE-A (LTE Advanced) allowing higher data rates than the basic LTE and which is also standardized under the control of 3GPP.\n\n[0006] For the future, 3GPP plans to further develop LTE-A such that it will be able to fulfill the technical requirements of 5G.\n\n[0007] As the 5G system may be based on LTE-A or NR, respectively, it is assumed that specific requirements of the 5G technologies will, basically, be dealt with by features and methods which are already defined in the LTE-A and NR standard documentation.\n\n[0008] Additionally, the transmission control protocol (\u201cTCP\u201d) is a very common protocol used in the internet and many applications supported in 5G networks will continue to use the transmission control protocol.\n\n[0009] It is known that the rate of data transmissions entering a network according to the transmission control protocol is controlled by several mechanisms, such as the slow-start mechanism. In the slow-start mechanism the transmission rate may be decreased significantly after a network congestion has been detected and may only start to increase slowly afterwards, which is may be recognized by users, for example, in the case of highly user centric wireless services, such as virtual reality, in which the gap between the end user and the network functions is almost minimal.\n\n[0010] Although there exist techniques for transmission rate control of data transmissions according to a transmission control protocol, it is generally desirable to improve the existing techniques.\n\nSUMMARY\n\n[0011] According to a first aspect the disclosure provides a network entity for a mobile telecommunications system, comprising circuitry configured to perform transmission rate control of data transmissions according to a transmission control protocol, wherein the transmission rate control is performed based on an output of a machine learning algorithm including a prediction of a congestion of the data transmissions.\n\n[0012] According to a second aspect the disclosure provides a user equipment for a mobile telecommunications system, comprising circuitry configured to use a service based on a transmission control protocol and to receive from a network entity a first MAC control element including a recommended bitrate which is based on an output of a machine learning algorithm including a prediction of a congestion of data transmissions according to a transmission control protocol and to adjust the transmission rate of the data transmissions according to the transmission control protocol in response to and based on the received first MAC control element.\n\n[0013] According to a third aspect the disclosure provides a user equipment for a mobile telecommunications system, comprising circuitry configured to coordinate activity across different layers, thereby a modem included in the user equipment obtains information about a transmission control protocol header.\n\n[0014] According to a fourth aspect the disclosure provides a user equipment for a mobile telecommunications system, comprising circuitry configured to transmit a buffer status report to a network entity indicating a smaller buffer size than it actually has.\n\n[0015] Further aspects are set forth in the dependent claims, the following description and the drawings.",
      "claims": "1. A network entity for a mobile telecommunications system, comprising circuitry configured to perform transmission rate control of data transmissions according to a transmission control protocol, wherein the transmission rate control is performed based on an output of a machine learning algorithm including a prediction of a congestion of the data transmissions. \n\n2. The network entity according to claim 1, wherein the transmission rate control is performed by controlling a data scheduling rate. \n\n3. The network entity according to claim 1, wherein the circuitry is further configured to generate a first MAC control element including a recommended bitrate and wherein the transmission rate control is performed by transmitting the first MAC control element to a user equipment using a service based on the transmission control protocol, which adjusts in response to and based on the transmitted first MAC control element the transmission rate of the data transmissions according to the transmission control protocol. \n\n4. The network entity according to claim 3, wherein the recommended bitrate is based on the output of the machine learning algorithm. \n\n5. The network entity according to claim 3, wherein the first MAC control element includes an averaging window time. \n\n6. The network entity according to claim 5, wherein the averaging window time is based on the output of the machine learning algorithm. \n\n7. The network entity according to claim 3, wherein the circuitry is further configured to receive a query for the recommended bitrate from the user equipment and to transmit in response to the received query the first MAC control element to the user equipment. \n\n8. The network entity according to claim 1, wherein the circuitry is further configured to receive a second MAC control element including a data rate preference from a user equipment using a service based on the transmission control protocol and wherein the transmission rate control is performed further based on the second MAC control element. \n\n9. The network entity according to claim 8, wherein the second MAC control element includes an uplink packet data convergence protocol queueing delay per at least one of channel quality indicator and 5G quality of service indicator. \n\n10. The network entity according to claim 8, wherein the second MAC control element changes at least one of radio link control and packet data convergence protocol parameters. \n\n11. The network entity according to claim 10, wherein the parameters include at least one of Poll-PDU, Poll-Byte and packet data convergence protocol discard timer. \n\n12. The network entity according to claim 1, wherein the circuitry is further configured to perform transmission control protocol data packet inspection. \n\n13. The network entity according to claim 12, wherein the machine algorithm includes a recurrent neural network. \n\n14. The network entity according to claim 13, wherein an output of the recurrent neural network includes at least one of a timing of a connection restriction start, a location of a connection restriction start, a type of a restricted service and a prediction of an uplink transmission rate control, a downlink rate control or both. \n\n15. The network entity according to claim 13, wherein an input of the recurrent neural network includes time series data. \n\n16. The network entity according to claim 15, wherein the time series data include radio conditions. \n\n17. The network entity according to claim 16, wherein the radio conditions include at least one of a synchronization signal-reference signal received power, a channel state information-reference signal received power, a synchronization signal-reference signal received quality, a channel state information-reference signal received quality, a channel quality indicator, a sounding reference signal measurement and a block error rate. \n\n18. The network entity according to claim 15, wherein the time series data include at least one of an error and a missing ACK at a radio link control layer. \n\n19.-27. (canceled) \n\n28. A user equipment for a mobile telecommunications system, comprising circuitry configured to coordinate activity across different layers, thereby a modern included in the user equipment obtains information about a transmission control protocol header. \n\n29. A user equipment for a mobile telecommunications system, comprising circuitry configured to transmit a buffer status report to a network entity indicating a smaller buffer size than it actually has.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0016] Embodiments are explained by way of example with respect to the accompanying drawings, in which:\n\n[0017] FIG. 1 illustrates schematically two embodiments of a radio access network;\n\n[0018] FIG. 2 illustrates in a diagram a slow-start mechanism of a transmission rate of data transmissions according to a transmission control protocol;\n\n[0019] FIG. 3 illustrates in a block diagram an embodiment of a recurrent neural network in a training stage;\n\n[0020] FIG. 4 illustrates in a block diagram an embodiment of a recurrent neural network in an inference stage;\n\n[0021] FIG. 5 illustrates in a diagram a time evolution of a transmission rate of data transmissions according to a transmission control protocol which is controlled by a network entity;\n\n[0022] FIG. 6 illustrates in a state diagram a first embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity;\n\n[0023] FIG. 7 illustrates in a state diagram a second embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity;\n\n[0024] FIG. 8 illustrates in a state diagram a third embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity;\n\n[0025] FIG. 9 illustrates in a state diagram a fourth embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity;\n\n[0026] FIG. 10 illustrates in a block diagram a user equipment and a network entity;\n\n[0027] FIG. 11 illustrates in a block diagram a multi-purpose computer which can be used for implementing a user equipment or a network entity;\n\n[0028] FIG. 12 illustrates in a block diagram an embodiment of a user equipment; and\n\n[0029] FIG. 13 illustrates in a state diagram a transmission of a buffer status report from a user equipment to a network entity.\n\nDETAILED DESCRIPTION OF EMBODIMENTS\n\n[0030] Before a detailed description of the embodiments under reference of FIG. 2 is given, general explanations are made.\n\n[0031] As mentioned in the outset, in general, several generations of mobile telecommunications systems are known, e.g. the third generation (\u201c3G\u201d), which is based on the International Mobile Telecommunications-2000 (IMT-2000) specifications, the fourth generation (\u201c4G\u201d), which provides capabilities as defined in the International Mobile Telecommunications-Advanced Standard (IMT-Advanced Standard), and the current fifth generation (\u201c5G\u201d), which is under development and which might be put into practice this year.\n\n[0032] One of the candidates for meeting the 5G requirements are termed New Radio (\u201cNR\u201d) Access Technology Systems. Some aspects of NR can be based on LTE technology, in some embodiments, just as some aspects of LTE were based on previous generations of mobile communications technology.\n\n[0033] A typical embodiment of an NR radio access network RAN 1a, as an example of a mobile telecommunications system, is illustrated in FIG. 1A. The RAN 1a has a macro cell 2, which is established by an LTE eNodeB 3, and an NR cell 4, which is established by an NR eNodeB 5 (also referred to as gNB (next generation eNodeB)).\n\n[0034] A UE 6 can communicate with the LTE eNodeB 3 and, as long as it is within the NR cell 4, it can also communicate with the NR eNodeB 5. This embodiment shows an NR EN-DC (\u201cE-UTRA-NR Dual Connectivity\u201d) deployment of the telecommunications system 1a. \n\n[0035] In some embodiments, the mobile telecommunications system is a NR standalone. Generally, a data collection at different nodes is known and a NWDAF (\u201cNetwork Data Analytics Function\u201d) entity is known. In some embodiments, the NWDAF entity may collect data from e.g. 5G core network, 5G RAN and UE measurements collected at RAN. Further, in some embodiments, there is a provision that data can be collected from an entity outside the 3GPP system, i.e. an application server may share data with the NWDAF entity. In some embodiments, these provisions are present in the network.\n\n[0036] Thus, another typical embodiment of an NR radio access network RAN in a mobile telecommunications system 1b is illustrated in FIG. 1B. The RAN has an NR cell 4, which is established by an NR eNodeB 5 (gNB).\n\n[0037] A UE 6 can communicate with the gNB 5, which is connected to a 5G core network (\u201c5GC\u201d) 8. A NWDAF entity 9 collects data from the 5GC 8, the gNB 5 and an application server 42 outside the mobile telecommunications system 1b. \n\n[0038] As mentioned in the outset, the transmission control protocol (\u201cTCP)) is a very common protocol used in the internet and many applications supported in 5G networks will continue to use the transmission control protocol. The rate of data transmissions entering a network according to the transmission control protocol is controlled by several mechanisms such as e.g. the slow-start mechanism.\n\n[0039] In some embodiments, the TCP window is configured in the transmitter and the receiver and has a sliding window. If the network is congested, the TCP window should be small and if the network is error free and a large bandwidth can be allocated, typically, the TCP window should be large. Other protocols such as packet data convergence protocol (\u201cPDCP\u201d) or radio link control (\u201cRLC\u201d) window operation may be aligned with the TCP window configuration. Both TCP and RLC support a sliding window mechanism. However, radio conditions may change dynamically and can cause a buffer to overflow leading to TCP slow-start mechanism. This may be recognized by the users, which, as mentioned in the outset, may be inconvenient for highly user centric wireless services, such as virtual reality, in which the gap between the end user and the network functions is almost minimal.\n\n[0040] One approach to address this issue, described in a paper (\u201cTCP-Aware Scheduling in LTE Networks\u201d, Shojaedin et al.), is that frequent resources are allocated to user equipments (\u201cUE\u201d) with a small buffer size. However, it has been recognized that this approach may lead to a resource waste due to inappropriate size of resource allocation or allocated resources not being used at the expense of resource scarcity for UEs with larger buffer sizes.\n\n[0041] Furthermore, it has been recognized that artificial intelligence (\u201cAI\u201d) and/or machine learning (\u201cML\u201d) is a powerful tool to learn, analyze and predict complex network scenarios, and, thus, machine learning is integrated with wireless communications in some embodiments. The application of ML and/or AI in wireless communications, i.e. mobile telecommunications system, is categorized as follows in some embodiments:\n\n[0042] First, an application of ML in a wireless system is to exploit intelligent and predictive data analytics to enhance situational awareness and the overall network operations, such as fault monitoring, user tracking, or the like across the wireless network.\n\n[0043] Second, beyond its powerful, intelligent and predictive data analytics functions, ML is used as a major driver of intelligent and data-driven wireless network optimization in order to address a variety of problems ranging from cell association and radio access technology selection to frequency allocation, spectrum management, power control, intelligent beamforming and the like.\n\n[0044] Third, beyond its system level functions, ML plays a key role at the physical layer of a wireless network, such as in coding and modulation design, at both the transmitter and the receiver level within a generic communication system.\n\n[0045] Fourth, the rapid deployment of highly user-centric wireless services, such as VR, in which the gap between the end-user and the network functions is almost minimal, ML assists in wireless networks that can track and adapt to the human user behaviour.\n\n[0046] It has further been recognized that ML and/or AI methods may be used in a scheduler implementation in some embodiments, for example, in a base station such as a gNB. In some embodiments, the ML and/or AI methods are used in scheduler in a network entity to predict a network congestion and/or radio conditions of a UE.\n\n[0047] Moreover, it has been recognized that ML and/or AI techniques may be extended to solve the TCP window stalling problem, in some embodiments, and it may be further extended to quick user datagram protocol internet connections (\u201cQUIC\u201d), if the window size is known to e.g. the gNB or if the scheduler can predict the window size.\n\n[0048] Hence, some embodiments pertain to a network entity for a mobile telecommunications system, including circuitry configured to perform transmission rate control of data transmissions according to a transmission control protocol, wherein the transmission rate control is performed based on an output of a machine learning algorithm including a prediction of a congestion of the data transmissions.\n\n[0049] The network entity may be a base station, such as an eNodeB, a NR gNB, or the like as a part of the mobile telecommunications system, which may be based on UMTS, LTE, LTE-A, or an NR, 5G system or the like. The entity may also be any other entity of a mobile telecommunications system and may be located anywhere in the system.\n\n[0050] The circuitry may include at least one of: a processor, a microprocessor, a dedicated circuit, a memory, a storage, a radio interface, a wireless interface, a network interface, or the like, e.g. typical electronic components which are included in a base station, such as an eNodeB, NR gNB, a user equipment, or the like. It may include an interface, such as a mobile telecommunications system interface which is adapted to provide communication to and/or from the mobile telecommunications system. It may also include a wireless interface, e.g. a wireless local area network interface, a Bluetooth interface, etc.\n\n[0051] As generally known, data transmissions according to the transmission control protocol are packet based transmissions and the transmission rate typically depends on the window size and the round-trip time. In some embodiments, the window size is the maximum number of bytes that the source can send in a set of packets for which it has not received acknowledgments. The destination computer sends back an acknowledgment for every correct packet that it receives. In some embodiments, the round-trip time is the time from the transmission of a packet until the reception of its acknowledgment by the source.\n\n[0052] Thus, the transmission rate may be controlled by adjusting the window size, the bitrate in a wireless transmission, or by influencing the round-trip time, which may depend on a quality of a connection, a network congestion, network scheduling, a busy server or base station or the like.\n\n[0053] The machine learning algorithm may be or may include or may be based on a neural network, a decision tree, a support vector machine or the like generating an output which is used by a scheduler in the network entity in order to perform the transmission rate control, i.e. to decide if transmission rate control by the network entity is required and take the necessary actions accordingly. The ML algorithm may be trained by supervised, unsupervised, reinforcement, deep learning strategies or the like. The ML algorithm may use historical network data in supervised and deep learning strategies.\n\n[0054] The ML algorithm outputs a prediction of congestion of data transmissions according to the transmission control protocol (for example a time when it is likely to occur and/or a probability for the congestion occurring and the like) and provides the output, for example, to a scheduler in the network entity, which decides about controlling the transmission rate. The prediction may be based on learning from TCP congestion mechanism, lower layer protocol configurations, UE radio conditions and the like.\n\n[0055] As an example:\n\n[0056] Assuming a TCP window size of 8kB is configured in the transmitter and receiver. Due to congestion and delay, the buffer occupancy at one instance stands at 7 kB and 1 kB of more data may trigger the TCP slow-start mechanism. The scheduler or similar entity in the network entity, for example, a gNB may perform the packet inspection and is aware of the configured TCP window size. Thus, in some embodiments, the circuitry (of the network entity) is further configured to perform transmission control protocol data packet inspection.\n\n[0057] This build up of data may be due to either losing packet(s) or slow scheduling:\n\n[0058] A packet loss at the TCP layer may also cause a missing ACK at RLC AM sub-layer. The RLC sub-layer may notice it at a slightly different time compared to TCP because TCP ACK and RLC ACK may have different timing (RLC ACK is based on Poll-PDU and Poll-Byte, i.e. number of PDUs and number of bytes before sending ACK) and the network/application may configure same or different values to TCP ACK. Also, the mechanism to configure these parameters is different, i.e. TCP window is configured dynamically with TCP packets whereas RLC parameters are configured by RRC and most likely at the time of bearer setup. The packet may be lost in PDCP sub-layer due to discard timer expiry.\n\n[0059] Thus, the scheduler can collate these events and a ML and/or AI models may learn and predict the future possibilities.\n\n[0060] If the buffer build up is due to congestion and packets are arriving later, then this may be detected by e.g. PDCP discard timer or UL delay parameter.\n\n[0061] Thus, the scheduler can collect these statistics and predict the future actions with certain level of certainty. A scheduler armed with such information may avoid buffer overflow or saturation and either slow down or speed up the packet scheduling.\n\n[0062] Consequently, in some embodiments, the transmission rate control is performed by controlling a data scheduling rate. The data scheduling rate can be controlled by the network entity and, thus, controls indirectly the transmission rate of data transmissions according to the transmission control protocol. Since TCP window is configured in the TCP packets itself and RLC window is configured using radio resource control (\u201cRRC\u201d), the cross layer alignment may not work properly, but with ML implemented near network entity scheduler, the ML may learn TCP traffic characteristics and treatment in the network and adjusts scheduling of data transmissions for such services accordingly.\n\n[0063] Generally, the TCP end point on the network side is unknown (somewhere in the internet) and another option may be that if, for example, the gNB (network entity) and the UE communicate and act accordingly, i.e. act to control the transmission rate. The straightforward option may be that gNB can communicate to the UE about any possible TCP head-of-line blocking or congestion or avoid triggering of slow-start mechanism.\n\n[0064] There is, for example, prior art on a new MAC (\u201cMedium Access Control\u201d) control element for informing the UEs about a recommended bit rate. However, it was agreed for a specific use case of MTSI (\u201cMultimedia Telephony Service for IMS\u201d) and ANBR (\u201cAccess Network Bitrate Recommendation\u201d) as specified in 3GPP TS 26.114 and TS 38.321. It is used by RTP/RTCP and sent by the gNB to the UE for both uplink (\u201cUL\u201d) and downlink (\u201cDL\u201d) recommended data rate. There, the averaging window time is fixed for 2000 msec. This is comparable to the rate control mechanism for audio.\n\n[0065] In some embodiments, the above MAC control element (herein: first MAC control element) may be modified to extend the rate control mechanism for TCP based applications in order to exercise the rate control when the TCP window is about to stall. In such embodiments, the averaging window time is configurable, so that it can be adjusted according to different applications and/or radio conditions.\n\n[0066] Hence, in some embodiments, the circuitry of the network entity is further configured to generate a first MAC control element including a recommended bitrate and wherein the transmission rate control is performed by transmitting the first MAC control element to a user equipment using a service based on the transmission control protocol, which adjusts in response to and based on the transmitted first MAC control element the transmission rate of the data transmissions according to the transmission control protocol.\n\n[0067] In some embodiments, the first MAC control element includes an averaging window time.\n\n[0068] The user equipment is configured accordingly, i.e. the user equipment can receive the first MAC control element and adjust the transmission rate accordingly. In response to the received first MAC control element the UE may adjust the TCP window size and/or the transmission bitrate and/or the averaging window time based on the received first MAC control element.\n\n[0069] In some embodiments, the recommended bitrate and/or the averaging window time is based on the output of the machine learning algorithm.\n\n[0070] In some embodiments, the network entity performs the signaling about congestion and a necessary adaption of transmission rate by the user equipment in order to exercise the transmission rate control when the TCP window is about to stall by other signaling procedures than the first MAC control element. In such embodiments, the signaling is based on RRC signaling, physical layer control signaling (DCI), an allocated grant or the like.\n\n[0071] In some embodiments, the circuitry (of the network entity) is further configured to receive a query for the recommended bitrate from the user equipment and to transmit in response to the received query the first MAC control element to the user equipment.\n\n[0072] Some further explanations regarding the recommended bitrate are described in 3GPP TS 38.321 (e.g. in section 5.8.10), which may be used/extended/modified for transmission rate control in TCP based applications.\n\n[0073] In addition, a new MAC control element (herein: second MAC control element) from a UE to a network entity may be introduced which indicates any data rate preference from UE's TCP layer point of view. The MDT (\u201cMinimization of Drive Tests\u201d) framework has introduced the UL PDCP queueing delay per QCI (\u201cChannel Quality Indicator\u201d) or 5QI (\u201c5G Quality of Service Indicator\u201d) as one of the parameters. It counts for AS delay, i.e. from the packet arriving at PDCP layer to receiving the UL grant+HARQ, RLC delay, F1 delay, PDCP reordering delay. This parameter can provide delay in the AS layer but may not address TCP window stalling issue due to different layers being involved. The new MAC control element may also change RLC or PDCP parameters like Poll-PDU, Poll-Byte or PDCP discard timer.\n\n[0074] Hence, in some embodiments, the circuitry is further configured to receive a second MAC control element including a data rate preference from a user equipment using a service based on the transmission control protocol and wherein the transmission rate control is performed further based on the second MAC control element. The second MAC control element provides, for example, further information about UE radio conditions and transmission channel.\n\n[0075] In some embodiments, the second MAC control element includes an uplink packet data convergence protocol queueing delay per at least one of channel quality indicator and 5G quality of service indicator.\n\n[0076] In some embodiments, the second MAC control element changes at least one of radio link control and packet data convergence protocol parameters.\n\n[0077] In some embodiments, the parameters include at least one of Poll-PDU, Poll-Byte and packet data convergence protocol discard timer.\n\n[0078] The MAC control elements are examples and actual signalling may take place, in some embodiments, via PDCP control PDU, L1 signalling or RLC control PDU. Even though RLC data or PDCP data PDU may also be used for this purpose, these formats may reduce the flexibility as new bits to indicate new format may have backward compatibility issues and the capability to understand new formats may be needed.\n\n[0079] As mentioned above, in some embodiments, the output of the machine learning algorithm includes a prediction about congestion of data transmissions according to the TCP. The network entity may realize (detect) that TCP is about to start congestion control (slow-start mechanism) from the ML output and the network (entity) scheduler will prevent from it, for example, by the change of transmission rate.\n\n[0080] In some embodiments, the machine algorithm includes a recurrent neural network.\n\n[0081] Generally, neural networks are organized into multiple layers, wherein each layer includes one or more nodes and wherein each node in one layer is connected to nodes in an immediately preceding and following layer. The layer that receives external data (input) is the input layer and the layer that produces the results and/or predictions (output) is the output layer. In between is an intermediate layer including one or more hidden layers. Each connection between the nodes is assigned with a weight. A trained neural network may be characterized by the trained weights. In a recurrent neural network (\u201cRNN\u201d) the input may be a sequence of data, such as a time series of data, and the recurrent network has an internal state. The nodes in an RNN loop the input data, for example, it uses the output of a first iteration as an input of the second iteration and so on. Thus, it may predict future events based on a time evolution of various input parameters, i.e. it is suitable for the prediction of time series data (for example the timing of a congestion of data transmissions).\n\n[0082] In some embodiments, a question is what are key factors of congestion, since these factors should be input to the ML algorithm. Generally, the ML algorithm can find autonomously the relevant input parameters among many ones. In that sense, any type of input may be fine. However, too many parameters may cause additional cost (e.g. a large number of neurons or layers in a neural network, as a result huge processing power is required). Therefore, in some embodiments, it is better to select the relevant input as of the best knowledge of a communication system designer.\n\n[0083] The following parameters may be relevant input parameters for the prediction of TCP congestion and output parameters:\n\n[0084] Input layer: [0085] Radio conditions: [0086] Synchronization Signal-reference signal received power (SS-RSRP); [0087] Channel State Information-reference signal received power (CSI-RSRP); [0088] Synchronization Signal-reference signal received quality (SS-RSRQ) [0089] Channel State Information-reference signal received quality (CSI-RSRQ); [0090] Channel Quality Indicator (CQI); [0091] Sounding Reference Signal (SRS) measurement; and [0092] Block error rate [0093] RLC layer: [0094] The Error or missing ACK at RLC layer. [0095] PDCP layer: [0096] Expiry of discard timer in PDCP layer. [0097] TCP layer: [0098] The base station, i.e. network entity, may read application layer data using deep packet inspection, then interpret the contents of TCP header; and [0099] TCP port number, it could be the clue of application (e.g. ftp download, Web, messaging, video streaming, video conference tool and the like). [0100] Traffic load: [0101] The historical data of traffic load with time stamp. The network congestion likely to occur in busy hour. [0102] Layer 2 measurement\u2014UL PDCP queuing delay\n\n[0103] Output layer: [0104] Time/date of restriction start; [0105] location of restriction start; [0106] Type of restricted services; and [0107] UL or DL or both rate control.\n\n[0108] Hence, in some embodiments, an output of the recurrent neural network includes at least one of a timing of a connection restriction start, a location of a connection restriction start, a type of a restricted service and a prediction of an uplink transmission rate control, a downlink rate control or both.\n\n[0109] In some embodiments, an input of the recurrent neural network includes time series data.\n\n[0110] In some embodiments, the time series data includes radio conditions.\n\n[0111] In some embodiments, the radio conditions include at least one of a synchronization signal-reference signal received power, a channel state information-reference signal received power, a synchronization signal-reference signal received quality, a channel state information-reference signal received quality, a channel quality indicator, a sounding reference signal measurement and a block error rate.\n\n[0112] In some embodiments, the time series data include at least one of an error and a missing ACK at a radio link control layer.\n\n[0113] In some embodiments, the time series data include an expiry of discard timer in a packet data convergence protocol layer.\n\n[0114] In some embodiments, the time series data include information from a transmission control protocol header.\n\n[0115] In some embodiments, the time series data include at least one of a traffic load with time stamp and an uplink packet data convergence queuing delay.\n\n[0116] In some embodiments, the recurrent neural network is trained based on historical training data, which may be obtained from historical network data including data for the above-listed input parameters and data about when TCP congestion happened.\n\n[0117] In some embodiments, the recurrent neural network is trained offline or during operation.\n\n[0118] In some embodiments, the training process is deployed inside a network entity (e.g. base station or the like) as described herein, including electronic components (circuitry) which are typically used for a training process a ML algorithm, i.e. neural network, such as a memory, a microprocessor, a graphical processing unit, or the like. In other embodiments the training process is deployed inside an external server/tool for network operation and maintenance (O&M). In some embodiments, the training process is handled offline. In other embodiments, the training process is handled during live network operation, wherein the server includes enough memory to store the historical (training) data. In some embodiments, the raw data of the network (historical data) is too large to store on a memory inside the network entity or the server. In such embodiments, the data is processed in advance of the training process, for example, by averaging or the like in order to reduce the size.\n\n[0119] In some embodiments, the trained ML algorithm, e.g. recurrent neural network having trained weights, is deployed for inference (actual operation for admission control) in the network entity. In such embodiments, the input to the ML algorithm, e.g. the recurrent neural network, is actual (real-time) data from live network monitoring, some static configurations and historical data (which may include live network monitoring data from previous iterations).\n\n[0120] Some embodiments pertain to a user equipment for a mobile telecommunications system, including circuitry configured to use a service based on a transmission control protocol and to receive from a network entity a first MAC control element including a recommended bitrate which is based on an output of a machine learning algorithm including a prediction of a congestion of data transmissions according to a transmission control protocol and to adjust the transmission rate of the data transmissions according to the transmission control protocol in response to and based on the received first MAC control element.\n\n[0121] The user equipment may be or may include a smartphone, a VR device, a laptop or the like. The circuitry may include at least one of: a processor, a microprocessor, a dedicated circuit, a memory, a storage, a radio interface, a wireless interface, a network interface, or the like, e.g. typical electronic components which are included in a user equipment to achieve the functions as described herein.\n\n[0122] In some embodiments, the circuitry (of the user equipment) is further configured to transmit a query for the recommended bitrate to a network entity.\n\n[0123] In some embodiments, the circuitry (of the user equipment) is further configured to transmit a second MAC control element to a network entity including a data rate preference.\n\n[0124] In some embodiments, the second MAC control element includes an uplink packet data convergence protocol queueing delay per at least one of channel quality indicator and 5G quality of service indicator.\n\n[0125] A normal handset (user equipment) may have different processors for modem and application. Thus, it has been recognized that an implementation of a user equipment may coordinate the activities across different layers, i.e. the AS (\u201cAccess Stratum\u201d) layer in a modem of the user equipment may be aware of the TCP header, which is generated/received in the application layer.\n\n[0126] Hence, some embodiments pertain to a user equipment for a mobile telecommunications system, including circuitry configured to coordinate activity across different layers, thereby a modem included in the user equipment obtains information about a transmission control protocol header.\n\n[0127] In some embodiments, from a user equipment point of view, the user equipment can send a buffer status report (\u201cBSR\u201d), which indicates a smaller buffer size than it actually has in order to ask for a smaller grant from a network entity, if the user equipment detects congestion of data transmissions. The buffer status report is generally known from LTE.\n\n[0128] Hence, some embodiments pertain to a user equipment for a mobile telecommunications system, including circuitry configured to transmit a buffer status report to a network entity indicating a smaller buffer size than it actually has.\n\n[0129] As above, the user equipment may be or may include a smartphone, a VR device, a laptop or the like. The circuitry may include at least one of: a processor, a microprocessor, a dedicated circuit, a memory, a storage, a radio interface, a wireless interface, a network interface, or the like, e.g. typical electronic components which are included in a user equipment to achieve the functions as described herein.\n\n[0130] In some embodiments, a network entity as described herein and a user equipment as described herein constitute a transmission rate control system for data transmissions according to a transmission control protocol and/or are part of a mobile telecommunications system (network).\n\n[0131] Returning to FIG. 2, which illustrates in a diagram a slow-start mechanism of a transmission rate of data transmissions according to a transmission control protocol.\n\n[0132] The time evolution of the transmission rate of data transmissions (i.e. TCP segments) according to the transmission control protocol in FIG. 2 shows the typical problematic case (which will be avoided by the techniques as described herein).\n\n[0133] At 10a the transmission rate is increased slowly (initial cwnd in TCP) and starts to increase exponentially at 10b up to a certain level. At 10c the transmission rate increases moderately when the congestion window reaches the TCP slow-start threshold (ssthresh in TCP).\n\n[0134] However, TCP has no idea when congestion is likely to occur, thus, it continues to increase the transmission rate until maximum allowed rate. At some point congestion occurs and a missing TCP ACK is detected at 10d, which enables the TCP to detect and react on the congestion by rapidly decreasing the transmission rate at 10e. When a low transmission rate is reached the slow-start mechanism starts again at 10f. \n\n[0135] This typical time evolution is useful as training data for the machine learning algorithm such as a recurrent neural network shown in FIG. 3 in training stage.\n\n[0136] FIG. 3 illustrates in a block diagram an embodiment of a recurrent neural network 20 in a training stage.\n\n[0137] In this embodiment, the recurrent neural network 20 (illustrated by the arrow going back on the same node) in the training stage is deployed in the network entity 7 and obtains input from a data storage device including historical data 21 at an input layer 22. In this embodiment, the input includes historical time series data including radio conditions, data/information from RLC, PDCP, TCP layer and traffic load, as described above.\n\n[0138] The nodes of the input layer 22 are connected to first nodes of an intermediate layer 23. The intermediate layer 23 performs calculations and the last nodes are connected to an output layer, which outputs a prediction for congestion occurrence (a time when it will occur and/or a probability for the congestion occurring) and other output values, as described herein.\n\n[0139] A loss function 25 compares the predicted result with the actual results obtained from the stored historical data 21 and uses a backpropagation algorithm to update the weights of the neural network 20 in order to increase the prediction accuracy of the recurrent neural network 20.\n\n[0140] FIG. 4 illustrates in a block diagram an embodiment of a recurrent neural network 30 in an inference stage.\n\n[0141] The neural network 30 corresponds to the trained recurrent neural network 20 of FIG. 3 and is deployed in the network entity 7 for inferencing, wherein the input layer 32, the intermediate layer 33 and the output layer 34 have the same structure as in FIG. 3. The recurrent neural network 30 obtains actual (real-time) data 31 from live network and measurement results (input data as described herein) and outputs the predictions about congestion of data transmissions according to a transmission control protocol to a scheduler 35 (which is part of the network entity).\n\n[0142] The scheduler 35 schedules the data in a scheduler queue 36 including data transmissions from a user equipment using a TCP based service. Based on the prediction of congestion of the data transmissions from the recurrent neural network 30 the scheduler 35 decides about transmission rate control of the data transmissions in the scheduler queue 36 and determines an action 37 for performing the transmission rate control, which are actions as described herein.\n\n[0143] The scheduler 35 can, for example, adjust the data scheduling rate of the data transmissions from a TCP based service in a way that the slow-start mechanism is avoided and a high transmission rate is achieved constantly.\n\n[0144] In other embodiments, there are variations of deployment of machine learning function such as input layer 32, the intermediate layer 33 and the output layer 34. A straightforward variation is a deployment inside the network entity (e.g. inside base station), as discussed. In some embodiments, it is high performance processing (e.g. cloud/edge computing) outside the network entity and connected to the network entity via interface (e.g. O&M network). The network virtualization/cloud based RAN could provide an alternative flexible deployment option.\n\n[0145] FIG. 5 illustrates in a diagram a time evolution of a transmission rate of data transmissions according to a transmission control protocol which is controlled by a network entity 7.\n\n[0146] At 11a the transmission rate is increased slowly (initial cwnd in TCP) and starts to increase exponentially at 11b up to a certain level. At 10c the transmission rate increases moderately when the congestion window reaches the TCP slow-start threshold (ssthresh in TCP).\n\n[0147] The network entity 7 from FIG. 4 continuously monitors the time evolution of the transmission rate of data transmissions according to the transmission control protocol and other input values (as described herein). Based on the prediction of congestion of the data transmissions from the recurrent neural network 30 the scheduler 35 adjust the data scheduling rate of the data transmissions from a TCP based service in a way that the slow-start mechanism is avoided and a high transmission rate is achieved constantly at 11d. \n\n[0148] FIG. 6 illustrates in a state diagram a first embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity 7.\n\n[0149] The UE 6 has an established TCP connection with a (server) PC 41 and the UE 6 is about to download data from the PC 41. At 50 the PC 41 sends the first data packets at a low transmission rate (as shown in FIG. 5) over the network via a Router R 40 and the network entity NE 7 (which is configured as in FIGS. 4 and 5), which acknowledges its receipt.\n\n[0150] At 51, the PC 41 increases the transmission rate (congestion window) which is illustrated by the arrows towards the UE 6. The NE 7 continuously monitors in parallel the network situation (radio conditions, TCP header information, etc. as described herein) in order to decide about transmission rate control of the data transmissions based on the output of the recurrent neural network (machine learning algorithm) including the prediction of congestion of the data transmissions according to the transmission control protocol.\n\n[0151] At 52, the PC 41 further increases the transmission rate (congestion window) and the recurrent neural network 30 in the NE 7 predicts that congestion is likely to occur when transmission rate is further increased (for example due to buffer overflow at the router R 40 or decreased radio link conditions or the like as described herein). Based on the output of the machine learning algorithm, the recurrent neural network 30, the NE 7 performs transmission rate control of the data transmissions according to the transmission control protocol by controlling the data scheduling rate.\n\n[0152] As a result, the transmission rate is held constant at 53, which is illustrated by the same number of arrows at 52 and 53, which avoids triggering of the TCP slow-start mechanism (as shown in FIG. 5).\n\n[0153] FIG. 7 illustrates in a state diagram a second embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity 7.\n\n[0154] The UE 6 has an established TCP connection with a server PC 41 and the UE 6 is about to upload data to the PC 41. At 60 the PC 41 sends the first data packets at a low transmission rate (as shown in FIG. 5) over the network via a Router R 40 and the network entity NE 7 (which is configured as in FIGS. 4 and 5), which acknowledges its receipt.\n\n[0155] At 61, the NE 7 generates a first MAC control element including a recommended bitrate and performs a transmission rate control by transmitting the first MAC control element to the UE 6 which uses a service based on the transmission control protocol and the UE 6 adjusts in response to and based on the transmitted first MAC control element the transmission rate of the data transmissions according to the transmission control protocol.\n\n[0156] At 62, the data transmissions proceed with the adjusted transmission rate, which is held constant at 63, which is illustrated by the same number of arrows at 62 and 63. Consequently, a triggering of the TCP slow-start mechanism (as shown in FIG. 5) is avoided.\n\n[0157] FIG. 8 illustrates in a state diagram a third embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity 7.\n\n[0158] The UE 6 has an established TCP connection with a server PC 41 and the UE 6 is about to upload data to the PC 41. At 70 the PC 41 sends the first data packets at a low transmission rate (as shown in FIG. 5) over the network via a Router R 40 and the network entity NE 7 (which is configured as in FIGS. 4 and 5), which acknowledges its receipt.\n\n[0159] At 71, the UE 6 transmits a query for the recommended bitrate to the NE 7, which in response to the received query transmits the first MAC control element (from FIG. 7) to the user equipment for performing transmission rate control of data transmissions according to a transmission control protocol.\n\n[0160] In another embodiment, the query for the first MAC control element may be transmitted from the UE6 to the NE 7 in a case where there is no response from the network.\n\n[0161] In other embodiments, it is likely that congestion is detected at the receiver, for example, in the entity PC 41, so that an NWDAF entity receives an input directly from PC 41, if an NWDAF is deployed.\n\n[0162] At 72, the data transmissions proceed with the adjusted transmission rate, which is held constant at 73, which is illustrated by the same number of arrows at 72 and 73. Consequently, a triggering of the TCP slow-start mechanism (as shown in FIG. 5) is avoided.\n\n[0163] FIG. 9 illustrates in a state diagram a fourth embodiment of a transmission rate control of data transmissions according to a transmission control protocol by a network entity 7.\n\n[0164] The embodiment is basically the same as the embodiment of FIG. 6 except for that the UE 6, which uses a service based on the transmission control protocol, transmits, at 80, a second MAC control element including a data rate preference to the NE 7, which performs the transmission rate control further based on the second MAC control element.\n\n[0165] The steps 81 to 84 correspond to the steps 50 to 53 of FIG. 6 except for that the transmission rate control, here by controlling the data scheduling rate in the NE 7, is based on the output of the machine learning algorithm (recurrent neural network 30) and the second MAC control element.\n\n[0166] An embodiment of a UE 6 and a network entity (NE) 7 (e.g. NR eNB/gNB) and a communication path 104 between the UE 6 and the NE 7, which are used for implementing embodiments of the present disclosure, is discussed under reference of FIG. 10.\n\n[0167] The UE 6 has a transmitter 101, a receiver 102 and a controller 103, wherein, generally, the technical functionality of the transmitter 101, the receiver 102 and the controller 103 are known to the skilled person, and, thus, a more detailed description of them is omitted.\n\n[0168] The NE 7 has a transmitter 105, a receiver 106 and a controller 107, wherein also here, generally, the functionality of the transmitter 105, the receiver 106 and the controller 107 are known to the skilled person, and, thus, a more detailed description of them is omitted.\n\n[0169] The communication path 104 has an uplink path 104a, which is from the UE 6 to the NE 7, and a downlink path 104b, which is from the NE 7 to the UE 6.\n\n[0170] During operation, the controller 103 of the UE 6 controls the reception of downlink signals over the downlink path 104b at the receiver 102 and the controller 103 controls the transmission of uplink signals over the uplink path 104a via the transmitter 101.\n\n[0171] Similarly, during operation, the controller 107 of the NE 7 controls the transmission of downlink signals over the downlink path 104b over the transmitter 105 and the controller 107 controls the reception of uplink signals over the uplink path 104a at the receiver 106.\n\n[0172] In the following, an embodiment of a general purpose computer 130 is described under reference of FIG. 11.\n\n[0173] The computer 130 can be implemented such that it can basically function as any type of network entity, base station or new radio base station, transmission and reception point, or user equipment as described herein. The computer has components 131 to 141, which can form a circuitry, such as any one of the circuitries of the base stations, and user equipments, as described herein.\n\n[0174] Embodiments which use software, firmware, programs or the like for performing the methods as described herein can be installed on computer 130, which is then configured to be suitable for the concrete embodiment.\n\n[0175] The computer 130 has a CPU 131 (Central Processing Unit), which can execute various types of procedures and methods as described herein, for example, in accordance with programs stored in a read-only memory (ROM) 132, stored in a storage 137 and loaded into a random access memory (RAM) 133, stored on a medium 140 which can be inserted in a respective drive 139, etc.\n\n[0176] The CPU 131, the ROM 132 and the RAM 133 are connected with a bus 141, which in turn is connected to an input/output interface 134. The number of CPUs, memories and storages is only exemplary, and the skilled person will appreciate that the computer 130 can be adapted and configured accordingly for meeting specific requirements which arise, when it functions as a base station or as user equipment.\n\n[0177] At the input/output interface 134, several components are connected: an input 135, an output 136, the storage 137, a communication interface 138 and the drive 139, into which a medium 140 (compact disc, digital video disc, compact flash memory, or the like) can be inserted.\n\n[0178] The input 135 can be a pointer device (mouse, graphic table, or the like), a keyboard, a microphone, a camera, a touchscreen, etc.\n\n[0179] The output 136 can have a display (liquid crystal display, cathode ray tube display, light emittance diode display, etc.), loudspeakers, etc.\n\n[0180] The storage 137 can have a hard disk, a solid state drive and the like.\n\n[0181] The communication interface 138 can be adapted to communicate, for example, via a local area network (LAN), wireless local area network (WLAN), mobile telecommunications system (GSM, UMTS, LTE, NR etc.), Bluetooth, infrared, etc.\n\n[0182] It should be noted that the description above only pertains to an example configuration of computer 130. Alternative configurations may be implemented with additional or other sensors, storage devices, interfaces or the like. For example, the communication interface 138 may support other radio access technologies than the mentioned UMTS, LTE and NR.\n\n[0183] When the computer 130 functions as a base station, the communication interface 138 can further have a respective air interface (providing e.g. E-UTRA protocols OFDMA (downlink) and SC-FDMA (uplink)) and network interfaces (implementing for example protocols such as S1-AP, GTP-U, S1-MME, X2-AP, or the like). The computer 130 is also implemented to transmit data in accordance with TCP. Moreover, the computer 130 may have one or more antennas and/or an antenna array. The present disclosure is not limited to any particularities of such protocols.\n\n[0184] FIG. 12 illustrates in a block diagram an embodiment of a user equipment 6.\n\n[0185] The UE 6 includes an application processor 150, a circuitry 151 and a modem 152. In the application processor, i.e. in the application layer, for example, TCP data transmission are generated. The circuitry coordinates the activity across different layers, thereby the modem 152 obtains information about the TCP header.\n\n[0186] The circuitry is shown as separate entity only for illustration purposes and may also be integrated in the application processor 150 or the modem 152.\n\n[0187] FIG. 13 illustrates in a state diagram a transmission of a buffer status report from a user equipment 6 to a network entity 7.\n\n[0188] The UE 6 is connected to a mobile telecommunications system and communicates via the NE 7.\n\n[0189] At 160, the UE 6 transmits a buffer status report to a network entity indicating a smaller buffer size than it actually has in order to ask for a smaller grant from the NE 7.\n\n[0190] At 161, the NE 7 transmits a new grant to UE 6 including transmission parameters the UE 6 should use based on the buffer status report received from UE 6.\n\n[0191] All units and entities described in this specification and claimed in the appended claims can, if not stated otherwise, be implemented as integrated circuit logic, for example on a chip, and functionality provided by such units and entities can, if not stated otherwise, be implemented by software.\n\n[0192] In so far as the embodiments of the disclosure described above are implemented, at least in part, using software-controlled data processing apparatus, it will be appreciated that a computer program providing such software control and a transmission, storage or other medium by which such a computer program is provided are envisaged as aspects of the present disclosure.\n\n[0193] Note that the present technology can also be configured as described below.\n\n[0194] (1) A network entity for a mobile telecommunications system, comprising circuitry configured to perform transmission rate control of data transmissions according to a transmission control protocol, wherein the transmission rate control is performed based on an output of a machine learning algorithm including a prediction of a congestion of the data transmissions.\n\n[0195] (2) The network entity of (1), wherein the transmission rate control is performed by controlling a data scheduling rate.\n\n[0196] (3) The network entity of (1) or (2), wherein the circuitry is further configured to generate a first MAC control element including a recommended bitrate and wherein the transmission rate control is performed by transmitting the first MAC control element to a user equipment using a service based on the transmission control protocol, which adjusts in response to and based on the transmitted first MAC control element the transmission rate of the data transmissions according to the transmission control protocol.\n\n[0197] (4) The network entity of (3), wherein the recommended bitrate is based on the output of the machine learning algorithm.\n\n[0198] (5) The network entity of (3) or (4), wherein the first MAC control element includes an averaging window time.\n\n[0199] (6) The network entity of (5), wherein the averaging window time is based on the output of the machine learning algorithm.\n\n[0200] (7) The network entity of anyone of (3) to (6), wherein the circuitry is further configured to receive a query for the recommended bitrate from the user equipment and to transmit in response to the received query the first MAC control element to the user equipment.\n\n[0201] (8) The network entity of anyone of (1) to (7), wherein the circuitry is further configured to receive a second MAC control element including a data rate preference from a user equipment using a service based on the transmission control protocol and wherein the transmission rate control is performed further based on the second MAC control element.\n\n[0202] (9) The network entity of (8), wherein the second MAC control element includes an uplink packet data convergence protocol queueing delay per at least one of channel quality indicator and 5G quality of service indicator.\n\n[0203] (10) The network entity of (8) or (9), wherein the second MAC control element changes at least one of radio link control and packet data convergence protocol parameters.\n\n[0204] (11) The network entity of (10), wherein the parameters include at least one of Poll-PDU, Poll-Byte and packet data convergence protocol discard timer.\n\n[0205] (12) The network entity of anyone of (1) to (11), wherein the circuitry is further configured to perform transmission control protocol data packet inspection.\n\n[0206] (13) The network entity of (12), wherein the machine algorithm includes a recurrent neural network.\n\n[0207] (14) The network entity of (13), wherein an output of the recurrent neural network includes at least one of a timing of a connection restriction start, a location of a connection restriction start, a type of a restricted service and a prediction of an uplink transmission rate control, a downlink rate control or both.\n\n[0208] (15) The network entity of (13) or (14), wherein an input of the recurrent neural network includes time series data.\n\n[0209] (16) The network entity of (15), wherein the time series data includes radio conditions.\n\n[0210] (17) The network entity of (16), wherein the radio conditions include at least one of a synchronization signal-reference signal received power, a channel state information-reference signal received power, a synchronization signal-reference signal received quality, a channel state information-reference signal received quality, a channel quality indicator, a sounding reference signal measurement and/or a block error rate.\n\n[0211] (18) The network entity of anyone of (15) to (17), wherein the time series data include at least one of an error and a missing ACK at a radio link control layer.\n\n[0212] (19) The network entity of anyone of (15) to (18), wherein the time series data include an expiry of discard timer in a packet data convergence protocol layer.\n\n[0213] (20) The network entity of anyone of (15) to (19), wherein the time series data include information from a transmission control protocol header.\n\n[0214] (21) The network entity of anyone of (15) to (20), wherein the time series data include at least one of a traffic load with time stamp and an uplink packet data convergence queuing delay.\n\n[0215] (22) The network entity of anyone of (13) to (21), wherein the recurrent neural network is trained based on historical training data.\n\n[0216] (23) The network entity of anyone of (13) to (22), wherein the recurrent neural network is trained offline or during operation.\n\n[0217] (24) A user equipment for a mobile telecommunications system, comprising circuitry configured to use a service based on a transmission control protocol and to receive from a network entity a first MAC control element including a recommended bitrate which is based on an output of a machine learning algorithm including a prediction of a congestion of data transmissions according to a transmission control protocol and to adjust the transmission rate of the data transmissions according to the transmission control protocol in response to and based on the received first MAC control element.\n\n[0218] (25) The user equipment of (24), wherein the circuitry is further configured to transmit a query for the recommended bitrate to the network entity.\n\n[0219] (26) The user equipment of (24), wherein the circuitry is further configured to transmit a second MAC control element to the network entity including a data rate preference.\n\n[0220] (27) The user equipment of (26), wherein the second MAC control element includes an uplink packet data convergence protocol queueing delay per at least one of channel quality indicator and 5G quality of service indicator.\n\n[0221] (28) A user equipment for a mobile telecommunications system, comprising circuitry configured to coordinate activity across different layers, thereby a modem included in the user equipment obtains information about a transmission control protocol header.\n\n[0222] (29) A user equipment for a mobile telecommunications system, comprising circuitry configured to transmit a buffer status report to a network entity indicating a smaller buffer size than it actually has."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 18,
      "claims_start": 18,
      "description_end": 18,
      "description_start": 9,
      "drawings_end": 8,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 18,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 18,
      "specification_start": 9,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 69185434,
    "foreign_priority": [
      {
        "app_filing_date": "2020-01-20",
        "app_number": "20152728.0",
        "country": "EP"
      }
    ],
    "guid": "US-20230010512-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0010/512",
    "intl_class_current_primary": [
      {
        "intl_class": "H04L",
        "intl_subclass": "47/25",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H04L",
        "intl_subclass": "41/147",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "H04L47/25",
      "H04L41/147"
    ],
    "inventors": [
      {
        "city": "Basingstoke",
        "country": "GB",
        "name": "SHARMA; Vivek",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Basingstoke",
        "country": "GB",
        "name": "WEI; Yuxin",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Basingstoke",
        "country": "GB",
        "name": "WAKABAYASHI; Hideji",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Basingstoke",
        "country": "GB",
        "name": "AWAD; Yassin Aden",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "SHARMA; Vivek et al.",
    "patent_title": "NETWORK ENTITY AND USER EQUIPMENT FOR TRANSMISSION RATE CONTROL",
    "publication_date": "2023-01-12",
    "publication_number": "20230010512",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2020-02-12",
    "appl_id": "17432212",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Tokyo",
        "country": "JP",
        "name": "Nippon Telegraph and Telephone Corporation",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "72143519!PG-US-20230008765",
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "21/55",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "63/20",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "63/1416",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/28",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "16/00",
        "version": "2019-01-01"
      },
      {
        "cpc_class": "H04L",
        "cpc_subclass": "63/1425",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "An estimation device includes: a collection section configured to collect related information when cyber threat intelligence of a maliciousness estimation target is input, the related information being related to the cyber threat intelligence and other cyber threat intelligence different from the cyber threat intelligence; a feature generation section configured to generate a feature based on the related information, the feature representing a feature of the cyber threat intelligence; a graph information generation section configured to generate graph information based on the related information and the other cyber threat intelligence, the graph information indicating a graph in which each of the cyber threat intelligence and the other cyber threat intelligence is a node and a relationship between the nodes is an edge; and an estimation section configured to estimate the maliciousness of the cyber threat intelligence by a graph convolutional neural network using the feature of the cyber threat intelligence when a graph indicated by the graph information has a graph structure between the cyber threat intelligence and the other cyber threat intelligence.",
      "brief": "TECHNICAL FIELD\n\n[0001] The present disclosure relates to an estimation device, an estimation method, and a program.\n\nBACKGROUND ART\n\n[0002] In recent years, security operations utilizing cyber threat intelligence (CTI) have been performed. Cyber threat intelligence is information that can be utilized for security operations such as attack detection and countermeasures by collecting and storing information about attackers, attack purposes, attack methods, and the like related to cyber attacks and threats and analyzing the information. Examples of the cyber threat intelligence include internet protocol (IP) addresses and domain names used by attackers, indicator of compromise (IoC) information such as uniform resource locators (URLs), information about countermeasures against attacks (Course of Action), malware hash values, and the like.\n\n[0003] Maliciousness is known as an index of such cyber threat intelligence and is used for determining the priority of the cyber threat intelligence, making a decision, and the like. Maliciousness is an index indicating the degree of likelihood that cyber threat intelligence such as a domain name or URL is used in a cyber attack and is represented by, for example, the probability that the cyber threat intelligence is malicious. Therefore, more prompt and accurate security operations and the like can be performed as the accuracy of maliciousness increases. Additionally, examples of information sources of cyber threat intelligence include cyber threat intelligence providing services of security companies, alert information from an attack detection device that detects attack communication, open-source intelligence (OSINT), and the like.\n\n[0004] As a conventional technique for calculating the maliciousness of cyber threat intelligence, a technique for estimating maliciousness according to a graph analysis method using a graph setting cyber threat intelligence as a \u201cnode\u201d and a connection between the related pieces of cyber threat intelligence as an \u201cedge\u201d is known. For example, a technique for calculating maliciousness using a PageRank method (Non Patent Literature 1) and a technique for estimating maliciousness by a graph propagation method (Non Patent Literature 2) are known. Further, a technique for calculating a threat score using a probability propagation method on a bipartite graph is also known (Non Patent Literature 3).\n\nCITATION LIST\n\nNon Patent Literature\n\n[0005] Non Patent Literature 1: Boukhtouta, Amine, et al. \u201cGraph-theoretic characterization of cyber-threat infrastructures.\u201d Digital Investigation 14 (2015): S3-S15 [0006] Non Patent Literature 2: WANG, Xin, et al. \u201cPoster: An Approach to Verifying Threat Intelligence Based on Graph Propagation.\u201d IEEE S&P 2018 Poster [0007] Non Patent Literature 3: \u201cDetecting Malicious Domains with Probabilistic Threat Propagation on DNS Graph\u201d Yuta Kazato, Kensuke Fukuda, and Toshiharu Sugawara, Computer Software 33.3 (2016): 3_16-3_28.\n\nSUMMARY OF THE INVENTION\n\nTechnical Problem\n\n[0008] However, in the existing technology, because the features of cyber threat intelligence cannot be used, the accuracy of estimating maliciousness may not be high.\n\n[0009] The present disclosure has been made in view of the above-described circumstances, and an object is to highly accurately estimate maliciousness of cyber threat intelligence.\n\nMeans for Solving the Problem\n\n[0010] In order to achieve the above-described object, an embodiment of the present disclosure provides an estimation device including: a collection section configured to collect, when cyber threat intelligence of a maliciousness estimation target is input, related information related to the cyber threat intelligence and other cyber threat intelligence different from the cyber threat intelligence; a feature generation section configured to generate, based on the related information, a feature representing a feature of the cyber threat intelligence; a graph information generation section configured to generate graph information based on the related information and the other cyber threat intelligence, the graph information indicating a graph in which each of the cyber threat intelligence and the other cyber threat intelligence is a node and a relationship between the nodes is an edge; and an estimation section configured to estimate the maliciousness of the cyber threat intelligence by a graph convolutional neural network using the feature of the cyber threat intelligence when a graph indicated by the graph information has a graph structure between the cyber threat intelligence and the other cyber threat intelligence.\n\nEffects of the Invention\n\n[0011] It is possible to highly accurately estimate the maliciousness of cyber threat intelligence.",
      "claims": "1. An estimation device, comprising: a collection section, including one or more processors, configured to collect related information when cyber threat intelligence of a maliciousness estimation target is input, the related information being related to the cyber threat intelligence and other cyber threat intelligence different from the cyber threat intelligence; a feature generation section, including one or more processors, configured to generate a feature based on the related information, the feature representing a feature of the cyber threat intelligence; a graph information generation section, including one or more processors, configured to generate graph information based on the related information and the other cyber threat intelligence, the graph information indicating a graph in which each of the cyber threat intelligence and the other cyber threat intelligence is a node and a relationship between the nodes is an edge; and an estimation section, including one or more processors, configured to estimate maliciousness of the cyber threat intelligence by a graph convolutional neural network using the feature of the cyber threat intelligence, when a graph indicated by the graph information has a graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n2. The estimation device according to claim 1, wherein the collection section is configured to collect, as the related information, at least one or more pieces of information among flow monitoring information collectable from a flow exporter and/or a flow collector, public information collectable from a server or Web site providing open-source intelligence, name resolution information collectable from a DNS server, and owner information collectable from a WHOIS server or RDAP server, and the feature generation section is configured to generate the feature based on the at least one or more pieces of information collected.  \n\n3. The estimation device according to claim 1, wherein the estimation section is configured to estimate the maliciousness of the cyber threat intelligence by a predetermined learning model obtained by learning in a method of supervised learning, using a feature amount of the cyber threat intelligence when the graph indicated by the graph information does not have the graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n4. The estimation device according to claim 1, wherein the feature includes at least one or more of: a length of a character string of a domain name included in the cyber threat intelligence; a length of a character string of an URL included in the cyber threat intelligence; a percentage of numbers in the domain name included in the cyber threat intelligence; a percentage of numbers in the URL included in the cyber threat intelligence; a percentage of consonants in the domain name included in the cyber threat intelligence; a percentage of consonants in the URL included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the domain name included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the URL included in the cyber threat intelligence; country-specific information of an IP address included in the cyber threat intelligence; and a number of A records and NS records when the IP address or domain name included in the cyber threat intelligence is resolved.  \n\n5. An estimation method that is performed by a computer, the estimation method comprising: collecting related information when cyber threat intelligence of an maliciousness estimation target is input, the related information being related to the cyber threat intelligence and other cyber threat intelligence different from the cyber threat intelligence; generating a feature based on the related information, the feature representing a feature of the cyber threat intelligence; generating graph information based on the related information and the other cyber threat intelligence, the graph information representing a graph in which each of the cyber threat intelligence and the other cyber threat intelligence is a node and a relationship between the nodes is an edge; and estimating maliciousness of the cyber threat intelligence by a graph convolutional neural network by using the feature of the cyber threat intelligence when a graph indicated by the graph information has a graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n6. A non-transitory computer readable medium storing one or more instructions that cause a computer to execute: collecting related information when cyber threat intelligence of an maliciousness estimation target is input, the related information being related to the cyber threat intelligence and other cyber threat intelligence different from the cyber threat intelligence; generating a feature based on the related information, the feature representing a feature of the cyber threat intelligence; generating graph information based on the related information and the other cyber threat intelligence, the graph information representing a graph in which each of the cyber threat intelligence and the other cyber threat intelligence is a node and a relationship between the nodes is an edge; and estimating maliciousness of the cyber threat intelligence by a graph convolutional neural network by using the feature of the cyber threat intelligence when a graph indicated by the graph information has a graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n7. The estimation method according to claim 5, further comprising: collecting, as the related information, at least one or more pieces of information among flow monitoring information collectable from a flow exporter and/or a flow collector, public information collectable from a server or Web site providing open-source intelligence, name resolution information collectable from a DNS server, and owner information collectable from a WHOIS server or RDAP server; and generating the feature based on the at least one or more pieces of information collected.  \n\n8. The estimation method according to claim 5, further comprising: estimating the maliciousness of the cyber threat intelligence by a predetermined learning model obtained by learning in a method of supervised learning, using a feature amount of the cyber threat intelligence when the graph indicated by the graph information does not have the graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n9. The estimation method according to claim 5, wherein the feature includes at least one or more of: a length of a character string of a domain name included in the cyber threat intelligence; a length of a character string of an URL included in the cyber threat intelligence; a percentage of numbers in the domain name included in the cyber threat intelligence; a percentage of numbers in the URL included in the cyber threat intelligence; a percentage of consonants in the domain name included in the cyber threat intelligence; a percentage of consonants in the URL included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the domain name included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the URL included in the cyber threat intelligence; country-specific information of an IP address included in the cyber threat intelligence; and a number of A records and NS records when the IP address or domain name included in the cyber threat intelligence is resolved.  \n\n10. The non-transitory computer readable medium according to claim 6, wherein the one or more instructions further cause the computer to execute: collecting, as the related information, at least one or more pieces of information among flow monitoring information collectable from a flow exporter and/or a flow collector, public information collectable from a server or Web site providing open-source intelligence, name resolution information collectable from a DNS server, and owner information collectable from a WHOIS server or RDAP server; and generating the feature based on the at least one or more pieces of information collected.  \n\n11. The non-transitory computer readable medium according to claim 6, wherein the one or more instructions further cause the computer to execute: estimating the maliciousness of the cyber threat intelligence by a predetermined learning model obtained by learning in a method of supervised learning, using a feature amount of the cyber threat intelligence when the graph indicated by the graph information does not have the graph structure between the cyber threat intelligence and the other cyber threat intelligence.  \n\n12. The non-transitory computer readable medium according to claim 6, wherein the feature includes at least one or more of: a length of a character string of a domain name included in the cyber threat intelligence; a length of a character string of an URL included in the cyber threat intelligence; a percentage of numbers in the domain name included in the cyber threat intelligence; a percentage of numbers in the URL included in the cyber threat intelligence; a percentage of consonants in the domain name included in the cyber threat intelligence; a percentage of consonants in the URL included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the domain name included in the cyber threat intelligence; information about whether or not a predetermined character string is included in the URL included in the cyber threat intelligence; country-specific information of an IP address included in the cyber threat intelligence; and a number of A records and NS records when the IP address or domain name included in the cyber threat intelligence is resolved.",
      "description": "BRIEF DESCRIPTION OF DRAWINGS\n\n[0012] FIG. 1 is a diagram illustrating an example of an overall configuration of a maliciousness estimation system according to an embodiment of the present disclosure.\n\n[0013] FIG. 2 is a diagram illustrating an example of a hardware configuration of a maliciousness estimation device according to the embodiment of the present disclosure.\n\n[0014] FIG. 3 is a flowchart illustrating an example of a maliciousness estimation process according to the embodiment of the present disclosure.\n\nDESCRIPTION OF EMBODIMENTS\n\n[0015] Hereinafter, an embodiment of the present disclosure will be described. In the embodiment of the present disclosure, the maliciousness of target cyber threat intelligence is estimated using a graph convolutional neural network (GCN: Graph Convolutional Networks or Graph Convolutional Neural Networks), by using a feature that represents a feature of the cyber threat intelligence, and a relationship (graph structure) between the cyber threat intelligence of a maliciousness estimation target (hereinafter, also referred to as \u201ctarget cyber threat intelligence\u201d) and other cyber threat intelligence. Further, in the embodiment of the present disclosure, the maliciousness of the target cyber threat intelligence is estimated using a predetermined model obtained by learning in supervised learning using the feature, when the target cyber threat intelligence does not have a relationship with the other cyber threat intelligence (that is, the target cyber threat intelligence is independent on a graph and does not have a graph structure with other cyber threat intelligence).\n\n[0016] Here, the following embodiment of the present disclosure will describe a maliciousness estimation system 1 by using the features of each cyber threat intelligence. The maliciousness estimation system 1 estimates maliciousness using a graph convolutional neural network when target cyber threat intelligence has a graph structure with other cyber threat intelligence and estimates maliciousness using a predetermined model obtained by learning in supervised learning when the target cyber threat intelligence does not have a graph structure with other cyber threat intelligence.\n\n[0017] Overall Configuration of Maliciousness Estimation System 1\n\n[0018] First, an overall configuration of the maliciousness estimation system 1 according to an embodiment of the present disclosure will be described with reference to FIG. 1. FIG. 1 is a diagram illustrating an example of an overall configuration of the maliciousness estimation system 1 according to the embodiment of the present disclosure.\n\n[0019] As illustrated in FIG. 1, the maliciousness estimation system 1 according to the embodiment of the present disclosure includes a maliciousness estimation device 10 and various types of external devices 20. The maliciousness estimation device 10 and various types of external devices 20 are communicatively connected to each other via an arbitrary network (for example, the Internet or an in-house Local Area Network (LAN)). Here, examples of the external device 20 include a cyber threat intelligence management device 21, an information management device 22, an attack detection device 23, an OSINT server 24, a DNS server 25, a WHOIS/RDAP server 26, and the like.\n\n[0020] When target cyber threat intelligence is input, the maliciousness estimation device 10 collects information related to the target cyber threat intelligence (hereinafter, also referred to as \u201crelated information\u201d), other cyber threat intelligence, and the like and estimates maliciousness of the target cyber threat intelligence by using the target cyber threat intelligence, the related information, other cyber threat intelligence, and the like. In this case, the maliciousness estimation device 10 generates the feature of the target cyber threat intelligence, determines whether or not the target cyber threat intelligence has a graph structure with other cyber threat intelligence, and then estimates maliciousness of the target cyber threat intelligence by either a graph convolutional neural network or a predetermined model obtained by learning in supervised learning according to the determination result. Note that the other cyber threat intelligence means cyber threat intelligence other than the target cyber threat intelligence.\n\n[0021] The cyber threat intelligence management device 21 is a database (DB) that stores and manages cyber threat intelligence registered, shared, analyzed, or used in the past (hereinafter, registration, sharing, analysis, and use are collectively referred to as \u201cuse, etc.\u201d). In addition to the cyber threat intelligence, the cyber threat intelligence management device 21 also stores and manages the maliciousness of the cyber threat intelligence (maliciousness estimated or set in the past) and the feature (feature generated in the past).\n\n[0022] The flow monitoring information management device 22 is a DB that stores and manages flow monitoring information collected from a flow exporter, a flow collector, or the like on a network.\n\n[0023] The attack detection device 23 is software or a network device for detecting and blocking cyber attacks, such as a fire wall or an Intrusion Detection System (IDS)/an Intrusion Prevention System (IPS).\n\n[0024] The OSINT server 24 is a server that provides public information about open-source intelligence. The OSINT server 24 provides public information about open-source intelligence in the form of, for example, a webpage or the like.\n\n[0025] The DNS server 25 is a server that performs name resolution (also referred to as \u201caddress resolution\u201d) between a domain name and an IP address.\n\n[0026] The WHOIS/RDAP server 26 is a server that provides owner information of an IP address or a domain name.\n\n[0027] Here, as illustrated in FIG. 1, the maliciousness estimation device 10 includes, as functional units, an input unit 101, an information collection unit 102, a feature generation unit 103, a graph information generation unit 104, a maliciousness calculation unit 105, and an output unit 106.\n\n[0028] The input unit 101 inputs target cyber threat intelligence. Note that the input unit 101 may input the target cyber threat intelligence from an arbitrary input source which is determined in advance. For example, the input unit 101 may input the target cyber threat intelligence stored in a storage device such as a memory or may input the target cyber threat intelligence received from other devices connected via a network.\n\n[0029] The information collection unit 102 collects related information and other cyber threat intelligence. Here, the information collection unit 102 includes a NW information collection unit 111, an OSINT collection unit 112, and a cyber threat intelligence collection unit 113.\n\n[0030] The NW information collection unit 111 collects related information which can be observed on the network. For example, the NW information collection unit 111 may inquire of the flow monitoring information management device 22 and then collect the flow monitoring information related to the target cyber threat intelligence. Further, for example, the NW information collection unit 111 inquires of the DNS server 25 and collects information (name resolution result information) obtained by resolving a domain name or an IP address included in the target cyber threat intelligence. Similarly, for example, the NW information collection unit 111 inquires of the WHOIS/RDAP server 26 and collects the owner information of the domain name or the IP address included in the target cyber threat intelligence.\n\n[0031] Note that, for example, the NW information collection unit 111 may inquire of the attack detection device 23 and collect alert information related to the target cyber threat intelligence as the related information. Because the alert information is also collected as the related information, the related information can be used, for example, in generating graph information from the graph structure showing the relationship among pieces of cyber threat intelligence (the target cyber threat intelligence and other cyber threat intelligence) or can be used as an evidence of the estimated maliciousness (that is, the maliciousness of the target cyber threat intelligence).\n\n[0032] The OSINT collection unit 112 collects public information about open-source intelligence. That is, the OSINT collection unit 112 inquires of the OSINT server 24 and collects public information related to the target cyber threat intelligence. Examples of the public information collected by the OSINT collection unit 112 include information indicating an autonomous system (AS) number result, information indicating a search result of GeoIP, information obtained from a reputation service, and the like.\n\n[0033] The cyber threat intelligence collection unit 113 collects other cyber threat intelligence. That is, the cyber threat intelligence collection unit 113 inquires of the cyber threat intelligence management device 21 and collects other cyber threat intelligence used in the past. Note that related information of other cyber threat intelligence may be also stored and managed in the cyber threat intelligence management device 21, and in this case, the cyber threat intelligence collection unit 113 may collect related information of other cyber threat intelligence in addition to the other cyber threat intelligence.\n\n[0034] The feature generation unit 103 generates the feature of the target cyber threat intelligence by using a domain name, URL, or the like included in the target cyber threat intelligence and the related information collected by the NW information collection unit 111 or the OSINT collection unit 112.\n\n[0035] The graph information generation unit 104 generates graph information (node information and graph information) from a graph structure indicating the relationship between pieces of the cyber threat intelligence (target cyber threat intelligence and other cyber threat intelligence) by using the target cyber threat intelligence and the related information and the other cyber threat intelligence that are collected by the information collection unit 102.\n\n[0036] The maliciousness calculation unit 105 determines whether or not the target cyber threat intelligence has a graph structure with the other cyber threat intelligence by using the graph information generated by the graph information generation unit 104. Then, in response to the determination result, the maliciousness calculation unit 105 estimates maliciousness of the target cyber threat intelligence by either a graph convolutional neural network or a predetermined model obtained by learning in supervised learning, by using the feature of pieces of cyber threat intelligence (the target cyber threat intelligence and other cyber threat intelligence). Here, the maliciousness calculation unit 105 includes a preprocessing unit 121, a graph convolutional calculation unit 122, and a supervised-learning calculation unit 123.\n\n[0037] As preprocessing, the preprocessing unit 121 divides a set of data indicating the feature of each cyber threat intelligence (hereinafter, also referred to as \u201cfeature data\u201d) into, for example, a teacher set, a verification set, and a test set. Here, the teacher set is a set of the feature data to be used in the graph convolutional neural network or learning a learning model in supervised learning. Further, the verification set is a set of the feature data to be used for verifying the learning result. On the other hand, the test set is a set of the feature data to be used for estimating maliciousness, and the feature data of the target cyber threat intelligence is included in the test set.\n\n[0038] Then, the preprocessing unit 121 creates a plurality of pieces of teacher data from the teacher set and creates a plurality of pieces of verification data from the verification set. Further, the preprocessing unit 121 creates the test data from the test set. Here, the teacher data and the verification data are data to which a label indicating the maliciousness is given to the input data input to the graph convolutional neural network or the learning model in supervised learning. At this time, labels included in the teacher data and the verification data may be calculated from the maliciousness of the feature data constituting each of the teacher data and the verification data. On the other hand, the test data is input data which is input to the graph convolutional neural network or the learning model in supervised learning (that is, no label is given to the test data). Note that, for the feature data of other cyber threat intelligence and the maliciousness for giving labels to the teacher data and the verification data, those stored and managed in the cyber threat intelligence management device 21 are used.\n\n[0039] The graph convolutional calculation unit 122 calculates (estimates) the maliciousness of the target cyber threat intelligence by the graph convolutional neural network using the input data (the teacher data, the verification data, and the test data) created by the preprocessing unit 121 and the graph information generated by the graph information generation unit 104.\n\n[0040] The supervised-learning calculation unit 123 calculates (estimates) the maliciousness of the target cyber threat intelligence according to the method of supervised learning using the input data (the teacher data, the verification data, and the test data) created by the preprocessing unit 121.\n\n[0041] The output unit 106 outputs the target cyber threat intelligence and the maliciousness calculated (estimated) by the maliciousness calculation unit 105. Note that the output unit 106 may output the target cyber threat intelligence and the maliciousness to an arbitrary output destination determined in advance. For example, the output unit 106 may use a display or the like as an output destination, may use a storage device such as a memory or the like as an output destination, or may use other devices (for example, the cyber threat intelligence management device 21 or the like) connected via the network as an output destination.\n\n[0042] Hardware Configuration of Maliciousness Estimation Device 10\n\n[0043] Next, the hardware configuration of the maliciousness estimation device 10 according to the embodiment of the present disclosure will be described with reference to FIG. 2. FIG. 2 is a diagram illustrating an example of the hardware configuration of the maliciousness estimation device 10 according to the embodiment of the present disclosure.\n\n[0044] As illustrated in FIG. 2, the maliciousness estimation device 10 according to the embodiment of the present disclosure includes, as hardware, an input device 201, a display device 202, an external I/F 203, a random access memory (RAM) 204, a read only memory (ROM) 205, a processor 206, a communication I/F 207, and an auxiliary storage device 208. These pieces of hardware are connected communicatively with each other via a bus B.\n\n[0045] The input device 201 is, for example, a keyboard, a mouse, a touch panel, or the like and is used when a user inputs various operations. The display device 202 is, for example, a display or the like and displays processing results (for example, the estimated maliciousness or the like) by the maliciousness estimation device 10. Note that the maliciousness estimation device 10 may not include at least one of the input device 201 and the display device 202.\n\n[0046] The external I/F 203 is an interface with an external recording medium such as a recording medium 203a. The maliciousness estimation device 10 can read from or write to the recording medium 203a via the external I/F 203. The recording medium 203a may store one or more programs and the like that realize each of the functional units (for example, the input unit 101, the information collection unit 102, the feature generation unit 103, the graph information generation unit 104, the maliciousness calculation unit 105, the output unit 106, and the like) included in the maliciousness estimation device 10.\n\n[0047] Examples of the recording medium 203a include a flexible disk, a compact disc (CD), a digital versatile disk (DVD), a secure digital memory card (SD memory card), a universal serial bus (USB) memory card, and the like.\n\n[0048] The RAM 204 is a volatile semiconductor memory that temporarily holds programs and data. The ROM 205 is a non-volatile semiconductor memory that can hold programs and data even when the power is turned off. The ROM 205 stores, for example, setting information related to an operating system (OS), setting information related to a communication network, and the like.\n\n[0049] The processor 206 is, for example, a central processing unit (CPU), a graphics processing unit (GPU), or the like and is an arithmetic device which reads out a program or data from the ROM 205, the auxiliary storage device 208, or the like onto the RAM 204 to perform processing. Each of the functional units included in the maliciousness estimation device 10 is realized by reading out one or more programs stored in the ROM 205, the auxiliary storage device 208, or the like onto the RAM 204 and performing processing by the processor 206.\n\n[0050] The communication I/F 207 is an interface for connecting the maliciousness estimation device 10 to the communication network. One or more programs that realize each of the functional units including in the maliciousness estimation device 10 may be acquired (downloaded) from a predetermined server or the like via the communication I/F 207.\n\n[0051] The auxiliary storage device 208 is, for example, a hard disk drive (HDD), a solid state drive (SSD), or the like and is a non-volatile storage device that stores a program or data. Examples of the program or data stored in the auxiliary storage device 208 include an OS, an application program that realizes various functions on the OS, and one or more programs that realize each of the functional units included in the maliciousness estimation device 10.\n\n[0052] The maliciousness estimation device 10 according to the embodiment of the present disclosure can realize various processes to be described later by including the hardware configuration illustrated in FIG. 2. Note that, in the example illustrated in FIG. 2, a case is shown in which the maliciousness estimation device 10 according to the embodiment of the present disclosure is realized by a single device (computer), but the present disclosure is not limited thereto. The maliciousness estimation device 10 of the embodiment of the present disclosure may be realized by a plurality of devices (computers). Further, a single device (computer) may include a plurality of the processors 206 or a plurality of memories (the RAM 204, the ROM 205, the auxiliary storage device 208, and the like).\n\n[0053] Maliciousness Estimation Process\n\n[0054] Next, a process (maliciousness estimation process) of estimating maliciousness of the target cyber threat intelligence will be described with reference to FIG. 3. FIG. 3 is a flowchart illustrating an example of the maliciousness estimation process according to the embodiment of the present disclosure.\n\n[0055] Step S101: First, the input unit 101 inputs the target cyber threat intelligence.\n\n[0056] Step S102: Next, the information collection unit 102 collects the related information and other cyber threat intelligence. That is, the information collection unit 102 collects, by the NW information collection unit 111, the related information which can be monitored in the network from the flow monitoring information management device 22, the attack detection device 23, the DNS server 25, the WHOIS/RDAP server 26, and the like. At the same time, the information collection unit 102 collects public information about open-source intelligence as related information from the OSINT server 27 by using the OSINT collection unit 112. Further, the information collection unit 102 collects other cyber threat intelligence from the cyber threat intelligence management device 21 by the cyber threat intelligence collection unit 113.\n\n[0057] Note that the related information and the other cyber threat intelligence collected in step S102 described above are stored in, for example, the auxiliary storage device 208 or the like of the maliciousness estimation device 10.\n\n[0058] Step S103: Next, the feature generation unit 103 generates the feature of the target cyber threat intelligence by using the domain name, URL, or the like included in the target cyber threat intelligence and the related information collected by the NW information collection unit 111 and the OSINT collection unit 112. Here, the feature is represented by, for example, a vector whose elements are numerical values of various features of the cyber threat intelligence. [0059] Examples of the features of the cyber threat intelligence include the following. [0060] Length of a character string of a domain name and a URL included in cyber threat intelligence [0061] Percentage of numbers in a domain name and a URL included in cyber threat intelligence [0062] Percentage of consonants in a domain name and a URL included in cyber threat intelligence [0063] Whether or not a domain name and a URL included in cyber threat intelligence contain a predetermined characteristic character string (for example, character string \u201cexe\u201d) [0064] The number of A records or NS records included in name resolution result information [0065] Country-specific information about an IP address included in cyber threat intelligence [0066] The related information may be used to quantify each of these features. For example, in quantifying the country-specific information of the IP address included in the cyber threat intelligence, information indicating a search result of GeoIP or the like may be used.\n\n[0067] Accordingly, for example, when the numerical values of each feature of the target cyber threat intelligence are x.sub.1, x.sub.2, . . . , x.sub.N, vectors [x.sub.1, x.sub.2, . . . , x.sub.N], whose elements are x.sub.1, x.sub.2, . . . , x.sub.N are generated as the feature of the target cyber threat intelligence. Here, N is a value determined in advance and indicates the number of features of the cyber threat intelligence. In addition, for example, in the case of quantifying whether or not a predetermined characteristic character string is included in a domain name or a URL, One-hot Encoder or the like may be used.\n\n[0068] Note that this feature of the cyber threat intelligence is an example, and various features may be used in addition to the above-described feature.\n\n[0069] Step S104: Next, the graph information generation unit 104 generates graph information from a graph structure indicating the relationship between pieces of the cyber threat intelligence (the target cyber threat intelligence and the other cyber threat intelligence) by using the target cyber threat intelligence and the related information and the other cyber threat intelligence which are collected in step S102 described above. Here, the graph information generation unit 104 generates the graph information by using the cyber threat intelligence (the target cyber threat intelligence and the other cyber threat intelligence) as a node and the relationship between the pieces of the cyber threat intelligence (nodes) as an edge. Note that the related information of the other cyber threat intelligence may be collected from, for example, the cyber threat intelligence management device 21 in step S102 described above, or the related information stored and managed in the cyber threat intelligence management device 21 in this step may be referred to or collected.\n\n[0070] Examples of the related cyber threat intelligence include the following.\n\n[0071] At least two cyber threat intelligence including an IP address obtained as a result of forward lookup of a same domain name For example, when an IP address A and an IP address B are obtained as a result of a forward lookup of a certain domain name, cyber threat intelligence A including the IP address A and cyber threat intelligence B including the IP address B become related cyber threat intelligence.\n\n[0072] At least two cyber threat intelligence including domain names sharing a same IP address For example, when a domain name C and a domain name D share the same IP address, cyber threat intelligence C including the domain name C and cyber threat intelligence D including the domain name D become related cyber threat intelligences.\n\n[0073] At least two cyber threat intelligence including domain names with a same company name or register name obtained as an inquiry result of WHOIS or RDAP For example, when a domain name E and a domain name F are the same company name as an inquiry result of WHOIS or RDAP, cyber threat intelligence E including the domain name E and cyber threat intelligence F including the domain name F become related cyber threat intelligence.\n\n[0074] Accordingly, a graph structure in which each cyber threat intelligence is a node and an edge is between the nodes of the related cyber threat intelligences can be obtained and the graph information generation unit 104 can generate graph information indicating the graph structure. Examples of the graph information include an adjacency matrix, a graph Laplacian, and the like.\n\n[0075] Step S105: Next, the maliciousness calculation unit 105 determines whether or not the target cyber threat intelligence has the graph structure with other cyber threat intelligence by using the graph information created in step S104 described above. That is, the maliciousness calculation unit 105 determines whether or not the node indicating the target cyber threat intelligence is connected to other nodes (that is, the nodes indicating other cyber threat intelligences) at the edge.\n\n[0076] When it is determined that the target cyber threat intelligence has the graph structure with other cyber threat intelligence in step S105, the maliciousness calculation unit 105 proceeds to step S106. On the other hand, when it is determined that the target cyber threat intelligence does not have the graph structure with other cyber threat intelligence (that is, when the node indicating the target cyber threat intelligence is independent on the graph), the maliciousness calculation unit 105 proceeds to step S108.\n\n[0077] Step S106: When it is determined that the target cyber threat intelligence has the graph structure with other cyber threat intelligence in step S105 described above, the maliciousness calculation unit 105 divides the feature data of each cyber threat intelligence into a teacher set, a verification set, and a test set by the preprocessing unit 121. Here, the preprocessing unit 121 may divide the feature data randomly, for example, so that the number of feature data included in the teacher set, the number of feature data included in the verification set, and the number of feature data included in the test set become a preset ratio (a ratio to the total number of feature data). Additionally, as described above, the test set includes the feature data of the target cyber threat intelligence (that is, data indicating the feature generated in step S104 described above).\n\n[0078] Then, the preprocessing unit 121 creates a plurality of teacher data from the teacher set and creates a plurality of verification data from the verification set. Further, the preprocessing unit 121 creates test data from the test set. Note that the maliciousness of the label given to a certain teacher data may be, for example, the average of the maliciousness of the feature data constituting the teacher data. The same applies to the verification data.\n\n[0079] Step S107: Next, the maliciousness calculation unit 105 calculates (estimates) the maliciousness of the target cyber threat intelligence by the graph convolutional neural network using the input data (the teacher data, the verification data, and the test data) created in step S106 described above and the graph information generated in step S104 described above by the graph convolutional calculation unit 122. In the graph convolutional neural network, when t is an index indicating the layer of the graph convolutional neural network, t=0 is an input layer, and t=T\u22121 is an output layer, the maliciousness is estimated by the following equation (1) for t=0, . . . , T\u22121.\n\nH.sup.(t+1)=\u03c3(AH.sup.(t)W.sup.(t))\u2003\u2003(1)\n\nHere, A is the graph information, W.sup.(t) is the parameter (weight) of the t-th layer, and is the activation function. Further, X=H.sup.(0) is the input data and a matrix composed of a predetermined number of feature data (for example, an M\u00d7N matrix if the predetermined number is M and the feature data is an Nth-order horizontal vector). Note that, as the activation function \u03c3, for example, a softmax function that outputs the maliciousness in the output layer using ReLU or the like other than the output layer (that is, a softmax function that outputs p and 1-p, where p is the maliciousness) is used.\n\n[0080] At this time, the graph convolutional calculation unit 122 estimates maliciousness by the followings (1-1) to (1-3).\n\n[0081] (1-1) First, the graph convolutional calculation unit 122 updates each parameter (weight) W.sup.(t) of the graph convolutional neural network by using each of the plurality of teacher data as the input data. That is, the maliciousness calculation unit 105 updates each parameter Wm by a known optimization method so that a predetermined error function (the error function may be referred to as an \u201cobjective function\u201d, \u201closs function\u201d, etc.) is minimized using the output result of the graph convolutional neural network when the label given to the teacher data and the teacher data are the input data.\n\n[0082] (1-2) Next, the graph convolutional calculation unit 122 verifies the estimation accuracy of the graph convolutional neural network by using each of the plurality of verification data as the input data. That is, the maliciousness calculation unit 105 verifies the estimation accuracy of the graph convolutional neural network by a known verification method using the output result of the graph convolutional neural network when the label given to the verification data and the verification data are the input data. In addition, for example, when the estimation accuracy of the graph convolutional neural network is not sufficient as a result of verification, it is possible to change the ratio of the learning set and the verification set and to perform the process again from step S106 described above.\n\n[0083] (1-3) Finally, the graph convolutional calculation unit 122 obtains an output result of the graph convolutional neural network by using the test data as the input data. This output result is the estimation result of the maliciousness of the target cyber threat intelligence.\n\n[0084] Note that, the above-described (1-2) may not be necessarily performed. In this case, the verification data may not be generated in step S106.\n\n[0085] Step S108: When it is determined that the target cyber threat intelligence does not have the graph structure with other cyber threat intelligence in step S105 described above, the maliciousness calculation unit 105 divides the feature data of each cyber threat intelligence into the teacher set, the verification set, and the test set and creates a plurality of teacher data, a plurality of verification data, and test data by the preprocessing unit 121 as in step S106 described above.\n\n[0086] However, for example, the ratio of the number of feature data included in the teacher set, the number of feature data included in the verification set, and the number of feature data included in the test set may be different from that of step S105 described above. That is, different values may be set as a ratio in step S105 described above and in this step. For example, the feature data of each cyber threat intelligence itself becomes the input data depending on the model used in the method of supervised learning. In such a case, a ratio is set so that the number of feature data included in the test set becomes \u201c1\u201d.\n\n[0087] Step S109: Next, the maliciousness calculation unit 105 calculates (estimates) the maliciousness of the target cyber threat intelligence according to the method of supervised learning by the supervised learning calculation unit 123 by using the input data (the teacher data, the verification data, and the test data) created in step S108 described above.\n\n[0088] Here, an arbitrary learning model can be used as a model (learning model) used in the method of supervised learning. For example, examples of the learning model include a support vector machine, a decision tree, and a learning model applied to ensemble learning (bagging and boosting).\n\n[0089] At this time, the supervised learning calculation unit 123 estimates maliciousness by the followings (2-1) to (2-3).\n\n[0090] (2-1) First, the supervised learning calculation unit 123 learns a learning model by the method of supervised learning using each of the plurality of teacher data as the input data.\n\n[0091] (2-2) Next, the supervised learning calculation unit 123 verifies the estimation accuracy of the learning model by the existing verification method using each of the plurality of verification data as the input data. In addition, for example, when the estimation accuracy of the learning model is not sufficient as a result of verification, it is possible to change the ratio of the learning set and the verification set and to perform the processes again from step S108 described above.\n\n[0092] (2-3) Finally, the supervised learning calculation unit 123 obtains an output result of the learning model by using the test data as the input data. This output result is the estimation result of the maliciousness of the target cyber threat intelligence.\n\n[0093] Note that the above-described (2-2) may not be necessarily performed. In this case, the verification data may not be created in step S108 described above.\n\n[0094] Step S110: After step S107 or step S109, the output unit 106 outputs the target cyber threat intelligence and the maliciousness estimated in step S107 or step S109 described above. Note that, for example, when the maliciousness estimated in step S107 or step S109 described above exceeds a predetermined threshold value, the output unit 106 may perform this step.\n\n[0095] Further, for example, when the maliciousness estimated in step S107 or step S109 described above exceeds a predetermined threshold value, the output unit 106 may transmit a predetermined notification to the terminal of the security operator or the like.\n\n[0096] Further, the output unit 106 may output, for example, a graph including a node indicating the target cyber threat intelligence, the feature of the target cyber threat intelligence, and the like in addition to the target cyber threat intelligence and the maliciousness. In this case, the output unit 106 may output the cyber threat intelligence in a predetermined format (for example, a standard description format such as a structured threat information expression (STIX) format).\n\nSUMMARY\n\n[0097] As described above, the maliciousness estimation device 10 according to the embodiment of the present disclosure collects various related information or cyber threat intelligences, determines whether or not the target cyber threat intelligence has a graph structure with other cyber threat intelligence, and in response to the determination result, estimates maliciousness of the target cyber threat intelligence using a graph convolutional neural network or a predetermined model obtained by learning in supervised learning and the feature of each cyber threat intelligence. Accordingly, when the target cyber threat intelligence has a graph structure with other cyber threat intelligence, it is possible to estimate the maliciousness with extremely high accuracy by using the feature of the target cyber threat intelligence and the graph structure. On the other hand, even when the target cyber threat intelligence does not have the graph structure with other cyber threat intelligence, it is possible to highly accurately estimate the maliciousness by using the feature of the target cyber threat intelligence.\n\n[0098] As a result, it is possible to detect and handle various cyber attacks at an earlier timing with high accuracy by using the maliciousness estimation device 10 according to the embodiment of the present disclosure. Further, it is possible to perform efficient security operations by reducing the operator's burden required for analysis and selection of a large amount of cyber threat intelligence and reducing false detection.\n\n[0099] The present disclosure is not limited to the above-described specific embodiments and can be modified or changed in various forms without departing from claims.\n\nREFERENCE SIGNS LIST\n\n[0100] 1 Maliciousness estimation system [0101] 10 Maliciousness estimation device [0102] 20 External device [0103] 21 Cyber threat intelligence management device [0104] 22 Flow monitoring information management device [0105] 23 Attack detection device [0106] 24 OSINT server [0107] 25 DNS server [0108] 26 WHOIS/RDAP server [0109] 101 Input unit [0110] 102 Information collection unit [0111] 103 Feature generation unit [0112] 104 Graph information generation unit [0113] 105 Maliciousness calculation unit [0114] 106 Output unit [0115] 111 NW information collection unit [0116] 112 OSINT collection unit [0117] 113 Cyber threat intelligence collection unit [0118] 121 Preprocessing unit [0119] 122 Graph convolutional calculation unit [0120] 123 Supervised learning calculation unit"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 12,
      "claims_start": 11,
      "description_end": 11,
      "description_start": 5,
      "drawings_end": 4,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 12,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 11,
      "specification_start": 5,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 72143519,
    "foreign_priority": [
      {
        "app_filing_date": "2019-02-20",
        "app_number": "2019-028716",
        "country": "JP"
      }
    ],
    "guid": "US-20230008765-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0008/765",
    "intl_class_current_primary": [
      {
        "intl_class": "H04L",
        "intl_subclass": "9/40",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "H04L9/40"
    ],
    "inventors": [
      {
        "city": "Musashino-shi, Tokyo",
        "country": "JP",
        "name": "KAZATO; Yuta",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "KAZATO; Yuta",
    "patent_title": "ESTIMATION APPARATUS, ESTIMATION METHOD AND PROGRAM",
    "publication_date": "2023-01-12",
    "publication_number": "20230008765",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "2020-07-24",
    "appl_id": "17785199",
    "applicants": [
      {
        "authority_type": "assignee",
        "city": "Suzhou, Jiangsu",
        "country": "CN",
        "name": "INSPUR SUZHOU INTELLIGENT TECHNOLOGY CO., LTD.",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "composite_id": "70301687!PG-US-20230009095",
    "cpc_inventive": [
      {
        "cpc_class": "G06F",
        "cpc_subclass": "13/1668",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "13/4027",
        "version": "2013-01-01"
      }
    ],
    "database_name": "PGPB",
    "derwent_week_int": 0,
    "document": {
      "abstract": "A data transmission method applied to an APB bridge for connecting an APB and an AHB, includes: dividing transmission into an address phase and a data phase according to a feature of an AHB; in the address phase, when the AHB meets an address transmission condition corresponding to a current operation, transmitting address information and control information, which are sent by the AHB, to an APB; and in the data phase, when the APB meets a valid data transmission condition corresponding to the current operation, sending received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB. According to the present application, the address information, the control information, and the data do not need to be cached, whereby the occupation of a storage space is reduced. Further disclosed are a data transmission apparatus and an electronic device having the above beneficial effects.",
      "brief": "[0001] This application claims priority to Chinese Patent Application No. 201911287932.8, filed on Dec. 15, 2019, in China National Intellectual Property Administration and entitled \u201cData Transmission Method and Apparatus, and Related Assembly\u201d, the contents of which are hereby incorporated by reference in its entirety.\n\nFIELD\n\n[0002] The present disclosure relates to the field of Advanced Microcontroller Bus Architecture (AMBA) buses, and particularly to a data transmission method and apparatus, and a related assembly.\n\nBACKGROUND\n\n[0003] A Baseboard Management Controller (BMC) chip is integrated with a processor that communicates with an external device through a data bus. Taking an Advanced Reduced Instruction Set Computer (RISC) Machine (ARM) processor as an example, the ARM processor communicates with an external device through an AMBA bus, and according to an AMBA protocol specification, needs to transmit data on an Advanced High performance Bus (AHB) to an Advanced Peripheral Bus (APB). Conversion and transmission of data on two buses are generally implemented through a bridge.\n\n[0004] Referring to FIGS. 1 and 2, FIG. 1 is a transmission timing diagram of the AHB, and FIG. 2 is a transmission timing diagram of the APB. It can be seen that, in an existing AHB transmission solution, for a current transaction, address information and control information are transmitted first, and then data is transmitted in a next clock cycle. The transmission of the address information and the control information needs only one clock cycle, and the transmission of the data needs multiple clock cycles. In an existing APB transmission solution, for the same transaction, the address information, the control information, and the data are transmitted at the same time. Therefore, the address information and control information sent by the AHB need to be cached in an APB bridge, and then the information and the data are transmitted after being completely received. As a result, excessive cache isolators are used, and a large storage space is occupied.\n\n[0005] Therefore, how to provide a solution to the foregoing technical problem is a problem currently needed to be solved by those skilled in the art.\n\nSUMMARY\n\n[0006] An objective of the present application is to provide a data transmission method and apparatus, and an electronic device. Address information, control information, and data do not need to be cached, whereby the occupation of a storage space is reduced.\n\n[0007] In order to solve the foregoing technical problem, the present application provides a data transmission method, applied to an APB bridge for connecting an APB and an AHB, including:\n\n[0008] dividing transmission into an address phase and a data phase according to a feature of the AHB;\n\n[0009] in the address phase, in response to the AHB meeting an address transmission condition corresponding to a current operation, transmitting address information and control information, which are sent by the AHB, to the APB; and\n\n[0010] in the data phase, in response to the APB meeting a valid data transmission condition corresponding to the current operation, sending received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.\n\n[0011] Preferably, after the step of dividing transmission into the address phase and the data phase according to the feature of the AHB, the data transmission method further includes:\n\n[0012] setting a first identifier corresponding to the address phase and a second identifier corresponding to the data phase; and\n\n[0013] judging whether a current transmission is in the address phase or the data phase according to whether the first identifier or the second identifier is valid.\n\n[0014] Preferably, the address transmission condition includes that HTRANS of the AHB is 2\u2032b02, and HREADY of the AHB is valid.\n\n[0015] Preferably, the current operation includes a read operation or a write operation.\n\n[0016] In response to the current operation being the read operation, the valid data transmission condition includes that:\n\n[0017] levels of PREADY, PENABLE, and PSEL are all high.\n\n[0018] In response to the current operation being the write operation, the valid data transmission condition includes that:\n\n[0019] the level of PREADY is high.\n\n[0020] Preferably, during the step of sending of received data to a bus corresponding to the current operation, the data transmission method further includes:\n\n[0021] assigning a value of PREADY to HREADY.\n\n[0022] Preferably, the data transmission method further includes:\n\n[0023] in response to the first identifier and the second identifier being both valid, determining that the current transmission is in the address phase.\n\n[0024] Preferably, the data transmission method further includes:\n\n[0025] in response to HREADY and HTRANS being both valid, determining that the first identifier is valid; and\n\n[0026] determining that the second identifier is valid in a next clock cycle after the first identifier is valid.\n\n[0027] Preferably, the data transmission method further includes:\n\n[0028] in response to the first identifier and the second identifier being both invalid, triggering HREADY to be converted to a high level.\n\n[0029] In order to solve the foregoing technical problem, the present application also provides a data transmission apparatus, applied to an APB bridge for connecting an APB and an AHB, including:\n\n[0030] a division module, configured to divide transmission into an address phase and a data phase according to a feature of the AHB;\n\n[0031] a first transmission module, configured to, in the address phase, in response to the AHB meeting an address transmission condition corresponding to a current operation, transmit address information and control information, which are sent by the AHB, to the APB; and\n\n[0032] a second transmission module, configured to, in the data phase, in response to the APB meeting a valid data transmission condition corresponding to the current operation, send received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.\n\n[0033] In order to solve the foregoing technical problem, the present application also provides an electronic device, including:\n\n[0034] a memory, configured to store a computer program; and\n\n[0035] a processor, configured to execute the computer program to implement any steps of the data transmission method as described above.\n\n[0036] The present application provides a data transmission method, applied to an APB bridge for connecting an APB and an AHB. A transmission process is divided into an address phase and a data phase first according to a feature of the AHB such that address information, control information, and data are transmitted in corresponding clock cycles respectively. In the present application, received information or data is transmitted when a condition corresponding to the current phase is met, whereby time for caching the address information, the control information, and the data is reduced, and the occupation of a storage space is thus reduced. The present application also provides a data transmission apparatus and an electronic device, which have the same beneficial effects as the data transmission method.",
      "claims": "1. A data transmission method, applied to an Advanced Peripheral Bus (APB) bridge for connecting an APB and an Advanced High performance Bus (AHB), comprising: dividing transmission into an address phase and a data phase according to a feature of the AHB; in the address phase, in response to the AHB meeting an address transmission condition corresponding to a current operation, transmitting address information and control information, which are sent by the AHB, to the APB; and in the data phase, in response to the APB meeting a valid data transmission condition corresponding to the current operation, sending received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.  \n\n2. The data transmission method according to claim 1, wherein after the step of dividing transmission into the address phase and the data phase according to the feature of the AHB, the data transmission method further comprises: setting a first identifier corresponding to the address phase and a second identifier corresponding to the data phase; and judging whether a current transmission is in the address phase or the data phase according to whether the first identifier or the second identifier is valid.  \n\n3. The data transmission method according to claim 1, wherein the address transmission condition comprises that HTRANS of the AHB is 2\u2032b02, and HREADY of the AHB is valid. \n\n4. The data transmission method according to claim 1, wherein the current operation comprises a read operation or a write operation; in response to the current operation being the read operation, the valid data transmission condition comprises that: levels of PREADY, PENABLE, and PSEL are all high; and  in response to the current operation being the write operation, the valid data transmission condition comprises that: the level of PREADY is high.   \n\n5. The data transmission method according to claim 1, wherein during the step of sending the received data to the bus corresponding to the current operation, the data transmission method further comprises: assigning a value of PREADY to HREADY.  \n\n6. The data transmission method according to claim 2, wherein the method further comprises: in response to the first identifier and the second identifier being both valid, determining that the current transmission is in the address phase.  \n\n7. The data transmission method according to claim 2, wherein the method further comprises: in response to HREADY and HTRANS being both valid, determining that the first identifier is valid; and determining that the second identifier is valid in a next clock cycle after the first identifier is valid.  \n\n8. The data transmission method according to claim 7, wherein the method further comprises: in response to the first identifier and the second identifier being both invalid, triggering HREADY to be converted to a high level.  \n\n9. (canceled) \n\n10. An electronic device, comprising: a memory, configured to store a computer program; and a processor, configured to execute the computer program to implement any steps of: dividing transmission into an address phase and a data phase according to a feature of an Advanced High performance Bus (AHB); in the address phase, in response to the AHB meeting an address transmission condition corresponding to a current operation, transmitting address information and control information, which are sent by the AHB, to an Advanced Peripheral Bus (APB); and in the data phase, in response to the APB meeting a valid data transmission condition corresponding to the current operation, sending received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.  \n\n11. The data transmission method according to claim 1, wherein under a circumstance that HTRANS is 2\u2032b01 and HREADY is valid with a high level, HADDR is assigned to PADDR, HWRITE is assigned to PWRITE, and meanwhile, PENABLE is enabled. \n\n12. The data transmission method according to claim 1, wherein under a circumstance that a level of PREADY is high, PENABLE is recovered to a low level, and HWRITE is a write control signal. \n\n13. The data transmission method according to claim 1, wherein under a circumstance that a level of HWRITE is high, the current operation is a write operation. \n\n14. The data transmission method according to claim 1, wherein under a circumstance that a level of HWRITE is low, the current operation is a read operation. \n\n15. The data transmission method according to claim 2, wherein under a circumstance that ahb_trans_head is valid and ahb_trans_data is invalid, the transmission is currently determined as in the address phase. \n\n16. The data transmission method according to claim 2, wherein under a circumstance that ahb_trans_data is valid and ahb_trans_head is invalid, the transmission is currently determined as in the data phase. \n\n17. The data transmission method according to claim 2, wherein under a circumstance that a level of HREADY is low and there is no ahb_trans_head, ahb_trans_data becomes invalid. \n\n18. The data transmission method according to claim 2, wherein under a circumstance that levels of HREADY and ahb_trans_head are all high, ahb_trans_data becomes valid. \n\n19. The data transmission method according to claim 2, wherein under a circumstance that ahb_trans_data and ahb_trans_head are both valid, the address phase is preferred. \n\n20. The data transmission method according to claim 1, wherein during multi-transaction transmission, the data phase of a current transaction is the address phase of a next transaction, and bug occurs. \n\n21. The electronic device according to claim 10, wherein the processor is configured to execute the computer program to implement any steps of: setting a first identifier corresponding to the address phase and a second identifier corresponding to the data phase; and judging whether a current transmission is in the address phase or the data phase according to whether the first identifier or the second identifier is valid.",
      "description": "BRIEF DESCRIPTION OF THE DRAWINGS\n\n[0037] In order to describe the technical solutions in embodiments of the present disclosure more clearly, the drawings required to be used in the prior art and the embodiments will be introduced briefly below. Apparently, the drawings in the description below are only some embodiments of the present disclosure. Those ordinarily skilled in the art may further obtain other drawings according to these drawings without creative work.\n\n[0038] FIG. 1 is a transmission timing diagram of an AHB according to the present application;\n\n[0039] FIG. 2 is a transmission timing diagram of an APB according to the present application;\n\n[0040] FIG. 3 is a schematic structural diagram of a data transmission system according to the present application;\n\n[0041] FIG. 4 is a flowchart of a data transmission method according to the present application;\n\n[0042] FIG. 5 is a timing diagram of an AHB-to-APB read operation according to the present application;\n\n[0043] FIG. 6 is a timing diagram of an AHB-to-APB write operation according to the present application;\n\n[0044] FIG. 7 is an AHB-to-APB multi-transaction timing diagram according to the prior art;\n\n[0045] FIG. 8 is an AHB-to-APB multi-transaction timing diagram according to the present application;\n\n[0046] FIG. 9 is a schematic structural diagram of a data transmission apparatus according to the present application; and\n\n[0047] FIG. 10 is a schematic structural diagram of an electronic device according to the present application.\n\nDETAILED DESCRIPTION\n\n[0048] The core of the present disclosure is to provide a data transmission method and apparatus, and an electronic device. Address information, control information, and data do not need to be cached, whereby the occupation of a storage space is reduced.\n\n[0049] In order to make the objective, technical solutions, and advantages of the embodiments of the present disclosure clearer, the technical solutions in the embodiments of the present disclosure will be described clearly and completely below in combination with the drawings in the embodiments of the present disclosure. Clearly, the described embodiments are not all but only part of embodiments of the present disclosure. All other embodiments obtained by those ordinarily skilled in the art based on the embodiments in the present disclosure without creative work shall fall within the scope of protection of the present disclosure.\n\n[0050] For ease of understanding the data transmission method of the present application, a system that the data transmission method of the present application is applicable for will be introduced below. Referring to FIG. 3, a schematic structural diagram of a data transmission system according to an embodiment of the present application is shown.\n\n[0051] As shown in FIG. 3, a high-bandwidth memory interface, a high-performance ARM processor, a high-bandwidth on-chip Random Access Memory (RAM), and a Direct Memory Access (DMA) bus master are mounted to an AHB, and a Universal Asynchronous Receiver Transmitter (UART), a keypad, a timer, a Parts In One (PIO, a novel barebone computer that may be assembled into an integrated computer freely), and other peripherals are mounted to an APB. The AHB is connected with the APB through an AHB to APB bridge, referred to as an APB bridge hereinafter for short. The data transmission method provided in the present application may specifically be implemented by the APB bridge. The APB bridge may be considered as a slave device of the AHB and a master device of the APB.\n\n[0052] Referring to FIG. 4, FIG. 4 is a flowchart of a data transmission method according to the present application. The data transmission method includes the following steps.\n\n[0053] In S101, transmission is divided into an address phase and a data phase according to a feature of an AHB.\n\n[0054] In S102, in the address phase, when the AHB meets an address transmission condition corresponding to a current operation, address information and control information, which are sent by the AHB, are transmitted to an APB.\n\n[0055] In S103, in the data phase, when the APB meets a valid data transmission condition corresponding to the current operation, received data is sent to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.\n\n[0056] Specifically, transmission is divided into two phases according to a feature of the AHB: one is an address phase, and the other is a data phase. In the present embodiment, a current operation may include a read operation or a write operation. Referring to FIGS. 5 and 6, FIG. 5 is a timing diagram corresponding to an AHB-to-APB read operation, and FIG. 6 is a timing diagram corresponding to an AHB-to-APB write operation. In the address phase, for example, when HTRANS is 2\u2032b01, and HREADY is valid (the level is high), HADDR is assigned to PADDR, HWRITE is assigned to PWRITE, and meanwhile, PENABLE is enabled. When a level of PREADY is high, PENABLE is recovered to a low level, and HWRITE is a write control signal (i.e., control information) of the AHB. When a level of HWRITE is high, the current operation is the write operation. When the level of HWRITE is low, the current operation is the read operation.\n\n[0057] Specifically, when the current operation is the read operation, referring to FIG. 5, the address phase is in a first clock cycle only. In the data phase, when a level of a PREADY signal of the APB is high, and data is valid, namely a level of PSEL is high and a level of PENABLE is high, the data is fed back to HRDATA of the AHB.\n\n[0058] Specifically, when the current operation is the write operation, referring to FIG. 6, an operation of the address phase is the same as the read operation. In the data phase, only when PREADY is valid, HWDATA of the AHB is directly assigned to PWDATA of the APB, and HREADY is made equal to PREADY.\n\n[0059] Further, after the transmission is divided into the address phase and the data phase, which are identified with ahb_trans_head and ahb_trans_data respectively, wherein ahb_trans_head is a first identifier, and ahb_trans_data is a second identifier. When ahb_trans_head is valid, and ahb_trans_data is invalid, it is determined that the transmission is currently in the address phase. When ahb_trans_data is valid, and ahb_trans_head is invalid, it is determined that the transmission is currently in the data phase. The ahb_trans_head is valid under the condition that both HREADY and HTRANS are valid. The ahb_trans_data is valid at a next beat of ahb_trans_head. The ahb_trans_data becomes invalid when the level of HREADY is low and there is no ahb_trans_head, and is valid when levels of the above signals are all high. When ahb_trans_head and ahb_trans_data are both invalid, HREADY may be triggered to be converted to a high level. That is, the level of the HREADY is high by default, and HREADY is equal to PREADY in other cases.\n\n[0060] It can be understood that, during multi-transaction transmission shown in FIG. 7 on the AHB, a data phase of a current transaction is also an address phase of a next transaction, and bug occurs easily during transmission. Therefore, when ahb_trans_data and ahb_trans_head are both valid, namely in a third cycle in FIG. 8, the address phase is preferred in the present application. That is, HADDR is assigned to PADDR in the third cycle, so PADDR changes to address B in a fourth cycle. As such, address phases and data phases of transactions are distinguished.\n\n[0061] The present application provides a data transmission method, applied to an APB bridge for connecting an APB and an AHB. A transmission process is divided into an address phase and a data phase first according to a feature of the AHB such that address information, control information, and data are transmitted in corresponding clock cycles respectively. In the present application, received information or data is transmitted when a condition corresponding to the current phase is met, whereby time for caching the address information, the control information, and the data is reduced, and the occupation of a storage space is thus reduced.\n\n[0062] Referring to FIG. 9, FIG. 9 shows a data transmission apparatus provided in the present application, applied to an APB bridge for connecting an APB and an AHB, including:\n\n[0063] a division module 1, configured to divide transmission into an address phase and a data phase according to a feature of the AHB;\n\n[0064] a first transmission module 2, configured to, in the address phase, when the AHB meets an address transmission condition corresponding to a current operation, transmit address information and control information, which are sent by the AHB, to the APB; and\n\n[0065] a second transmission module 3, configured to, in the data phase, when the APB meets a valid data transmission condition corresponding to the current operation, send received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.\n\n[0066] It can be seen that, according to the present embodiment, a transmission process is divided into an address phase and a data phase first according to a feature of the AHB such that address information, control information, and data are transmitted in corresponding clock cycles respectively. In the present application, received information or data is transmitted when a condition corresponding to a current phase is met, whereby time for caching the address information, the control information, and the data is reduced, and the occupation of a storage space is further reduced.\n\n[0067] As a preferred embodiment, the data transmission apparatus further includes:\n\n[0068] an identifier setting module, configured to set a first identifier corresponding to the address phase and a second identifier corresponding to the data phase; and\n\n[0069] a phase judgment module, configured to judge whether the current transmission is in the address phase or the data phase according to whether the first identifier or the second identifier is valid.\n\n[0070] As a preferred embodiment, the address transmission condition includes that HTRANS of the AHB is 2\u2032b02, and HREADY of the AHB is valid.\n\n[0071] As a preferred embodiment, the current operation includes a read operation or a write operation.\n\n[0072] If the current operation is the read operation, the valid data transmission condition includes that:\n\n[0073] levels of PREADY, PENABLE, and PSEL are all high.\n\n[0074] If the current operation is the write operation, the valid data transmission condition includes that:\n\n[0075] the level of PREADY is high.\n\n[0076] As a preferred embodiment, the data transmission apparatus further includes:\n\n[0077] an assignment module, configured to assign a value of PREADY to HREADY.\n\n[0078] As a preferred embodiment, the phase judgment module is specifically configured to, when the first identifier and the second identifier are both valid, determine that the current transmission is in the address phase.\n\n[0079] As a preferred embodiment, when HREADY and HTRANS are both valid, the first identifier is valid. It is determined that the second identifier is valid in a next clock cycle after the first identifier is valid.\n\n[0080] As a preferred embodiment, the data transmission apparatus further includes:\n\n[0081] a triggering module, configured to, when the first identifier and the second identifier are both invalid, trigger HREADY to be converted to a high level.\n\n[0082] In another aspect, the present application also provides an electronic device. Referring to FIG. 10, a schematic composition structure diagram of an electronic device according to an embodiment of the present application is shown. The electronic device 2100 of the present embodiment may include a processor 2101 and a memory 2102.\n\n[0083] Optionally, the electronic device may further include a communication interface 2103, an input unit 2104, a display 2105, and a communication bus 2106.\n\n[0084] The processor 2101, the memory 2102, the communication interface 2103, the input unit 2104, and the display 2105 communicate with one another through the communication bus 2106.\n\n[0085] In the embodiment of the present application, the processor 2101 may be a Central Processing Unit (CPU), an application specific integrated circuit, a digital signal processor, a field-programmable gate array or another programmable logic device, etc.\n\n[0086] The processor may call a program stored in the memory 2102. Specifically, the processor may execute operations executed by an electronic device side in the following embodiment of a data transmission method.\n\n[0087] The memory 2102 is configured to store one or more than one program. The program may include a program code that includes a computer operation instruction. In the embodiment of the present application, the memory at least stores programs for implementing the following functions:\n\n[0088] dividing transmission into an address phase and a data phase according to a feature of an AHB;\n\n[0089] in the address phase, when the AHB meets an address transmission condition corresponding to a current operation, transmitting address information and control information, which are sent by the AHB, to an APB; and\n\n[0090] in the data phase, when the APB meets a valid data transmission condition corresponding to the current operation, sending received data to a bus corresponding to the current operation, wherein the bus is the APB or the AHB.\n\n[0091] It can be seen that, according to the present embodiment, a transmission process is divided into an address phase and a data phase first according to a feature of the AHB such that address information, control information, and data are transmitted in corresponding clock cycles respectively. In the present application, received information or data is transmitted when a condition corresponding to a current phase is met, whereby time for caching the address information, the control information, and the data is reduced, and the occupation of a storage space is further reduced.\n\n[0092] In a possible embodiment, the memory 2102 may include a program storage region and a data storage region. The program storage region may store an operating system, an application program needed by at least one function (such as a validity judgment function), etc. The data storage region may store data created according to use of a computer.\n\n[0093] In addition, the memory 2102 may include a high-speed RAM, and may also include a nonvolatile memory, such as at least one disk memory device or other volatile solid-state memory device.\n\n[0094] The communication interface 2103 may be an interface of a communication module, such as an interface of a Global System for Mobile communications (GSM) module.\n\n[0095] The present application may further include the display 2104, the input unit 2105, etc.\n\n[0096] Certainly, the structure of the electronic device shown in FIG. 10 does not form any limitation on the electronic device in the embodiment of the present application. In practical applications, the electronic device may include more or fewer components than those shown in FIG. 10, or some components are combined.\n\n[0097] It is also to be noted that relational terms in the specification, such as first and second, are used only to distinguish an entity or operation from another entity or operation and do not necessarily require or imply the existence of any practical relation or sequence between these entities or operations. Moreover, terms \u201cinclude\u201d and \u201ccontain\u201d or any other variation thereof is intended to cover nonexclusive inclusions, whereby a process, method, object, or device including a series of elements not only includes those elements but also includes other elements which are not clearly listed, or further includes elements intrinsic to the process, the method, the object, or the device. With no more restrictions, an element defined by statement \u201cincluding a/an\u201d does not exclude the existence of the same other elements in a process, method, object, or device including the element.\n\n[0098] The disclosed embodiments are described above to enable those skilled in the art to implement or use the present application. Various modifications to these embodiments are apparent to those skilled in the art. The general principle defined herein may be implemented in other embodiments without departing from the spirit or scope of the present application. Therefore, the present application will not be limited to these embodiments shown herein but is consistent with the largest scope consistent with the principles and novel characteristics disclosed herein."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 11,
      "claims_start": 10,
      "description_end": 10,
      "description_start": 7,
      "drawings_end": 6,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 11,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 10,
      "specification_start": 7,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 70301687,
    "foreign_priority": [
      {
        "app_filing_date": "2019-12-15",
        "app_number": "201911287932.8",
        "country": "CN"
      }
    ],
    "guid": "US-20230009095-A1",
    "image_file_name": "00000001.tif",
    "image_location": "us-pgpub/US/2023/0009/095",
    "intl_class_current_primary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "13/40",
        "version": "2006-01-01"
      }
    ],
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "13/16",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06F13/40",
      "G06F13/16"
    ],
    "inventors": [
      {
        "city": "Suzhou, Jiangsu",
        "country": "CN",
        "name": "WANG; Zhaohui",
        "postal_code": "N/A",
        "state": "N/A"
      },
      {
        "city": "Suzhou, Jiangsu",
        "country": "CN",
        "name": "LIU; Tongqiang",
        "postal_code": "N/A",
        "state": "N/A"
      }
    ],
    "inventors_short": "WANG; Zhaohui et al.",
    "patent_title": "DATA TRANSMISSION METHOD AND APPARATUS, AND RELATED ASSEMBLY",
    "publication_date": "2023-01-12",
    "publication_number": "20230009095",
    "type": "US-PGPUB"
  },
  {
    "app_filing_date": "1975-07-25",
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "defensive publication",
    "composite_id": "24398621!MXOC-US-T0949002",
    "cpc_inventive": [
      {
        "cpc_class": "G11C",
        "cpc_subclass": "15/04",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "G06F",
        "cpc_subclass": "12/0866",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "ACCPE INTERNL\n\nEXTFANAL\n\nA AN A NTSRL\n\nCHANNELS\n\n(1) A is lensity dynamic mode operat d its associated read!\n\n(2) write, aerating elements are combis y such as a disk file, to fori Lge system. Complete with a Xgic between the random a ial memory, the corn- binatic -quential storage may be ad("
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 0,
      "claims_start": 0,
      "description_end": 0,
      "description_start": 0,
      "drawings_end": 4,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 4,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 0,
      "specification_start": 0,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 24398621,
    "guid": "US-T949002-I4",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/T0/949/002",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "G06F",
        "intl_subclass": "12/08",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G11C",
        "intl_subclass": "15/04",
        "version": "2006-01-01"
      },
      {
        "intl_class": "G11C",
        "intl_subclass": "15/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "G06F12/08"
    ],
    "patent_title": "OCR SCANNED DOCUMENT",
    "publication_date": "1976-08-03",
    "publication_number": "T949002",
    "type": "USOCR",
    "us_class_current": [
      "711/112"
    ]
  },
  {
    "composite_id": "23002839!OC-US-04388879",
    "cpc_additional": [
      {
        "cpc_class": "D10B",
        "cpc_subclass": "2503/06",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "D05D",
        "cpc_subclass": "2305/12",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "D05D",
        "cpc_subclass": "2207/04",
        "version": "2013-01-01"
      }
    ],
    "cpc_inventive": [
      {
        "cpc_class": "D05B",
        "cpc_subclass": "41/00",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "D05B",
        "cpc_subclass": "33/00",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "United States Patent [ig] Everall, Jr. et al. [54] APPARATUS                FOR                 SEQUENTIALLY FABRICATING PILLOWCASES OR LIKE PRODUCRS [75] Inventors: Palmer   B.   Everall,   Jr.;    Jack    R. Lowery,    Sr.,    both    of    Lancaster, S.C. [73] Assignee: Springs Mills, Inc., Fort Mill, S.C. [21] Appl. No.: 263,686 [22] Filed: May 14, 1981 [51] Int. Cl.3 DOSB 13/00 [52] U.S. Cl 112/10; 112/121.29- 112/304;112/30i [58]    Field    of     Search 112/10,      It      121.29 11   2/121.12,   121.15,    303,    304,    307:    121.26: 121.27 [56]                       References Cited U.S. PATENT DOCUMENTS 2,940,404 6/1960 Damo n 112/10 3,126,848 3/1964 Gasto nguay 112/10 3,223,059 12/1965 Jacobs 112/121.12 3,224,394 12/1965 Dobner et al 112/10 3,227,118 1/1966 Gore 112/10 3,310,207 3/1967 Gore 112/10     X 107      1 ?14 ?JO [11] 493889879 [451 Jun.21,1983 3,773,002 11/1973  Burton 112/121.12 4   214,541   7/1980   Zeigler,   Jr.   et   al 112/121.10 X 4:224,883   9/1980   Zeigler,   Jr.    et    al 112/121.10 Primary Examiner-14.  Hampton Hunter Attorney,    Agent,    or    Firm-Bell,     Seltzer,     Park &     Gibson ABSTRACI' [57] Apparatus for sequentially fabricating pillowcases or like products wherein mechanisms are provided for automatically feeding continuous tubular material from a supply means to a cutter mechanism and for cutting the material into- individual pieces of predetermined desired lengths for either \"regular cut\" pillowcases in which each pillowcase is cut a predetermined desired length or \"Panel cut\" pillowcases in which each pillowcase is cut a predetermined desired length from a printed panel design.  The apparatus includes mechanisms for clamping one of the cut ends of each cut piece while allowing the remaining portion of the cut pieces to hang down in a generally verticay-extending position and for conveying the cut pieces sequentially past a stitching mecbanism to complete fabrication of the pillowcases. 11 Claims, 15 Drawing Figures 50 40 10\n\nU. S. Patent        Jun. 21, 1983        Sheet I of 14         493889879 < LO\n\nU. S. Patent Jun. 21, 1983 yg,-, LL Sheet 2 of 14 00 H 493889879 C\\i\n\nU.S. Patent Jun. 21, 1983 Sheet 3 of 14                         41,3889879 LL) 00 --M  TN I\n\nU. S. Patent Jun. 21, 1983 Sheet 4 of 14 493889879 L I N M% F                          I Fllil 31 0 33 13@l C: I L n c o Li-\n\nU. S. Patent  Jun. 21, 1983        Sheet 5 of 44 4@3889879 I r a Ai L(\"r a II El                       Li\n\nU. S. Patent   Jun. 21, 1983       Sheet 6 of 14 493889879 ru t7- ro7 13 00\n\nU. S. Patent        Jun. 21, 1983        Sheet 7 of 14        493889879 0 c LE Li c@j -zzi-      ini 01, CO LO . . . . .... ... ...................\n\nU.S. Patent              Jun. 21, 1983 Sheet 8 of 14              493889879 Ln co co C\\i\n\nU. S.   Patent Jun. 21, 1983               Sheet 9 of 14        493889879 C@ 4&  ul C\"i W -i  !:!DN :LLUO z 0- 9:t  ikz LLi 10                                                        ui  @0- oLU ;j4 co CLZU 4 LU l@z -z\n\nU. S. Patent        Jun. 21, 1983        Sheet 10 of 14        493889879 155 ro 60, 1 70 P H O T O 177 ELE CM C CE H 7) PLJ--,4-1-THRU 66 2- COUNT-C-P, BOTTOM CLAMP\n\nU. S. Patent   Jun. 21, 1983 @ol IT 120VAC 104- 4 4 ENABLE- .-r %j TM pi JL 1 194- SEWINC-A - ;j m NippOLLM  .50 OTOR C@LLJ@TCH-BPA eA Sheet II of 14      493881,879 MAGNETIC,    103 51-/,AK I =!I..j 'ZI5 I)Or-F- 72                 1% r------- 189 TOIAL 1 COUNTER POLSE- 70,, GENERA         )30\n\nU. S. Patent       Jun. 21, 1983        Sheet 12 of 14        493889879 OB COWEYOP, GS 1991-1 CUTTINQ ENI@ SEWINCT   ts RE\\/ERSNC:i  i6ro 54 -M-\",@CcLiTTER ACROSS Pp,ox. ?-4 33 5CRAY OT OR C-UTTY= POWER CUTREP, NIP RO   46 s-43 OPENI- LOS   I COLOR  I AIR,   -y- 116 POW SUPP 117@ 11 0 T r RT T 5TOP, 14?-, \"AuTomATtc, RUN\" 143 L U T C H R E C I . C I J T )51 )50   14 4- 151?  1 PANE: L6 I 0 ------ 0   - FOFD CR amp \"MP -@I 5-5 B RAKE- 156\n\nU. S. Patent   Jun. 21, 1983         Sheet 13 of 14 493889879 To? CL&.mp 160                      160 'PPIOX. 163- 'PRO        CUTTF-P, 160                  CL)TTER, ACPC)52-> c o FO'RWAIR PEVE I 1 4 12-5 [BO 'BOMM CLAMP 17,3                   66.) )70 171 PUSH-THRU PIAOTO ELECTP- IC-CELL TO CONTIPOLCONVE YOR CLAMI)bMN'PLAT E- AM iLl2c\n\nU. S. Patent      Jun. 21, 1983      Sheet 14 of 14 493889879 t7l'17   17@                       )79 A) R 176 1 BS 73 205 24V-;.#- ..   Co m. bc +  18 7 191 204- 20 PHOT O ELECTRIC C 208 @-2J4- P)3 Aip, 80 ACCUMULATOR PHOTO ELECTRIC CEL                STOP /-,\\ (O  Irr- ACCUMLJLATOR FULL\n\n4,388,879 APPARATUS FOR SEQUENTIALLY FABRICATING PILLOWCASES OR LIKE PRODUC]RS 5 FIELD OF THE INVENTION This invention relates to apparatus for sequentially fabricating pillowcases or like produc@ts wherein continuous tubular material is automatically handled and cut into individual pieces of predetermined desired lengths 10 for either \"regular cut\" pillowcases in which each pil@ lowcase is cut a predetermined desired length, or \"panel, cut\" pillowcases in which each pillowcase is cut a predetermined desired length from a printed panel design. e cut pieces past  a  stitching  mechanism  with  the  end  to be stitched  clamped  and  the  remaining  portion   of   the cut pieces  hanging  down  in   a   generally   vertic ally-extending position. BACKGROUND OF THE INVENTION 20 Although   various   apparatus   have   been   proposed   and commercialized  over  the   years   for   use   in fabricating pillowcases  or  like  products,  such   apparatus   have suf- fered  from  one  or  more  drawbacks  including   lack   of a 25 desired  amount   6f   automation,   size   of   the  - apparatus requiring   excessive   floor   space   in    a manufacturing plant, inability  to  automatically  fabricate  either \"regular cut\"  or.  \"panel  cut\"  pillowcases  on  the  same apparatus, etc. 30 Accordingly, it is the  object  of  this  invention  to  pro- vide  an  apparatus   for   sequentially   fabricating pillow- cases  or  like  products   which   performs   its fabrication operations   automatically   and   which   can   be   - regulated for  fabricating  either  \"regular  cut\"  or  \"panel  cut\" pil- 35 lowcases. It is a further  object  of  this  invention  to  provide  an apparatus   for   sequentially   fabricating   pillowcases or like  products  which  provides   improved   handling   of the material  to  reduce  the  size  of  the  apparatus  so  as to 40 reduce  floor  space  required  for   the   apparatus   in the manufacturing plant; SUMMARY OF THE INVENTION By  this  invention,  it  has  been  found  that  the above 45 objects  can  be  accomplished  by   providing   apparatus for sequentially   fabricating   pillowcases   or   like products automatically  from   a   continuous   flat   tubular material wherein   mechanisms   feed   the   material   from   a supply mechanism  to  a  cutter   mechanism   for   transversely cut- 50 ting the material into individual pieces. The   apparatus   also   includes   improved   means   opera- tively  connected  with  and   automatically   controlling the operation   of   the   cutting   mechanism   and   the feeding mechanisms  for   sequentially   cutting   the   material into 55 individual  pieces   of   predetermined   dimensions   for the pillowcases.   This   control    means    includes mechanisms for  regulating  the  control  means  to  control  the cutting mechanism   and   the   feeding   mechanism   for fabricating either  \"regular  cut\"  pillowcases   in   which   the pillow- 60 case  is  cut  a  predetermined  desired   length,   or \"panel cut\"  pillowcases  in  which  each  pillowcase  is  cut  a pre- determined desired length ' from spaced separate printed panel designs on the material. The apparatus further includes improved conveying 65 means for sequentially receiving the individual cut pieces of material, clamping one of the transverse cut ends thereof while allowing the remainder of the cut pieces of material to hang downwardly in a generally vertically-extending position, and conveying and sopositioned cut pieces sequentially in a generally straight-lined path of travel past a stitching mechanism for stitching the clamped transverse cut end of each piece of material while being conveyed by the conveyor mechanisms to complete fabrication of the pillowcases. The apparatus may further include mechanisms for doffing the fabricated pillowcases from the conveyor mechanisms and for stacking the pillowcases into individual stacks of a predetermined number of pillowcases and for accumulating a predetermined number of stacks for removal by an operator. Some of the objects and advantages of this invention having been set forth, other objects and advantages wi]I appear when taken in conjunction with the accompanying drawings, in which: FIG. 1 is a perspective view of a pillowcase fabricated with the apparatus of this invention; FIG. 2 is a perspective schematic view, with frame and other portions omitted for clarity, of an apparatus in accordance with this invention; FIG. 3 is a partial side elevational view, taken generally in the direction of the arrow 3 in FIG. 2; FIG. 4 is an enlarged side elevational view of a portion of the apparatus shown in FIG. 3, with parts of the covering frame portion removed; FIG. 5 is a side elevational view of a portion of the apparatus of FIG. 2 and taken in the direction of the arrow 5 in FIG. 4; FIG. 6 is a side elevational view of a portion of the apparatus of FIG. 2 and taken generally in the direction of the arrow 6 in FIG. 4, FIG. 7 is a sectional view, taken generally along the line 7-7 of FIG. 5; FIG. 8 is a sectional plan view, taken generally along the line 8-8 of FIG. 5; FIG. 9 is an eievational view of a portion of the apparatus of FIG. 2 and taken generally in the direction of the arrow 9 in FIG. 2; FIG. 10 is a schematic mechanical/electrical view of a portion of the apparatus illustrated in FIG. 2; FIG. 11 is a schematic electrical/mechanical view illustrating operation of the mechanisms utilized for clamping the material during the cutting operation and placing the cut pieces of material in the conveyor mechanism for conveying away the pieces from the cutting mechanism after cutting; and FIGS. 12 A-D are continuation views of an electrical schematic of the apparatus of this invention, particularly the control mechanisms therein. DETAILED DESCRIPTI ON OF INVENTIO N In the drawings, one embodiment of an apparatus for sequentially fabricating pillowcases P (as shown in FIG. 1) or like products, such as bags and other similarily constructed products, is schematically illustrated.  However, it is to be understood that other apparatus utilizing the novel features of this invention could be provided. Broadly (as schematically illustrated in FIG. 2), the apparatus of this invention includes means, generally indicated at 10, for supplying continuous flat tubular material M. This flat tubular material M may be a continuous sheet of textile fabric normally utilized for pillowcases P which has been previously folded longitudi- The   apparatus   includes   mechanisms   for   conveyiiig   th' 15   BRIEF DESCRIPTIONS OF THE DRAWINGS\n\n49388,879                             4 nally about its central longitudinal axis A and then side seamed at B along the other longitudinal edge to form flat, two layer, tubular material M. This flat, folded, side seamed, two layer, tubular material M is then fed by feeding means, generally indicated at 11, from the supply means 10 to cutting means, generally indicated at 12, for transversely cutting the continuous material M into individual pieces L. Electrical and mechanical control means, to be described in detail below and schematically illustrated in the drawings, are operatively connected with and control the operation of the cutting means 12 and the feeding means 11 for sequentially cutting the continuous material M into individual pieces L of desired predetermined dimensions for the pillowcases P. Such control s, as will be described hereinafter, include pr ' mean                                                      ovi- sions for regulating the control means to control the feeding means 11 and cutting means 12 for fabricating (1)  \"regular cut\" pillowcases P in which each of the pillowcases are of a predetermined desired length and (2)  \"panel cut\" pillowcases P in which the continuous material M has spaced-apart printed panel designs D and in which it is desired to cut each pillowcase a predetermined distance or centrally between the printed panels D. Conveying means, generally indicated at 13, @re provided for sequentially receiving the individually cut pieces L, clamping one of the transverse cut ends El thereof, while allowing the remainder of the cut pieces L to hang downwardly in a generally vertically-extending position, and conveying the so-positioned cut pieces L sequentially in a generally straight-line path of travel.  Means, generally indicated at 14, are provided for stitching the clamped transverse cut end El of each piece L while being conveyed by the conveyor means 13 to complete fabrication of the- pillowcases P (as shown in FIG. 1), which are closed on three sides and have an open end E2. From the conveyor means 13, the pillowcases P maY be doffed onto a stacking and an accumulating means, generally indicated at 15, for forming individual stacks of a predetermined number of pillowcases P, as will be described below. For a better understanding of the above described broad features of the apparatus of this invention, a mechanical sequence of operation will now be described. To start-up operation of the apparatus, an operator first places a roll of continuous material M onto a driven let-off mechanism 20, forming a part of the supply means 10 and consisting of a pair of friction drive rolls 21, 22 which receive the roll of material M on the top surface. thereof.  These rolls 21, 22 are geared together and driven through shaft 23 from motor 24.  The end of the material M is then threaded through driven nip- rolls 26,27 which are driven by a chain and sprocket mechanism 28 from the shaft 23 of motor 24.  The nip-rolls 26, 27 feed the material M into an automatic scray mechanism 30 which consists of a sheet-metal pan 31 that is balanced through a weighted pivot mechanism 32 so that when it fills up with the desired amount of material M it causes the nip-rolls 26, 27 and driven let-off 20 to stop.  As the material M is pulled out of the pan 31, the pan 31 raises to a point that a proximity switch 33 is closed and causes the nip-rolls 26, 27 and driven let-off 20 to start-up again through the motor 24, in a manner to be discussed in more detail below.  This procedure repeats as long as the apparatus is running.  These de- vices are all mounted on a frame section Fl (see FIG. 3) and form the supply means 10. The end of the continuous material M is then threaded through a drag bar tensioning mechanism 40 over another drag bar 41 through an optical color or shade photoelectric cell scanning device 43 to be described more fully below, over another drag bar 44 and down to feeding nip-rolls 45, 46.  Nip-rolIs 45, 46, as well as drag rolls 41, 44, scanning device 43 and drag bar 10 device 41 are mounted on a frame section F2, (see FIG. 3), which is generally in alignment with the frame section Fl carrying the material supply means 10 so that the material M travels in a first, generally straight-line, path of travel from the supply means 10 to the feeding 15 nip-rolls 45, 46.  The material is carried in a generally inverted U-shaped path of travel from the drag bar mechanism 40 to the feeding nip-rolls 45, 46.  The scanning device 43 is positioned in generally the apex of the inverted U-shaped path of travel so that an operator 20 may easily obtain access thereto by walking under the inverted U-shaped path of travel of the material M. The feeding nip-rolls 45, 46 are rotatably mounted on the frame F2, (as shown in FIGS. 3 and 4).  The roll 46 is pivotally mounted for movement by a pneumatic piston 25 and cylinder mechanism 47 (see FIG. 4) for movement into and out of feeding engagement with the roll 45 for threading of the material M therebetween.  The roll 45 is suitably driven by a chain and sprocket drive 48 from a clutch/brake device 49 and motor 50. 30 After the material M is threaded though the feeding nip-rolls 45, 46 it passes to a cut line (see FIG. 10) defined by the path of travel of a horizontally movable rotary cutter 52 forming part of the cutting means 12.  The cutter 52 is a driven rotary cutter which is suitably 35 mounted on a chain and sprocket device 53 driven by motor 54 to move back and forth transversely across the material M for transversely cutting the material M into individual pieces L for continued fabrication of individual pillowcases P. 40 If \"regular cut\" pillowcases P are being fabricated, the end of the material M is placed approximately I inch pass the cut line of the rotary cutter 52 to allow for a small trim-off to obtain a straight edge prior to automatic operation of the apparatus.  If \"panel cut\" pillow- 45 cases P are being fabricated, the material M is placed to the cut line so that, when trimmed-off, the proper distance from the cut edge to the panel print design D will be provided. The apparatus has  now  been  threaded  up  and  is  ready 50 for automatic run, as will be discussed below in a description of the electrical sequence.  When the desired lengths of material M has been fed for either a \"regular cut\" or \"panel cut\" pillowcase P, the nip-feeding rolls 45, 46 will automatically stop in a manner to be de- 55 scribed below.  When the nip-rolls 45, 46 stop, a top clamp 60 which is carried by a pneumatically-operated piston and cylinder mechanism 61 mounted on frame portion F2, (see FIGS. 4 and 10) is activated to clamp the material M against a stationary plate 63 on the frame 60 portion F2 to hold the material M just below the cut line of the rotary cutting mechanism 52.  At the same time, the rotary cutting mechanism 52 is activated which causes the cutting mechanism to move across and cut the material M to form an individual piece L which is 65 held at the top cut edge El by the clamp 60. After the piece L has been cut, a bottom clamp 65, carried by a pneumatic piston and cylinder mechanism 66 on the frame portion F2, clamps the cut piece L\n\n5                          4,388,879 6 against a stationary plate 67  on  the  frame  portion  F2  just pillowcases P then  continue in  their  sequential  path  of below a first belt conveyor 68.  This action activates a push-through bar 70 which is carried by a pneumatic piston and cylinder mechanism 71 mounted on the frame portion F2 to push the material M between the first belt conveyor 68 and a top hold plate 73.  The top hold plate 73 is pivotally mounted on the frame portion F2 and is moved into and out of clamping engagement with the first conveyor belt 68 by a pneumatic piston and cylinder mechanism 74 mounted on the frame portion F2.  As the push-through bar 70 starts to extend, it deactivates the top clamp 60 thereby releasing the top cut edge El of the cut piece L. This will allow the push-through bar 70 to push the cut edge El of the piece L through the belt conveyor 68 and top hold clamp 73 to extend approximately one inch past the belt conveyor 68. As the push-through bar 70 retracts after pushing the cut edge El of the cut piece L through the first belt conveyor 68 and top hold plate 73, it activates the top hold plate 73 which comes down and clamps the top cut edge El of the piece L to the belt conveyor 68.  This belt conveyor 68 is mounted on a portion of the machine frame F3 (see FIGS. 5 and 6) by suitable pulley mechanisms and ' is continuously driven in a manner to be described below.  With the top hold plate 73 down, the bottom hold clamp 65 is deactivated and the cut piece L now begins to move in a second, generally straight-line, path of travel at a generally 90' angle to the first path of travel of the material M prior to being cut.  The cut piece L is positioned, as described above, to hang downwardly from the belt conveyor 68 in a generally vertically-extending position so as to conserve space and provide a more compact apparatus. When the trailing end of the cut piece L has moved away from the cutting means 12, the top hold plate 73 is deactivated by the piston and cylinder mechanism 74 moving the top hold plate 73 back to its upward position.  The next cut piece L in the sequence is then pushed through the belt conveyor 68 and top hold plate 73, as described above, after having been cut and clamped by the respective mechanisms. The cut pieces L are spaced approximately four inches apart sequentially as they are fed forwardly by the first belt conveyor 68 ' The sequence continues to repeat itself to sequentially place cut pieces L in the above described position for being fed forwardly in the above described path of travel. A second belt conveyor 75 is positioned to cooperate with the first belt conveyor 68 (see FIGS. 1 and 6) to receive the clamped end El of the cut piece L between it and the first belt conveyor 68 as each cut piece L is fed forwardly sequentially in its path of travel.  The second belt conveyor 75 is in the form of an endless belt and is suitably mounted by pulley mechanisms carried by the frame portion F3 and is continuously driven in a manner to be described below. The stitching means 14 is a suitable sewing machine mounted adjacent the belt conveyors 75 and 68 so that the clamped end El of the cut pieces L are trimmed and stitched as they sequentially pass the sewing machine 14.  A chain of stitching is produced between cut pieces L which is cut by an automatic chain thread clipper 77 located at the exit end of the sewing machine 14.  This clipper 77 is activated by the trailing edge of each cut length, in a manner to be described below. After the cut pieces L have been sewed and the stitch chains cut, the pillowcases P have been formed.  The travel by the second belt conveyor 75 which now has clamped the sewn and cut end El of the pillowcase P against a plate 79 carried by the frame portion F3. 5 The pillowcases P then move sequentially in front of an air doffing mechanism 80 that automatically doffs the pillowcases P over a bar 82 of the accumulating stacking means 15 by blowing the pillowcases P from the conveyor 75 and plates 79 and over the bar 82 (indi- 10 cated schematically in FIG. 2), in a manner to be described more fully below.  The air pressure on the air doffing mechanism 80 can be regulated to allow for different lengths of pillowcases P and is controlled in a manner to be described below. 15 The accumulating and stacking means 15 further includes numerous bar holders 82 carried by respective pairs of driven chains 84, 85.  The accumulating and stacking mechanism 15 provides for the stacking of approximately 60 pillowcases P over a bar holder 82 by 20 the air doffing mechanism 80 and is then advanced.  This moves the full bar 82 from the stacking section, composed of chain drives 85, to the accumulating section, composed of chain drives 84, and brings up an empty bar 82 on which the next stack of pillowcases P will be 25 doffed.  This procedure continues as long as the machine is running and the accumulating section will hold approximately I I stacked bundles. Referring now particularly to FIGS. 11 and 12A-12D ' an electrial sequence of operation will be 30 described.  The main power supply 101 to the machine is 220 VAC, 3 Phase, 60 hz and is fed through a 15 Amp circuit breaker 102.  Power is then fed to a magnetic starter 103 which is pulled in when the operator depresses an \"Enable\" push button switch 104.  This \"En- 35 able\" switch 104 puts power on the entire electrical system. The apparatus is threaded up with material M, as described above. The scray nip-rolls 26, 27 are driven by the motor 24, as described above, which is a i H.P., 40 220/115 VAC, 1 phase gear motor controlled through the pivot action of the scray mechanism and the proximity switch 33.  The sequence described above repeats itself as long as the apparatus is running. Opening and closing of the nip-rolls 45,  46  for  thread- 45 ing is controlled by a push button switch 106 (FIG. 12B) which, when depressed, applies power to solenoid valve 107 to control air to the piston and cylinder mechanism 47 for pivoting the nip-roll 46 into closed position with the nip-roll 45. When the push button switch 106 is 50 retracted, the solenoid valve 107 is deenergized causing the piston and cylinder mechanism 47 to open the niproll 46 with respect to the nip-roll 45 for threading-up. The nip-roll 45 is driven by the motor 50 (FIG. 12A), as described above, which is a I H.P., 220/115 VAC, 1 55 phase gear motor in tandem with a suitable electroid- clutch/brake 49.  The nip-rolls 45, 46 are driven by energizing the clutch coil of the clutch/brake 49 and stopped by energizing the brake coil of the clutch/brake 49, as will be described below. 60 After the apparatus has been threaded up with material M, the operator depresses a \"Start\" push button switch 110 (FIG. 12B) which energizes and latches in coil relays 112, 113, 114.  Coil relays 112 and 113 are latched in through the machine \"Stop\" push button 65 switch 115, coil relays 116 and 117.  With relays 112 and 113 energized, power is supplied to motor 120 and motor 121.  The motor 120 drives the first belt conveyor 68 and the mbtor 121 drives the second belt conveyor\n\n7                          4,388,879 75 (see also FIGS. 2 and 10).  These motors 120, 121 continue to run until either the operator depresses the \"Stop\" switch 115 or other events occur, as will be described below.  Relay 114 is latched in through a magnetically actuated, microswitch 123 which is located on the top hold clamp piston and cylinder 61- (schematically illustrated in FIG. 11).  At this point in the cycle, the top hold clamp 60 is retracted magnetically holding microswitch 123 closed.  This function served by microswitch 123 is used only in the initial startup of the cycle and once the apparatus is in automatic cycle, microswitch 123 serves no purpose, as described more fully below.  With relay 114 energized, power is applied to solenoid valve 125 (FIG. 12C) which causes the bottom hold clamp 65 to retract by operation of the piston and cylinder mecbanism 66. The apparatus is now ready to set up for fabricating either \"regular cut\" or \"panel cut\" pillowcases P. For this purpose, the desired length of the cut lengths L and ultimate pillowcases P must be set up.  In either case, the nip-rolls 45, 46 are used as a measuring device to determine the dimension of the length of the cut pieces L. This is accomplished through a suitable, commercially available, rotary pulse generator, schematically indicated at 130 (FIG. 12A).  The rotary pulse generator 130 produces 600 pulses per input revolution and is driven through a 2/1 ratio from the motor 50, also driving the feeding nip-rolls 45, 46, by a suitable belt and pulley drive 132 to produce 1,200 pulses per revolution of the feeding nip-rolls 45, 46.  This means that, as the feeding nip-rolls 45, 46 feed material M, 0.0107 inch of material M is fed per pulse produced by the pulse generator 130.  These electricai pulses are fed to a suitabie commercially available, dual adjustable preset output, pulse counter 134 which can be preset for fabricating either .,regular cut\" or \"panel cut\" pillowcases P. To preset for fabricating \"regular cut\" pillowcases P, a number is programmed into the number one output preset of the pulse counter 134 which is equivalent to the finished length of the particular size pillowcase P being fabricated@ This number represents the number of electrical pulses from the pulse generator 130 required to feed the desired length of material M past the cutter means 12.  This number is lower than the actual number representing the finished length of the pillowcase P. For example, to cut the material M into a piece L for a pillowcase P which is 36 inches long, the actual number of pulses representing 36 inches is 3,365.  A preset of- approximately 2,950 pulses is required for such a finished length of a pillowcase P of 36 inches.  This difference in the number of pulses is due to the reaction time for the relays involved and the brake of the clutch/brake mechanism 49 to actually stop the feeding niprolls 45, 46 from feeding material M. When the pulse counter 134 receives enough pulses to reach this preset number, the number one output of the pulse counter is energized which energizes coil relay 135.  The function of relay 135 to fabricate \"regular cut\" pillowcases P will be discussed below. A number is also programmed into the number two output preset of the pulse counter 134 that represents the maximum allowed length of material M that may be fed for cutting a piece L for a particular size pillowcase P.  This number represents the actual maximum length allowed.  No allowance for reaction time is needed.  If the pulse counter 134 receives enough pulses to reach this second preset number, the number two output of the pulse counter is energized which in turn energizes relay 116.  If this happens, the apparatus will stop since one of the contacts of relay 116 is in series with the \"Stop\" push button switch 115 (FIG. 12B) and acts to shut off power to the apparatus when energized.  A suitable indicator light may be provided to light up when the apparatus is stopped and the operator must then make corrections to the number one output preset of the electrical pulse counter 134 to correct this excessive length situation. 10 To set up for fabricating \"panel cut\" pillowcases P, a number is programmed into the number one output preset of the electrical pulse counter 134 that represents the portion of the printed panel design D on the material M whicb the scanning device 43 is not to read.  For 15 example, (see FIGS.  I and 2) the portion which the scanning device 43 is not to read would be that portion between printed panel designs D in the continuous material M. The operator must measure the portion not to be read by the scanning device 43 and convert the num- 20 ber of inches into equivalent number of eiectrical pulses produced by the pulse generator 130 for feeding that length of material M. This number of electrical pulses is then programmed into the number one output preset of the pulse counter 134.  As in the set-up for \"regular cut\" 25 pillowcases P, when the electrical pulse counter 134 has received enough pulses to reach this preset number, the number one output of the counter 134 is energized which in turn energizes coil relay 135, the function of which will be discussed below.  The procedure for set- 30 ting up the number two output preset of the electrical pulse counter 134 for fabricating \"panel cut\" pillowcases P is identical to that discussed above for fabricating \"regular cut\" pillowcases P. The results of the electrical pulse counter reaching output preset two in fabri- 35 cating \"panel cut\" pillowcases P are the same for fabricating \"regular cut\" pillowcases P, discussed above. After the desired length of cut pieces L has been set up, in the manner discussed above, the apparatus is ready for automatic cycle.  To start the automatic cycle, 40 the operator closes the \"Automatic Run\" push button switch 140 (FIG. 12B) whicb completes a series circuit through a magnetically actuated, microswitch 141 positioned to be closed by the bottom clamp 65 being retracted in the manner discussed above (FIG. 11).  This 45 series circuit starts the feed of the material M by activating coil timing relay 142 which controls coil relay 143 for fabricating \"regular cut\" pillowcases P and coil relay 144 for fabricating \"panel cut\" pillowcases P. If the apparatus is set up for  fabricating  \"regular  cut\" 50 pillowcases P, the output of a set of timed contacts in timing relay 142 is routed to relay 143 and coil relay 145 through push button switch 147 (FIG. 12B), which is closed for fabricating \"regular cut\" pillowcases P and energizes these relays. Relays 143 and 145 are then 55 latched in through relay 135, which is controlled by the number one output of the dual preset electrical pulse counter 134.  When relay 143 is energized, the clutch coil of the clutch/brake 49 is energized to start the feeding operation of the nip-rolls 45, 46.  When relay 145 60 is energized, the brake coil of the clutch/brake 49 is open. When the preset number of the number one output of the electrical pulse counter 134 is reached, relay 135 is energized which releases the latch on relays 143 and 145.  When relay 143 deenergizes, the clutch coil of 65 the clutch/brake 49 opens and when the relay 25 deenergizes, the brake coil of the clutch/brake 49 energizes stopping the feeding operation of the nip-rolls 45, 46. The desired length of a piece L has now been fed.\n\n9                          4,388,879 10 If the machine is set up for fabricating \"panel cut\" pillowcases P, the output of the same set of @timed contacts as used for fabricating \"regular cut\" pillowcases P is routed to relays 144 and 145 through push button 149 (FIG. 12B), which energizes these relays. Relays 144 and 145 are then latched in through coil relay 150 at this point of the cutting cycle.  Relay 150 is energized at the same time relay 144 is energized by a separate set of timed contacts in timing relay 142.  Relay 150 is then latched in through coil relay 151 located in the scanning device 43 power supply (FIG. 12B).  Coil relay 151 is controlled by the scanning device 43 reading a color or shade contrast on the material M. The color or shade scanning device 43 is a well understood,- commercially available mechanism.  With relay 144 energized, the clutch coil of clutch/brake 49 closes starting the drive to the feeding nip-rolls 45, 46. After the desired length of material M has been fed to reach the number one preset of the pulse counter 134, relay 135 is energized by the number one output of the electrical pul5e counter 134.  Relay 135 energizes a coil relay 153 for the same set time.  The contacts in relay 153 control a latch and inhibit module mounted inside the scanning device 43 power supply.  Until relay 153 is energizedi the scanning device 43 is inhibited from reading a color contrast and relay 151 is latched in an energized state.  When relay 153 is energized for the time duration and then released, the latch and inhibit function of the scanning device 43 is released and the scanning device will now be able to read a color or shade contrast.  The latch on relay 151 was released when the latch and inhibit module function was released and in turn released the latch on relay 150.  At this point, the latch on relays 144 and 145 are maintained through relay 151.  The clutch coil of clutch/brake 49 remains energized and the material M continues to be fed. When the point on the printed panel design D that is to be read reaches the scanning device 43, the scanning device reads this point and energizes relay 151.  The latch on relays 144 and 145 are released and the relays deenergized.  The clutch coil of clutch/brake 49 is now opened and the brake coil of the- clutch/brake 49 closes causing the feeding nip-rolls 45, 46 to stop feeding material M. When relay 151 was energized by the scanning device 43 reading the printed design D, the latch and inhibit module function of the scanning device 43 was again applied which latches in relay 151 and inhibits the scanning device 43 from reading again until the function is removed on the next cycle.  The desired length of material M for a cut piece L of a \"panel cut\" pillowcase P has now been fed by the feeding nip-rolls 45, 46 to the desired length. The material M is now ready to be cut and the cutting cycle consists of activating the top clamp 60 and the rotary cutter 52 as described above.  These functions are controlled in two different ways depending upon whether the machine is set up for fabricating \"regular cut\" or \"panel\" \"cut\" P. With the apparatus set for fabricating \"regular cut\" pillowcases P, coil relay 155, which controls the top clamp 60 is energized by the momentary signal from relay 135 which is activated when the number of electrical pulses preset into preset number one of the electrical pulse counter 134 is reached.  Relay 155 is then latched in through a magnetically actuated, microswitch 156 which is located on the retracted end of the piston and cylinder 71 controlling the push-through bar.70 (see FIG. 11).  With the push- through bar 70 in its retracted position, microswitch 156 is magnetically held closed thereby maintaining the latch on relay 155.  A set of contacts in relay 155 energizes solenoid valve 158 which controls the pneumatic piston and cylinder 61 for causing the top clamp 60 to go in and clamp the material M to plate 63.  Another set of contacts in relay 155 activates coil timing relay 160 from which a set of timed contacts energizes coil relay 161.  Relay 161 is -then latched in through coil relays 162, 163.  Relays 162, 163 10 are controlled by proximity switches 164,165 which are located on each end of the cutting stroke of cutter 52 (FIGS. 10 and 12C) and suitably mounted on frame portion F3'.  Prior to receiving the cut signal, the cutter 52 is located in front of one of the proximity switches 15 164, 165 thereby maintaining relay 162 or relay 163 energized. With either of these relays energized, the latch circuit on relay 161 is open. When relay 161 receives the timed signal from timing relay 160, the signal must be of sufficient time duration to allow the cutter 52 20 to move away from the proximity switch 164, or 165 which thereby completes the latch circuit.  With relay 161 energized, a run signal is applied to a conventional motor controller 166 which starts up the drive motor 54 to the cutting mechanism 12.  Relay 161 will remain 25 latched in until the movable cutter 52 reaches the other side of its cutting stroke. When this happens, the other proximity switch 164, 165 is closed thereby opening the latcb circuit on relay 161.  The run signal is then removed from motor controiler 166 to stop motor 54 and 30 movement of the movable cutter 52. At this point, relay 155 is still maintained energized holding the top clamp 60 in clamping position.  Relays 162, 163 are also used to place the cutter 52 in a forward-reverse movable state.  When the cutter 52 is in 35 front of proximity switch 165, coil relay 167 is energized through relay 163 which is controlled by proximity switch 165.  Relay 167 is latched in through relay 162 and places motor controller 166 in a forward drive state.  When the motor controller 166 is signalled to start the 40 cutting mechanism drive motor 54, it drives in a forward direction.  When the cutter 52 reaches the other side and closes proximity switch 164, relay 162 is energized which, as stated above, stops the cutter assembly drive and also opens the latch on relay 167.  This places 45 motor controller 166 in a reverse drive state.  On the next cutting cycle, the movable cutter 52 will return to the other side. With the apparatus set up for fabricating \"panel cut\" pillowcases P, relay 151 was energized when the scan- 50 ning device 43 read the beginning of panel design D and activated timing relay 160 which energized relay 161 in the same fashion and for the same operation as described above with respect to fabricating of \"regular cut\" pillowcases P. Relay 155 is energized by a set of 55 contacts in relay 161, instead of relay 135 as was the case in fabricating \"regular cut\" pillowcases P. Relay 135 is out of this circuit controlling the top clamp 60 by means of push button switch 147.  The latch on relay 155 is identical to that described above with respect to fabri- 60 cating of \"regular cut\" pillowcases P. The forwardreverse circuit motor controller 166 remains the same as described above with respect to- fabricating of \"regular cut\" pillowcases P. In fabricating of  either  \"regular  cut\"  or  \"panel  cut\" 65 pillowcases P, the movable cutter 52 itself is driven by a 24 VDC motor 168.  A 24 vdc power supply supplies power to bus bars 169 carried by the frame portion F3 along the length of the cutting stroke of the movable\n\n4,388,879                            12 cutter 52 (FIG. 10).  A brush assembly 166 travels these bus bars 169 and feeds the power to the motor 168.  This motor 168 runs continuously as long as the \"Enable\" push button switch 104 is depressed and all access doors of the frame assembly F3 are closed.  These access doors to the frame assembly providing access to the cutting means 12 have limit switches attached to them and the doors must be closed before the cutting means 12 will run.  This is a safety feature designed into the apparatus. After the material M has been cut sequentially into pieces L, as described above, these lengths L are ready to be carried to the stitching means 14 by the first belt conveyor 68. As described above, relay 114 was energized by closing of the \"Start\" push button switch 110 and latched in through microswitch 123 located at the retracted end of the pneumatic piston and cylinder 74 of the top clamp 73.  When the top clamp 73 moves into clamping position, solenoid valve 158 is energized by relay 155.  At this time, microswitch 123 is allowed to open thereby releasing the latch on relay 114.  With relay 114 deenergized, the bottom clamp 64 goes in and clamps the material M against stationary plate 67 just below the first belt conveyor 68.  The cut piece L is now clamped above and below the push-through bar 70.  When the bottom clamp 65 clamps the cut piece L to the stationary plate 67, magnetically actuated, microswitch 172, which is located on the extended end of the piston and cylinder 66 of the bottom clamp 65 (FIG. 11), closes placing power through a set of normally closed contacts in coil relay 170 to solenoid valve 171.  With solenoid valve 171 energized, pneumatic pressure is applied to the piston and cylinder 71 to extend the push-through bar 70. When the piston and cylinder 71 is extended, magnetically actuated microswitch 156 on the retracted end of the piston and cylinder 71 (FIG. 11) is opened thereby releasing the latch on relay 155.  With relay 155 deenergized, the top clamp 73 is retracted.  The top edge of the cut piece L is now free to be pushed through by the push-through bar 70. When the push-through bar piston and cylinder 71 is fully extended and has the cut end of the cut length L pushed through between the conveyor belt 68 and top plate 73, magneticaily actuated microswitch 173, which is located on the extended end of the push-through bar pistion and cylinder 71, closes energizing relay 170.  Relay 170 is latched in through microswitch 169.  The set of normally closed contacts in relay 170 that energized solenoid valve 171 are now open and the solenoid valve 171 is deenergized.  This retracts the piston and cylinder 71 and the push-through bar 70.  As the piston and cylinder 71 retract, magnetically actuated, microswitch 175, which is located at the midstroke of the piston and cylinder 71, is momentarily closed which energizes coil relay 176. Microswitch 175 was closed when the piston and cylinder 71 extended, but a set of normally open contacts between the switch and relay 176 prevented relay 176 from energizing on the extension stroke.  Relay 176 is then latched in through switch contacts of photoelectric cell device 177, which is mounted on the frame portion F3 and located above the point where the cut edge of the cut piece L is pushed through the belt conveyor 68 and the top plate 73.  When this cut edge was pushed through, the cut piece L broke the beam in the photoelectric cell device 177, thereby closing its contacts and providing a latch on relay 176.  With relay 176 energized and latched, solenoid valve 179 is energized (see FIG. 11) to extend piston and cylinder 74 for moving top plate 73 into clamping engagement with the first conveyor belt 68. Magnetically actuated microswitch 180, which is located on the extended end of the piston and cylinder 74 of the top plate 73, is closed by the clamping action of the top plate 73 and energizes solenoid valve 125 to operate piston and cylinder 66 for retracting the bottom hold clamp 65. It should be noted here that from this point on in the automatic cycle of the apparatus microswitch 180 alone 10 controls bottom clamp solenoid valve 125.  As discussed above, relay 114 was used through microswitch 123 to operate the bottom clamp 65 in the initial machine startup. After initial start-up, relay 114 and microswitch 123 will not be used again until the apparatus is stopped and 15 has to be restarted. With the top plate 73 in clamping position with the first belt conveyor 68, and the bottom clamp 65 retracted, the cut pieces L are then fed to the stitching means 14.  It should be further noted that, when the 20 bottom clamp 65 is retracted, magnetically actuated microswitch 182, located at the midstroke of the bottom clamp piston and cylinder 66 was closed momentarily to reset a commercially available counter 183 (FIG. 11), which will be discussed more fully below.  When the 25 bottom clamp 65 and piston and cylinder 66 are fully- retracted, microswitch 141 is closed to again start the next cutting cycle.  As the cut pieces L, clamped between belt conveyor 68 and plate 73 and hanging in a generally vertical position, are conveyed toward the 30 stitching means 14, the trailing edge of the cut length L moves away from photoelectric cell device 177.  When the beam of this photoelectric cell device 177 is completed, the latch of relay 176 is removed and solenoid valve 179 is deenergized allowing the top plate 73 to 35 come up and release its clamping engagement.  This movement of top plate 73 opens microswitch 180 energizing solenoid valve 125 which extends piston and cylinder 66 to extend the bottom clamp 65.  Prior to the time that the trailing edge of the cut piece L reached 40 photoelectric cell device 177, material M had been fed past the rotary cutter 52 to the desired length, clamped by the top clamp 73, cut by the cutter 52, and is waiting to be clamped by the bottom clamp 65 and pushed through the belt conveyor 68 and top plate 73 by the 45 push-through bar 70 on the next cycle. As the cut piece L are sequentialiy conveyed toward the stitching means 14, the leading edge breaks the beam of a photoelectric cell device 185 suitably mounted on frame portion F3, (FIGS. 5 and 6).  This photoelectric 50 cell device 185 controls an electronic circuit that in turn controls three functions. When the beam of this photoelectiic cell device 185 is broken, the electronic circuit energizes relays 187, 188, and 189 (see FIG. 12D).  Relay 187 controls an air blast that blows down the 55 leading edge of the cut piece L to insure that it goes under the presser foot of the sewing machine 14.  This relay 187 energizes for a set period of time and in turn energizes solenoid valve 191 that produces the air blast (shown schematically in FIG. 12D).  Relay 188 controls 60 a clutch/brake mechanism 193 (see FIG. 12D) connected with the sewing machine motor 194 (see FIG. 12A).  When relay 188 energizes, it energizes the clutch coil of clutch/brake mechanism 193 to start the sewing machine motor 194 running.  The sewing machine 14 65 will continue running as long as the beam of photoelectric cell device 185 is broken. Relay 189 controls counters, to be discussed below, that count the pillowcases P per stack and the total pillowcases P. When this relay\n\n13                         4,388,879 14 189 is energized, the contacts in the relay provide the switching necessary to activate coil timing relay 196, which controls the counter for pillowcases P per stack, to be discussed below, and coil timing reiay 197, which controls a counter for counting the total pillowcases P, to be discussed below. The cut pieces L continue to move past the sewing machine 14 and the cut end El is stitched.  As the leading edge of the cut piece L Ieaves the sewing machine 14, it breaks the beam of another phoioelectric cell device 199 suitably mounted on frame portion F3 (see FIGS. 5 and 6).  This photoelectric cell device 199 controls another electronic circuit that controls coil relays 201, 202.  Relay 201 c6ntrols the sewing machine motor clutch/brake 193 on the exit end of the sewing machine 14. The output of this relay is parallel with the output of relay 188 so that the sewing machine 14 will continue to run through its motor 194 until all of the cut pieces L pass the sewing machine 14.  When the trailing edge of the cut piece L passes this photoelectric cell device 199 and allows the beam to go back complete, the next cut piece L has already broken the beam of photoelectric cell device 185 on the entering end of the sewing machine 14, thereby maintaining the sewing machine 14 running.  If there is no cut piece L advancing when the beam of the photoelectric cell device 199 on the exit end of the sewing machine 14 goes compiete, then both beams are complete which activates the brake coil of the clutch/brake mechanism 193 to stop the sewing machine motor 194 and the sewing machine 14. Relay 202 controls chain thread clipper 77.  When the trailing edge of a cut length L that has been stitched passes the photoelectric cell device 199 on the exit end of the sewing machine 14, relay 202 is energized which energizes solenoid valve 204 for activating the chain thread clipper 77 through a cutting stroke.  This relay 202 is a timed relay through a suitable resistive-capacitive circuit designed into the electronic circuit.  When solenoid valve 204 is energized through relay 202, the clipper 77 clips the chain and then returns to its original position through the timed output of relay 202.  The cut pieces L have now been fabricated into pillowcases P which have been cut, stitched, chain clipped and counted and continue to move by the second belt conveyor 75 toward the doffing end of frame portion F3.  It should be noted here that if pillowcases P are being run that are to be stitched by sewing machine 14, \"Sew/No Sew\" push button switch 205 is in a depressed position.  This allows all functions described above to operate.  If pillowcases P are being fabricated that are not to be stitched, push button switch 205 is in its released position.  This allows only the count function to operate since there is no need of the air blast, sewing machine run or thread clipping functions. After the pillowcases P leave the sewing means 14, the belt conveyor 75 carries the pillowcases P to the doffing end of the apparatus.  When the leading edge of a pillowcase P breaks the beam of a photoelectric cell device 207, suitably mounted on the end of the apparatus frame portion F3, the photoelectric cell device 207 activates coil timing relay 208 (FIG. 12D) which energizes solenoid valve 210 and solenoid valve 211.  Solenoid valve 210 activates a pneumatic piston and cylinder mechanism 213 (FIGS. 1 and 12D) which pivots a mechanical linkage 214 for moving conveyor belt 75 away from clamping engagement with plates 79 for releasing the clamped cut and stitched end El of the pillowcase P for doffing.  Solenoid valve 211 activates a suitable valve for supplying air to the doffing mechanism 80, which is in the form of a pipe having apertures therein, for emitting a blast of air therefrom for blowing the pillowcases P over the bar holder 82 of the accumu- lating and stacking means 15.  Solenoid valves 210 and 211 are energized for the time duration set up on coil timing relay 208.  This action is repeated for each pillowcase P sequentially received at the doffing mechanism 80 and sensed by the photoelectric cell device 207. 10 After 60 pillowcases P, which is a desired number for one stack, has been placed on the bar holder 82 of the accumulating and stacking mechanism 15, the fuli bar 82 is advanced forward and an empty bar 82 is brought up from the bottom of the accumulating and stacking 15 mechanism 15.  The number of pillowcases P placed on a bar 82 is controlled by a counter mechanism 196 (FIG. 12A) which may be any suitable, commercially available, well understood counter mechanism.  This counter mechanism 196 has a manual preset that determines the 20 number of pillowcases P per bundle.  As the counter mechanism 196 receives a count signal from relay 189, as disclosed above, the counter mechanism 196 c6unts backwards from the manual preset number and when the counter mechanism 196 reaches 0, an internal relay 25 therein energizes which in turn energizes coil timing relay 215 that is electrically connected through the internal relay.  The internal relay is energized for a time duration programmed into the counter mecbanism 196.  The counter mechnism 196 is also wired up to give an 30 automatic reset back to the preset number when 0 is reached. When timing relay 215 is energized, a set of timed contacts energizes a magnetic motor 220 (FIGS. 2 and 12A) for advancing the chains 84, 85 on the accumulat- 35 ing and stacking section.  The holding contacts of motor 220 are held in through proximity switch 221 located on the accumulating and stacking apparatus.  With motor 220 energized, the chains 84, 85 will advance the full stack of pillowcases P on the bar holder 82 and bring up 40 a new empty bar hoider 82 into doffing position.  The motor 220 continues to run until the empty bar 82 reaches proximity switch 221 to indicate that the empty bar 82 is at the proper stopping point.  Proximity switch 221, which is normally closed, opens the holding 45 contact circuit in magnetic motor 220 to deenergize and stop the motor 220.  This action will repeat again when counter mechanism 196 signais another full stack of pillowcases P. The   accumulating   and   stacking   mechanism   15   will 50 hold a maximum of eleven full stacks of pillowcases P. Ten full stacks will be at the accumulating end of the mechanism 15 and the elevent h stack will be in the process of being stacked at the stacking end of the mechanism 15. Wben the tenth stack moves to the accu- 55 mulating end of the mechanism 15, it breaks the beam of a photoelectric cell device 223 that completes the count circuit of counter mechanism 235. At this point, counter mechanism 235 receives a count signal each time timer 160 energizes. When counter mechanism 235 receives 60 enough count signals to reach a preset number in the counter mechanism 235, relay 225 is energized and latched in by the output signal of counter mechanism 235.  The contacts in relay 225 open the circuit between timing relay 142 and microswitch 141.  This prevents 65 timing relay 142 from energizing again and therefore will not let the drive for the feeding nip-rolls 45, 46 feed further material M for fabricating pillowcase P. The contacts in relay 225 also open the circuit that energizes\n\n15                           4,388,879 16 and latches in the motor 220 for the accumulating and stacking mechanism 15 to prevent the accumulating mechanism 15 from operating when counter mechanism 196 indicates that the accumulating and stacking section is filled.  The apparatus will not start up again until the accumulating and stacking mechanism 15 is doffed of at least two stacks of pillowcases P at the end of the accumulating section thereof. This wiII allow the beam of photoelectric cell device 223 to be complete thereby not letting the counter mechanism 235 count.  Counter mechanism 235 must be reset since relay 225 is stili energized and latched in preventing the apparatus from running.  After counting mechanism 235 has been reset, the apparatus is started back up in the automatic cycle by depressing the \"Start\" push button switch 110. In the normal operating cycle, the operator can stop the apparatus by two methods.  One by depressing the apparatus \"Stop\" push button switch 115.  This stops all conveyor belt motor drives and the sewing machine drive.  The feeding nip-roll drive will finish feeding material M started but will stop after it has been fed to length. This method of stopping is used for such things as jam-ups.  Another method of stopping the apparatus is by pressing the \"Automatic Run\" push button smiitch 140.  This will allow all pillowcases P already in the normal cycle to finish the complete cycli@.  No other cycles for fabricating pillowcases P will begin and this method of stopping is for such things as machine adjustments, length corrections, color scanning device location changes, standard length changes and changing patterns. In the drawings and specification, there has been set forth a preferred embodiment of the invention and aIthough specific terms are employed, they are used in a generic and descriptive sense only and not for purposes of limitation. What is claimed is: 1.  Apparatus for sequentially fabricating pillowcases or like product comprising: means for supplying continuous flat tubular material; means for transversely cutting the continuous material into individual pieces; means for feeding the continuous material from said supply means to said cutting means; means operatively connected with and automatically controlling the operation of said cutting means and said feeding means for sequentially cutting the material into individual pieces of desired predetermined dimensions for the pillowcases; conveying means for sequentially receiving the individual cut pieces of material, clamping one of the transverse cut ends thereof while allowing the remainder of the cut pieces of material to hang downwardly in a generally vertically-extending position, and conveying the so-positioned cut pieces sequentially in a predetermined path of travel; and means for sequentially stitching the clamped transverse cut end of each piece of material to complete fabrication of the pillowcases while each piece of material is being conveyed by said conveyor means with the remainder of each piece of material in the generally vertically-extending position. 2.  Apparatus for sequentially fabricating pillowcases or like products comprising: means for supplying continuous flat tubular material; means for transversely cutting the continuous material into individual pieces; means for feeding the continuous material from said supply means to said cutting means; means operatively connected with and automatically controlling the ciperation of said cutting means and said feeding means for sequentially cutting the material into individual'pieces of desired predetermined dimensions for the pillowcases, said control means including means for regulating said cutting means and said feeding means for fabricating either 10 regular cut pillowcases in which each pillowcase is cut a predetermined desired length or panel cut pillowcases in which each pillowcase is cut a predetermined desired length from a printed panel design on the pillowcase; 15 conveying means for sequentially receiving the indi- vidual cut pieces of material and conveying the cut lengths sequentially in a predetermined path of travel; and means for sequentially  stitching  one  of  transverse  cut 20 ends of each of the cut pieces while being con- veyed by said conveyor means to complete fabrication of the pillowcases. 3. Apparatus for sequentially fabricating pillowcases 25 or like products comprising: means for supplying continuous flat tubular material; means for transversely cutting the continuous material into individual pieces; means for feeding the continuous material from said 30 supply means to said cutting means; means operatively connected with and automatically controlling the operation of said cutting means and said feeding means for sequentially cutting the material into individual pieces of desired predeter- 35 mined dimensions for the pillowcases, said control means including means for regulating said cutting means and said feeding means for fabricating either regular cut pillowcases in which each pillowcase is cut a predetermined desired length or panel cut 40 pillowcases in which each piliowcase is cut a predetermined desired length from a printed panel design on the pillowcase; conveying means for sequentially receiving the individual cut pieces of material, clamping one of the 45 transverse cut ends thereof while allowing the remainder of the cut pieces of material to hang downwardly in a generally vertically-extending position, and conveying the so-positioned cut pieces sequentially in a predetermined path of 50 travel; and means for sequentially stitching the clamped transverse cut end of each piece of material to complete fabrication of the pillowcases while each piece of material is being conveyed by said conveyor means 55 with the remainder of each piece of material in the generally vertically- extending position. 4. Apparatus, as set forth in claim 1, 2 or 3 further including: means for doffing the fabricated pillowcases from 60 said conveyor means; and means cooperating with said doffing means for re@teiving the doffed pillowcases, for stacking the pillowcases into individual stacks of a predetermined number of pillowcases, and for accumulat- 65 ing a predetermined number of stacks for removal by an operator. 5. Apparatus, as set forth in claims 1, 2 or 3, in which said material supply means comprises\n\n17                         4@3881879 18 a pair of friction drive rolls mounted  for  rotation  in side-by-side position  for  receiving  a  roll  of  the continuous  flat  tubular  material  on  top   surfaces thereof and for frictionally rotating the roll  of  ma- terial for unwinding the material, 5 nip-rolls rotatably  mounted  for  feeding  the  continu- ous material being unwound, drive means connected  with  said  friction  drive  rolls and said feeding nip-rolls for rotatably  driving  said rolls, 10 scray  means  for  receiving  the   continuous   material from  said  feeding  nip-rolls  and   having   counter- balancing   means   -@novably   mounting   said   scray means  for   downward   movement   under   the   weight of the material received therein, and 15 control  means  operatively  connected  with  said  drive means  and  scray  means  for  stopping  operation   of said drive means, and thus said  friction  drive  rolls and said  feeding  nip-rolls,  when  said  scray  means contains   a   predetermined   amount   of    m'aterial 20 therein  and  for  starting  operation  of  said  drive means when  said  scray  means  does  not  contain  the pred I etermined amount of material therein. 6. Apparatus, as set forth in claims 1, 2 or 3, in which 25 said  material   feeding   means   compnses   selectively driven nip-rolls  rotatably  mounted  for  feeding  the material  therefrom  in  a  generally  downward  verti- cally-extending position . past said cutting means, said cutting means  comprises  a  driven  rotary  cutter, 30 and a driven means  carrying  said  cutter  for  selec- tive  horizontal  back  and  forth   movement.   trans- versely  across  the  material  fed  by  said  material feeding means, and said control  means  including  preset  adjustable  means 35 for determining the amount  of  material  fed  by  said nip-rolls past said cutting  means  and  for  actuating said control  means  for  @sequentially  stopping  said nip-rolls to stop the feed of maten@al after a  desired length of material  has  been  fed  past  said  cutting 40 means,  actuating  said  cutting  means  to  effect   a cutting cycle thereof, and  again  starting  said  nip- rolls for feeding of material. 7. Apparatus, as set forth in claim 6, in which said  preset  adjustable  means  in  said  control  means 45 comprises  electrical  pulse  generating  means  opera- tively  connected  with  said  feeding  nip-rolls   for producing  a   predetermined   number   of   electrical pulses for each revolution of said  nip-rolls,  and  an electrical pulse  counter  means  connected  with  said 50 pulse  generating  means  and  having   an   adjustable preset  output  means  operatively  connected  in  said control means  for  actuation  thereof  after  counting an adjustable preset number of electrical pulses. 8. Apparatus, a@ set forth in claim 6, in whicb 55 said conveying  means  includes  a  driven  endless  belt conveyor  means  mounted  for  rotation  to  define  an upper generally  horizontally-extending  surface  of  a width  much  narrower  than  the  length  of  the   cut pieces of material and  positioned  below  said  rotary 60 cutter,  and  an  elongate,   generally   horizontally- extending,  top  hold  plate  means  positioned   above said  upper  surface  of  said   conveyor   means   and mounted  for  selective  sequentiai  pivotal   movement into and out of  engagement  with  said  belt  conveyor 65 means for receiving and  clamping  one  of  the  trans- verse cut ends of  each  cut  piece  after  cutting  by said cutter, and said apparatus further including clamping and positior)ing means cooperating with said cutting means and said conveying means and being controlled by said control means for ciamping the material during operation of said cutting means and for positioning the cut end of the cut piece between said belt conveyor means and said top hold plate after operation of said cutting means. 9. Apparatus, as set forth in claims 2 or 3, in which said control means includes preset adjustable means for determining the amount of material fed by said feeding means past said cutting means and for actuating said control means for sequentially stopping said feeding means to stop the feed of material after a desired length of material has been fed past said cutting means, actuating said cutting means to effect a cutting cycle thereof, and again starting said feeding means for feeding of material, said preset adjustable means comprising electrical pulse generating means operatively connected with said feeding means for producing an electrical pulse for each predetermined increment of length of material fed by said feeding means, an electrical pulse counter means connected with said pulse generating means and having an adjustable preset output means operatively connected in said control means for actuation thereof after counting an adjustable preset number of electrical pulses corresponding to the desired length of material to be fed by said feeding means, and an optical color or shade photoelectric cell scanning device incorporated in said control means and selectively operable when said apparatus is fabricating the panel cut pillowcases only and connected with said pulse counter for being actuated thereby after said pulse counter has counted a preset number of puises corresponding to the length of material between printed panel designs and for reading the location of such printed panel design and actuating said control means for fabricating panel cut pillowcases. 10. Apparatus, as set forth in claims 1 or 3, in which said material feeding means comprises selectively driven nip-rolls rotatably mounted for feeding the material therefrom in a generally downward vertically-extending position past said cutting means, said cutting means comprising a driven rotary cutter, and a driven endless chain and sprocket means carrying said cutter for selective horizontal back and forth movement transversely across the material fed by said material feeding means, said conveying means including a driven endless belt conveyor means mounted for rotation to define an upper generally- horizontally-extending surface of a width much narrower than the width of the cut pieces of material and positioned below said rotary cutter, and an elongate, generally horizontallyextending, top hold plate means positioned above said upper surface of said conveyor means and mounted for selective sequential pivotal movement into and out of engagement with said conveyor means for receiving and clamping one of the transverse cut ends of each cut pieces after cutting by said cutter, and said apparatus further including clamping and positioning means cooperating with said cutting means and said conveying means and being controlled by said control means for clamping the material dur-\n\n19                         4,388,879 20 ing operation of said cutting means and for positioning the cut end of the cut piece between said belt conveyor means and said top hold plate after operation of said cutting means. 11.  Apparatus for sequentially fabricating pillowcases or like products comprising: means for supplying continuous flat tubular material; means for transversely cutting the continuous material into individual pieces comprising a driven rotary cutter, and a driven means carrying said cutter for selective horizontal back and forth movement transversely across the material; means for feeding the- continuous material from s . aid supply means past said cutting means comprising selectively driven nip-rolls rotatably mounted for feeding the material therefrom in a generally downward vertically- extending position past said cutter; conveying means including a driven endless belt conveyor means mounted for rotation to define an upper generally horizontally-extending surface of a width much narrower than the length of the cut pieces of material and positioned below said rotary cutter, and an eiongate, generally horizontallyextending, top hold plate means positioned above said upper surface of said conveyor means and mounted for selective pivotal movement into and out of engagement with said belt conveyor means for receiving and clamping one of the transverse cut ends of each cut piece after cutting by said cutter while allowing the remainder of the cut pieces of material to hang downwardly in a generally vertically-extending position, and conveyin the so-positioned cut pieces sequentially in a predetermined path of travel from said cutting means; clamping and positioning means cooperating with said cutting means and said conveying means for clamping the material during operation of said cutting means and for- positioning the cut end of the cut piece between said belt conveyor means and said top hold plate; means for sequentially stitching the clamped transverse cut end of each of the cut pieces to complete fabrication of the pillowcases while each cut piece is being conveyed by said conveyor means with the remainder of each cut piece in the generally vertically-extending position; and control means operatively connected with and automatically controlling the operation of said cutting means, said feeding means, said conveying means, said clamping and positioning means and said stitching means for sequential operation thereof including means for regulating said cutting means and said feeding means for fabricating either regu- 10 lar cut pillowcases in which each pillowcase is cut a predetermined desired length or panel cut pillowcases in which each pillowcase is cut a predetermined desired length from printed panel designs on the material, said control means including preset 15 adjustable means for determining the amount of material fed by said nip-rolls past said cutter and for actuating said control means for sequentially stopping said nip-rolls to stop the feed of material after a desired length of material has been fed past 20 said cutting means, actuating said cutting means to effect a cutting cycle thereof, actuating said clamping and positioning means, actuating said conveying means and again starting said feeding means for anotber cycle; and 25 said preset adjustable means comprising electrical pulse generating means operatively connected with said feeding nip-rolls for producing a predetermined number of electrical pulses for each revolution of said nip-rolls, an electrical pulse counter 30 means connected with said pulse generating means and having an adjustable preset output means operatively connected in said control means for actuation thereof after counting an adjustable preset number of electrical pulses corresponding to the 35 desired length of material to be fed by said feeding means, and optical color or shade photoelectric cell scanning device means incorporated in said control means and selectively operable when said apparatus is fabricating the panel cut pillowcases 40 only and connected with said pulse counter for being actuated thereby after said pulse counter has counted a preset number of pulses corresponding to the length of material between printed panel designs and for reading the location of sucb printed 45 panel design and actuating said control means for fabricating panel cut pillowcases. 50 55 60 65"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 25,
      "claims_start": 23,
      "description_end": 23,
      "description_start": 16,
      "drawings_end": 15,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 25,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 23,
      "specification_start": 16,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 23002839,
    "guid": "US-4388879-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/04/388/879",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "D05B",
        "intl_subclass": "33/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "D05B",
        "intl_subclass": "41/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "D05B33/00"
    ],
    "inventors": [
      {
        "name": "EVERALL JR PALMER B"
      },
      {
        "name": "LOWERY SR JACK R"
      }
    ],
    "inventors_short": "EVERALL JR PALMER B et al.",
    "patent_title": "Apparatus for sequentially fabricating pillowcases or like products",
    "publication_date": "1983-06-21",
    "publication_number": "4388879",
    "type": "USOCR",
    "us_class_current": [
      "112/470.36",
      "112/304",
      "112/307"
    ]
  },
  {
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "composite_id": "PP003823!MXOC-US-PP003823",
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "claim_statement": "I claim:",
      "claims": "1. A new and distinct variety of plum tree, substantially as illustrated and described, which is vigorous, spread-ing, dense, round-topped, hardy, freely-suckering, foliated with oblanceolate leaves having an acuminate tip, blooming early with white, uniform flowers borne in small clusters on evenly dispersed spurs, and a productive bearer of large, substantially globose, clingstone fruit having purple skin with a silver-grey bloom, and deep red flesh; the tree being characterized, in addition to its profuse suckering and ease of asexual rcproduction from rooted suckers and hardwood cuttings, by bearing fruit-of excellent quality-having a general exterior appearance similar to the Santa Rosa, but ripening a few days later and having distinctive deep red flesh. No references cited. ROBERT E. BAGWILL, Primary Examiner",
      "description": "(1) 3,823 PLUM@ TRIEE Z arnline P'ayce Hlarris, B akersfield, Calif., assignor to Plumwood Associates, Bakcersfield, Califf.\n\n(2) Filed )71cc. 9, 1974, Ser. NoT. 531l,091f Irt. CI. AOfIh 5/03 1l Claimn\n\nABSTRACT OF THIE DISCLOSURE\n\n(3) A plum tree which is vigorous, spreading, dense, round- opped, hardy, freely suckering, foliated with oblanceolate eaves having an acuminate tip, blooming early with white, uniform flowers borne in small custers on evenly dispersed spurs, and a productive bearer of large, sub- tantially globose, clingstone fruit having purple skin with tsilver-grey bloom, and deep red flesh.\n\nORIGIN OF THE VARIETY\n\n(4) The present variety of plum tree originated, in the yard )f my residence in Bakersfield, Kern County, Calif., ap- arently as a fortuitous seedling. Curious as to the kind )f tree the seedling would produce, I permitted it to ;row-and continued to carefully observe it-after dis- ;overing its presence in my yard. In maturity, the tree- hen recognized as a plum tree-bore fruit having char- Lcteristics which appeared novel, and I then undertook isexual reproduction of the tree in order to ascertain 'ts potential for commercial growing.\n\nASEXUAL REPRODUCTION OF THE VARIETY\n\n(5) As the original tree was freely suckering, asexual re- ,roduction was first accomplished by me, in the yard of ny residence located as aforesaid, by severing and re- lanting rooted suckers or shoots which emerged from, the \"oot system of such original tree. Subsequently, the va- \"iety was further asexually reproduced, on my behalf, by commercial nursery, by bud scions propagated on peach >eedings in such nursery. In both instances, the repro- iuctions ran true to the original tree in all respects, and miere healthy, vigorous, and productive.\n\nSUMMARY OF THE VARIETY\n\n(6) The present variety of plum tree is vigorous, spreading, sense, round-topped, hardy, freely suckering, foliated Nith oblanceolate leaves having an acuminate tip, bloom- 'ug early with white, uniform flowers borne in small ;lusters on evenly dispersed spurs, and a productive nearer of large, substantially globose, clingstone fruit zaving purple skin with a silver-grey bloom, and deep *ed flesh; the tree being characterized, in addition to its profuse suckering and ease of asexual reproduction from rooted suckers and hardwood cuttings, by bearing fruit- )f excellent quality-having a general exterior appear- rnce similar to the Santa Rosa (unpatented), but ripen- '.ng a few days later and having distinctive deep red flesh.\n\n(7) By further comparison, the fruit and the flesh, of the present variety, bear general resemblance to the Mariposa (Plant Patent No. 1ll), while the skin, bloom, and pit ire more similar to the Elephant Heart (unpatented).\n\n(8) BRIEF DE.SCRIPr1ION OI1' 1'HF DRAWING The drawing is an illustration, by photogr aphic repro- 3uction in color, of a twig with leaves and fruit, and separate fruit, one of the latter being partially cut away to expose the flesh.\n\n(9) DESCRIPIlION OF'1 HF, VARIF i Y T1he botanical details of this new distinct variety of plum tree with color definitions (except those in com- :J.S. CI. Plt.-3S mon color terms) referenced to Maerz and Paul Dict ion- ary of Color-are as follows:\n\nTREE:\n\n(10) Vigor.-Moderate to vigorous.\n\n(11) Growth.-Spreading.\n\n(12) Density.-Dense.\n\n(13) Forrn-Round-topped. Tree readily adapts to open- vase tree-training system.\n\n(14) Hardiness .-Hardy.\n\n(15) Production.-Productive.\n\nTRUNK:\n\n(16) Size.-Medium to stocky.\n\n(17) Texture.-Shaggy. External bark finely and evenly 15 checked.\n\n(18) Color.--Slate to dull brown.\n\nBRANCHES:\n\n(19) Size.-Medium to stocky.\n\n(20) Color.-Slate to dull brown.\n\n(21) Texture.-B ark evenly checked and noticeably thicker than Santa Rosa.\n\n(22) Len ticels: Number-Mvedium. Size-Uniform.\n\nLEAVES:\n\n(23) Size.-Medium to large. Average length-4\". Average width-17/s\".\n\n(24) Form .-Oblanceoiate. Acuminate tip.\n\n(25) Texture-Medium.\n\n(26) Color.-Top side-Medium to deep green (23-L- 10). Gloss: Medium to dull; under side-lighter green (22-K-7).\n\n(27) V'eins.-Distinct. Light green.\n\n(28) Margin.-Finely and unevenly crenate.\n\n(29) Glands.-Only on the petiole end at leaf.\n\n(30) Leaf Glands.-small. Nonuniform. Attached to basal end of leaf.\n\n(31) Stipules.-N one.\n\nFLOWER BUDS:\n\n(32) Size.-At full bud stage-I/\" to 5/is' in diameter.\n\nFLOWERS:\n\n(33) Blooming Period.-Early, as compared with other varieties.\n\n(34) Stem.-Medium to thin. Average length-s/16\".\n\n(35) 45~ Petals. - Evenly grouped. 5 count. Flat. Average length-3/s \". Average width-5/46\". White. Obtuse.\n\n(36) Pistil and Anthers.-Equal in length.\n\n(37) Form .-Unif orm when fully open.\n\n(38) Aroma.-Distinct. Sweet. Attractive to bees.\n\n(39) Flower Dispersion.-Uniform.\n\n(40) Flower Clusters.-Small. Usually 2 or 3 on evenly dispersed spurs.\n\nFRUIT:\n\n(41) Maturity W~hen Described. - Fully matured, firm ripe.\n\n(42) Date of First Picking.-June 20th.\n\n(43) Date of Last Picking.-July 4th.\n\n(44) 00 Size.-Large. Average diameter axially-21/ \". Average transversely in suture plane- 2 /a\".\n\n(45) Form.- Nearly perfectly round; i.e., substantially globose.\n\n(46) Suture. -Smooth. Well filled fromi base to apex.\n\n(47) fi Ventral Surface.- Smooth.\n\n(48) Cavity.=-Small. Distinctly deep.\n\n(49) Base.- Broad. Uniform. Fjirm.\n\n(50) Apex.- -Round. Without point or depression.\n\n(51) Apex Marking. A distinct widening at the suture surface is present on the very apex of all fruit; such surface being dull yellow and smooth. No fruit cracking evident at apex of fruit.\n\n(52) Thiclness.-Medium to thin.\n\n(53) Texture .-Smooth.\n\n(54) Tenacity.-Tenacious to flesh.\n\n(55) Color.-Purple red (55-L-12) shading darker to deep purple (56-E-11) .\n\n(56) Bloom.-Moderate to light. Easily removed. Silver- grey (38-E-2).\n\nFLESH:\n\n(57) Color.-Red (7-L-4). Surface of Pit Cavity: Deep- maroon.\n\n(58) Amygdalirz.-Moderate.\n\n(59) Juice.-Moderate. Mellow. Sweet.\n\n(60) Texture.-Medium firm.\n\n(61) Fibers.-Few. Fine. Tender.\n\n(62) Ripens.-Evenly.\n\n(63) Flavor.-Mellow. Swcet. Subacid. Distinct.\n\n(64) A roma.-Pronounced.\n\n(65) Eating Quality.-Excellent.\n\nSTONE:\n\n(66) Type.-Cling. Flesh adheres to all surfaces of the stone.\n\n(67) Fibers.-Very short.\n\n(68) Size. - Small. Average length - 1/ . Average width-li/ls\". Average thickness-3s \".\n\n(69) Form.-Oblong. Similar to Elephant Heart and Satsuma (unpatented).\n\n(70) Base-Evenly tapered to hilum.\n\n(71) Hilum.-\"small. Narrow. Oblong.\n\n(72) Apex.-Acuminate to rounded. Very small point.\n\n(73) Sides.-Uniform.\n\n(74) Suzrface.-Roughly pitted on both surfaces.\n\n(75) Ridges.-Pronounced. Rough. Sharp.\n\n(76) Ventral Edge.-Thin. Rough.\n\n(77) Pits.-Well developed.\n\n(78) Color.-Beige (12-I-7) .\n\n(79) USE: Dessert-Market-local and long distance ship- Sping.\n\n(80) KEEPIN'G QUALITY: Good.\n\n(81) SHIPPING QUALITY: Good.\n\n(82) RESISTANCE TO DISEASES AND INSECTS: Equal to othzr commercial varieties.\n\n(83) ]o( The tree and its fruit herein described may vary in slight detail due to climatic and soil conditions undcr which the variety may be grown; the present description being of the variety as grown in the Central Valley of 1, California."
    },
    "document_structure": {
      "abstract_end": 2,
      "abstract_start": 2,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 2,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 3,
      "claims_start": 3,
      "description_end": 3,
      "description_start": 2,
      "drawings_end": 1,
      "drawings_start": 1,
      "front_page_end": 2,
      "front_page_start": 1,
      "page_count": 3,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 3,
      "specification_start": 2,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "guid": "US-PP03823-P",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/PP/003/823",
    "patent_title": "OCR SCANNED DOCUMENT",
    "publication_date": "1975-12-23",
    "publication_number": "PP03823",
    "type": "USOCR",
    "us_references": [
      {
        "cited_by_examiner": false,
        "patentee_name": "N/A",
        "pub_month": "1975-12-01",
        "publication_number": "PP003823"
      }
    ]
  },
  {
    "app_filing_date": "1973-07-23",
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "reissue",
    "composite_id": "26798577!MXOC-US-RE028436",
    "cpc_inventive": [
      {
        "cpc_class": "E21B",
        "cpc_subclass": "12/02",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "E21B",
        "cpc_subclass": "44/00",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "E21B",
        "cpc_subclass": "47/00",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "claim_statement": "We claim:",
      "claims": "1. The method of determining bit damage in rotary drilling comprising rotating a drill string having a bit attached thereto in a hole and monitoring the rotary torque for characteristic oscillations, said characteristic oscillations having a frequency not more than twice and not less than half the expected frequency of torsional oscillation of the drill string, and said characteristic oscillations having an amplitude substantially in excess of the oscillations produced while rotating an undamaged bit.\n\n2. The method ofclaim 1further characterized by activating an alarm signal in response to the occurrence of said characteristic oscillations.\n\n3. The method of determining bit damage in rotary drilling comprising driving an electric motor to rotate a drill string having a bit attached thereto in a hole, producing a signal proportional to the current to the electric motor as indicative of rotary torque, passing said signal through a filter to suppress undesired frequencies more than twice and less than half the expected frequency of torsional oscillation of the drill string to produce an information signal, recording the said information signal passed through said filter and monitoring said information signal for oscillations characteristic of bit damage.\n\n4. The method ofclaim 3further characterized in that the expected period of torsional oscillation of the 00 drill string is between 3 and 12 seconds.\n\n5. The method ofclaim 3further characterized in that the information signal is recorded on a strip chart.\n\n6. The method ofclaim 5further characterized in that the information signal is recorded and monitored only at predetermined sample time intervals.\n\n7. Apparatus for use in well drilling comprising a drill string, drive means for rotating said drill string, torque reasuring means connected to said drive means for producing a signal proportional to the torque produced by said drive means, filter means connected to said torque measuring mieans, said filter rmeans selected to suppress relatively high frequency signals having a period near to the rotary table period and to suppress relatively low frequency signals of less than one twenty-fourth cycle per second and recording means connected to said torque measuring means through said filter means for recording the signal passed by said filter means.\n\n8. Apparatus for use in well drilling comprising a drill string, electric drive means for rotating said drill string, shunt means connected to said electric drive means for producing s signal proportional to the current utilized by said electric drive means, band pass filter means connected to said shunt means, said band pass filter selected to suppress relatively high frequency signals having a period near to the rotary table period and to suppress relatively low frequency signals of less than one twenty-fourth cycle per second, and recording means connected to said shunt means through said band pass filter for recording the signal passed by said band pass filter.\n\n9. The apparatus ofclaim 8further characterized in that said recording means includes strip chart means and means for periodically actuating said strip chart means for a limited sample period. 10. The apparatus ofclaim 8further characterized in alarm means responsive to actuation by oscillations having a period near the expected period of the drill string and having an amplitude substantially in excess of the amplitude of oscillations produced while rotating an undamaged bit.\n\n11. Apparatus f or use in well drilling with a drill string having a rotary table and drive means for rotating said drill string comprising torque measuring means for producing a signal proportional to the torque produced by drive means rotating a drill string, means for operatively connecting said rotary torque measuring means to a drive means for rotating a drill string, filter means connected to said torque measuring means, said filter means selected to suppress relatively high frequency signals having a period near to the rotary table period and to suppress relatively low frequency signals of less than one twenty-fourth cycle per second and recording means connected to said torque measuring means through said filter means for recording the signal passed by said filter means.\n\n12. Apparatus for use in well drilling with a drill string having a rotary table and electric drive means for rotating said drill string comprising shunt means for producing a signal proportional to the current utilized by an electric drive means rotating a drill string through a rotary table, means for operably connecting said shunt means to an electric drive means for rotating a drill string, band pass filter means connected to said shunt means, said band pass filter selected to sup press relatively high frequency signals having a period near to the rotary table period and to suppress relatively low frequency signals of less than one twenty-fourth cycle per second, and recording means connected to said shunt means through said band pass fiter for recording the signal passed by said band pass filter.\n\n13. The apparatus ofclaim 12further characterized in that said recording means includes strip chart means and means for periodically actuating said strip chart means for a limited sample period.\n\n14. The apparatus ofclaim 12further characterized in including alarm means connected to said band pass filter means and responsive to actuation by oscillations having a period near the expected period of the drill string and having an amplitude substantially in excess of the amplitude of oscillations produced while rotating an undamaged bit. I5. Apparatus for determining bit damage in rotary drilling comprising means for rotating a drill string having a bit attached thereto in a hole and means for monitoring the rotary torque for characteristic oscillations, said monitoring means being sensitive only to characteristic oscillations having a frequency not more than twice and not less than half the expected frequency of torsional oscillations of the drill string, and said characteristic oscillations having an amplitude substantially in excess of the oscillations produced while rotating an undamaged bit.\n\n16. The apparatus of claim 15 further characterized by an alarm signal means connected to means for monitoring the rotary torque oscillations and operable in re-sponase to the occurrence of said characteristic oscillations.\n\n17. Apparatus for use in rotary drilling comprising a drill string having a bit attached thereto, an electric motor connected to said drill string, means for driving said electric motor to rotate said drill string having a bit attached thereto in a hole, means for producing a signal pro portional to the current of the electric motor as indicative of rotary torque, filter means receiving said signal to suppress undesired frequencies more than twice and less than half the expected frequency of torsional oscillation of the drill string to produce an information signal and recording means for recording the information signal passed through said ,filter means.\n\n18. The apparatus ofclaim 17further characterized in that strip chart means records said signal. References Cited The following references, cited by the Examiner, are of record in the patented ifile of this patent or the original patent. UNITED STATES PATENTS 3,052,117 9/1962 Miller et al.--------_73-136 R 3,417,611 12/1968 Dean et al.--------_73-136 R 3,581,564 6/ 1971 Young, Jr.-----------_73-151 FOREIGN PATENTS 617,065 2/1949 Great Britain--------_73-70.1 JERRY W. MYRACLE, Primary Examiner U.S. Cl. X.R. 175-39",
      "description": "(1) 28,436\n\nMETHOD OF DETERMINING DOWNUHOLE\n\nOCCURRENCES IN WELL DRILLING USING\n\nROTARY TORQUE OSCILLATION MEASURE-\n\nMENTS\n\n(2) Albert L. Vitter, Jr., New Orleans, and Hugh C. Mc- Donald, Monroe, La., assignors to Chevron Research Company, San Francisco, Calif.\n\n(3) Original No. 3,703,096, dated Nov. 21, 1972, Ser. No.\n\n(4) 101,739, Dec. 28, 1970. Application for reissue July 23, 1973, Ser. No. 381,956 Int. CI. E2lb 47/00 U.S. Cl. 73-151 18 Claims Matter enclosed in heavy brackets [] appears in the original patent but forms no part of this reissue specification; matter printed in italics indicates the additions made by reissue.\n\nABSTRACT OF THE DISCLOSURE\n\n(5) Method and apparatus are used to detect and measure oscillation of the rotary torque generated while well drill- ing, particularly oscillations of a frequency near the expected frequency of torsional oscillation of the drill string.\n\n(6) The method and apparatus are particularly \"useful in indicating bit damage.\n\nFIELD OF THE INVENTION\n\n(7) The present invention is directed to rotary well drilling and, more specifically, the present invention is directed to obtaining, at the surface, indications of drill bit damage utilizing rotary torque oscillation measurements.\n\nBACKGROUND OF THE INVENTION\n\n(8) Wells are drilled for oil utilizing a drill bit connected to the surface by a string of drill pipe. The drill pipe is rotated to cause the bit to rotate and cuttings are circulated up the well annulus by means of a circulating fluid. As the drill string is rotated, the drill bit tends to wear down and eventually fail. In deep hole drilling (e.g. 10,000 feet) balance must be made between operating the drill bit until a failure occurs which necessitates a fishing job, and pulling the bit prematurely from the well, i.e., while there is still useful life in the bit, because in deep hole drilling, half a day, or more, may be required to pull the bit from the hole, place a new bit on the drill string, and run it back into the hole. It is, therefore, desirable to get maximum use out of each drill bit. Heretofore no satisfactory method has been found to determine when the drill bit is damaged, and should be pulled.\n\n(9) One form of rotary drilling utilizes roller cones which are connected to the bit body and which are bearing mounted so as to roll on the drilled rock face when the drill bit is rotated. Usually a roller cone bit has three or four roller cones. As noted above, it is desirable to operate these bits to a point just short of complete failure, i.e., it is generally desirable to operate the bit just short of the point where the roller cones become detached from the bit body. In operation, the bearings of the roller cones tend to freeze after a period of use.\n\n(10) Even then, the bits with the frozen roller cones may still be operated for a short additional period. However, if the frozen bits are operated for too long a time, the bushings are likely to be destroyed and the cones will fall free of the bit body when the drill pipe is pulled. If this occurs, a \"fishing job\" must be performed to remove the roller cones from the hole before further drilling may proceed. 't he fishing job requires a complete roundtrip with a drill string having special fishing apparatus connected to its lower end. 'this opeation is time-consuming and expensive and is to be avoided, if all possible.\n\n(11) Several types of methods have been tried in the past to determine when it is desirable to pull a bit in order to place a new bit on the drill string.- One of the most promising methods involved embedding radioactive tracers in the grease in the roller cones of a bit. The tracers are released into the drilling fluid stream when the roller cones fail. The tracers are then detected \"by monitoring apparatus at the surface. This method, however, is somewhat inadequate in that the travel time of the mud up the annulus may be as much as an hour and, therefore, much additional damage can be done to the bit before anything is detected at the surface. The nearest approach to the method of the present invention has perhaps been the observation of drill pipe torque changes at the sur- face, but these were changes in the average value of the torque as observed on a meter in front of the driller or on a drilling recorder. (See e.g., the advertisement of Technical Oil Tool Corporation, p. 4,571 of the Composite Catalog of Oil Field Equipment and Services. 1970-71 Edition. Published by World Oil, Houston, Tex. Also see p. 3,237 of same publication for an advertisement ad of Martin-Decker Corp. on the same subject.) Changes in the average torque can result from many causes, the commonest being a change in the formation being drilled, and so they are not a reliable indication of bit damage.\n\n(12) There is still need, therefore, for a reliable method of detecting bit damage.\n\nBRIEF DESCRIPTION OF THE INVENTION\n\n(13) The present invention provides a method of detecting bit damage during rotary drilling by utilizing, as an indication of such damage, very particular oscillatory changes in rotary torque produced while drilling. In accordance with the invention, the rotary torque produced during rotary drilling is monitored for characteristic oscillations of a frequency in the neighborhood of the expected frequency of torsional oscillation of the drill string. These oscillations usually have a frequency significantly different from the fundamental frequency of rotation of the rotary drill string and the harmonics of that frequency, so that they may be unambiguously detected. The expected period of torsional oscillation of the drill string in most drilling operations of interest is of the order of 3 to\" 12 seconds.\n\n(14) In accordance with the present invention the method of determining bit damage in rotary drilling includes rotating a drill string having a bit attached thereto in a hole and monitoring the rotary torque for characteristic oscillations, said oscillations having a frequency not more than twice and not less than half the expected frequency of torsional oscillation of the drill string and said oscillations having an amplitude substantially in excess of the amplitude of any oscillations produced by rotation of an undamaged bit.\n\n(15) The preferred apparatus of the present invention includes a drill string and electric drive means for rotating the drill string. A shunt is connected to the electric drive means for proucing a signal prouportional to the current utilized by the electric drive means during rotation of the drill string. A filter means is connected to the shunt. The filter is selected to suppress relatively high frequency signals having a period near to the rotary table period and to suppress relatively low frequency signals of less than one twenty-fourth cycles per second. A recording means is connected to the shunt through the filter for recording the signal passed by the filter.\n\n(16) In a preferred embodiment of the present invention 65rotary torque is measured by determining the current required to operate the electric motor which rotates the drill string. A shunt is provided between the generator and the rotary drive electric motor from which a continuous measurement of current is made. 'lhe signal obtained is filtered through an appropriate band pass filter selected to pass frequencies depending, as detailed Later, on the dimensions and masses of the various portions of 'the drill string. The signal is recorded on a suitable strip chart recorder. It has been found that significant oscillations in the passed signal are indicative of bit damage. These oscillations may be observed on the strip chart.\n\nOBJECTS OF THE INVENTION\n\n(17) A. particular object of the present invention is to provide a driller with a method of determining bit damage utilizing oscillatory changes in rotary torque which characteristically occur as a result of such damage. Further tibjects and advantages of the present invention will become apparent from the following detailed description read in view of the accompanying drawing.\n\n(18) BRIEF DESCRIPTION OF THE DRAWING 1.\n\n(19) FIG. 1 is a schematic view and illustrates a preferred embodiment of apparatus assembled in accordance with the present invention.\n\n(20) FIG. 2 is a facsimile of a portion of a strip chart recording revised for clarity and illustrates oscillations'\n\n(21) in rotary torque recorded against time and is typical of records produced ini accordance with the present invention.\n\n(22) .DETAILED DESCRIPTION OF THE PREFERRED\n\nEMBODIMENT\n\n(23) Refer now to FIG. 1 where a schematic illustration of the preferred embodiment of apparatus assembled in accordance with the present invention is shown. A well 10 is being drilled by means of rotating bit 12 with drill string 14. Bit 12 has rotary cones 16 which rotate against the drilled rock face 18 as the bit is rotated. The rotary cones 16 have teeth which engages the rock face 18 and break it into chips. These chips are carried to the surface *by means of drilling mud which comes down thuough drill string 14 and circulates back up annulus 11 and out the overflow pipe 13. The drill pipe is rotated at the surface by means of rotary table 20. The topmost portion of the pipe, the so-called kelly, is square in external cross section so that it may be gripped for rotation, but may still move freely through the rotary table vertically. The rotary table 20 is rotated by electric motor 24 through appropriate linkage 26. Power for the electric motor, ,is supplied through suitable circuitry by generator 28.\n\n(24) In accordance with the present invention it has. been found that as bearings of the drill bit cones 16 begin to fail, characteristic oscillations are produced in the rotary torque of drill string 14. These oscillations have a period longer than the fundamental period of rotation of the rotary table 20 and approximately equal to the expected period of torsional oscillation of the drill string.\n\n(25) In order to more fully understand the present invention a description is now given of a method of calculating the expected period of torsional oscillatiojn of a drill string. The description is based upon readily available material in a well-known textbook, \"Vibration Problems in Engineering\" by S. Timoshenko, 3rd edition,. Van.\n\n(26) Nostrand Company, 1955.\n\n(27) BAs discussed by Timoshenko on pages 9 and 10, the periods of torsional oscillation in bodies such as a drill string are given by equations of the type of Equation (1) (Timoshenko's Equation (9), page 10):\n\n(28) r=2'r I/K (1) r-the period of torsional oscillation Ih-an effective moment of inertia 0 K -an effective spring constant (torque per radian of 6 twist) For a simple rigid disk, 'l imoshenko calculates the moment of inertia from Equation (2) (an unnumbered equation on 'timoshenko's page 11):\n\n(29) I=-Wfl/Sg W--weight of the disk 1) -diameter of the disk g= gravitational acceleration For the effective spring constant of a simple solid shaft Timoshenko gives Equation (3) (Timoshenko's\" Equation (a), page 10):\n\n(30) K= (rd4G/32L) d=diameter of shaft L-length of shaft G=the shear modulus (3) The drill string of interest here comprises a long, relatively flexible drill pipe, and a set of relatively stiff drill, collars attached _to this drill pipe above the bit.\n\n(31) Thus two complications must be introduced into the simple Timoshenko formulas. These complications are introduced by methods which Timoshenko has outlined\"\n\n(32) or indicated in his text. Equation (2) is readily converted to an equation, for a hollow cylinder instead of a solid disk in the following manner: First, Equation (2,) is written in an expanded form in terms of a specific weight, w, and disk height, h: , I==(whirD2/4) (D2/8g-) (4) Then for a hollow cylinder of two different diameters, D and d:\n\n(33) I=(l/8g) (whwr/4) (D4-d4)'\n\n(34) But D4-d4=(D2-d2) (D2=d2) So I=(1/8 g) (whir/4) (D2-d2)\n\nor\n\n(35) (5) (6) where in Equation (6), W is now the weight of the hollow cylinder.\n\n(36) Equation (6) is the basic formula for computing the moments of Inertia of the parts of the drill string in the problem of interest, but for the upper part of the string, the drill pipe itself, the effective moment of inertia is less than that indicated by Equation (6) because the upper part of the drill pipe is relatively stationary during the oscillation. The drill pipe itself participates actively in the oscillation only toward its bottom. Timo- shenko shows in his pages 29 and 30 how, with the use of Rayleigh's method, one may calculate the effective moment of inertia of a simple shaft undergoing this type of motion. His analysis shows that, in such a cir- cumstance, an expression of the type of Equation (6) should be multiplied by a factor of one-third.\n\n(37) For the system concerned here it is convenient then to write down an equation of the type of Equation (6) involving three terms, one for the drill pipe, and one for each of two sections of drill collar, denoted respectively by the subscripts 1, 2 and 3:\n\n(38) I=1g [wl(Die-I-dis)\u00b1Iw2 (D2z-+d2)- W(a--3 (7) The remaining necessity at this point is to formulate an expression for the effective spring constant to insert in Equation (1). Equation (3) may be readily converted to an equation for a hollow cylinder of two different diameters, D and d:\n\n(39) qrG k= (D~-d~) 32L (4) but the system concerned has three different lengths, of different elastic properties. imoshenko \"show on page 11 how a composite solid shaft, composed of two lengths of .diffrent diameters, acts as if it were a shaft of the single smualler diameter but of an \"equivalent\"\n\n(40) length less than the actual composite length. The stiffer, part of larger diameter acts as if it were shorter than its actual length. By reasoning identical to that of Tlimo- shenko, Equation (8') may be converted to Equation (9):\n\n(41) Co _ rG_(Dld d14)_ (1 d4 32 FIJI-1,2 (Da4d24) T (D4d4 1 9) (2) D1a-dla k I3 C ' D -dl4L ztDi -d1\u00b0L3 32L imD24-d24 2 - 3 k 32X 12 X10#in?2 197 in.4 X (1,GOO12)in.+{(1200X12 (196,000 in.\n\n(42) r 2rJ -2rJ1600#~ in. se_ 7.as i1he discussion thus far considers 1 effect rotating in a vacuum or a fluid insignificant amount of drag on the tc of the drill string. In practice the drill in a heavy drilling miud and will in fay the sides of the hole it has drilled. 1'\n\n(43) the sticking is overcome, and the bit rotates impulsively through a relatively large angle. This sends a torsional pulse up the drill pipe which must be reflected at its 5l5 upper end. Then, if the pulse is strong enough, or if it is followed by a sufficient number of similar pulses, at random intervals, torsional resonance is set up in the drill pipe.\n\n(44) Refer now to FIG. 1 for a detailed description of detection apparatus useful to detect the torsional socilla- tions of interest. In the electrical path from the power source 28 to the motor 24, a shunt 30 is inserted so that the power current or a known fraction of that current may be continuously measured. A signal proportional to the instantaneous current strength is transmitted from shunt 30 to filter 34. This filter is preferably a band pass filter whose upper and lower cutoff frequencies may be varied. Generally then, when the expected period of torsional oscillation of the drill pipe is expected to be between 3 and 12 seconds the upper pass frequency may be set to suppress relatively high frequency signals having a period near the rotary table period and the lower pass frequency mlay be set to suppress relatively low frequency signals of less than one twenty-fourth cycle per second, In a more specific instance, if the expected perod of torsional oscillation of the drill pipe is expected to be about 8 seconds (a frequency of one-eighth second), the upper pass frequency may be set at, say, 0.25 cycles per second and the lower frequency cutoff at, say, ,0.06 cycles per second. The signal passed by the filter 34 is used to activate the strip chart recorder 38, which marks on the chart 39. The stylus 40 of the strip chart ,recorder produces a record 42 of the current variations which are indicative of the torsional oscillations of drill string 14. If desired, an alarm device such as light 44 and/or bell 43 may be activated by a threshold filter responsive to the signal passed through band pass filter 34.\n\n(45) FIG. 2 represents a typical section of strip chart which was produced utilizing the method of the present invention. In FIG. 2 the oscillatory component of the current for the rotary drive motor is plotted as a tunction of time. Clock times may be marked on the chart by the recorder itself. Corresponding drilling depths may also sometimes be advantageously entered on the chart. The recorded current represents the variation of the torque on the drill string. The normal torque variation on the drill string is illustrated in the lower portion of the chart below the vicinity of the chart where the time indication is 4:16 P.M. There, a substantially constant torque is being required by, and imposed upon, the drilling apparatus. In the portion of the chart beginning above the 4:30 P.M. marking, relatively large characteristic torsional oscillations begin, indicating that the drill bit is now damaged. These characteristic oscillations are substantially greater in amplitude than the minor oscillations which occurred when drilling with an undamaged bit, i.e., before 4:16 P.M. In the particular strip chart in question each vertical chart division represents 1 recording minute. Thus the period of the oscillations is about 10 seconds. It has also been found that it is sufficient in field practice to record the torque for a 1 minute interval every 15 minutes. Thus as shown in FIG. 2, after one minute of recording between 4:00 P.M. and 4:01 P.M. a fourteen minute break pis taken before actuating the torque recorder for another 1 minute interval. As illustrated in FIG. 2 evidently the oscillations began some- tinie after 4:16 P.M. since characteristic oscillations are shown beginning with the 4:30 P.M. recording. These characteristic oscillations have an amplitude substantially in excess of the minor oscillations occurring before 4:16 P:M. as a result of rotating an undamaged bit in the well.\n\n(46) The proper time to replace a bit is a function of many variables including the bole depth, the formation being drilled, and the probable cost of a complete failure, which would require a subsequent fishing job. The start of oscillations which are recorded in accordance with the method of the present invention, however, informs the driller that the bit is damaged and immediate attention must be given to whatever considerations should precede actual pulling of the pipe. In some instances, the driller will want to pull and replace the bit within two hours after he determines that the oscillations are continuing.\n\n(47) The following Field Examples illustrate the effectiveness of the present invention in indicating bit damage.\n\n(48) As a result of these Examples it was decided that in future operations, in the drilling of similar wells, a bit should be pulled after 2 drilling hours beyond the initial appearance of oscillations in torque.\n\n(49) FIELDI EXAMPLE I (BIr PUTLIED BEFFORE;\n\n(50) .\"FAILURE,) A straight hole was drilled in the South 'timbilier Block 86 Field. 'The hole was 8 / inches in diameter and was being drilled below 95/s inch protective pipe set at 14,953; feet. At this depth, roundtrip time averaged 9-10 hours. 'The rotating roller bits used in this operation had long tooth tungsten carbide inserts. Generally, their life on the bottom is approximrately 40 to 60 rotating hours.\n\n(51) The practice was to use bit weights of 40 to 45 thousand pounds and a rotary speed of 45 to 50 r.p.m. Due to the long length of protective pipe in this hole, oscillations in torque on the surface were quite apparent as the bit bearings became damaged. After 43 hours of drilling, characteristic oscillations of the type illustrated in FIG.\n\n(52) 2 appeared on the torque record. The bit was run 3 more hours in the manner specified above, i.e., with 40 to 45 thousand pounds bit weight and at 45 to 50 r.p.m.\n\n(53) rotary speed. The bit was then pulled from the hole and no cones were lost from the bit in the hole. However, the lower bearings from two of the cones were left in the hole and one bit cone was cracked. It is believed that if additional drilling had been done for any extent of time, one or more of the roller cones would have been left in the hole necessitating an undesirable fishing job.\n\n(54) FIELD EXAMPLE II (BIT PULLED AFTER FAILURE ) A hole was being drilled below 16,000 feet. The weight on the bit was 20 thousand pounds and the rotational speed was 87 r.p.m. The hole proceeded from 16,194 feet to 16,280 feet where the weight on the bit was 42 thousand pounds and the rotational speed was 78 r.p.m. At this depth, characteristic oscillation of the rotary torque appeared on the strip chart. Drilling continued for 13 hours during which time the oscillation also continued on the strip chart. At the end of this time the bit was pulled and three cones were left in the hole. An expensive fishing job was required to remove the cones from the hole.\n\n(55) Although only certain preferred embodiments of the present invention have been described in detail, the invention is not meant to be limited to these embodiments only, but rather to the scope of the appended claims."
    },
    "document_structure": {
      "abstract_end": 2,
      "abstract_start": 2,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 2,
      "bib_start": 2,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 6,
      "claims_start": 5,
      "description_end": 5,
      "description_start": 2,
      "drawings_end": 1,
      "drawings_start": 1,
      "front_page_end": 4,
      "front_page_start": 1,
      "page_count": 6,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 5,
      "specification_start": 2,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 26798577,
    "guid": "US-RE28436-E",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/RE/028/436",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "E21B",
        "intl_subclass": "12/02",
        "version": "2006-01-01"
      },
      {
        "intl_class": "E21B",
        "intl_subclass": "12/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "E21B",
        "intl_subclass": "44/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "E21B",
        "intl_subclass": "47/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "E21B12/02"
    ],
    "patent_title": "OCR SCANNED DOCUMENT",
    "publication_date": "1975-06-03",
    "publication_number": "RE28436",
    "type": "USOCR",
    "us_class_current": [
      "73/152.59",
      "175/39"
    ],
    "us_class_issued": [
      "073/152.49",
      "175/39",
      "073/152.59"
    ]
  },
  {
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "composite_id": "26217627!MXOC-US-04016076",
    "cpc_inventive": [
      {
        "cpc_class": "B01D",
        "cpc_subclass": "17/04",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "17/045",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "B01D",
        "cpc_subclass": "17/047",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "C02F",
        "cpc_subclass": "1/5236",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "(1) United States Patent [9 Uefeuvre [11] 4,016,076 [45] Apr. 5, 1977 [54] PROCESSING OF EMULSIONS [75] Inventor: Antoine Augustin Joseph Lefeuvre, Ennery, France [73] Assignee: Societe d'Etudes et de Realisations Industrielles, Le Chesnay, France [22] Filed: Mar. 21, 1974 [21] [30] [52] [51] [58] [56] Appl. No.: 453,544 Foreign Application Priority Data Mar. 21, 1973 France .................... 73.10045 Jan. 23, 1974 France .................... 74.02173 U.S. Cl..... .................... 210/61; 210/62;\n\n(2) 25 2/3 30 Int. C.2 ............................... C02B 1/18 Field of Search ............. 210/21, 22, 61, 62;\n\n(3) 252/330, 358 References Cited\n\nUNITED STATES PATENTS\n\n(4) 1,847,413 3/1932 1,984,432 12/1934 2,588,794 3/1952 2,807,654 9/1957 3,262,566 7/1966 Pollock.................... 210/21 X Robinson ................ 252/330 X Barton ....................... 210/21 Grimmett et al ........... 210/21 X Peters.................... 252/3 30 X 3,300,405 3,363,399 3,446,732 3,689,406 3,741,908 1/1967 1/1968 5/1969 9/1972 6/ 1973 Black.......................... 210/21 Schmauch et al ......... 252/330 X Gasser et al.................. 210/44 Ohta et al.................... 210/23 Dailey ...................... 252/330 Primary Examiner-Frank A. Spear, Jr.\n\n(5) Attorney, Agent, or Firm-Armstrong, Nikaido &\n\n(6) Wegner [571\n\nABSTRACT\n\n(7) In order to effect the separation of two liquids, particularly two liquids in the form of an emulsion, one of the liquids, the dispersed component of the emulsion, is concentrated in the form of an emulsion which is inverted with respect to the initial emulsion, i.e. is an emulsion of the previously dispersing component in the previously dispersed component. Thereafter the inverted emulsion can be separated off from the remainder of the initial mixture. Chemical agents and agitation may be employed to promote the formation of the inverted emulsion. The process is especially applicable to oil-in-water emulsions known as soluble oils.\n\n(8) 2 Claims, 1 Drawing Figure\n\nI\n\nPROCESSING OF EMULSIONS\n\nBACKGROUND OF THE INVENTION\n\n(9) I. FIELD OF THE INVENTION This invention relates to improvements in or relating to the processing of emulsions and provides novel processes in this field and novel apparatus for performing such processes.\n\n(10) 2.D3ESCRIPTION OF THE PRIOR ART The difficulty of separating mixtures of nonmiscible liquids increases when the constituents of the mixtures have a reduced tendency to separate naturally and when there is a considerable difference between the proportions of the liquids in the mixture. Many mixtures used in industry, however, exhibit these conditions; a liquid is often dispersed in the form of an emulsion having a certain stability in another liquid at a concentration which, though low, is too great for the emulsion to be simply thrown away, either because of the resulting enviromental pollution of for reasons of expense in an industrial process where the constituent in low concentration is expensive. Processing is therefore required to remove the liquid at low concentration from the liquid at high concentration.\n\n(11) In the case, for example, of the mixture conventionally called soluble oil and consisting of a stable emulsion of oil in a large amount of water, e.g. 0.5% oil-in- water, it is impossible to discharge spent soluble oil directly into a drain since this may seriously pollute the water into which the drain discharges. The oil must therefore be withdrawn so that the water effluent is purified; the withdrawn oil is then destroyed e.g. by incineration.\n\n(12) In conventional methods of processing or \"breaking down\" soluble oils, attempts have been made to separate the oil and water directly. This separation is extremely difficult and the installations required are bulky since they have limited efficiency.\n\nSUMMARY OF THE INVENTION\n\n(13) According to the present invention, there is provided a method of separating a first liquid from a second liquid through which the first liquid is distributed, comprising concentrating the first liquid in the form of an emulsion in which the second liquid is the dispersed component, and separating the said emulsion from the remainder of the second liquid.\n\n(14) Applied to the separation of an oil-in-water emulsion, this method can achieve higher efficiency. Oil is not separated in the form of oil but is converted into a water-in-oil emulsion very concentrated in oil. Experience shows that this emulsion has a strong tendency to separate from water naturally. The residual water is practically oil-free and can be discharged directly into a drainage system. The concentrated oil emulsion can be destroyed by incineration.\n\n(15) A soluble oil, for example, in the normal form in which it is used, i.e. a stable emulsion of oil in water (called the \"right\" form of emulsion) consists of a small volume of oil dispersed in fine droplets in a large volume of water. The concentration of oil is the concentration of the miixture. If, in the same mixture, the oil is collected in the formi of a water-in-oil emulsion (called the \"left\" formj of emiulsion and normally called \"mayonnaise\") the mixture in fact consists of two mix- tures, the first being the mayonnaise containing a high concentration of oil and the second being a unore or less emulsified mixture of oil in water wherein the oil concentration is low, much lower than in the starting mixture and lower in proportion to the completeness with which the mayonnaise has formed.\n\n(16) In the method according to the invention applied in soluble oil, oil is collected in mayonnaise form by inverting the form of the emulsion from the right to the left form.\n\n(17) Thus according to the invention, in another aspect, there is provided a method of processing an emulsion in order to separate the dispersed and dispersing components therein, comprising the step of converting the initial emulsion into a mixture of two concentrates which are richer in respectively the dispersed compo- 1nent and the dispersing component than the initial emulsion, the said concentrate richer in the dispersed component being in the form of an emulsion inverted with respect to the initial emulsion, having the said dispersing component dispersed in the said dispersed component.\n\n(18) In conventional methods of processing soluble oils an attempt is made, in outline, to cancel the effect of the emulsifiers maintaining the stability of the oil-in-water emulsion. In the method according to the invention, on the other hand, the aim is to produce an emulsifying effect, but one in the water-in-oil direction. This method appears to draw maximum advantage from a favourable effect, i.e. a certain natural tendency of the 30mayonnaise to increase, i.e. to capture the oil droplets 30dispersed in water.\n\n(19) In practice the steps that will be taken to invert the form of the emulsion vary and should be adapted to each case, since the soluble oils for processing may 35differ in their original composition (which is always complex, more particularly with regard to the additives which they contain), their concentration with respect to water, and the changes in their constituents during use (such changes are usually classified as \"ageing\").\n\n(20) Preferably, the method includes subjecting the initial emulsion to the combined effect of at least one additive and agitation, the combined effect promoting the formation of the inverted emulsion. Suitable additives which may be added in the method are salts, pH mod ifi- ers, emulsifiers or agents modifying the additives to the oils. Mechanical means suitable for agitating the medium are agitators, mixing or circulating pumps passing the liquid through a filter, ultra-sonics, etc.\n\n(21) The mayonnaise can be separated from the oil-free water by natural decantation, which can be accelerated by various methods e.g. flotation by water saturated with air, electro-flotation centrifuging, coalescence, etc. The process can be continuous and heating may not be required.\n\n(22) In the additive, divalent or trivalent metal ions are preferred, though monovalent ions may be highly effective.\n\n(23) The method of the invention can be applied to mixtures other than soluble oils, inter alia in order to recover a liquid forming the low-concentration component of a mixture. This component may be concentrated in the form of the miain constituent of a mnayon- naise emulsion. FEither the liquids in the miixture for treatmient are themiselves suitable as consitutents of the, mayonnaise emulsion, or a substance having a selective affinity for the constituents to be withdrawn and adapted to give a miayonnaise with the other constituent is added to the miixture.\n\n(24) The following non-limitative examples of the method aspects of the invention show how the invention may be put into practice.\n\nEXAMPLE I\n\n(25) A new sample of a soluble oil (sold under the name ESSO EZL 341 ) was diluted to 5% in water to give an emulsion whose constitutents were to be separated.\n\n(26) The emulsion had a pH of 8.5. 3 grams per liter of calcium chloride were added and 30 liters of the resulting mixture were placed in a tank and agitated by means of a pump (600 liters per hour) which conveyed the liquid through a 30-micron filter the function of which was possibly to produce internal agitation in the fine ducts forming the filter. The oil and water constituents travelled through the filter and returned to the tank where they were taken up by the pump. After 15 minutes' agitation in a closed circuit, oil drops were observed to form and increase in size (this was perceptible to the eye through a transparent wall). The drop- lets, consisting of an emulsion of water in oil, rose in the bath to a place where agitation was less intense.\n\n(27) After half an hour, agitation was stopped and the contents of the tank conveyed through a pasteboard filter (coalescer) having a pore size of 5 microns. The water collected at the base of the filter was completely clear and contained less than 15 ppm (part per million) of oil, whereas practically pure oil was discharged from the top of the separator.\n\n(28) EXAMPLE 2 A mixture of spent soluble oils (sold under the name MOTUL - COUPEX PN and PNC3 - LABO SB 223) having an oil concentration of approximately 4%, was processed by the method described in Example 1. The pH was slightly reduced (to 9) by adding sodium carbonate before agitation began. During agitation the pH may be brought to 7.5. The resulting purified water contained 20 ppm of oil.\n\n(29) EXAMPLE 3 A soluble oil (sold under the name ESSO EZL 341), spent after being used for several months, was processed by the method described in Example 1. Before calcium chloride was introduced, it was necessary to add a water-in-oil emulsifying agent (a product sold under the name CEPRETOL MO by Societe Rhone Progil), since the oils had aged considerably, probably because of modifications in the additives, inter alia emulsifying agents, which they contained. The pH was brought to 9, as in Example 2, and reduced to 7.5 during agitation. As before the separation yield was good, since the oil-free water contained less than 24 ppm of oil. The use of calcium chloride, or more generally of the salt of an alkaline earth metal, has the advantage of preventing hydroxide sludge forming when the pH is brought to near neutrality.\n\n(30) Where an oil-in-water emulsion (the right form), conventionally called a soluble oil, is processed, it may be advantageous to add two reagents. A first reagent modifies the physicochemical conditions in the direction favourable the left formi, whereas a second reagent effectively inverts the formi, i.e. produces the left form of emulsion of mayonnaise. Such a process is illustrated by the accomipanying drawing referred to below. In its apparatus aspect, the invention is especially though not exclusively applicable to a process of this kind which is performied continuously.\n\n(31) According to the invention in the apparatus aspect, there is provided apparatus for the continuous processing of material in emulsion form by the method described above, including a. means providing a main flowpath for the material b. supply means to provide a flow of the material in the said flowpath c. means adapted to inject at controlled rate at least one reagent into the said flow of material d. means adapted to intimately mix said injected reagent with the flow of material.\n\n(32) Preferably means for subjecting the material to agitation are provided. The functions of mixing the added reagent and of agitation may be combined.\n\n(33) The main flowpath may include a volume in which agitation is performed, a reagent being preferably injected immediately upstream of or even inside the fractional volume.\n\n(34) In an advantageous arrangement, the mixing and the action of the reagent are brought about or prolonged by recycling inside the volume, preferably in countercurrent to the main flow. This recycling is particularly advantageous if the emulsion entering the fractional volume meets the emulsion which has already been ivre;tersligconditions are favourable for inversion.\n\n(35) The last-mentioned feature is efficient and used little energy, since the amount mixed can be kept small, corresponding to the fractional volume.\n\n(36) The fractional volume can be a small chamber disposed in the main flowpath. Agitation in the chamber can be produced by various means such as an agitator, a vibrator, ultra-sonics, etc.\n\n(37) Where recycling is performed, the fractional chamber may be all of the main flowpath where recycling occurs; the reagent is preferably injecting upstream of the circulation pump, either in the main circuit or in a loop or shunt circuit used for recycling.\n\n(38) The apparatus may be simplified by omitting the chamber, i.e. by connecting the shunt circuit directly to the main circuit. An arrangement of this kind is shown by way of example in the accompanying drawing, in which the single figure is a diagram of a non-limitative embodiment of the apparatus according to the invention.\n\nDESCRIPTION OF A PREFERRED EMBODIMENT\n\n(39) The apparatus has a main flowpath 1 provided with a supply pump 2 conveying the emulsion from an inlet la to an outlet lb and having a delivery rate d. Shunt circuits 3, 4 forming loops off the main circuit are provided with circulating pumps 5, 6. The first reagent is injected at a controlled rate by a proportioning pump (not shown) at 7 into the shunt circuit 3 upstream of pump 5. The second reagent is injected at 8 into shunt circuit 4 upstream of pump 6. Each circulation pump 5, 6 has a delivery rate greater than d.\n\n(40) The apparatus operates continuously as follows:\n\n(41) Emulsion is introduced into the main circuit by pump 2 and enters shunt circuit 3 where the first reagent (e.g.\n\n(42) the salt of an alkaline earth imetal and/or an emiulsifier) is injected at 7, and travels through the loop comiprising shunt circuit 3 and the portion of the main circuit between the two ends of the shunt. The am~ount of recycling of the emulsion, i.e. the statistical numiber of timies that a particle of emiulsion travels through the loop, is higher in proportion as the delivery rate of recycling depends on the length of the shunt.\n\n(43) After leaving shunt 3, the emulsion flows along the main circuit at the rate d. The second reagent (e.g. the salt of a trivalent metal or a pH modifier) is introduced in shunt 4 and produces the inversion. The device continuously discharges a mixture of water and of mayonnaise particles containing a variable amount of water.\n\n(44) The water and mayonnaise can subsequently be separated by natural decantation or by any other, more rapid method, such as flotation by water saturated with air.\n\n(45) The arrangement illustrated has the advantage of great simplicity and the apparatus can be compact, light and cheap. Experience shows that the device can process a greater variety of soluble oils (natural or concentrated) so that the process is widely applicable, and that some tolerance is permissible in the proportioning of the reagents, so that the process is highly reliable.\n\n(46) While the invention has been illustrated above by reference to several examples and a preferred but non- limitative embodiment thereof, it will be understood by those skilled in the art that various changes may be without departing from the spirit and scope of the invention and it is intended to cover all such changes and modifications by the appended claims.\n\n(47) 1. A continuous method of processing a stable emulsion of oil-in-water comprising the steps of:\n\n(48) a. providing a main flowpath of an initial oil-in-water 5emulsion at a slightly alkaline pH, b. diverting a portion of the emulsion from the main flowpath into a first shunt circuit, c. injecting the first additive selected from the group consisting of salts of an alkaline earth metal at a to controlled rate into said first shunt circuit, d. joining the first shunt circuit to said main flowpath of initial emulsion thereby forming a mixture of said first additive and initial emulsion.\n\n(49) e. diverting a portion of the mixture formed in (d) from the main flowpath into a second shunt circuit, f. injecting a second additive selected from the group consisting of salts of divalent and trivalent metals at a controlled rate into said second shunt circuit to cause inversion of said initial emulsion, g. joining said second shunt circuit to the main flow- path, and h. allowing the resultant mixture from step (g) to separate into separate layers of water and a water- in-oil emulsion.\n\n(50) 2. The process of claim I wherein the rate of flow in said first and second shunt circuits is greater than the rate of flow in said main flowpath."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 5,
      "claims_start": 5,
      "description_end": 5,
      "description_start": 3,
      "drawings_end": 2,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 5,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 5,
      "specification_start": 3,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 26217627,
    "guid": "US-4016076-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/04/016/076",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "C02F",
        "intl_subclass": "1/52",
        "version": "2006-01-01"
      },
      {
        "intl_class": "B01D",
        "intl_subclass": "17/04",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "C02F1/52"
    ],
    "inventors": [
      {
        "name": "LEFEUVRE ANTOINE AUGUSTIN JOSE"
      }
    ],
    "inventors_short": "LEFEUVRE ANTOINE AUGUSTIN JOSE",
    "patent_title": "Processing of emulsions",
    "publication_date": "1977-04-05",
    "publication_number": "4016076",
    "type": "USOCR",
    "us_class_current": [
      "516/141"
    ]
  },
  {
    "app_filing_date": "1975-02-12",
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "defensive publication",
    "composite_id": "27400852!MXOC-US-T0942010",
    "cpc_inventive": [
      {
        "cpc_class": "C08L",
        "cpc_subclass": "81/00",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "(1) UNITED STATES PATENT AND TRADEMARK OFF+ICE Published at the request of the applicant or owner in accordance with the Notice of Dec. 16, 1969, 869 O.G. 687. The abstracts of Defensive Publication applications are identified by distinctly numbered series and are arranged chronologically.\n\n(2) The heading of each abstract indicates the number of pages of specification, including claims and sheets of drawings contained in the application as originally filed. The files of these applications are available to the public for inspection and reproduction may be purchased for 30 cents a sheet.\n\n(3) Defensive Publication applications have not been examined as to the merits of alleged invention. The Patent and Trademark Office makes no assertion as to the novelty of the disclosed subject matter.\n\n(4) PUBLISH ED JAINUARY 6, 1976 942 O.G. 191 T942,O1a NOVEL. CUREFS FORs B3LOCKED POYSUL.FIDE POLYMER~F1 S Julian R. Panelk, N ewtown, Pa., and Michael J. Kohut, Trenton, N.J., assignaors to Thiok~ol Chnemical Corpo- ration, Bristol, Pa.\n\n(5) Continuation of application Ser. No. 403,576, Oct. 4, 1973, which is a continuation of application Ser. No.'\n\n(6) 255,349, May 22, 1972, both now abandoned. This application Feb. 12, 1975, Ser. No. 549,256i mt. Cl. BO~d 3/10 U.S. Cl. 427--340 No Drawing. 17 Pages Specification '\n\n(7) A process for providing a curing system for aldehyde and ketone blocked polysulfide polymers is disclosed wherein ammonium compounds and substituted amines are used to cure the blocked polysulfide polymers after impregnation of leather by the blocked polysulfide polymer,"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 0,
      "claims_start": 0,
      "description_end": 0,
      "description_start": 0,
      "drawings_end": 0,
      "drawings_start": 0,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 1,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 0,
      "specification_start": 0,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 27400852,
    "guid": "US-T942010-I4",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/T0/942/010",
    "intl_class_current_primary": [
      {
        "intl_class": "C08L",
        "intl_subclass": "81/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "C08L81/00"
    ],
    "patent_title": "OCR SCANNED DOCUMENT",
    "publication_date": "1976-01-06",
    "publication_number": "T942010",
    "type": "USOCR",
    "us_class_current": [
      "427/341",
      "427/340"
    ]
  },
  {
    "composite_id": "24921114!OC-US-04082996",
    "cpc_inventive": [
      {
        "cpc_class": "H03F",
        "cpc_subclass": "3/19",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "United States Patent [igi Hinn [541 VIDEO AMPLIFIER INCLUDING AN A-C COUPLED          VOLTAGE          FOLLOWER           OUTPUT STAGE [75] Inventor: Werner Hinn, Zollikerberg, Switzerland [73] Assignee: RCA    Corporation,    New    York, N.Y. [21] Appl. No.: 727,044 [22] Filed: Sep.27,1976 [51] Int. Cl.2 ......................... H04N    5/68; H04N     9/18 [52] U.S.   Cl  358/242;358/184; 358/65 [58] Field of Search 358/21,  40,  39,   64, 358/65,  34,  184,   172,   242;   330/32 [56]                  References       Cited U.S.          PATENT           DOCUMENTS 2,819,334 1/1958 Squire s 358/21 3,803,503 4/1974 Greut man 330/32 Primary     Examiner-John     C.      Martin Attorney, Agent, or Firm-Eugene M. Whitacrel- William   H.    Meagher;    Kenneth    R.    Schaefer [57]                     ABSTRACR A  video   amplifier   suitable   for   providing   amplified im- IB + B+-AV                                       36 30 3 R AM )LIFIER     AMPLI I         60                  46 28 +12v 26 48 22  20  18  50 16 14   12@'   2 F-- G  R SIGNAL    VREF PROCESSING CIRCUITS loj 490829996 [451          Apr. 4, 1978 age-representative signals to an image reproducing device having a capacitive input impedance component comprises a first wideband amplifier stage which provides video signal voltage gain substantially greater than unity. A second wideband amplifier stage provides approximately unity video signal voltage gain and includes at least one amplifying device having input, output and common terminals. An output load circuit comprising at least first and second resistors is coupled in series relation between the output and common terminals through the power supply.  Direct coupling means are vided for supplying amplified video signals from the pro output terminal to the image reproducing device.  Amplified signals are capacitively coupled from the first stage to the input terminal of the second stage. A further capacitor is coupled from the input terminal of the second stage to a junction between the first and second resistors of the output load so as to couple video signal excursions of a polarity tending to reduce conduction of the amplifying device to the output terminal and the load. 8 Claims, 1 Drawing Figure B+ B + I  I 38  40 34 42 6V 64-6 0 v\n\nU. S. Patent April 4, 1978 49082@996 C\"i 7 ------ co C.0 co C - 0 c : : , + m 0 C.0          co cl@j U,-)        U'> +                                  C.0                U-.) C.C> r%-.)                                        .   C.0 co U-) co col rlo                C= LAJ LL- --V C%J Cj C%i LLJ LL- C.D C.D C%.i L L J cm cli L-i LL- ei =E C.,j                             CC)\n\n4,082,996                             2 VIDEO AMPLMER INCLUDING AN A-C COUPLED VOLTAGE FOLLOWER OUTPUT STAGE T'his invention relates to video ampiifler circuits and, in particular, to circuits of this type which exhibit stable operating characteristics and are suitable for driving a color picture tube which has a relatively wide cutoff (black level) tolerance range. Several conflicting requirements, including wide bandwidth, a relatively higii direct operating supply voltage and low quiescent power dissipation must be met in designing semiconductor video amplifiers for driving modem color picture tubes.  Specifically, color picture tubes such as the unitized PI (precision in-line) gun type frequently require that associated video amplifiers have a relatively wide- adjustment range for the direct component of output voltage in order to compensate for a relatively wide tolerance of beam current cutoff voltage of the several guns of the picture tube.  In a typical PI tube, an adjustment range of fifty to sixty volts may be required for the beam cutoff voltage while a signal voltage swing of the order of 110 to 130 volts above the cutoff level (black to white) may be necessary.  Additional supply voltage variations and practical limitations of the amplifier and picture tube operating- characteristics require that the direct supply voltage (B +) for the video output amplifiers be of the order of 200 volts. Recently, low cost, wide bandwidth, relatively low power video amphfler transistors have been made avaflable which are designed to operate without the use of heat sinks (thereby reducing output capacitance and improving frequency response).  It is desirable from a cost and performance standpoint to employ such devices in video output stages.  However, a supply voltage of the magnitude described above could either result in a level of power consumption which would exceed the operating limits of such devices or could adversely affect the frequency response of the ampefiers. The present invention permits one to utihze such low power, low cost, high bandwidth transistors in a system where the direct supply voltage required for the video output stage is greater than a normal supply voltage for such transistors.  These conditions may also be encountered where video amplifiers are constructed in monolithic integrated circuit form. Alternatively, a video amphfler preceding the output stage may be operated at lower supply voltages and hence lower power dissipation when the prcsent invention is employed. In accordance with the present invention, a video amplifier circuit suitable for driving an image reproducing device comprises a first wideband ampfifier stage for providing a video signal voltage gain substantially greater than unity.  The apparatus further comprises a second wideband amplifier stage for providing approximately unity video signal voltage gain, the second stage comprising at least one amplifying device having input, output and common terminals.  An output load circuit comprising at least first and second resistors is coupled in series relation between the output and comrnon terminals.  Means are provided for coupling amplified output signals from the output terminal to the image reproducing device.  A first capacitor is provided for coupling amplified signals from the first stage to the input terminal of the second stage. A second capacitor is coupled from the input terminal of the second stage to a junction between the first and second resistors of the load circuit for coupling video signal excursions of a polarity tending to reduce conduction of the amplifying device to the output terminal. In the accompanying drawing, a portion of a color television receiver is shown in schematic diagram forin including three video amplifier- arrangements con- 10 structed in accordance with the present invention. Referring to the drawing, low level television signal processing circuits 10, which may be of a conventional forrn, provide red (R), green (G) and blue (B) imagerepresentative signals via respective adjustable drive 15 control resistors 12, 14, 16 and associated biasing networks 18, 20, 22 to a red (R) video signal amplifier 24, a green (G) video signal amplifier 26 and a blue (B) video signal amplifier 28, respectively.  Each of the amplifiers 24, 26, 28 may employ, for example, wide 20 bandwidth, low power dissipation devices such as the BF422, BF423 complementary type transistors in a Class B arrangement. Amplified red, green and blue image-representative signals are coupled from amplifiers 24, 26 and 28 via 25 respective capacitors 30, 32 and 34 to video driver or output stages 36, 38, 40 which are constructed in accordance with the present invention. Red, green and blue image-representative signals having- controllable direct voltage levels (as will appear 30 below) are coupled from amplifiers 36, 38, 40 to respective cathodes of an image reproducing cathode ray tube 42. The illustrated cathode ray tube 42 is of the PI type and therefore includes, for all three guns, a common 35 first contrdl grid (GI) bias source and a common second control grid (G2) adjustable bias source. The output stages 36, 38, 40 are substantially identical and therefore only the red signal amplifier 36 will be described in detail.  Output stage 36 comprises a transis- 40 tor 44 arranged in a voltage follower (common collector) configuration with input signals coupled via capacitor 30 to a base electrode and output signals direct current coupled from the emitter electrode of transistor 44 to the red (R) signal cathode of cathode ray tube 42. 45 The emitter circuit of transistor 44 includes a series combination of resistors 46 and 48, a variable D.C. level adjusting resistor 50 and a resistor 52 coupled to a reference potential (ground).  A keyed clamping circuit- comprising cascaded transistors 54, 56 a collector load resis- 50 tor 58, a coupling resistor 60 and a filter capacitor 62 is arranged to compare a fraction of the output voltage (at the junction of resistor 48 and D.C. level adjusting resistor 50) with a direct reference voltage (VREF) during each line blanking interval.  The collector of transis- 55 tor 56 is coupled via resistor 60 to the junction of couphng capacitor 30 and the base of output transistor 44 to maintain a desired direct voltage level at the base of transistor 44 and thereby set the direct component of output voltage coupled to the cathode of cathode ray 6o device 42.  Keying pulses which occur at the line scanning rate (period of 1H) are supplied to a terminal 64 (the collector of transistor 54 and the collector of similar devices in output stage 38, 40).  The keying pulses typically are derived from an associated line deflection 65 system (not shown) and are coincident in titne with the blanking interval of each line scanning period.  Appropriate voltage levels are illustrated on the waveform adjacent terminal 64.\n\n3                                4,082,996 4 In    accordance    with    one    aspect    of    the present    inven- tion,   a   capacitor   66   is   coupled   between   the   base of    volt- age   follower   transistor   44   and   a   tap   on   the emitter    load resistance   (i.e.,   between   resistors   46   and    48    in the    FIG URE)@    As    will    appear    below,    capacitor     66 allows     the 5 load     capacitance     68     associated     with     the cathode     of image reproducer 42 to discharge via resistor 46, cap .ac- itor   30   and   the   preceding    amplifier    stage    24 to    imprpve the    response    of    the    voltage    follower   - transistor    44     to negative-going output signal voltage changes, 10 As   is   shown   in   the   drawing,    the    collector    of follower transistor   44    is    coupled    to    an    operating voltage    supply (B+)   while   the    preceding    amplifier    24    is coupled    to    a lower     operating     voltage     supply     (B+     - AV).      Typical values  for   such   supplies   are   +   190   volts   and   + 160   volts, 15 respectively. In    the    operation    of    the    illustrated    - arrangement,     the amplifier      24      provides      red      imag e-representative      output signals    having    the    peak-to-peak    voltage    swing (e.g.,     130 volts)    required    for    driving    cathode    ray    tube 42.    These 20 signals   are   developed    with    respect    to    the supply    voltage of   amplifier   24   (e.g.,   +   160   volts)    which,    in the    case where   cathode   ray   tube   42   is   of   the   PI   type, is   insuffi- cient    to    provide    the    required    range    of cutoff     adjust- ment.   The   signals   are    A.C.    coupled    via capacitor    30    to 25 the   base   of   voltage    follower    transistor    44. For    positive- going      output      voltage      signal      swings, transistor      4 . 4 1 supplies   the   current    required    to    charge    the load    capaci- tor   68   (e.g.,   approximately    12    picofarads)    which is    asso- ciated   with   the    red    cathode    electrode    of    the tube    42. 30 Transistor    44    serves     to     reduce     the effective     cathode capacitance    seen    by    the    preceding    amplifier 24     by     a factor   equal   to   the   current    gain    (hf,)    of transistor    44 which  may   be   of   the   order   of   50.   The   transistor 44   itself may   be   of    the    type    BF392    which    exhibits    a capacitance 35 of   the   order   of   less   than   2   picofarads.   The overar    effect of   interposing    output    stage    36    between amplifier    24    and cathode   ray   tube   42   is   to   lower   the   rise   time of   signals supplied to cathode ray tube 42. With     regard     to     negative-going      output signal      tran- 40 sients,   a   simple   voltage    follower    normally    is arranged    to be   driven   towards    cutoff    and    the    discharge    of the    ca- pacitive   output   load   68   occurs    through    the enu'tter    resis- tor.   The   effective   emitter   resistor   of   follower 44    is    the series   combination   of   resistors   46,   481   50,    52. The    total 45 of  these   resistors   may   be   as   high   as   20,000 ohms   in   order to   keep    power    consumption    of    this    stage relatively    low. This    relatively    high    value    of     effective emitter     resis- tance,     however,     would     significantly     reduce the     ability of    a    conventional    voltage     follower     to reproduce     large 50 negative-going     output     signal     transients.     That is,      the cathode   capaci.tance   68    of    tube    42    could    only be    dis- charged   with   a   relatively   long   time    constant    via such    an emitter   resistor.   Signal   fall   tiines   could    thus be    substan- tially   greater   than    signal    rise    times.    In    the ihustrated 55 configuration,     capacitor     66     allows     the     load (cathode) capacitance    68    to    discharge    via    relatively small    resistor 46   (e.g.,   270   ohms),   large    capacitor    30    (e.g., I    micro- farad)    and    the    output    impedance     of     preceding amplifier stage 24 fast enough during negative-going transients to 60 obtain substantially equal rise and fall tiines. The direct component of the output voltage at the emitter of transistor 44 is set by means of the clamping circuit compnsmg transistors 54, 56 and associated components. The clamping circuit is keyed on during 65 each line blanking interval and compares that @fraction of the direct output voltage developed across resistors 50, 52 with a reference voltage (VREF).  Transistors 54 and 56 conduct in response to the difference between such voltages to change the charge on coupling capacitor 30.  In this way, the direct voltage level of the video signal produced at the emitter of transistor 44 is \"restored\" during each line blanking interval to a desired level.  This level is adjustable by means of resistor 50 during \"set-up\" of the receiver to correspond to an appropriate black level.  The black level voltage may be vane . d over a range of the order of 50 to 60 volts as is required for PI cathode ray tubes.  It should also be noted that the signal swing provided at the output of unity voltage gain transistor 44 will be substantially equal to that provided by the preceding amplifier 24 but will be developed with respect to the appropriate adjustable black level voltage independent of the direct voltage level at the output of preceding amplifier 24. Furthermore, any temperature drift in the preceding amplifier 24 will have no effect on the direct voltage at the output of transistor 44.  The amphfier stages 24, 26, 28 all may be operated with substantially equal bias conditions selected to provide a desired linearity with a niimmum supply voltage (B+ - AV).  The- amplifiers 24, 261 28 may thereby be arranged to provide substantially equal rise times so as to avoid differential rise time errors in the three signal channels which would result in an image having an appearance similar to that caused by convergence errors. Any temperature dnft associated with transistor 44 itself will have negligible effect on fts direct output voltage since transistor 44 provides only unity voltage gain. Furthermore, temperature drift attributable to transistors 54 and 56 may be expected to be relatively minor and also to be accompanied by like drifts in the clamp circuits associated with amphfiers 38 and 40.  SubstantiaRy no differential drift and therefore no resultant color shift may be expected from temperature drift of the clamp circuits.  It is also possible, if desired, to compensate for any comrnon drift in the clamp circuits by providing an opposite drift in the reference voltage (V    ). REF While the invention has been described in terms of a prefeffed embodiment, various modifications may be made within the scope of the invention. Component values for one typical circuit configuration are listed below. C apacitors 30, 32, 34 1 mi crofarad R esistor 46 2 70 o hms R esistor 48 18 ,000 o hms R esistor 50 1, 000 o hms (adjustable) R esistor 52 1, 000 o hms R esistor 58 4 7,OW o hm R esistor 60 4 7,000 o hms 62 100 microfarads 66 2.2 microfarads R e V oltage (Vp @ EF) + 6.8 v olts B + +1 90 v olts B + - AV +1 60 v olts What is claimed is: 1.  Video amplifier apparatus suitable for providing amplified image- representative signals to an image reproducing device having a capacitive input impedance component, the apparatus comprising: first wideband amplifier stage for providing a video signal voltage gain greater than unity; and second wideband amphfier stage for providing approximately unity video signal voltage gain, said second stage comprising at least one- amplifying device having input, output and common termi-\n\n5                         4,082,996 6 nals, an output load circuit comprising at least first and second resistors doupled in series relation between said output and common terminals, means for coupling amplified signals from said OutPut terminal to said image reproducing device, a first capacitor for couphng amplified signals from said first stage to said input terminal of said second stage, and a second capacitor coupled from said input temiinal to a junction between said first and second resistors for coupling video signal excursions of a polarity tending to reduce conduction of said amplifying device to said output terminal. 2.  Video amplifier apparatus in accordance with claim 1, wherein: said first resistor is of substantially lower resistance value than said second resistor. 3. Video amplifier apparatus in accordance with claim 2, wherein: said second capacitor exhibits a sufficiently low iinpedance at signal frequencies corresponding to image edge transitions to produce substantiary equal signal rise and fall times at said output terminal. 4. Video amplifier apparatus in accordance with claim 3, wherein: the capacitance of said first capacitor is  of  the  same order of magnitude as the capacitance of said second capacitor. 5.  Video amplifier apparatus in accordance with 5 claim  3, wherein: said first wideband amplifier stage includes means for providing a first direct operating voltage; and said second wideband amplifier stage includes means for providing a second direct operating voltage 10 greater than said first. 6.  Video amplifier apparatus in accordance with claim  5,  wherein: said output load circuit comprises at least one adjustable resistance element for varying direct voltage 15 at said output terminal. 7.  Video amplifier apparatus in accordance with claim  6,  wherein: said second wideband amplifier stage further comprises keyed clamping means coupled to said first 20 capacitor for periodically restoring a direct voltage component in said image- representative signals coupled to said second stage. 8.  Video amplifier apparatus in accordance with claim  7,  wherein: 25 said adjustable resistance element is coupled to said clamping means for varying said direct voltage component and thereby varying black level of images produced at said image reproducing device. 30 35 40 45 50 55 60 65"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 5,
      "claims_start": 4,
      "description_end": 4,
      "description_start": 3,
      "drawings_end": 2,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 5,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 4,
      "specification_start": 3,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 24921114,
    "guid": "US-4082996-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/04/082/996",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "H03F",
        "intl_subclass": "3/189",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H03F",
        "intl_subclass": "3/19",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "H03F3/189"
    ],
    "inventors": [
      {
        "name": "HINN WERNER"
      }
    ],
    "inventors_short": "HINN WERNER",
    "patent_title": "VIDEO AMPLIFIER INCLUDING AN A-C COUPLED VOLTAGE FOLLOWER OUTPUT STAGE",
    "publication_date": "1978-04-04",
    "publication_number": "4082996",
    "type": "USOCR",
    "us_class_current": [
      "348/380",
      "348/809"
    ]
  },
  {
    "composite_id": "25772776!OC-US-04311002",
    "cpc_inventive": [
      {
        "cpc_class": "B65H",
        "cpc_subclass": "51/14",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H01B",
        "cpc_subclass": "13/0003",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "D07B",
        "cpc_subclass": "3/005",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "H01B",
        "cpc_subclass": "13/04",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "United States Patent [igi Hoffmann et al. [54]   FORMING STRANDED STOCK [75]   Inventors: Ernst Hoffinann; Friedrich Schatz, both of Langenhagen; Artur Unger, Lennep,    all    of    Fed.    Rep. of    Germany [73]   Assignee: Kabel-und Metallwerke Gutehoffnungshutte AG, Hanover, Fed. Rep. of Germany [21] Appl. No.: 182,492 [22] Filed:           Aug. 29, 1980 Related U.S. Application Data [63] Continuation-in-part of Ser.  No. 944,582, Sep. 21, 1978, abandoned. [30] Foreign Application Priority Data Sep. 22, 1977 [DE]   Fed. Rep. of Germany 2742662 Oct. 24, 1979 [DE)   Fed. Rep. of Germany 2942924 [51]   Int. Cl.3 liOlB        13/04 [52]   U.S. Cl 57/293;57/294 [58]   Field of Search 57/293,294 [56] References Cited U.S.             PATENT              DOCUMENTS 3,025,656   3/1962 Cook 57/293 3,052,079   9/1962 Henning 57/293 3,169,360   2/1965  Corrall et al 57/294 [11)                493119002 [451           Jan. 19,  1982 3,426,519         2/1969 Unger    et     al 57/294      X 3,481,127 12/1969 Vogelsberg 57/294 3,491,525 1/1970 Sugi 57/294 3,855,777 12/1974 Durkee    et    al 57/293      X 4,012,894 3/1977 Akachi    et    al 57/293 4,056,925 11/1977 Vogelsberg 57/293 FOREIGN PATENT DOCUMENTS 2514033 10/1976  Fed.  Rep. of Germany Primary           Examiner-John           Petrakes Attorney,     Agent,     or      Firm-Smyth,      Pavitt, Siegemund      & Martella [57]                          ABSTRACT Plural strands, e.g., conductors, having a particular cross section, are drawn off spools, run through straightening rolls, and are combined in a bundle in a first stranding point. Brackets on a caterpillar-like capstan grip the bundle and hold it while turning the bundle on the stranding axis.  The bundle continues towards a second stranding point spaced particularly from the capstan.  The stationary (nonrevolving) stranding points can be established by nipples or by additional, nonrevolving capstans, in which case slidable brackets are used to compensate speed differentials.  The revolving capstan may rotate continuously or reverse periodically to obtain different types of stranding patterns. 23 Claims, 6 Drawing Figures 7      9\n\nU.S. Patent  Jan. 19, 1982         Sheet I of 3 493119002 00\n\nU.S. Patent jan. ig, 1982@l              Sheet 2 of 3        493119002 O@ /C @.o@ /C\n\nU. S. Patent Jan. 19, 1982              Sheet 3 of 3 493119002 ci 1\"77   -7Z7@7Z@ z                             z to\n\n49311,002 FORMING STRANDED STOCK This application is a continuation-in-part of applica@ tion Ser. No. 944,582, filed Sept. 21, 1978, now abandoned. BACKGROUND OF THE INVENTION The present invention relates to contin-uously stranding elongated stock such as condtictors, etc., particu@ larly of the variety having larger cross-section and/or complex profiles in cross-section, to 6btain a stranded cable, rope or the like. Equipment for stranding elongated stock, particularly of the heavy variety, uses storage drums or spools from which the stock is continuously paid, and these drums or spools are mounted on a rotating device.  The individual strands of stock are run together at a stranding point and stranded.  If the @trands themselves are stranded filaments or the like, a higher order stranding element is produced. It is inevitable that the known stranding machines require movement of heavy components which limits the production speed.  Furth6rmore, the known equipment requires an inordinate amount of space whild the storage capacity for the stock being stranded together is quite limited.  These features, in tum, limit the length of the product being made.  A typical stranding device is shown, e.g. in U.S. Pat.  No. 3,106,815. The aforementioned drawbacks have been attempted to be alleviated by means of stranding the stock with reversing pitch.  Each strand is guided by particular means which reverse rotation (see German printed patent application No. 25 14 033).  The resulting eqtiipment is indeed more economical and requires less space, and less consideration can be given to inertia forces resulting from rotation.  Also, set up and preparation times are considerably reduced. The known equipment as referred to in the precedirig paragraph are disadvantaged by torsion forces in strands of larger cross-section.  If the strands are insillated, the insulation may be damaged.  In the case of cable, damage to the insulation cannot be tolerated. DESCRIPTION bF THE INVENTION It is an object of the present invention to strand elongated stock to obtain a stranded configuration, such as a cable, a rope or the like, which is free from the defi6iencies outlined above and wherein particularly the strands are not damaged in the process. It is another object of the present invention to obtain a stranded product on a continuous basis and v@ith little expenditure in machinery and operating p@rsonnel. In accordance with the preferred embodiment of the invention, it is suggested@to withdraw the strarfds from individual, reelable, but otherwise stationary moiinted spools and, preferably, to straighten the strands before twisting them together.  The straightening niay include or-ienting the strands to each other, if they are particularly contoured and sh6uld be assembled in co'ntour matching configuration.  The strands are combined in a bundle; the combining head miy constitute a first stranding point. Downstream from that stranding point, the bundle is gripped and held on an axis for a certain length or a particular travel path while the means@ for gripping and holding revolve about a stranding axis; upon release, the twisted bundle is rtin throiigh a nonrevolving second stranding point.  The term \"stranding 2 point\" is to mean a point passed through by a bundle, and on one side a revolving (twisting) motion is imparted upon the bundle while no' such motion is imparted upon the @undle on the other side of that point. 5 The means for gripping and holding will preferably include plural brackets which grip and clamp the bundle in pairs, hold it positively for a certain length of common travel 'and release it thereafter, but all the while revolving about the stranding axis.  The brackets, 10 or bracket elements, may be mounted on two endless belt4ike carriers constructed in caterpillar fashion and each carrying -the spaced brackets. These brackets cooperate in oairs, one Oer carrier of each pair, to hold and grip the bundle, whereby the brackets of each pair close 1 5 on the bundle as it arrives, hold it without twisting, and carry it along for the storage length, and release it as the brackets return.  Positive igripping of the bundle avoids damage to the surface of the @ strands. In a preferred form, th brac         e ements are 20 mounted@on endless chains an are configured to function in pairs in a gearlike fashion.  One bracket element of a pair has a detent, the other one carries an elastic pin which is received by the detents of the other element as long as they run together in a common path into which 25 the bundle is fed and is being held by the pin and against the detent. These carriers (such as endless chains) and the brackets thereof constitute a capstan being preferably mounted in a frame and rotating about the stranding 30 axis.  Depending upon the type of stranding required, the frame, i.e@, -the mears for gripping aiid holding, may rotate continuously, resulting in a cbntinuous mode of helically looping the strands about each other and the common stranding axis.  In other cases, it is desirable to 35 more@ or less frequently reverse the rotation and to reverse the pitch, after less than one'twisting turn or several thereof.  This way, one obtains different types of stranding patterns, such as a so-called meander pattem, or an S-Z pattern.  For reasons below -and considering 40 S-Z stranding-the second stranding point is to be spaced from the means for gripping and ding y a distance which is an@ even- numbered multiple  of the length of the bundle in between twist reversals which may coin6ide with the length the bundle is held.  The 45 revolving speed may differ in the two directions resulting@in@@different pitch values.  Also, the number of unidirectional turns or, more, generally, the total angular displacement of the revolving capstan in between reversals may be different in the@ two directions. : 50 T@p @unolin-g of the strands can be sei)arated from the first stranding @oint by means of anotlier capstan which grips and holds the buridie for a certain length but does not iotate.  Ugon@adjusting and synchronizing the capstans, particularly as far as opening and closing of 55 brackets is concerned, the length of the buiidle between the capstans in any instaiit can be made constant.  Analogously, the second stranding point downstream from the revolving capstan can be established by another capstan.  The spacings between the capstans may be 60 varied to change the lay and pitch of twisting.  However, the rule concernirig spacing between the second stranding point and the end of the revolving capstan should be observed (even multiple of the travel length :between reversals).  The purpose thereof isi briefly, the 65 following.  In the case of so-called S-Z stranding, it was obs@rved that the stranded bundle has a certain waviness.  That deficiency is of no consequence operation- ally, for example in the case of a cable; but the  cable  has\n\n4,311,002                            4 a rather odd, unpleasant appearance and may be difficult to handle.  Appearance and ease of handling are improved if the spacing between the revolving capstan and the,second stranding point is an even-numbered multiple of the travel path length for the bundle in between reversals. If three such capstans are disposed in series, only the middle one revolving, the propagation speed of the stock from capstan to capstan reduces in downstream direction.  All three capstans should be driven at the same constant speed for reasons of ensuring the requisite synchronization.  The brackets should be mounted 6n the carriers to permit additional longitudinal movement during the time of engagement with the stock to compensate the above-mentioned speed,differential of the stock. Downstream from the second stranding point, one may wrap a ribbon or the like around the cable for preventing untwisting.  However, upon holding and stranding the twisted stock, e.g. by a downstream, nonrevolving capstan, such wrapping may not be needed.  Additionally or altematively, a second lay may be stranded on top of the stock after leaving the device described above. As far as further improvement is concerned, it was found advisable to jacket the bundle already upstream from the second stranding point, downstream from the revolving capstan.  Jacketing may include extruding a layer and filling the gaps between the stranded elements, and it may further include wrapping a ribbon around the bundle, possibly prior to extrusion, Also, the bundle may be marked at that point in a suitable fashion. Stranding may be carried out in steps in that a second revolving capstan is provided downstream from the first revolving one, but rotating in opposite direction.  This way, the lay (pitch) is doubled.  If the two capstans revolve uniformly, the path between them serves as temporary storage. In special cases, it may be desirable to open and reclose the brackets; during the open period they change their azimuthal position relative to the bundle.  The brackets, therefore, will exert torsion upon the bundle for twisting it interinittently only which results in a non-uniform speed of stranding. The preferred embodiment of the invention, the objects and features of the invention, and further objects, features and advantages thereof, will be better understood from the following description taken in connection with the accompanying drawings. DESCRIPTION OF THE DRAWINGS FIG. 1 is a schematic view of an apparatus for stranding stock in accordance  with the preferred embodiment of the invention; FIG. 2 is a schematic view of a modified and improved device for that purpose; FIG. 3 is a schematic view of a different modification; FIG. 4 is a perspective view of clamping elements used in either of the aforementioned examples, preferably in the one shown in FIG. 2. FIG. 5 is a view similar to FIG. 1, but showing details of a further improvement; and FIG. 6 is a schematic but still more detailed view of an improved example for a capstan that can be used in any of the apparatus' described in the several figures. Proceeding now to the detailed description of the drawings, FIG. 1 illustrates several drums or spools 2, journalled for rotation about their own axis, but being otherwise stationarily mounted in a plant or the like. - Individual strands 1 are taken from these drums or spools and pass through sets of straightening rollers 3. The straightened and, possibly, oriented strands 1 are fed to a stranding head or nipple 4 which, in the essence, combines the strands in a bundle la.  The number of strands to be bundled and stranded is, of course, not limited to three. The bundle la is  received  by  the  stranding  device  5 10 arranged downstream from nipple or head 4. The device 5 is essentially constructed as a capstan being comprised of two endless chains 6 carrying clamping jaws or brackets 7, which cooperate in pairs, one jaw or bracket per chain of each pair.  The brackets grip the 15 bundle positively in form closed engagement.  The chains are driven by drives 8 and synchronized as to speed and phase by a linking belt, sprocket belt 16 or the like.  Reference numeral 9 refers to reversing pulleys or sprockets for each chain. 20 The capstan chains with brackets, drives and reversing rolls 9 are mounted in a frame 10 having hollow shafts which are joumalled in bearings 11 and 12, to permit rotation of the frame so that the capstan 5 revolves about the stranding axis.  The frame is driven by 25 a motor 14 such as a d.c. four quadrant motor, in that the motor drives a transn- iission gear 15 which, in tum, drives a belt, sprocket chain or the like, 13. The motor 14 is a reversible one (or the transmission is constructed for reversing the direction of rotation), 30 reversal being provided by a controller 14a, so that the frame and stranding device 5 can rotate in one or the opposite direction as indicated by the two arrows.  If stranding is carried out without reversing pitch, a reversibility of the drive is not required.  The distan ce 35 between the capstan and the head 4 may be adjustable to vary the lay, i@e. the number of twists per unit length.  The drives 8 could be disposed outside of the frame so that they do not revolve with the capstan proper.  A transmission is needed in this case to drive the chains 40 whereby the chain speed is the composite of drive speed and revolving speed. A second stranding head or nipple 17 is disposed downstream from device 5, behind which a ribbon 19 is helically wrapped around the stranded conductors. A 45 spool 18 from which, the ribbon 19 is paid, revolves about the stranding axis thereby rotating about its own axis as the ribbon is paid and wr4pped. The device operates as follows..The strands 1 may be regular solid conductors or stranded conductors to 50 obtain a higher order of stranding.  The invention is of particular interest for stranding particularly profiled conductors having, e.g.- pie-shaped cross-sections. The conductors may be insulated so that gentle handling of the insiilation envelope is required.  In either case, the 55 conductors 1 are, reeled off the drums or spools 2 and pass through straightening rollers 3. The rollers in each set are staggeredly arranged and they are oriented to straighten the conductors and to orient, for example, each sector in accordance with the position it is to have 60 in the bundle.  Straightening is particularly important if the strands have solid and rather large cross-sections.  Sector or pie-shaped conductors are notably stiff and require straightening. The bundle  la  results  from  the  combining  action  by 65 nipple or head 4. The two chains 6 run continuously so that each bracket is moved in a position vis-a-vis another one and together they grip the incoming bundle la, hold it for a certain distance, whereafter the brackets\n\n4,311,  002 5 open.  The brackets grip the bundle positively without permitting relative movement between the strandg and the bracket elements.  The surface area of contact must be chosen to be sufficiently large so that the per unit surface area clamping force is not too high.  This aspect is also important for ensuring th@,t only very low torsion forces act on the insulation.  It must not be forgotten that stranding does require the exertion of twisting torsion upon the conductors undemeath the insulation. During the period of time in which the stock is held by brackets, the frame 10 rbtates about the stranding axis so that each bracket pair closes about the bundle in a disposition which is angularly offset by a particular amount with regard to the disposition the frame had when the preceding bracket pair closed about the bandle. The number of twists as so imparted depends on the dimensions of the apparatus, on the relative speed of stock and the revolving capstan,' and on the distance between capstan 5 and head 4. As stated, one may strand the conductors in the so-called ceander pattern, in which each twisting turn covors at the most 360' in between reversals. Reversals after 18b' are quite common in other cases.  Less frequent reversals result in the so-called S-Z pattern, wherein the conductors are stranded about each othe@ in several helical turns, before the sense of winding and direction of stranding is changed for plural turns in the opposite'direction, etc.  The pattem actually results by chosing the instants and phases of reversing the capsian revolution. The brackets hold the conductor bundle immobile in the capstan, except for the movement forced upon the bundle at the point of engagement with the brackets on account of the translatory and revolving movement of the brackets.  Therefore, upon bding released, but while traveling from the point of release to nipple 17, an additional twist is provided because the bundle upstream is held by a still closed pair of brackdts which continues to revolve.  One obtains, therefore, a double-twist stranding. The wrapping device 18 provides the stranded stock with a helical winding for preventing the stranded conductors from untwisting.  The stranded and wrapped stock can be processed further downstream, for example, by extruding insulation on and around the assembly or by jacketing -the assembly 'otherwise. Particularly, the stranded stock can be used as a core, about which are twisted further strands. It should be noted that the effective stranding length is a continuously variable one.  A certaih length of the bundle la is twisted in any instant by cooperation of head 4 and a pair of brackets 7. When the tWo bra6kets of a pair close about the bundle, they have a particular distance from the nipple 4 and begin to twist the portion of the bundle between brackets and head as a whole, while receding from the nipple on account of the chain movement.  The effective stranding length is,- therefore, continously lenghtened until the next pair grips the bundle etc.  In cases, this variation in effective length may be deemed undesirable. FIG. 2 illustrates an example in which the effective stranding length remains (or can be made to remain) constant throughout.  The equipment includes storage spools 23 for strands 24, sets 25 of straightening rollers and a nipple 26 which, however, does not participate in the stranding action but merely forms a bundle 27. The bundle is gripped by the brackets of a first capstan 21 constructed as the capstan 5 in FIG. 1 but without rotating about the bundle axis.  The@individual brackets serve 6 as stranding nipples, particularly the downstream most pair of brackets of capstan 21 which is still closed (the one next downstream has already opened wi serve as stranding nipple, holding the bundle azimuthally immo- bile but feeding a free bundle portion beyond to the stranding capstan. Reference numeral 20 refers to the stranding capstan assembly being similar to the one identified by numeral 5 in FIG. 1, and including a frame and revolving drive 10 means as described' The two arrows indicate the fact that this capstan 20 revolves about the bundle and stranding axis.  The chain drives for the capstans 20 and 21 are synchronized in speed and phase; more about that aspect below.  Particularly, a bracket pair of capstan 20 15 closes on the bundle exactly when a bracket pair of capstan 21 releases the bundle. This way, the length of stock between the Izist one of brackets on non-revolving capstan 21, being the stranding nipple and holding the stock, and the first pair of brackets of revolving capstan 20 20 holding the stock, remains exactly the same even though the brackets participating change. Another non-revolving capstan 22 is provided downstream from capstan 20, and is synchronized thereto in exactly the same fashion, so that the brackets of capstan 25 22 serve as downstream stranding nipples.  A stationary downstream nipple is not used here; rather the device 28 wraps the stranded stock right after being released from brackets of capstan 22.  The capst'an 22 operates, in fact, also ag a straightening device, preventing the stranded 30 stock from untwisting, so that wrapping may not be needed. The devic6 22 functions additionally as capstan proper, pulling the stock through the equipment and the individual strands off the storage spools 23, though load 35 relief may be provided for the latter.  The capstans 20 and 21 may, in fact, be dragged along by the stock in engagement therewith, though load relief may also be provided here.  As long as the brackets close positively around the stock, slippage will not occur, and the brack- 40 ets open and close in synchronism from capstan to capstan.  It may, however, be more advantageous to drive additionally all of the capstans.  In that particular case the following aspect has to be observed. Due  to  the  stranding  operation,  the  effective   transla- 45 tory sp6ed of the stock downstream is reduced from capstan to capstan.  This is apparent if one realizes that the twisting amounts to a helical coiling of each individual strand which reduces its effective length in the direction of propagation. Thus, the linear speed of capstan 50 21 is@larger than the linear speed of capstan 20, and the lineat speed of the latter is still larger than the linear speed of capstan 22, assuming, of course, that the clampitig brackets always positively engage the stock without relative movement. The speed relation is established 55 automatically if the devices 20 and 21 are passive, do iiot actively drive th6 stock, @but are being drive by it. It is of advantage, however, to drive all capstans, and for reasons of achieving synchronism, constant uniform speed should be imparted on all of them. Accordingly, 60 compensation of the difference in speed of the stock along the line must be provided for.  In particular, one must provide for relative movement between the chain drives of the capstans 20, 21 and their clamping brackets. 65 FIG. 4 illustrates a clamping bracket of the type used on the capstans 20, 21.  Each clamping bracket 40 is slidably mounted on a carriage 41.  The carriage 41 is secured to an endless chain of the respective capstan,\n\n7                            4,311,002 the   bracket   40   as   positively   engaging    the    bundle or stranded  stock,  is   shifted   gradually   on   the   carriage 41. The  bracket  40   is   spring   biased,   and   the   said shifting occurs  against   the   force   of   the   spring   42.   The spring returns  the  bracket  on  the  carriage   to   the   other position 5 upon disengagement from the stock. The    displacement    a    bracket    undergoes    is directly proportional  to  the  length   of   the   path   a   bracket trans- lates   in   engagement   with   the   stock   (and    the opposite bracket   on   the   companion    chain).    The    displacement is 10 further.  proportional  to  (f-1)   wherein   f   is   the stranding factor.   Actually,   the   displacement   is   the   direct product of  these   two   factors.   Please   note   that   stranding occurs . upstream and downstream from capstan 20. The   displacement   of   the   several    brackets,    i.e. the 15 force   needed   to   effect   the   displacement,   must   be pro- duced   by   the   downstream   most   capstan.   But   all capstans are  driven  so  that   they   all   participate   in   dragging and pulling  the  stock   through   the   device;   the   capstan 22   ' merely    provides    additionally     the     displacement forces 20 needed  to  move   all   engaging   brackets   40   of capstans   20 and 21 against the respective springs. FIG.   3   illustrates   how   the   temporary    storage capac- ity   of   the   stranding   equipment   itself   can   be increased without   interfering    with    the    features    outlined above. 25 The   elements   30,   31,   32   and   33,    respectively, corre- spond  to  parts  1,  2,  3  and  4  in  FIG.  1,  producing  a bun- dle   34   to   be   stranded   downstream   from   nipple   33. The bundle  is  gripped   by   a   first   revolving   capstan   35 con- structed    and    operated    like    capstan    5    above. Down- 30 stream   from   capstan   35    is    provided    another revolving capstan   36.   Another   strand   nipple   37   is   provided down- stream   from   capstan   36.   Stranding    is    provided between the   nipples   and   the   capstans   which   run   in synchronism. Of    course,    they    should    also    reverse simoultaneously. 35 Reference  numeral  38  refers   again   to   a   device   for wrap- ping the stranded stock. This     machine,     employing     two     revolving capstans, establishes  a  variable  storage  capacity   for   the   stock. The capacity   can   be   varied   by   varying   the   distance between 40 the  two  capstans   35,   36.   This   feature,   in   tum, permits the  selection  of  the  lays  number  of   the   stranding   in one direction.  The   capstan   36   does   not   have   to revolve   at the   same   speed   as   capstan   35.   Moreover,   the   two cap- stans  35  and  36   may   actually   revolve   in   opposite direc- 45 tions. A   specific   improvement   will    now    be    explained with reference   to    FIG.    5    which    improvement    is basically applicable  to  all   of   the   serveral   examples   above. Plural strand   elements   50   are   paid   from   a    stationary supply 50 (rotating  drums);   these   elements   may   be   of   large, e.g., section-shaped   cross   sections,   or   each   one   of   the ele- ments   50   may    be    a    conductor    composed    of stranded filaments,   i.e.   stranded.wire   bundels.    Guide    sheaves or pulleys  52  run  these  strand   elements   to   a   stranding head 55 53  defining  the  first  stranding  point.  Rolls   or pulleys   52 may    also    be    provided    here    for     stretching and/or straightening   of   the   elements   50   prior   to   entering the first stranding head. Head  53   combines   the   strandirig   elements   in   a bundle 60 and   feeds   them   to   a   stranding   device   54   placed down- stream   from   this   first   stranding    point.    The stranding device   54   is    comprised    of    two-serial, caterpillar-type capstans  55  and   56,   any   of   which   is   shown   in greater detail  in  FIG.  6  and  will  be   described   more   fully below. 65 Suffice  it  to  say  that  each   capstan   includes   bracket ele- ments  which  grip  the   bundle   67,   hold   if   for   a certain storage path length and release it again. As described earlier and quite analogously, each of the capstans 55 and 56 revolve on and about the longitudinal stranding axis, either continuously, or in alternating directions, e,g., for obtaining S-Z type stranding.  For example, the direction may be changed whenever the bundle has traversed a distance equal to the storage length in the capstans 55 and 56.  The twisted bundle as held by the capstans 55 and 56 is not twisted or stranded further while being so held.  This path and storage length for the bundle extends specifically from the point where a bracket of capstan 55 grips the arringing bundle up to the point where a bracket of capstand 56 releases the bundle.  This storage Iength is denoted lp. The stranded bundle as released by capstan 56 is fed through a wrapping device 57 which wraps a holding and retaining ribbon around the stranded bundle.  Moreover, the device 57 may include an extruder, extruding a suitable synthetic around it and into the gaps between the stranded elements to, thereby, embed the stranded elements into an inner core.  The wrapping will occur upstream from the extrusion. In addition, the bundle as so jacketed may be suitably marked for id entification. A second stranding point, 58, is established downstream from device 57.  This particular stranding point is also a caterpillar-like capstan, except that it does not revolve on the stranding axis.  Thus, the bundle receives an additional twist in the space between revolving capstan 56 and nonrevolving capstan 58.  Neither extrusion nor ribbon wrapping will interfer with the additional stranding. The stranded stock is redirected by a sheave 60 and wound onto a drum 61.  This is the simplest operation after the stranding.  However, additional working steps may be performed on the stranded stock.  For example, an armoring may be provided around the stranded bundle, and an outer jacket may be extruded, this jacket to be made of a wear-resistant material such as PVC The point in which a bracket of capstan 58 engages the bundle 59 is spaced from the point in which a bracket or capstan 56 releases the bundle by a distance equal to 2nl, wherein n is a positive integer and 1, is the distance the bundle travels in between reversals for stranding in an S-Z pattern.  The storage length lp in the capstan may be equal to that length 1,.  The total distance chosen, as between capstan 56 and the second stranding point, depends upon particulars of the desired stranding.  As stated above, the stranding 59 will have zero waviness, or remains very small, for the following reason. In the example of FIG. 5, it is assumed that the first stranding point is placed rather closely to the capstan 55.  Moreover, it is assumed that capstans 55 and 56 revolve in one particular direction for the time it takes a bundle increment to traverse the storage path length Lp, whereupon the direction of rotation is reversed for a like period.  This way, one alternates between S-twists and Z-twists and 1,=IP. In this specific example, the second stranding point (entrance to capstan 58) is located at a distance from capstan 56 which is equal to twice the reversal length.  Thus, in any instance equal quantities of S-twists and Z-twists (i.e., one each) is located in this second stranding path.  Mathematically speaking, the total twist on the path from capstan to second stranding point is always zero.  The FIG. 1 shows an instant in which the device just about reverses from Z-twist to S-twist.  Accordingly, the Z-twist portion that now will gradually enter and pass through the second stranding point (58) will be gradually untwisted, but additional S-twist is\n\n9                            4,311,002 10 imparted upon the S-twist portion.  That additional Stwist is removed again from the bundle when and after the direction of revolving the capstans 55 and 56 is again reversed toward producing a Z-twist.  As a conseqtience, the objectional waviness is, in fact, reduced. it was found moreover that waviness is reduced more for still longer lengths, i.e., spacing between revolving capstan and downstream stranding point. However, the spacing must be an even multiple of the distance any bundle increment travels in between reversals. FIG. 6 shows one of the capstans, which may be any of the capstans 5, 20, 21, 22, 35, and 36, bearing in mind, however, that in some instance bracket mounts as shown in FIG. 4 should be used to compensate differences in speed of the stock along the propagation path.  On the other hand, the speed of the chain drive motors such as 66 and 66a could be controlled accordingly.  T'his particular capstan is basically comprised of two chains, 64 and 64a respectively looped around pairs of sprockets, 65 and 65a. One sprocket in each instance is driven by a drive, 66 and 66a, respectively.  Bracket and jaw elements 62 are mounted to these chains in a particular distance from each other, but in such a way that these jaw and bracket elements face each other for gripping cooperation of the bundle.  The bracket elements have detents 70, and elastic pins 68 are inserted in and affixed to the detents of the bracket elements on chain 64. The pins are preferably made of rigid highdensity polyethylene. T-he elements 62 are fastened to the chains by means of suitable fasteners 63.  These fasteners are centered with respect to pins 68, and vice versa; also, the fasteners on chain 64 are centered to the detents 70 of the respective jaw and bracket elements 62 on that chain.  Thus, the pins 68 will engage oppositely located detents in gear-like fashion (see center line 71 in one instance).  Also, the centers of the pins 68 run along a particular path 69 which merges with the corresponding path 69a of the other chain. Those are the paths of contact of the capstan with the stranded bundle 67 runs in these merging path portions and is positively engaged by the pins 68 and the respective opposing detents 70, matching the pins' contours.  Engagement is analogous to holding by a chuck and resemble a gear engagement. The positive engagement of pins and detents of opposing bracket elements ensures automatically uniformity in the movement of both chains; an equalization ,-ear, or the like, is not needed.  Of course, the two drives should be adjusted toward uniform motion as accurately as possible.  This arrangement is particularly advantageous in the case of a revolving capstan (55, 56, 35, 36, etc.) as it perinits both chain systems of the capstan to have similar mass.  Also, the armatures of the two drives 66 and 66a can be electrically connected in parallel so that the motors can be controlled, e.g., electronically in a very simple fashion. The invention is not limited to the embodiments described above; but all changes and modifications thereof, not constituting departures from the spirit and scope of the invention, are intended to be included. We claim: 1.  Apparatus for stranding elongated stock, compris- ing: a plurality of stationarily mounted, reelable support spools storing individual strands to be unwound from the spools upon rotation thereof- means for combining the strands as unwound in a bundle and for providing a first stranding point through which the bundle passes; means disposed for positively holding the strands of the bundle as a bundle for a particular travel path as the bundle moves along the travel path together and in engagement with the means for holding, said means for holding stranding the strands together about an axis, but outside the travel path between 10 the means for positively holding and the first stranding point, while stranding does not occur on and along the particular travel path due to said positive holding; means   defining   a   second    stranding    point    through 15 which the stranded bundle passes by operation of rotation as between the means for i)ositively holding and the second stranding point; and means for moving the bundle through the first and second stranding points, the means for holding 20 engaging the bundle as so moved through the travel path without stranding a portion of the bundle while on the travel path due to the positive holding. 2.  Apparatus as in claim 11 in which the holdiilg 25 means revolves about said axis. 3.  Apparatus as in claim 2, in which said holding means revolves in alternating directions. 4.  Apparatus as in claim 2, said means for holding including a plurality of clamping brackets running in 30 axial direction and gripping said bundles over said travel path. S.  Apparatus as in claim 4, the means for holding including pairs of clamping brackets holding the bund e from opposite sides. 35 6. Apparatus as in claim 5, the brackets being mounted on two endless belt-like carriers. 7. Apparatus as in claim 1, and including means for holding the stranded bundle beyond said second strand- ing point. 40 8. Apparatus as in claim 1, and including means for straightening each said strands as unwound, upstream from said means for combining. . 9. Apparatus as in claim 1, said means for holding including a pair of caterpillar-like lements with clamp- 45 ing brackets establishing a capstan for holding said bundle over said travel path; frame for said capstan journalled for rotation on said axis; and means for driving said frame. 50 10.  Apparatus  as  in  claim   9,   said   second  - stranding point being established by a second capstan. 11.  Apparatus as in claim 9 or 10, and including a further capstan between said means for combining and said rotating capstan. 55 12. Apparatus for stranding elongated stock, compris- ing: a plurality of spools, storing individual strands to be unwound from the spools; means for combining the strands, as unwound from 60 the spools, in a bundle and providing a first, relatively nonrotating stranding point; capstan means; including plural clamping bracket means, each engaging the bundle positively and moving with the bundle for a particular travel path 65 before disengaging from the moving bundle; means defining a second nonrotating stranding point, said capstan means being dispose,d between the first and second stranding point;\n\n11                         4,311,002 12 means for causing the capstan means to rotate to obtain stranding of the bundle as between the first stranding point and a point of engagement of the bundle with one of the clamping means, and to obtain additional stranding as between a point of engagement of the bundle with another one of the clamping means, just prior to release of the bundle by the latter clamping means, and the second stranding point, there being no stranding of a portion of the bundle as between the one clamping means and the other clamping means on accoutit of said positive engaging of the bundle; and means for moving the bundle through the stranding points and the travel path  as the bracket means engage the moving bundle. 13. Apparatus as in claim 12, said capstan means including two capstans arranged in series, each defining a portion of the travel path. 14. Apparatus as in claim 13, said capstans rotating in opposite directions to obtain additional stranding of the bundle extending between them. 15. In an apparatus for stranding elongated stock paid individually from spools toward a head combining them into a bundle, there being means for pulling the stock through the head and moving the stock axially, the combination comprising: a capstan, including a plurality of bracket means being- independently driven and provided for engaging individually the bundle as moved by the means for pulling, each bracket means upon engaging the bundle holding the bundle while traveling in positive engagement therewith for a particular travel path, said bracket means disengaging from the bundle as the bundle continues beyond said travel path; means for rotating the capstan about an axis along the travel path so that a portion of the bundle, as held by some of the bracket means in any instant in positive engagement therewith, is rotated as a whole without being stranded; and means being relaiively stationary as far as capstan rotation is concemed and being placed for engagement with the bundle at a point displaced from the capstan so that the bundle is stranded as between the last-mentioned means and the capstan. 16. In an apparatus for stranding elongated stock paid individually from spools, there being means for pulling the stock and moving the stock in axial direction, the combination comprising: a stationary head, said means for pulling, pulling the stock through the head and combining the stock into a bundle; capstan including a pair of bracket means, said bracket means being independentl,Y driven for engaging individually the bundle as moved by the means for pulling, each bracket means upon engag- ing the bundle holding the bundle while traveling in positive engagement therewith for a particular travel path, said bracket means disengaging from the bundle as the bundle continues beyond said travel path; 10 means for rotating the capstan about an axis along the travel path so that a portion of the bundle, as held by the bracket means, in any instant in positive engagement therewith, is rotated as a whole without being stranded; and 15 said capstan being disposed for said engagement with the bundle at a point displaced from said head so that the bundle is stranded as between the head and the capstan. 17. In an apparatus, as in claim 15, said relatively 20 stationary means being a second capstan which holds the bundle for the length of a travel path and moves therewith, so that stranding occurs as between the two capstans and different portions of the bundle are held by the two capstans. 25 18. In an apparatus as in claim 15, or 17, including further, relatively stationary means, the two relatively stationary means being placed- respectively upstream and downstream to the capstan, to obtain doublestranding action, once upstreamed and additionally down- 30 stream from the capstan. 19. Apparatus as in claim 2, 3, or 12, wherein the means for holding or the capstan means includes reversible drive means and control means for the reversible drive means, the second stranding point being spaced 35 from the means for holding or the capstan means by a distance being an even- numbered multiple of the travel path of the bundle in between reversals by the control means. 20. Apparatus as in claim I or 12, including means for 40 wrapping around a ribbon, disposed upstream from the second stranding point. , 21. Apparatus as in claims 4, 12, 15, or 16, said bracket means being mounted on the two juxtaposed endless driven chains, each of the clamping bracket means in- 45 Iuding a pair of clamping brackets, one bracket per chain, for gripping the bundle. 22' Apparatus as in claim 21, wherein one of the brackets of a pair has a detent, the other one a matching pin for holding the bundle against the detent. 50 23. Apparatus as in claim I or 12, including means for extruding a filler jacket around and into the stranded bundle, and being disposed upstream from the second stranding point. 55 60 65"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 10,
      "claims_start": 9,
      "description_end": 9,
      "description_start": 5,
      "drawings_end": 4,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 10,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 9,
      "specification_start": 5,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 25772776,
    "guid": "US-4311002-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/04/311/002",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "B65H",
        "intl_subclass": "51/14",
        "version": "2006-01-01"
      },
      {
        "intl_class": "B65H",
        "intl_subclass": "51/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01B",
        "intl_subclass": "13/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "D07B",
        "intl_subclass": "3/00",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01B",
        "intl_subclass": "13/04",
        "version": "2006-01-01"
      },
      {
        "intl_class": "H01B",
        "intl_subclass": "13/02",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "B65H51/14"
    ],
    "inventors": [
      {
        "name": "HOFFMANN ERNST"
      },
      {
        "name": "SCHATZ FRIEDRICH"
      },
      {
        "name": "UNGER ARTUR"
      }
    ],
    "inventors_short": "HOFFMANN ERNST et al.",
    "patent_title": "Forming stranded stock",
    "publication_date": "1982-01-19",
    "publication_number": "4311002",
    "type": "USOCR",
    "us_class_current": [
      "57/294"
    ]
  },
  {
    "applicants": [
      {
        "city": "N/A",
        "country": "US",
        "state": "N/A",
        "zip_code": "N/A"
      }
    ],
    "application_type": "utility",
    "composite_id": "22769394!MXOC-US-03857398",
    "cpc_inventive": [
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/3956",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/365",
        "version": "2013-01-01"
      },
      {
        "cpc_class": "A61N",
        "cpc_subclass": "1/056",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "claim_statement": "What is claimed is:",
      "claims": "1. Apparatus for electrically stimulating the heart comprising: electrode means; sensing means having an input connected to said electrode means for sensing cardiac electrical signals; pulse rate detector means for detecting the average rate of occurrence of the QRS complexes of said cardiac electrical signals over a predetermined time period; pulse duration detector means for detecting the Q to S period of each said QRS complex in said cardiac electrical signal, said pulse duration detector 1 5 means being connected to said pulse rate detector means to permit rate detection for pulses whose duration is greater than a predetermined value; threshold logic means connected to said pulse rate 20 detector means and said pulse duration detector means for providing an output pulse when the average rate of occurrence of said QRS complexes exceeds a predetermined value and a predetermined number of said Q to S periods exceed a predetermined time period; and defibrillator means connected to the output of said logic means for generating a defibrillating pulse on said electrode means in response to receiving a pulse from said threshold logic means.\n\n2. Apparatus according toclaim 1and further including disable means connected between the output of said defibrillator means and said sensing means for disabling said sensing means when said defibrillating pulse is generated.\n\n3. Apparatus according toclaim 1and further including pacer means having the input thereof connected to said sensing means for generating pacing pulses on said electrode means when the rate of occurrence of said QRS complexes falls below a predetermined level.\n\n4. Apparatus according toclaim 3and further including disable means connected from the outputs of said defibrillator means and said pacer means to said sensing means for disabling said sensing means when either one of said defibrillating pulses or one of said pacing pulses is generated.\n\n5. Apparatus according toclaim 4and wherein said electrode means includes first, second and third spaced electrodes; said input of said sensing means connected to said first and second electrodes; the output of said pacer means connected to said first and second electrodes; and the output of said defibrillator means connected to said first and third electrodes.\n\n6. Apparatus according toclaim 5further including a catheter; said first electrode mounted at the distal end of said catheter; and said second and third electrodes spaced in succession along said catheter. 60 7. Apparatus for electrically stimulating the heart 60con~iprising: electrode ileans; sensing iieans having the input thereof connected to said electrode means for sensing cardiac electrical signals; amplitude detector means connected to said sensing means for passing pulses greater than a predeter-miined aiiplitude; pulse duration detector means connected to said sensing means for detecting the duration of the pulses in said cardiac electrical signal; pulse rate detector means connected to said amplitude detector means and said pulse duration detector means for detecting the average rate of occurrence of those pulses passed by said amplitude detector means which have a pulse duration greater than a predetermined value, as measured by said duration detector means; threshold logic means, including timing means controlled by said pulse duration detector means, connected to the outputs of said pulse rate detector means for generating an output pulse when said average rate exceeds a predetermined threshold in a given time as measured by said timing means; and defibrillator means connected to the output of said logic means for generating a defibrillating pulse on said electrode means in response to receiving a pulse from said threshold logic means.\n\n8. Apparatus according to claim 7 and further includ-ng disable means connected between the output of said defibrillator means and said sensing means for disabling said sensing means when said defibrillating pulse is generated.\n\n9. Apparatus according to claim 7 and further including pacer means having the input thereof connected to the output of said amplitude detector means for generating pacing pulses on said electrode means when the pulse rate at the output of said amplitude detector means falls below a predetermined value. 10. Apparatus according toclaim 9and further including disable means connected from the outputs of said defibrillator means and said pacer means when either one of said defibrillating pulses or one of said pacing pulses is generated.\n\n11. Apparatus according to claim 10 and wherein said electrode means includes first, second and third spaced electrodes; said input of said sensing means connected to said first and second electrodes; the output of said pacer means connected to said first and second electrodes; and the output of said defibrillator means connected to said first and third electrodes.\n\n12. Apparatus according toclaim 11further including a catheter said first electrode mounted at the distal end of said catheter; and said second and third electrodes spaced in succession along said catheter.",
      "description": "(1) United States Patent [ 19j Rubin t[11 3,857,398 [45] Dec. 31, 1974 [541 ELECTRICAL CARDIAC DEFIBRILLATOR [76] Inventors: Leo Rubin, 30 1-17 Spring St., Red Bank_ N.J_07701 _ __ [22] Filed: Dec. 13, 1971 [21]j AppI. No.: 207,148 [52] U.S. Cl. ... 128/419 D, 128/2.06 A, 128/2.06 F, 128/419 P [51 ] Int. CIl................................... A6ln 1/36 [ 58 ] Field of Search ....128/419 D, 419 P, 419 R, 128/421, 422, 2.06 A, 2.06 F, 2.06 R References Cited\n\nUNITED STATES PATENTS\n\n(2) 3,547,108 3,528,428 3,614,955 3,144,019 3,352,300 3,236,239 3,460,542 12/1970 9/1970 10/1971 8/1964 11/1967 2/1966 8/1969 Seiffert.................... 128/419 D Berkovits .................. 128/419 P Mirowski.................. 128/419 D Haber..................... 128/2.06 A Rose ...................... 128/2.06 A Berkovits.................. 128/419 D Gemmer................... 128/419 P\n\nOTHER PUBLICATIONS\n\n(3) Stratbrucker et al., \"Rocky Mountain Engineering So- ciety,\" 1965, pp. 57-61.\n\n(4) _ 12\\ Primary Examiner-William E. Kamm Attorney, Agent, or Firm-Harry M. Saragovitz et al.\n\n(5) [57]\n\nABSTRACT\n\n(6) The combination of a defibrillator, demand pacer, analyzer-control device, and a set of electrodes for detecting and analyzing electrical activity of the heart and for electrically stimulating the heart. The cardiac electrical pulses detected by a first pair of electrodes_ placed in the heart are analyzed by pulse rate and pulse duration detectors, to determine the average pulse rate and pulse width over some predetermined period. A threshold logic circuit connected to the outputs of the pulse rate and duration detectors produces an output pulse when the pulse rate of the heart exceeds a predetermined average rate and when simultaneously each of the pulses in the period has a pulse duration longer than a given period. This output pulse energizes the defibrillator which in turn applies a defibrillating pulse to the heart via a second pair of electrodes. A demand pacer, having the input thereof connected to the analyzer-control, receives the heart pulses and on demand produces pulses at the first pair of electrodes to stimulate the heart when the heart rate falls below a predetermined value.\n\n(7) 12_Claims, 5 Drawing Figures XL___-\n\nJ\n\n(8) [56]\n\nELECTRICAL CARDIAC DEFIBRILLATOR\n\n(9) The invention described herein may be manufac- tured, used and licensed by or for the Government for governmental purposes without the payment to us of any royalty thereon.\n\nBACKGROUND OF THE INVENTION\n\n(10) The present invention relates to cardiac stimulators and more particularly to therapeutic instruments which electrically stimulate the heart transvenously.\n\n(11) In the field of cardiology it has been the general practice to employ electrical stimuli to the heart for the treatment of certain coronary heart diseases. The defibrillator has found widespread use in the treatment of those coronary heart diseases which culminate in ventricular tachycardia or ventricular fibrillation. When rapid and irregular heart beats are detected, therapeutic defibrillation is instituted by applying a relatively high energy electrical pulse to the heart which produces a synchronization of the fibers of the heart and a subsequent return to a normal rhythm. In the event that spontaneous rhythm does not occur subsequent to defibrillation, a pacer is then used to stimulate the heart at a normal rhythm.\n\n(12) The above procedures are normally carried out in intensive care units which are equipped with heart monitoring equipment. Operation of the monitoring equipment usually requires the constant attention of specially trained personnel who are capable of making quick judgement decisions. For example, some cardiac monitoring equipments have warning devices such as lights or buzzers which are energized when a patient's heart beat exceeds a predetermined rate. The actual heart beat or electrical activity is also displayed on a CRT. When the warning device is energized, a judgement decision is made by the attendant after studying the CRT display as to whether or not defibrillation is necessary or whether a physician should be called to make an additional analysis. If defibrillation is judged to be in order, the attendant then places the defibrillator electrodes on the patient and energizes the defibrillator.\n\n(13) It has been determined that in most cases, the sooner defibrillation is instituted, the smaller will be the amount of electrical energy required to accomplish defibrillation. It has also been shown that myocardial damage will be a minimum when the defibrillating energy is a minimum. Therefore attempts have been made to completely automate the above defibrillating process so that the time lost in the defibrillation procedure is a minimum, thereby minimizing myocardial damage.\n\n(14) Of course, a completely automatic defibrillator has other advantages. For example, the attending physicians may be relieved of many tasks and in some cases, the patient can be removed from the intensive care unit. Also, an automatic defibrillator and demand pacer coniibination nmay be imiplanted in the body of the patient, who can then actually leave the hospital and carry on mtany of his normal daily functions.\n\nSUMMARY OF THIE INVENTLION\n\n(15) I he general object of this invention is to provide a therapeutic instrum ent which automatically detects and analyzes the electrical activity of the heart and responds to certain predetermrined parameters of the QRS complexes by electrically stimulating the heart so that abnormal' heart beats can be corrected. To attain this, the present invention contemplates a combination of a defibrillator, pacer and a unique heart beat analyz- er-control device which analyzes automatically the QRS complexes of a heart beat and, when necessary, applies proper defibrillator or pacer pulses to the heart.\n\nDETAILED DESCRIPTION OF THE DRAWINGS\n\n(16) Other objects and features of the invention will becomes apparent to those skilled in the art as the disclosure is made in the following description of a preferred embodiment of the invention as illustrated in the ac- IS5 companying sheets of drawings in which:\n\n(17) FIG. 1 is a block diagram of a preferred embodiment of the invention;\n\n(18) FIG. 2 illustrates a diagrammatic view of the appara- 20tus in situ;\n\n(19) 20FIG. 3 shows a sectional view of a portion of the device shown in FIG. 2; and FIGS. 4 and 5 are waveforms useful in describing the invention.\n\nDETAILED DESCRIPTION OF THE PREFERRED\n\nEMBODIMENT\n\n(20) Referring now to the drawing, there is shown in FIG.\n\n(21) 1 an automatic cardiac stimulator 9 having a defibrillator 10, a demand pacer 11, and an analyzer-control 12.\n\n(22) In general, the analyzer-control 12 will sense and analyze voltages generated in the heart via terminals 14 and 15. If the sensed voltages have predetermined characteristics, as will be described later, the analyzer- control 12 will energize defibrillator 10 via line 18.\n\n(23) The demand pacer 11 and the defibrillator 10 are well known devices and will not be described in detail.\n\n(24) Briefly, the demand pacer 11 is simply a heart stimula- for which may include a timing device and a pulse generating circuit. If after some predetermined time, say one second, a heart pulse has not been sensed by analyzer-control 12, or if bradyarrythmia occurs (viz.\n\n(25) pulse rate less than 50 beats/minute) then the pulse generating circuit of pacer 11 would transmit a pulse or pulses to the heart via terminal 15 to stimulate the heart.\n\n(26) Likewise, the defibrillator 10 includes a pulse circuit which transmits a relatively strong pulse to the heart via terminal 16 for therapeutically shocking the heart after some predetermined condition in the heart beat has been sensed by analyzer-control 12.\n\n(27) When in use, the terminals 14, 15 and 16 are connected to three electrodes 30, 31 and 32 respectively which are mounted on the catheter 35 shown in FIG.\n\n(28) 3. As shown in FIG. 2, the automatic cardiac stimulator 9 may be implanted in a patient with the catheter 35 extending into the heart via the subclavian vein and the superior vena cava. \"t he ground or reference electrode 30, mounted on the tip of catheter 35, is positioned at the apex of the right ventricle. Electrode 31 is positioned in the right ventricle and electrode 32 is positioned in the right atriuni;. I he pacing pulses fromi de- 65imand pacer 11 are applied to the right ventricle across 65electrodes 30 and 31. TChe defibrillating pulses from<\n\n(29) defibrillator 10 are applied transvenously across electrodes 30 and 32. \"1\"he analyzer-control 12 senses the voltages in the right ventricle which appear across electrodes 30 and 31.\n\n(30) The detailed description and operation of the automatic cardiac stimulator 9 will now be presented with particular reference to FIGS. 1, 4 and 5. In both FIGS.\n\n(31) 4 and 5, the letters AK identify waveforms which appear at similarly identified output points in the block diagram of FIG. 1. The waveforms in FIG. 4 represent the typical response of analyzer-control 12 to a heart undergoing ventricular tachycardia or ventricular fibrillation with a subsequent return to a normal rhythm.\n\n(32) It is one of the functions of the analyzer-control 12 to determine the rate of occurrence of the QRS complexes and the duration of the Q to S periods as they appear in waveforms A. If the average rate of occurrence of the QRS complexes exceeds a predetermined value over a given time interval, for example 150 per minute for 10 seconds, and if the duration of each of the 0 to S periods in the given time interval exceeds a given period, for example 120 milliseconds, then a pulse will be generated by analyzer-control 1l2 on line 183 to energize defibrillator 10.\n\n(33) In FIG. 4, waveform A represents a typical voltage wave generated in a heart beating with a normal rhythm. The QRS complex is shown as a sharp impulse having a relatively short Q to S period followed by a T wave having an amplitude substantially less than the R value. In FIG. 5, the first part of waveform A, i.e., the portion to the left of the line X, represents a typical voltage wave generated in a heart undergoing ventricular tachycardia or ventricular fibrillation. The QRS complexes occurring to the left of line X occur at a relatively high rate and the Q to S periods are substantially longer than in the normal case. Although the QRS complexes have been flattened, the R amplitude is still greater than the amplitude of the T wave.\n\n(34) The portion of waveform A to the right of line X in FIG. 5 again represents a heart beating with a normal rhythm. As will be seen later, it is assumed that the heart is defibrillated and resumes normal rhythm at the time represented by line X.\n\n(35) As an aid in visualizing the operation, it may be assumed that each of the vertical lines in FIGS. 4 and 5 are spaced one second apart and that the distance from the left side of FIG. 5 to line X covers a period of ten seconds.\n\n(36) Analyzer-control 12 includes a transmission gate 40 having the transmission input thereof connected to terminal 15. Two enable inputs to gate 40 are connected to the outputs of pacer 11 and defibrillator 10 via inverters. Therefore, if neither the pacer 11 nor defibrillator 10 are operating, then gate 40 is in the enable state and the voltages on terminal 15 are transmitted to the inputs of pulse shapers 41 and 42. The transmission gate 40 may also have appropriate circuitry for conditioning the input signals A with proper amplification and filtering. For examiple, in some cases, filtering of induced voltages caused by devices such as electric razors, automobile ignition systems, etc. may be necessary. It is also assumed that the proper gain is applied to all signals where necessary.\n\n(37) T he QRS complexes in waveform A are converted into well defined rectangular pulses by pulse shaper 41 to produce waveform B at the output. Il he pulse shaper 41 ;ilay be a base clipper or the like which produces a constant positive voltage at the output for all input voltages which exceed a predetermined value. The clipping voltage would be set at a value greater than the maximum amplitude of the normal T waves. Signal B will therefore be a series of rectangular pulses occurring in phase with the QRS complexes and having pulse widths less than the Q to S periods. The pulse shaper 41 therefore functions as a device for detecting the occurrence of the QRS complexes.\n\n(38) It is pointed out that the pulse shaper 41 may alternatively include a differentiator which produces a well defined rectangular pulse each time the slope or first derivative of the input wave goes through some relatively high positive value, since the QRS complexes always has a slope greater than the slope of the T wave.\n\nIS\n\n(39) The Q to S periods of the QRS complexes in waveform A are detected via the pulse shaper 42. Pulse shaper 42 may include a hard clipper which forms a series of well defined positive rectangular pulses having leading and trailing edges occurring in phase with all zero crossings in waveform A. The output of pulse shaper 42 is represented by waveform C.\n\n(40) The output C of pulse shaper 42 is connected to the input of a pulse duration detector 43 which produces an output signal D having a maximum amplitude which is a function of the pulse duration of the pulse in waveform C. Detector 43 might, for example, include a capacitor which is charged through a large resistor and a diode by the pulses in waveform C and is discharged sharply by the trailing edges of the pulses in waveform C.\n\n(41) The output D of pulse duration detector 43 is connected to the input of a third pulse shaper 44 which generates a series of well defined rectangular pulses, 35one pulse each time the amplitude of waveform D exceeds the horizontal dashed line. Here again, the pulse shaper 44 may be a base clipper. The predetermined value below which the waveform D is clipped, i.e., the height of the horizontal dashed line represents a pulse duration in waveform C which is less than the value to be detected, e.g. 120 milliseconds. Therefore, the pulses in waveform C appear in phase with all positive sections of waveform A having a time duration at or near the base or zero line greater than the specified value, e.g. 120 milliseconds.\n\n(42) A flip-flop 45 has the set and reset inputs connected to the outputs B and C of pulse shapers 41 and 42 respectively. The flip-flop 45 is set on the leading edge 50of a pulse and is reset on the trailing edge of a pulse.\n\n(43) Therefore, the output F of flip-flop 45 will be a series of rectangular pulses having leading edges in phase with the leading edges of the pulses in waveforms B and trailing edges in phase with the trailing edges of the pulses in waveform C which in turn are in phase with the S points of the QRS complexes of the waveform A.\n\n(44) The output F of flip-flop 45 is connected to the input of a one shot 47 which generates a series of pulses having the leading edges in phase with the leading edges of the pulses in the input waveform F and having a fixed duration and amiplitude. The output waveforrji G of one shot 47 is fed to the input of a pulse rate detector 50 which in turn has an output K having an amtplitude w hich grows at a rate proportional to the integral of the positive pulses in waveforiii G and slowly approaches zero along a negative slope when waveform G is zero.\n\n(45) One embodiment of the detector 50 would be a capacitor which is charged by waveform G through a first resistor and diode and is discharged through a second resistor and diode when waveform G is zero. The RC time constants of the discharging circuit and the charging circuit could be equal or different but would be chosen such that when the average ratc of occurrence of the pulses in waveform G over a given time interval, e.g. 10 seconds, is less than some predetermined value, e.g. 150 per minute, then the output voltage K of detector 50, i.e., the charge on the capacitor, would be less than some predetermined threshold. The characteristics of waveform K will be described later in greater detail. The output of detector 50 is connected to a threshold detector 55 which generates a positive output when the output of pulse rate detector 50 exceeds the aforementioned predetermined value shown by a horizontal dashed line in FIG. 5.\n\n(46) Pulse rate detector 50 also includes a set device 51 and a reset device 52. Set device 51 is energized by the trailing edges of the pulses in waveform G to enable the detector 50. Reset device 52 is energized by the output of OR gate 60 to reset the output of detector 50 to zero and disable the detector 50 until it is again enabled by set device 51.\n\n(47) The set device 51 and reset device 52 might include a flip-flop plus some logic elements. When reset device 52 is energized, a logic gate may be closed connecting ground to the charged capacitor in detector 50, thereby dumping the charge on the capacitor to ground. When the set device 51 is energized, the ground would be removed from the capacitor which can now be charged with the pulses in waveform G as described earlier. The conditions under which detector 50 is set and reset is important to the inventive concept and will also be described in greater detail. The particular circuit details to accomplish the function is arbitrary and are therefore not shown.\n\n(48) The output E of pulse shaper 44 and output F of flip- flop 45 constitute the two inputs to AND gate 61. It is pointed out again that each time a QRS complex appears in waveform A, a corresponding pulse appears in waveform F. Also, the trailing edges of the pulses in waveform F are in phase with the S points in waveform A. Further, the pulses appearing in waveform E are a result of positive wave sections in waveform A having zero crossings or base widths greater than some predetermined value, e.g. 120 milliseconds. Therefore there will be an output pulse from AND gate 61 each time both of these conditions occur simultaneously, i.e., there is a QRS complex and the Q to S period is greater than the prescribed value, e.g. 120 milliseconds.\n\n(49) Since in the example of FIG. 4 the heart beat is assumed normal, i.e., the Q to S time periods are all less than the limit, e.g. 120 milliseconds, there will be no output from AND gate 61 and therefore no waveform H appears in FIG. 4.\n\n(50) hlowever in the exaiinple of FIG. 5, the heart beat is initially assumed not noriiial and the Q to S tiiiie periods are assumed greater than the limit, e.g. 120 miilli- seconds. In this case, there will be an output from AND gate 61 and the waveform :: to the left of line X will be identical to waveform E.\n\n(51) 1ihe output h1 from AND gate 61 is inverted and anded in AND gate 62 with the output I of trailing edge detector 64 which is connected to the output F of flip- flop 45, 1 he trailing edge detector 64 niiay be imiple- mented with a differentiator which produces a positive impulse when the slope of the input goes highly negative. The output J of AND gate 62 is connected to the set side of flip-flop 63 via OR gate 60. The output H from AND gate 61 is connected to the reset side of flip-flop 63. The flip-flop 63 will be set by those trailing edges of the pulses in waveform F which do not occur in phase with a pulse in waveform H. In other words, if at a particular time waveform H is zero and a trailing edge of a pulse in waveform F occurs, then an impulse will appear in waveform J which will set the flip-flop 63 via OR gate 60.\n\n(52) In the example of FIG, 4, all of the impulses in waveform I will be passed by AND gate 62, since the output I1S H from AND gate 61 is zero. Therefore, in this exam- ple, waveforms I and J will be identical.\n\n(53) In the example of FIG. 5, there will be no output from AND gate 62 during the period left of the line X, because all of the impulses in waveform I occur when waveform H is positive.\n\n(54) The flip-flop 63 is reset on the leading edges of the pulses in waveform H. Of course, after the flip-flop 63 is reset, a second reset pulse will have no effect on the flip-flop 63. The same is also true of the set pulses when flip-flop 63 is in the set state.\n\n(55) To summarize, there will be an impulse in waveform J each time a QRS complex occurs having a Q to S distance less than the prescribed time limit, e.g. 120 milliseconds. Therefore, if flip-flop 63 is in the reset state, it will be set by the first occurrence of a normal QRS complex. On the other hand, if flip-flop 63 is in the set state, it will be reset by the first occurrence of a QRS complex which is not normal.\n\n(56) The output of flip-flop 63 will be integrated over time by an integrator 65 which may simply be an RC circuit.\n\n(57) The output of integrator 65 is connected to the input of a threshold detector 66 which produces an output pulse when the input from integrator 65 reaches a predetermined value. This value is chosen such that the output pulse from the threshold detector 65 appears some predetermined time after flip-flop 63 is reset by a pulse in waveform H. For example, if the predetermined time is ten seconds, as is assumed in the example in FIG. 5, and if for a ten second period all of the Q to S periods are greater than the prescribed time limit, e.g. 120 milliseconds, then a pulse will appear at the output of threshold detector 66. If at any time in a 10 second interval a normal QRS complex should ap- 50pear, the flip-flop 63 will be set, the output of integra- t0 or 65 will be reset to zero, and measurement of the 10 second interval will be terminated. The next measurement of a 10 second interval is initiated with the next occurrence of a QRS complex in waveform A which has a Q to S period greater than the prescribed time 55limit.\n\n(58) As mentioned earlier, the output of threshold detector 55 will be positive when the waveform K exceeds the horizontal dashed line in FIG. 5. In the exaiiiple of FIG. 4, the output of pulse rate detector 50 is represented by the solid line sawtooth path in waveform K.\n\n(59) I he output of detector 50 starts to increase along a linear path (the capacitor begins to charge up) but is interrupted by the imipulses J which resets the detector 6550 via the output of OR gate 60 (the capacitor is rap- 65idly discharged). If the detector 50 was not reset by a pulse J, then the output K would follow the dashed line sawtooth path in waveforizi K. In other words, the capacitor in detector 50 would be charged by the positive pulses in waveform G and discharged when waveform G is zero. In any case, the waveform K in FIG. 4 never reaches an appreciable positive value (the threshold) for two reasons: (1) the rate of occurrence of the QRS complexes is less than the limiting value and (2) all of the Q to S periods are less than the limiting value. If only the first reason existed and all the Q to S periods were actually greater than the prescribed value, the waveform K would follow the dashed line sawtooth path and the threshold would still not be reached since the rate of occurrence of the QRS complexes is low.\n\n(60) On the other hand, if the rate were high but the Q to S periods were short, output K of detector 50 would still not reach the threshold value because detector 50 would be reset by waveform J via OR gate 60.\n\n(61) Because the rate of occurrence of the pulses in waveform G to the left of line X in FIG. 5 is greater than the prescribed value and because the pulse rate detector 50 is not reset during this ten second period, the output K of detector 50 gradually climbs and passes the threshold value represented by the horizontal dashed line. If during the period to the left of line X in FIG.\n\n(62) 5 a QRS complex had appeared which had a Q to S distance less than the prescribed value, detector 50 would be reset and the output K would instantaneously go to zero.\n\n(63) When the output of threshold detector 66 becomes positive indicating that the prescribed ten second time period is over, the pulse rate detector 50 is reset to zero by the output of detector 66 via OR gate 60. The flip-flop 63 is also set by the output of threshold detector 66 via OR gate 60. This situation is indicated by the line X in FIG. 5. At this time, the defibrillator 10 is energized via AND gate 70 and line 18, because the outputs from both threshold detectors 55 and 66 are positive. In other words, if over the prescribed time inter- val, e.g. 1 0 seconds, the average rate of occurrence of the QRS complexes is greater than the prescribed aver- age, as measured by threshold detector 55, e.g. 150 per minute, and if each and every QRS complex appearing in that same 10 second interval has a Q to S time period greater than the prescribed time, e.g., 120 millisec- onds, then the output of AND gate 70 goes positive and defibrillator 10 is energized. Also, the analyzer- control 12 is reset to the original state.\n\n(64) Finally, the demand pacer 11 uses the pulses in waveform B as a control to provide stimulating pulses to the heart via terminal 15 when a prescribed time period, say one second, is exceeded in which a QRS complex is not detected or when bradyarrythmia occurs. Defibrillators and demand pacer devices are well known and are therefore not described in detail.\n\n(65) Obviously many modifications and variations of the present invention are possible in the light of the above teachings. Of course, it should be understood, that the specific values used for the heart beat rate, QS period etc. are only examples. In the present example, it is assumed that each and every Q to S period must exceed a predetermined value during the measured period before defibrillation is performed. In some cases, the device iiiay be readily miodified such that all but two, three, etc. Q to S periods, for examiple, exceed the given valve. It is therefore to be understood that within the scope of the appended claiiiis, the invention iiiay be practiced otherwise than as specifically described."
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 10,
      "claims_start": 9,
      "description_end": 9,
      "description_start": 6,
      "drawings_end": 5,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 10,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 9,
      "specification_start": 6,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 22769394,
    "guid": "US-3857398-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/03/857/398",
    "intl_class_currrent_secondary": [
      {
        "intl_class": "A61N",
        "intl_subclass": "1/372",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61N",
        "intl_subclass": "1/39",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61N",
        "intl_subclass": "1/05",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61N",
        "intl_subclass": "1/365",
        "version": "2006-01-01"
      },
      {
        "intl_class": "A61N",
        "intl_subclass": "1/375",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "A61N1/372"
    ],
    "inventors": [
      {
        "name": "RUBIN L"
      }
    ],
    "inventors_short": "RUBIN L",
    "patent_title": "ELECTRICAL CARDIAC DEFIBRILLATOR",
    "publication_date": "1974-12-31",
    "publication_number": "3857398",
    "type": "USOCR",
    "us_class_current": [
      "600/516",
      "607/5",
      "600/519"
    ]
  },
  {
    "composite_id": "23954668!OC-US-03993582",
    "cpc_inventive": [
      {
        "cpc_class": "A01M",
        "cpc_subclass": "13/00",
        "version": "2013-01-01"
      }
    ],
    "database_name": "USOC",
    "derwent_week_int": 0,
    "document": {
      "description": "United States Patent ilgi Curtis [541 PULSE             FOG             GENERATOR [751 Inventor: Russell R. Curtis, Indianapolis, Ind. [73] Assignee:        Curtis        Dyna        Products Corporation, Westfield, Ind. [221     Filed:          July 26, 1974 [21] Appl. No.: 492,035 [521 U.S. Cl 252/359       A;       239/78 [511 Int. Cl.2 BOlj          13100 [58] Field of Search 239/129,    77     78,     5, 239/8,    9;    60/3,    7,    319; 252@359     A [561                      References           Cited UNITED STATES PATENTS 2,768,031       10/1956    Tenney 239/129 26                  -37 28 36- @4IJ 21 22 47 27                                              44 -                                               43   11 fill            399939582 [451 Nov.  23,  1976 2,857,332 10/1958 Tenney 239/129 21959,214 11/1960 Durr 239/77 Prinwry Examiner-Hiram H. Bemstein Attorney, Agent, or Firm-Woodard, Weikart, Emhardt & Naughton [57]                          ABSTRACT Disclosed is a portable, pulse-jet engine powered fog producing device in which the hot engine components are mounted on one side of a vertical plate and the ftiel and insecticide supply tanksare mounted on the opposite side of the piate and in which the engine components are fon-ned to utilize natural convection currents to rapidly cool the engine after shut-down. 3 CUms, 20 Drawing Figures -_,29 38- 31 39 41 13 17 0 18 14\n\nU.S. Patent Nov. 23, 19'76 CD OD cli tD Sheet I of 7399939582 tD CD (NJ (NJ  LL.\n\nU. S. Patent  Nov. 23, 1976 39993@582 Sheet 2 of 7 29           26 61 10   0 27 Fig. 2 51 31 63 66 29    62 Fig. 9A\n\nU.S. Patent   Nov. 23, 1976       Sheet 3 of 7   3 9931,582 81 92 31 53                                       310 318 37                              2 @-25c                   72 30C          28                                      -390 1 3 36-               -42 300               39 25o                 4V 34 \"-33 2i 42           24 32@ 1 4 67 Fig.3\n\nU. S. Patent  Nov. 23, 1976      Sheet 4 of 7 399939582 31 53 291                 38 56 26 f 5@2 54 61 -14 21 32- -24 27 4e. 67                     -67 Fig. 4 10                   13 Fig. 5\n\n7 3 9 9 9 3 9 5 8 2 U. S. Patent   Nov. 23, 1976      Sheet 5 of 21 37 22b 72 22a 220 -   22e 38a 22d-- -22 229 71 22 C 22b Fig. 9 16a Fig.7 10 28 13 -17 28e 14 28d  2 c 28b- a    16 a n n @36 42 34 F i g . 6 Fig.8\n\nU. S. Patent    Nov. 23, 1976            Sheet 6 of 7 399939582 30d 22p 22' 25 21 22j 22  2m 32 31--25 g  I 22k 7 22-F                   33                  32-@   - 22f 22- 22-                                    22e -   33- 22d e 229 -22c 22c                  22                      22 20 2 22b \\22a Fig. 10 Fig. I I 14 82 81 3 1d - 9 2 3 1 83 3ic- 4@   -91 3if  - 310 31e. Fig. 12 82 81 Fig. 13\n\nU. S. Patent  Nov. 23, 1976    Sheet 7 of 7 399939582 87a 81 87 86 84 4a 83 30f' 30e 309 Fig.   14 ,30 c Fi g. 17 107 108   106 109 102 30  25c  101 - 104 25b 38  @103 39 25a Fig. 15 30a 30 b 30.( @94   96 Fig. 18 3 1b 96 97 97 9 Fig. 16 Fig. 16A\n\n3,993,582 PULSE FOG GENERATOR BACKGROUND OF THE INVENTION Fogging   devices,   used   to   generate   an   insecticide fog, 5 fot  example,   and   utilizing   the   pulse-jet   (resonant inter- mittant   combustion)   principle,   are    well    known    in the prior  art.  An  example   of   such   structure   is disclosed   in Tenney  et  al  U.S.  Pat.   No.   2,857,332   and   the   pulse jet engine   operation   is   there   explained   in   some detail.    A 10 fogging    apparatus    utilizing     a     resonant, intermittant combustion   device,   a   fuel   supply,   an   insecticide supply and  a  starting  device  is  disclosod  in  Curtis  U.S.   Pat. No. 3,151,454.   The   apparatus   of   the   present    invention rep- resents   an   improved   version   of   such   prior   art devices. 15 The   apparatus   of   the    present    invention    provides a thermal   barrier   between   the   engine   and    the    fuel and insecticide   tank   components.   The   engine    has    a housing configuration   and   air   vent   orientation   s'uch    that con- vection   air   currents   reduce   or   eliminate   the substantial 20 teniperature   rise   of   parts    of    the    apparatus occuring after   engine   shut-down,   a   difficulty   which    has plagued prior   art   devices.   The    fuel    valve    component provides improved   fuel   air   atomization   and    mixing    and substan- tially   eliminates   back-flow   or    blow-back    of    raw fuel 25 through   the   air   intake,   a   condition   heretofore inherent in  pulse   jet   engine   operation.   The   valve   which controls flow-inducing   pressurizing    of    the    insecticide    tank is provided  with  a  vent   which   functions   to   bleed   off pres- sure  in   the   insecticide   tank   after   engine  - shut-down,   an 30 ,added   safety   feature.   The   control   valve,    metering the flow  of  insecticide  at  a  selected   rate,   utilizes   an inte- grailly  formed   conically   shaped   valve   member   of   a suit- able   elastomeric   material   with    a    circular    bead above the  conical  portion  so  that  sealing  of   both   the   stem and 35 the  valve  seat  is   accomplished   by   the   single   valve mem- ber.   Several   constructional   features,    including    a vapor trap  in  the  fuel  feeding  line,  prevent  fuel  and   fuel vapor from   migrating   into    proximity    with    hot    engine parts immediately     after     engine     shut-down.     Post-shut down 40 ignition  of  fuel,  a  difficulty  in  prior   art structures,   is thus   prevented.   The   apparatus    is    provided    with addi- tional   constructional    improvements    referred    to    in the subsequent, detailed description of the device. BRIEF DESCRIPTION OF THE DRAWINGS 45 FIG.   1   is   a   schematic   view   of   the   primary compo- nents  of  the   pulse   fog   generator   of   the   present inven- tion. FIG.  2  is  a  side  view  of  the  pulse  fog  generator  of the 50 present   invention    as    actually    assembled    and    not in schematic form. FIG.  2A  is  a  fragmentary,  top   plan   view   of   a portion of the structure shown in FIG. 2. FIG.  3  is  a  side  view  of  the  structure  shown  in  FIG. 2 55 but   taken   from   the   opposite   side   thereof   and with    a portion broken away. FIG.  4  is  an  end  view  of  the   structure   shown   in FIG. . 2, taken from the right hand end of FIG. 2. FIG.  5   is   a   fragmentary,   detailed   view   of   the dis- 60 charge  tube  or  duct   portion   of   the   apparatus   and illus- trating  its  connection   to   the   exhaust   tube   and   the ex- haust tube housing. FIG.  6   is   a   fragmentary   view   of   the   discharge tube and the outer housing, with a portion of the outer hous- 65 ing broken away to reveal the inner housing. FIG. 7 is a sectional, side view of the insecticide fluid tank or reservoir. 2 FIG. 8 is an enlarged side sectional view of the check or unidirectional valve controlling pressurization of the insecticide fluid tank shown in FIG. 7. FIG. 9 is a bottom plan view of the upper portion of the fuel and air inlet valve. FIG. 10 is a side sectional view of the upper portion of the fuel and air inlet valve taken generally along the line 10-10 of FIG. 9. FIG. 11 is a side sectional view of the Valve taken generally along the line 11-11 of FIG. 9. FIG. 12 is a side sectional view of the manually operated insecticide fluid control valve. FIG. l@ is a top plan view of the structure shown in FIG. 12. FIG. 14 is an end view of a handle structure shown in FIG. 12 and taken generally along the line 14-14 of FIG. 12. FIG. 15 is a side sectional view of an insecticide fluid control valve which is a modified form of the valve shown in FIG. 12 and utilizing otily one valve closure member. FIG. 16 is a side view, partially in section, illustrating a modified form of the valve 'closure shown in both FIGS. 12 and 15. FIG. 16A is an enlarged, fragmentary view of the valve closure member of FIG. 16 seated in the conical cavity in the valve body. FIG. 17 is a perspective view of a-cam member utilized in the assembly. FIG. 18 is a perspective view of the valve actuating rod which cooperates with the cam shown in FIG. 17. DESCRIPTION OF THE PREFERRED EMBODIMENTS Referring primary to FIG. 1, the various components of the pulse fog generator device are schematically shown with interconnecting ttibing.  The apparatus includes a sinuous exhaust tube 10, communicating at one end with a combustion chamber 11, the combustion chamber having an electric glow coil, known in the prior art, identified at 12 and functioning as a flame holder.  The other end of the sinuous tube extends through a discharge tube 13.  The end of the sinuous tube 10 is spaced from the discharg@ tube 13 to provide an annular air passage to atffio phe@re. The exhaust tube 10, although sinuous, lies in a single vertical plane and is enclosed with a flat, generally rectangular outer housing 14 and an inner housing 16 having the same general configuration as the housing 14 but nested in the outer housing, the walls of the housing 14 and 16 being spaced from each other.  As may be seen in FIGS. 5 and 6, the inner housing 16 and the outer housing 14 have 6utwardly extending perpheral flanges which are stacked and joined together by any suitable means such as bolts 16a.  The outer housing 14 is provided with a series of air intake apertures 17 which function to admit air into the space between the housings.  It will be noted that the discharge tube 13 extends from and is in communication with the interior of the housing 16 but extends through the housing 14 in sealed relation thereto.  The inner housing 16 is provided with air intake apertures 18 and, it will be noted, these are below. the sinuous exhaust tube 10.  Cooling convection currents of air move through the apertures 17, through the apertures 18, sweep over the sinuous tube 10 and exit through the annular space within the discharge tube 13 and these convection air currents circulate whether or not the resonant intermittent com-\n\n3,993,582 3 bustion device is in operation, the convection air currents thus serving to cool the exhaust tube rapidly after shut down of the combustion device. As previously mentioned the exhaust tube 10 a sinuous configuration.  Since a design-specified length of the exhaust tube must be retained to produce satisfactory noise level and operating characteristics in a pulse-jet engine, the sinuous configuration provides a means for incorporating the required exhaust tube length in relatively restricted space.  The sinuous exhaust tube 10 differs from prior art structures, however, in that the longitudinal centerline of the combustion chamber I I and formed tube 10 lie in a single common plane thus making possible the utilization of the relatively flat, vertically disposed tube housings 14 and 16 and the thin overall contour of the complete assembly. The hottest portions of the exhaust tube 10, during engine operation are at the radiused ends of the sinuous tubes and, as will be evident from FIGS.  I and 6, the air intake apertures 17 in the outer housing 14 and one set of apertures 18 in the inner housing 16 are located adjacent the tube bends.  It will also be apparent from FIG. 1 that the combustion chamber and attached exhaust tube entrance are located at a lower corner of the nested housings 14 and 16.  The cooling air moving along the outside of tube 10 is discharged from the tube 13 adjacent an upper, diagonally opposite corner of the nested housings.  The hottest operating area of the engine, combustion chamber 11, is thus placed near the bottom of the engine housings and removed from the air intake apertures 17 and the air discharge tube 13.  This arrangement has particular utility in that it is conductive to dissipating engine heat by convection air flow after the engine has been shut down.  During engine operation and, particularly, after engine shut down air entering housing 14 through the set of apertures 18, just below combustion chamber II, receives heat from the combustion chamber, rises and moves over the sinuous tube 10 and out the tube 13.  This produces a relatively rapid cooling of the engine after shut down, eliminating the undesirable post shut down temperature rise of the engine and adjacent parts which is characteristic of prior art devices.  The convection flow of cooling air described, it will be noted, continues after engine shut down and until the engine has cooled.  This utilization of thermal convection to provide cooling air after engine shut down is a unique feature of the struc- ture herein described. Flow of fuel and air into the combustion chamber II is controlled by a fuel control valve indicated generally at 21 having an upper portion 22 and a lower portion 23.  An electrical spark ignition means having electrodes extending into the combustion antechamber 46 is identified at 24.  A manually operable charging air pump is indicated generally at 26 and a combustion fuel supply tank is indicated at 27. A check valve indicated generally at 28 functions to pressurize the upper portion of the. insecticide fluid reservoir or tank 29 in a fashion to be subsequently described in detail. A manually adjustable insecticide fluid metering valve is indicated generally at 31, the valve 31 controlling the movement of insecticide fluid in the gases discharged through the exhaust tube 10, these gases issuing as insecticide fog from the discharge tube 13. A fuel supply line 32 connects the fuel tank 27 with the fuel valve 21.  Air is supplied through the valve 21 from atmosphere when the combustion device is in operation, however, upon start-up air is supplied to the 4 combustion chamber through the valve 21 by means of the air tube or conduit 33 which receives air from the manual pump 26 when operated.  The pump 26 also supplies air through the tube 34 to the intake side of the valve 28.  Air pressure, through the tube 36 is thus presented in the upper portion of the fuel tank 27 and provides pressurization of the interior of the fuel tank to assure fuel feeding.  Air is also supplied through the tube 37 for pressurizing the upper portion of the insec- 10 ticide fluid tank 29 thereby assuring the flow of insecticide through the tube 38 to the valve 31.  The tube 39 leads from the outlet of the valve 31 to the insecticide fluid injection fitting 41.  During operation of the combustion device, that is, after initial start-up, the tube 42 15 supplies combustion chamber pressure through the body of valve 28, to the tube 36 for pressurizing the fuel tank 27 and insecticide tank 29 after manipulation of the hand pump 26 has ceased, it being understood that, as is conventional, the hand pump is utilized 20 merely to supply start-up combustion air and to initially pressurize the fuel tank. The valve 21 includes conventional petal valve 43 which permits one-way introduction of a combustible fuel air mixture through the apertures 44 into the com- 25 bustion antechamber 46, the apertures 44 functioning as combustion chamber pressure controlled fuel delivery apertures.  The valve also includes a conical deflector 47. The  valve   21,   combustion   chamber   11   and   exhaust 30 tube 10 function in conventional fashion as a resonant intermittent combustion device.  The operation of such combustion devices is well known in the prior art and is explained'in detail in Tenney et al U.S. Pat.  No. 2,857,332.  After initial ignition of the air-fuel charge, 35 pulsating combustion is maintained by the engine without the necessity of further operation of the ignition means with the exhaust gases moving through the sinuous tube 10 and out the discharge end of the tube. The   components    shown    somewhat    schematically    in 40 FIG. 1 will now be described in more detail and initial reference is to FIGS. 2, 2A, 3 and 4. As will be evident from these figures, the components are mounted on the side faces of a vertically oriented, shaped and horizontally ribbed plate 51 of relatively lightweight sheet 45 metal. This vertical plate 51 has attached to it all of the other components and assemblies of the complete apparatus.  As may be seen in FIG. 2A, the plate 51 is shaped to enclose the insecticide tank 29 and is pro- 50 vided with horizontally extending ribs which rigidify the support provided by the plate.  The plate 5 1, though made of thermally conducting sheet metal, serves as a thermal barrier between the hot engine parts, supported from one side of the plate, and the other compo- 55 nents (fuel and insecticide tanks and engine ignition apparatus composed of the battery and ignition coil which must be protected from any substantial temperature rise during and subsequent to operation of the engine.  The engine assembly is attached to the plate by 60 four brackets 52 (FIG. 4) which support the exhaust tube housing 14 and hold it spaced from plate 51.  The heat generated in the engine assembly (to be subsequently described in detail) can move to the plate 51 only by convection through the air space provided by 65 spacing brackets 52, by radiation and by conduction through the brackets.  The thin cross-section brackets 52 provide only a small heat conductive path to the plate, air moving upwardly, by convection, between the\n\n3,993,582 5 plate 51 and the engine assembly tends to.remove, heat from between the plate and the hot engine assembly, and loss of heat by radiation from the engine enclosure assembly is controlled by coating the en,closure with an aluminum pigmented finish. The valve 28 and the valve 31 and also disposed to the right of the central plate 51 (as vielwed in FIG. 4).  A carrying handle 53 extends upwardly from the plate and a wire guard 54 overlies,the exhaust tube.housing 14, the protecting guard or grill being attached at its upper end to a deflector plate 56 (FIG. 4) which extends sidewardly in inclined relation to the plate 51, and serves to deflect heat away from the handle area., Referring to PIG. 2, a co@tainer 61 for accommo'dating a battery and ignition coil used to energize. the spark means 24 and the glow coil 12 is- supporte,d on the left-hand side of the plate 51 (as viewed in FIG. 4).  The hand pump 26 and the fuel tank 29 are also mounted on this side of the central, vertical Olate 5 1. The insecticide tank 29 is accommodated with.in a cavity 62 formed by the shaped portion 63 of the central support plate 51, and the shield 66 (FIG@ 2A) formed by a plate having vertical undulations, thQ shield being secured at its corners to,plate 51 by metal clamps.  The insecticide tank 29 is formed, of semi-rigid plastic and is sized so that when unpressurized,it slips easily into the cavity (FIG. 2A but, when pressurized, resists removal from the cavity because of the press.ure exerted by the bulging tank walls against the wall of the receiving cavity.  The shield 66 provides further means for spacing or insulating the insecticide.tank from the heat generating combustion device disposed on the opposite side of the plate 51.  The vertical undulations in the shield 66 provide thermal chimneys or passages through which cooling air,. entering at the base olf the apparatus, may move upwardly by convection through the shield undulations to exit at the top of the asser@bly removing heat which has reached the plate 51 and thus further insulating ihe insecticide tank 29 from the heat source provided by the engine.  The', shield 66 thus provides a convection, path for cooling air, spaces the tank 29 away from plate 51 and strengthens the total structure. Support legs 67 extend.from the central plate 51 and from the wire guard 54 to support the comppnents above the floor or other supporting surface sufficient to permit air to enter below the engine, housings and move upwardly by convection. As may best be seen in FIG. 5 the discharge tube 13 is provided with a- surrounding wire guard 13a. It is desirable that the insecticide liquid tank 29,lbe easily removable from the apparatus when empty 6r when it is to be interchan ed with a duplicate tank holding a different, or additional, insecticide fluid and this feature requires that the pressurizing air line and the insecticide fluid delivery line be easily and quickly detachable from the tank.  The structure particularly adapted to accomplish this is shown i.n detail in FIG. 7. As shown in FIG. 7 the lines 37 and 38 are connected to a common removable junction block 71 which is secured to a mating assembly by a single finger-t ight screw 72, the mating assembly being secured to the insecticide tank. The mating, assembly consists of a backup plate 73, a face plate 74 on the outside of the container, a nipple 37a adapted to be attached to the line 37 and a fiipple 38a adapted to be attached to the line 38.  The two nipples 37a and 38a extend through the face plate 74 and through the cbntai @ner wall en- 6 gaged by the backup plate 73, an o-ring or gasket seal being disposed between the face plate and the container and an o-ring or gasket seal being disposed be, tween the appropriate shoulder of the block 71 and the n ipples.  To remove or replace the insecticide fluid tank 29, the screw 72 may be loosened, the junction block 7,1 and attached tubes 37 and 38 removed and the replacement tank, having nipples and mounting plates extending from the appropriate one of its sides installed 10 in the tank receiving cavity and the junction block 71 again tightened onto the new of replacement fluid tank.  A dummy junction block may be provided for sealing the insecticide tank either before or after it has been I ins . erted in the supporting cavity, 15 Referring now to FIG. 8, the insecticide pressure tank controlling check valve 28 will now be described in detlail. The.lowerportion ofthe valve connects to the tubes 34, 36 and 42, previousi descr@ibed, and a cylindrical valve portion 28a, extending upwardly from the 20 transverse passage through the valve,, has supported in it a tubular valve member 28b formed of rubber or similar elastomer.  The valve member is formed so hat it has two inclined faces (one of which is shown in FIG. 8 and- identified at 28c) topped by a slot 28d formed in 25 the member at the junction of the two inclined faces. The tubular member 28a has a reduced nipple portion 28e which extends through and is supported on the mounting plate 76.  The reduced portion 28e has clamped to it the adjacent end of the flexible tube 37 30 leading to the upper portion of the insecticide fluid tank 29.  There is a small aperture 28f formed in the inclined face 28c of the member 28b, this aperture serving as a by-pass to thQ slot 28d and permitting pres@sure to bleed back after short time interval from the 35 do.wilsilream side of the valve member 28b o,nce com- bustioh has ceased in combustion chamber 11.  It will be unders,tood that pressure pulses extending in, the tubular member 28a below the valve slot 28d will cause air or c@xhaust gases to pass,through the slot 28d into the 40 tube 37 but the pressure pulses are passed in one direc- ton only and pressure can not reversely move through the slot. 28d, although pressure extending downstream of the valve member 28b can be relieved through the aperture 28f, a preferred time interval for bleed-off of 45 this pressure being 45 seconds.  At start-up pressure pulses fr m the pump 26, transmitted through the tube 34, and, during combustion pressure pulses from the combustion chamber 1 1, transmitted through the tube 42, are utilized to pressurize the top of the insecticide 50 fluid supply tank 29.  The top of the fuel tank 27 is, of course, pressurized through the line 36 directly by line 34 from the pump 26 and by line 42 from the combustion chamber 11 without passing through the check valve member 28b. 55 , In prior art devices it is conventional to tap pressure from the pulse jet engine, while it is operating, to pressurize both the fuel supply tank and the insecticide tank.  Pressure, pulses are taken from the engine and directed through a one-way or check valve and deliv- 60 ered to the insecticide tank, pressurizing the tank and thus inducing the desired flow of insecticide to the engine exhaust tube during engine operation.  With such arrangements it is necessary to provide an additional.safety valve (usually pressure r,esponsive) in the 65 insecticide line from the tank which closes should the engine cease, operation for, any reason.  The necessity for this additonal valve occu,rs because the pressure in 'the inse@ticide tank, bulit-up during the normal opera-\n\n3,993,582 7 tion of the engine, remains after the engine stops due to a malfunction or for any other reason, and dangerous flow of oil-based insecticide continues to the hot engine after combustion has ceased unless an engine operation responsive safetly valve is provided in the insecticide line.  The aperture 28f in the valve member 28b eliminates the necessity for this additional safety valve.  While the aperture does not materially affect the build up of tank pressure by engine pressure pulses through the slot 28d, it does permit the pressure in the tank to bleed back across the member 28b in a short time interval of the order of 45 seconds.  Upon engine shut down, the insecticide tank is rapidly depressurized through the aperture 28f and insecticide flow to the engine ceases even though the manually operated insecticide flow control valve, to be subsequently described, is held open.  The aperture 28f thus obviates the necessity of providing a conventional, additional engine combustion responsive safety valve in the insecticide fluid line. Referring to FIGS. 9, 1 0 and 1 1, the fuel valve 21 will now be described in detail.  Ag pointed out with reference to FIG. 1, the valve has a lower casting 23 which encloses an antechamber 46, the chamber being defined by the horizontal plate 44a (FIG. 1) in which are formed the apertures 44.  A conical deflector 47 is centered on the upper side of the plate and a pin extending through the plate supports a flexible petal valve member 43, the valve functioning to permit an air-fuel mixture to move downwardly through the apertures 44 but flexing to pervent movement of exhaust gases back through the supports 44.  The upper portion of the valve 21, the portion identified at 22 in FIG. 1, is shown in detail in FIGS. 9, 10 and 11.  As will be evident from comparing FIGS. 9, 1 0 and 1 1, the casting 22 provides a generally frusto-donical chamber overlying the inlet side of the apeetures 44 (FIG. 1).  The chamber, as will be evident from FIG. 9, is elongated along two of its sides and these elongated sides are formed by inclined surfaces 22a which give it its generally conical configuration.  The end surfaces 22b of the chamber are slightly curved transversely but extend vertically upward.  The frusto-conical chamber is identified at 20 in FIG. 10.  The smaller end of the chamber 20 is defined by a transverse baffle wall 22c having a central opening 22d (FIGS. 9 and 11) therein.  A further baffle wall 22e is spaced from the baffle member 22c vertically and extends over the major portion of the central opening 22d, as will be evident from FIG. 9. A central bore extends vertically through the casting 22 and accommodates a spud 22f from the lower portion of which extends a fuel supply orifice tube 22g, the tube extending freely through a central aperture in the baffle wall 22e with its axis aligned with the longitudinal axis of the chamber 20.  The orifice spud 22f at its upper end provides a valve seat 22k against which the lower end 22m of stem member 22j closes, as will subsequently be described.  Air under pressure is introduced through the tube 33 into the area below the spud 22f and passes through the annular space between the tube 22g and the central opening in the baffle wall 22e.  Fuel is introduced under pressure into the area above the spud 22f through the tube 32 and moves downwardly through the orifice tube 22g to be discharged at the lower end of the orifice tube into the airstream flowing vertically downwardly around the tube. In prior art fuel valve structures the starting and running air supply flows in a direction aligned with the axis of the venturi passage and the fuel metering orifice 8 is conventionally positioned normal to this axis callsf'ng the fuel liquid to be introduced into the air stream at a right angle to the direction bf flow of the air streani.  Conventionally, the fuel enters the air stream either at its center or at its periphery and normal to the direction of air flow.  It is also conventional in small pulse jet engines to arrange the fuel-air entrance ports (such as ports 44 of FIG. 1) to the combustion chamber in a circular pattern about the axis of the venturi passage 10 and combustion chamber, with a central, conical deflector (such as deflector 47 of FIG. 1) to direct the fuel-air mixture to the ports.  With this arrangement, injection of fuel at right angles to the air stream generally results in an uneven distribution of fuel in the air 1 5 stream causing an over-rich mixture to reach some entrance ports while an over-lean mixture reaches other ports.  While this condition can be endured when the engine is running, it can present a formidable problem in starting a cold engine.  Ideally, upon starting a 20 completely homogenous mixture of fuel and air should be delivered to all of the ports for admission to the combustion chamber.  The fuel valve control structure of the present invention closely approacbes this ideal condition by positioning the fuel stem 22g, not normal 25 to the air flow but in line with the center of the conical deflector 47 and of the starting air stream.  Referring to FIG. 10, starting air enters the inlet casting through the tube 33 and enters a cavity that surrounds the fuel stem 22g.  From this cavity starting air is directed down- 30 wardly in a cylindrical flow path surrounding the fuel stem and aligned with it, the fuel stem being directed toward the apex of the conical deflector 47 (FIG. 1). This arrangement provides easier starting of the engine and assures that a homogenous fuel-air mixture contin- 35 ues to flow to the engine combustion chamber during operation of the engine. The purpose of the baffle wall arrangement will be explained with reference to FIG. 11.  In FIG ' I 1 the normal flow of air into the chamber 20 is indicated by 40 broken line arrows downwardly directed.  It is generally known in the art that although engine (petal) valves, such as provided by apertures 44 and petal member 43 (FIG. 1), are considered to be positively sealing on the explosion cycle of the engine, they do, in fact, permit a 45 small amount of back flow of air fuel mixture, such back flow being indicated by the generally upwardly directed, solid line arrows in FIG.  I 1. In prior art valve structures this has resulted in some \"spitting\" of raw fuel back out of the air. passage (the open upper end) of 50 the valve as the engine operates, the magnitude of this unwanted flowing out of fuel being a function of the sealing efficiency of the valve formed by the petal member 43 and the apertures 44.  Since the petal member 43 must flex with each pulse of the engine and must 55 flex easily so as to admit the fresh change of air to the engine with each pulse, sealing efficiency with respect to back flow through the valve is sometimes sacrificed somewhat.  The arrangement of the baffle walls 22c and 22e in the valve herein described substantially elimi- 60 nates the potentially dangerous and inefficient throwing back of fuel out through the open upper end of the valve assembly, that is, through the normal air intake passage.  As the solid line arrows in FIG. 11 indicate, any back flow of gases with entrained fuel droplets 65 must move in a generally tortuous path, and none can flow in a straight line path except for that which moves through the small enlatgements at each end of the aperture 22d (FIG. 9).  Any gases thus moving out of\n\n3,993,582 9 the open upper end of the valve assembly are, prior to attaining that location, stripped of any fuel droplets because of the somewhat tortuous path which it must take.  The construction is such as to give only a very reduced straight line path from the chamber 20 to the open upper end of the valve, that is, to atmosphere. In relatively small pulse jet engines, particularly those of the type and size used in small portable fogging devices such as that described herein, the fuel flow is relatively small and an extremely small fuel metering orifice is required.  Such small orifices are particularly sensitive to the presence of dirt or other foreign particles in the fuel.  A relatively small particle can totally restrict the orifice disrupting the fuel flow to the engine.  It is known in the prior art to employ a cleanout needle (slightly smaller than the orifice) positioned to enter the orifice and eject foreign particles therefrom.  In the prior art devices the orifice and cleanout needle are positioned horizontally.  In such arr'angement the foreign particles in the fuel are carried to the orifice and held there by fuel and act to restrict or halt the flow of fuel to the engine.  When the engine stops, in these prior art structures, the fuel pressure decays and releases the particle which falls back into the cavity below the orifice to again be carried to the orifice by the fuel flow when the engine is restarted.  The cleanout needle is thus ineffective in discharging the particles from the fuel system and the needle is effective to force the particle through the orifice only when the particle remains lodged in the orifice.  In the valve shown in FIGS. 9, 10 and 11 and the cleanout needle 22h is positioned above the vertically directed orifice tube 22f and is carried by the member 22j which is movable within the central bore of the upwardly extending portion of the casting 22, ihe member 22j being sealed in the bore by means of an o-ring seal. When the needle is at its lowermost portion within the orifice the surface 22m of the member 22j closes against the surface 22k of the member 22f to seal off or close the orifice.  A resilient means taking the form of the compression spring 25 urges the member 22j and hence needle 22h upwardly.  A rod 25a having an enlarged lower end 30d is pinned to yoke member 22p carried at the upper end of member 22j (FIGS. 10 and 18). As may be seen in FIGS. 3 and 18, the rod 25a extends upwardly through a horizontal, stationary plate 30.  The rod 25a, shown in detail in FIG. 18, provides a means for manually moving the needle downwardly into the orifice, when necessary, to dislodge particles therefrom.  Enclosing the major portion of the length of rod 25a is a compression spring 30a which bottoms on a shoulder 30b (FIG. 18) at the upper margin of the enlarged lower end 30d of the rod.  The upper end of the spring holds a shouldered washer 30c upwardly against the underface of the stationary plate 30.  The spring 30a thus functions to urge the rod 25a downwardly and biases the surface 22m (FIG. IO) into engagement with surface 22k to close off the orifice member 22f in the fuel valve when the rod is in its lowermost position.  Controlled movement of the rod 25a by the operator of the apparatus is accomplished by means of the cam member 30c (FIG. 17), mounted on the upper face of plate 30, and having an upper edge providing a detent portion 30e which merges into a downwardly inclined portion 30f and a lower detent portion 30g. As may best be seen in FIG. 18, the upper end 25b extends sidewardly and carries member 25c whose curved face forms a lever which rides the cam surfaces 30e, 30f and 30g, the lowermost 10 position of rod 25a, and the position in which needle 22h (FIG, 10) extends into orifice member 22f, is established by the seating of member 25c in the detent position 30g on the cam 30c.  In this structure, the cleanout needle and the metering orifice are both positioned vertically so that foreign particles collect at the orifice insuring that they can be ejected through the orifice by the cleanout needle.  To eject the particles collected at the fuel metering orifice, the operator 10 vigorously strokes the starting air pump 26 with the ignition switch in off position. This causes pressure in the fuel tank to rise delivering fuel through line 32 to the space above the fuel metering orifice.  With the tank pressurized, and continuing to stroke the air pump, the 15 operator moves the lever 25c up and down the cam surface 30f (FIG. 18) rapidly, repeating this action several times and continuing to stroke the air pump.  With each downward stroke of the rod 25a fuel present in the cavity of the orifice is pressurized by the de- 20 scending surface 22m (FIG. 10), forcing fuel and any foreign particles to the orifice where they are either blown through the orifice or wedged therein.  As previously mentioned, at the bottom of the downward stroke of the member 25a, the cleanout needle 22h enter the 25 orifice and pushes any foreign particles wedged therein through the orifice.  By repeatedly moving the lever member 25c up and down the cam surface, all foreign particles present in the cavity above the orifice are either hydraulically or mechanically ejected through 30 the orifice.  The spring 30a holds the lever 25c against the cam surface and overcomes the upward force of spring 25 (FIG. 10) to exert a net downward force sealing the fuel supply off at surfaces 22m and 22k when lever 25c is in the lower detent 30g (FIG. 17). 35 This provides a positive closing off of the flow of fuel to the antechamber and to the combustion chamber to provide immediate shutdown of the- combugtion device, and this rapid closure of the fuel valve by a simple manual nudge of the lever 25c down the inclined cam 40 surface has obvious safety advantages over the necessity to screw-turn downwardly a control knob to shut the fuel valve, as required in conventional fogging devices. The insecticide  fluid  control  valve  will  now  be  de- 45 scribed in detail with reference to FIGS. 12, 13 and 14.  The control valve 31 includes bL housing 31a having a rectilinear fluid flow passage 31b, which is circular in cross section, extending thrbugh the housing. Supported in a suitable bore in the housing is a valve mem- 50 ber 31c, the valve member, however, being adjustable movable within the bore transversely to the passage 31b.  A conical cavity is formed in the housing having a longitudinal axis which intersects the centerline of the passage 3 lb, the junction of the passage and the conical 55 cavity lying completely upon the inclined side surface of the conical cavity.  The valve member 31c includes a stem portion 31d which extends exteriorly of the valve housing and carries a valve closure element 31e.  The closure element 3le is unitarily niolded of rubber of a 60 similar elastomeric material and @has a conical configuration closely fitting within the conical cavity, as indicated in FIG. 12, thus blocking the passage when the stem is in its downward extreme position.  The closure element is provided with an integral, generally annular 65 collar 31f spaced above the conical portion of the member. This collar 31f engages the bore within which the valve member moves and provides a dynamic seal for the valve member.\n\n3,993,582 The upper end 31d of the valve member extends freely through an opening in a lever 81 which is generally channel-shaped in cross section as will be evident from FIG. 14.  The stem portion 3 Id carries an enlarged head which has a larger diameter than the aperture through which the stem 31d extends and there is thus provided at the adjacent end of the lever 81 on a oneway or unidirectional connection of the lever and stem so that upward movement of the lever end raises the valve member 31c, however, downward movement of the adjacent end of the lever 81 does not, itself, move the valve member 31c downwardly but permits it to be moved downwardly by the compression spring 31g.  The lever 81 is pivoted about the transverse pin 82 which extends through the supporting bracket 83 As may be seen in FIG. 14, a pin 84 having an enlarged head 84a extends freely through an aperture in the supporting bracket 83 and, as may best be seen in FIG. 14, the depending flange portion of the lever 81 is provided with a cut-out portion which accommodates an enlarged shoulder 86 on the pin 84, the shoulder providing a radially extending abutment which, with the parts in the position shown in FIG. 14, closely engages the cut-out portion of the lever 81.  When the head 84a of the pin 84 is moved leftwardly, as viewed in FIG. 14, the radial abutment 86 is moved leftwardly to its broken line position of FIG. 14 removing the abutment 86 from engagement with the lever 81 and permitting the lever to be pivoted counterclockwise, as viewed in FIG. 12, by manually depressing the end of the handle opposite its unidirectional attachment to the valve stem 31d. A coil spring 87 (FIG. 14) acts as both a torsion and compression spring and functioning as a compression spring serves to bias the pin 84 rightwardly as viewed in FIG. 14 so that the abutment 86 engages and locks the lever 81 against pivotal motion.  The end 87a of the spring is extended to engage the underside of the lever 81 and functions to bias the lever in a clockwise direction as viewed in FIG. 12, so that moving the handle portion of the lever 81 downwardly requires overcoming the torsional bias of the spring 87.  The spring 87 thus provides a means both urging the assembly into locked position when the handle lever 81 is free and also urging the handle lever into released position so that valve 3le can close.  In addition to permitting the operator to immediately stop flow of insecticide ' by releasing lever 81, should the engine shut-down for any reason, this arrangement provides a \"dead man\" effect for the manual control of the insecticide control valve.  If the handle lever 81 is released, for any reason, it is immediately re- locked by movement of abutment 86 into engagement with the lever and return of th6 lever to released position.  Manual reopening of the insecticide control valve can again occur only by the two step procedure of pushing head 84a of pin 84 leftwardly (as viewed in FIG. 14) and rocking lever 81 counterclockwise (as viewed in FIG. 12) about its pivot 82. Referring to FIG. 12 the valve body 31a also accommodates a further valve member 91 which, at its lower end, carries a conical, ela tomeric portion which is sized to closely fit into a second conical cavity intersecting the longitudinal  axis of the passage 31b.  The upper end of the valve member 91 carries a head 92 and adjustment of the head 92 on the member 91 serves to establish the effective length of the valve member and thus the position of the conical lower portion of the valve member in the passage 31b, This 12 establishes the maximum flow of insecticide fluids through the passge 31b to the fluid line 39 after the valve member 31c has been moved to open position by the handle lever 81.  The valve member 31c thus func- tions as an on-off valve and the valve member 91 functions as a metering valve. Referring to FIG. 16 and 16A, a modified form of the valve members is indicated at 94.  The valve member carries a unitarily molded clastomeric member 96 10 .@,hich has a conical configuration and is provided with two diemetrically opposite, flattened portions 97.  As may best be seen in FIG. 16A the surface area of each of the flattened portions is larger than the area enclosed by the junction of the fluid passage 31b with the 1 5 . n of the conical cavity receiving the member 96 margi in the valve body 31a.  With this arrangement, when the closure member 96 is in its lowermost position in the conical valve body cavity, the sharpened, and possibly burred, edges of the junction of the passge 3 lb with the 20 adj'acent conical sidewall of the cavity are not contacted by the surface of the elastomeric valve member 96, this being evident from FIG. 16A.  This arrangement prevents any cutting or abrasion of the elastomeric valve member 96 as it moves into its- lowermost 25 or passage-closing position. Referring to FIG. 15 a form of the insecticide fluid control valve which is modified from that of the structure shown in FIG. 1.2 will now be described. The struc- 30 ture of FIG. 15 differs, primarily, from that of FIG. 12 in that it utilizes a single valve member 101, movable in the valve housing 102, to control the flow of insecticide fluid through the rectilinear passage 103 in the valve housing.  The valve member 101 carries a conically 35 shaped, elastomeric element 104 and structurally is identical to the valve member 31c of FIG. 12.  The upper end of the valve member 101 carries an adjustment knob 106 which can be adjusted upwardly or downwardly upon the threaded end of the valve mem- 4o ber 101 and establishes the amount of upward travel imparted to the valve member 101 by actuation of the handle lever 107 which is pivoted at 108.  The single valve member 101 thus acts as a shut-off valve in its lowermost position and, since its upper most position is 45 limited by the adjustable knob 106 which is picked-up by the handle lever 107, it functions also as a metering valve for the insecticide fluid flowing through the passage 103 from the tube 38 to the tube 39.  The compression spring 109 serves to bias the valve member 101 50 into closed position and is the counterpart of spring 3 lg of FIG. 12. Prior art devices are customarily provided with an insecticide fluid control valve which includes only a metering valve adjustably threaded into flow obstruct- 55 ing position to determine the maximum rate of flow of fluid, the counterpart of valve member 91 of FIG. 12.  Such valves do not provide for shutting the flow of fluid off rapidly should combustion cease in the pulse-jet engine but, instead, require that the adjustable meter- 60 ing element be threaded or turned down to completely closed position.  Both of the control valve assemblies of FIG. 15 provide, in addition to the metering valve function (establishing a rate of flow of insecticide fluid), an on-off function which permits the flow of insecticide 65 fluid to be immediately shut off by release of the operating handle.  Flammable insecticide fluid is thus prevented from escaping to the hot engine exhaust tube upon engine shut-down.\n\n3,993,582 13 Returning to FIG. 3, th& structure forming a vapor trap in the insecticide fluid supply line 39 will now be described.  In the structure of FIG.@3, the portion of the tube 39 which extends through the housings and into the fitting 41 (FIG. 1), which communicates with the interior of the exhaust tube 10, is formed by a rel ative ly small diameter, rigid, stainless steel tube 39a, as shown in FIG. 3. The rigid tube 39o is formed, so that it has a portion 39b, adjacent its discharge end communicating with the fitting 41, which is disposed at an elevated level with respect to the remaining portion of the tube adjacent the engine.  This configuration thus forms a vapor trap cutting off the drainage of insecticide fluid into the exhaust tube upon cessation of combusti6n in the combustion device.  If insecticide fluid standing in the tube 39 and its extension 39a, after closure@of v@ive 31, were permitted to drain into the hot exhaust tube 10 after the combustion device has been stopp6d, combustion of this fluid might take place and present fire hazard to the operation.  The vapor trap provided by the configuration of the stainless steel tube 29a preVents liquid from flowing into the combustion device after the combustion device has ceased operation.  At shut down of the combustion device, the stainless steel tube 39a will normally be at an elevated temperature and will cause the insecticide fluid trapped between the valve 31 and the fitting 41 to be partially vaporized and to collect at the high point in the tube indicated at 39b in FIG. 3 and thus forming an insulating barrier. To review the operation, at starting, the starting air pump 28 is stroked repeatedly which builds up ',@he pressure in the fuel tank 27 sufficiently to force fuel into the valve 21 through the line 32.  A transparent portion of tube 32 (identified at 32a in FIG. 4) gives a visual indication that f@el is flowing to valve 21.  With lever 25c in upper detent 30e on the cam 30c holding member 22j in its position of FIG. 10, fuel is directed into the atomizing air stream through the tube 22g (FIG. 10), the air being initially supplied from the pump through the line 33.  The fuel and air mixture moves through the petal valve apertures 43 into the antechamber 46 (FIG. 1).  The burning gases are swept downstream into the combustion of the gases.  Due to the burning action of the gases moving through the combustion chamber, a low pressure area is created in the antechamber 46 which then deflects the fingers of the petal valve 43 to their open position shown in FIG.  I and a new charge of airfuel mixture is swept past the spark plug 24 and into the combustion chamber where the combustion cycle is again repeated.  Shut down of the combustion device (as distinguished from stoppage of the flow of insecticide which is accomplished by releasing lever 81) is accomplished by moving lever 25c (FIG. 18) from the upper position 30e on cam 30c to the lower detent position 30g (FIG. 17). The structure of the present invention provides an improved mounting arrangement for the components (on either side of the vertical plate 51) and provides an arrangement for the exhaust tube 10 and its nested housings 14 and 16 which permits convection air to sweep the tube 10 and cool it after the engine has been shut down.  The fuel control valve 21 is arranged so as to eliminate back flow or blowback of raw fuel.  The insecticide tank pressure can bleed off after the apparatus has been shut down so that pressure is not retained in this tank for a prolonged period, the bleed-off occuring through the aperture 28f in the check valve member 28b (FIG. 8). The insecticide fluid metering valve 14 ,31 incorporates a dual sealing valve stem and utilizes a single spring for both compression and torsion functions.  The cleanout needle 22h (FIG. 10), be,ing vertically arranged, has advantages over prior art structures as previously mentioned.  Fuel is supplied through the tube 22g along the centerline of the valve and is not introduced at right angles to the air tream in starting as is conventional with prior art structures. I claim: 10 1. A pulse fog generator including a resonant intermittent combustion device composed of a combustion chamber a sinuous exhaust tube- communicating with said co@;bustion chamber, one end of said tube discharging the gases resulting from combustion, a fuel 15 tank and means for injecting fuel from the tank into the combustion chamber, a container providing a reservoir of fluid, means to inject said fluid into the exhaust stream from the combustion chamber and being dispersed thereby in a finely divided condition, electrical 20 ignition means for the combustion device and a manually operated air pump for providing an initial air charge @to said combustion chamber and to said container and thus producing charging pressure for the fluid to be injected intb the exhaust stream, a vertical 25 plate supporting the combustion chamber and exhaust tube on one of its side faces and supporting the container, fuel tank, air punip and ignition means on the other side face so that the plate acts as a barrier between the combustion chamber element and the other 30 components, said exhaust tube being enclosed in a housing supported in spaced relation to the adjacent face of said plate to thereby permit circulation of convection air flow between said housing and said plate, said means for injecting fuel into the combustion cham- 35 ber including a fuel valve interposed between said fuel tank and said combustion chamber, said fuel valve having combustion chamber pressure controlled fuel delivery apertures for delivering an atomized fuel-air mixture to the combustion chamber, said fuel valve 40 in.cluding means providing a generally frusto-conical chamber overlying the inlet side of said fuel delivery apertures with the smaller end of said chamber farthest from said fuel delivery apertures, the smaller end of said end of said chamber being defined by a transverse 45 baffle wall having a central opening therein, a further baffle wall spaced from said first mentioned baffle wall and extending over the major portion of said central opening, a fuel supply orifice tube extending freely through a central aperture in said further baffle wall 50 and aligned with the longitudinal axis of said frusto- conical chamber, means for supplying fuel under pressure to said orifice tube and means for supplying air to the area surrounding said tube so that as air flows into said frusto-conical chamber it flows parallel to said 55 orifice tube to entrain fuel droplets issuing from the tube, the reverse flow of combustion gases through said- frusto-conical chamber being impeded by said first mentioned baffle wall and the said extension of said further baffle wall over said central opening of the first 60 mentioned baffle wall. 2.  A pulse fog generator as claimed in claim 1 in which said frusto- conical chamber is disposed vertically With said orifice tube extending vertically into said chamber, a member supported for vertical movement 65 above the upper end of said orifice tube, a clean-out wire extending from said member for protrusion into the bore of said orifice tube when said member is moved vertically downward, and resilient means urging\n\n3,993,582 15 said member into a limiting upward position in which said wire is spaced from said tube bore. 3. A pulse fog generator including a resonant inter- mittent combustion device composed of a combustion chamber, a sinuous exhaust tube communicating with said combustion chamber, one end of said tube discharging the gases resulting from combustion, a fuel tank and means for injecting fuel from the tank into the combustion chamber, a container providing a reservoir of fluid, means to inject said fluid into the exhaust stream from the combustion chamber and being dispersed thereby in a finely divided condition, electrial ignition means for the combustion device and a manually operated air pump for providing an initial air charge to said combustion chamber and to said container and thus producing charging pressure for the fluid to be injected into the exhaust stream, a vertical plate supporting the combustion chamber and exhaust tube on one of its side faces and supporting the container, fuel tank, air pump and ignition means on the other side face so that the plate acts as a barrier between the combustion chamber element and the othee components, said exhaust tube being enclosed in a 16 housing supported in spaced relation to the adjacent face of said plate to thereby permit circulation of convection air flow between said housing and said plate, a tube providing communication between said combus- tion chamber and the extreme upper portion of the interior of said fluid container and above the fluid level therein, and a check valve permitting the flow of gases into said container but substantially impeding the flow 10 of gases out of the container to thereby pressurize the ihterior of said container upon the occurence of pressure pulses within said combustion chamber but to permit the interior of said container to return to atmospheric pressure after a time delay determined by the 15 rate of reverse flow through said check valve subsequent to shut-down of said combustion device, said check valve taking the form of a tubular member formed of elastomeric material, the bore of said tubular member terminating at one end in a slot which is closed 20 except when the bore is under a predetermined pressure, and a by-pass aperture formed in said tubular member which remains open independently of the open or closed position of said slot. 25 30 35 40 45 50 55 60 65"
    },
    "document_structure": {
      "abstract_end": 1,
      "abstract_start": 1,
      "amend_end": 0,
      "amend_start": 0,
      "bib_end": 1,
      "bib_start": 1,
      "cert_correction_end": 0,
      "cert_correction_start": 0,
      "cert_reexamination_end": 0,
      "cert_reexamination_start": 0,
      "claims_end": 16,
      "claims_start": 15,
      "description_end": 15,
      "description_start": 9,
      "drawings_end": 8,
      "drawings_start": 2,
      "front_page_end": 1,
      "front_page_start": 1,
      "page_count": 16,
      "ptab_end": 0,
      "ptab_start": 0,
      "search_report_end": 0,
      "search_report_start": 0,
      "specification_end": 15,
      "specification_start": 9,
      "supplemental_end": 0,
      "supplemental_start": 0
    },
    "family_identifier_cur": 23954668,
    "guid": "US-3993582-A",
    "image_file_name": "00000001.tif",
    "image_location": "uspat/US/03/993/582",
    "intl_class_current_primary": [
      {
        "intl_class": "A01M",
        "intl_subclass": "13/00",
        "version": "2006-01-01"
      }
    ],
    "intl_class_issued": [
      "A01M13/00"
    ],
    "inventors": [
      {
        "name": "CURTIS RUSSELL R"
      }
    ],
    "inventors_short": "CURTIS RUSSELL R",
    "patent_title": "Pulse fog generator",
    "publication_date": "1976-11-23",
    "publication_number": "3993582",
    "type": "USOCR",
    "us_class_current": [
      "239/78"
    ]
  }
]