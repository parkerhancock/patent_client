interactions:
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 1, "hl_snippets": "2", "op": "OR", "q": "(\"RE43633\")[PN]",
      "queryName": "(\"RE43633\")[PN]", "highlights": "0", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "(\"RE43633\")[PN]"}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '712'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.0.5 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":1,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"(\"RE43633\")[PN]","queryName":"(\"RE43633\")[PN]","userEnteredQuery":"(\"RE43633\")[PN]","viewName":"tile","op":"OR","highlights":"0","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"(\"RE43633\")[PN]","error":null,"terms":["\"re43633\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":32,"highlightingTime":0,"cursorMarker":"AoJwwPKkx7kCNzIyNzI4Mjg4IVVTLVVTLVJFMDQzNjMz","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-RE43633-E","publicationReferenceDocumentNumber":"RE43633","compositeId":"22728288!US-US-RE043633","publicationReferenceDocumentNumber1":"RE043633","datePublishedKwicHits":null,"datePublished":"2012-09-04T00:00:00Z","inventionTitle":"System
        and method for linking streams of multimedia data to reference material for
        display","type":"USPAT","mainClassificationCode":"704/3","applicantName":null,"assigneeName":["Sentius
        International LLC"],"uspcFullClassificationFlattened":"345/157;704/10;704/9;707/999.003;715/255;707/758;704/1;345/160;715/856","ipcCodeFlattened":"G06F17/30;G06F17/28","cpcInventiveFlattened":"G09B19/06;G09B5/065","cpcAdditionalFlattened":null,"applicationFilingDate":["2009-06-08T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":["2005-02-24T00:00:00Z","1994-02-16T00:00:00Z"],"primaryExaminer":"Hudspeth;
        David","assistantExaminer":["Spooner; Lamont"],"applicationNumber":"12/480556","frontPageStart":1,"frontPageEnd":4,"drawingsStart":5,"drawingsEnd":9,"specificationStart":10,"specificationEnd":15,"claimsStart":15,"claimsEnd":22,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":4,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":22,"pageCountDisplay":"22","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/RE/043/633","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"Bookman;
        Marc et al.","familyIdentifierCur":22728288,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        RE43633 E","derwentAccessionNumber":null,"documentSize":197845,"score":14.146106,"governmentInterest":null,"kindCode":["E"],"urpn":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"urpnCode":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"descriptionStart":10,"descriptionEnd":15,"publicationReferenceDocumentNumberOne":"RE043633"}],"qtime":28}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 18 Jan 2023 17:22:45 GMT
      Server-Timing:
      - intid;desc=525cc21f8d3a5719
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: ''
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 1, "hl_snippets": "2", "op": "OR", "q": "(\"RE43633\")[PN]",
      "queryName": "(\"RE43633\")[PN]", "highlights": "0", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "(\"RE43633\")[PN]"}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '712'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.0.5 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":1,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"(\"RE43633\")[PN]","queryName":"(\"RE43633\")[PN]","userEnteredQuery":"(\"RE43633\")[PN]","viewName":"tile","op":"OR","highlights":"0","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"(\"RE43633\")[PN]","error":null,"terms":["\"re43633\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":50,"highlightingTime":0,"cursorMarker":"AoJwwPKkx7kCNzIyNzI4Mjg4IVVTLVVTLVJFMDQzNjMz","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-RE43633-E","publicationReferenceDocumentNumber":"RE43633","compositeId":"22728288!US-US-RE043633","publicationReferenceDocumentNumber1":"RE043633","datePublishedKwicHits":null,"datePublished":"2012-09-04T00:00:00Z","inventionTitle":"System
        and method for linking streams of multimedia data to reference material for
        display","type":"USPAT","mainClassificationCode":"704/3","applicantName":null,"assigneeName":["Sentius
        International LLC"],"uspcFullClassificationFlattened":"345/157;704/10;704/9;707/999.003;715/255;707/758;704/1;345/160;715/856","ipcCodeFlattened":"G06F17/30;G06F17/28","cpcInventiveFlattened":"G09B19/06;G09B5/065","cpcAdditionalFlattened":null,"applicationFilingDate":["2009-06-08T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":["2005-02-24T00:00:00Z","1994-02-16T00:00:00Z"],"primaryExaminer":"Hudspeth;
        David","assistantExaminer":["Spooner; Lamont"],"applicationNumber":"12/480556","frontPageStart":1,"frontPageEnd":4,"drawingsStart":5,"drawingsEnd":9,"specificationStart":10,"specificationEnd":15,"claimsStart":15,"claimsEnd":22,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":4,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":22,"pageCountDisplay":"22","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/RE/043/633","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"Bookman;
        Marc et al.","familyIdentifierCur":22728288,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        RE43633 E","derwentAccessionNumber":null,"documentSize":197845,"score":14.156972,"governmentInterest":null,"kindCode":["E"],"urpn":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"urpnCode":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"descriptionStart":10,"descriptionEnd":15,"publicationReferenceDocumentNumberOne":"RE043633"}],"qtime":45}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 18 Jan 2023 17:22:46 GMT
      Server-Timing:
      - intid;desc=e6f29b477f8cd73b
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.0.5 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://ppubs.uspto.gov/dirsearch-public/patents/US-RE43633-E/highlight?queryId=1&source=USPAT&includeSections=True
  response:
    body:
      string: '{"guid":"US-RE43633-E","publicationReferenceDocumentNumber":"RE43633","compositeId":"22728288!US-US-RE043633","publicationReferenceDocumentNumber1":"RE043633","datePublishedKwicHits":null,"datePublished":"2012-09-04T00:00:00Z","inventionTitle":"System
        and method for linking streams of multimedia data to reference material for
        display","type":"USPAT","mainClassificationCode":"704/3","applicantName":null,"assigneeName":["Sentius
        International LLC"],"uspcFullClassificationFlattened":"345/157;704/10;704/9;707/999.003;715/255;707/758;704/1;345/160;715/856","ipcCodeFlattened":"G06F17/30;G06F17/28","cpcInventiveFlattened":"G09B19/06;G09B5/065","cpcAdditionalFlattened":null,"applicationFilingDate":["2009-06-08T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":["2005-02-24T00:00:00Z","1994-02-16T00:00:00Z"],"primaryExaminer":"Hudspeth;
        David","assistantExaminer":["Spooner; Lamont"],"applicationNumber":"12/480556","frontPageStart":1,"frontPageEnd":4,"drawingsStart":5,"drawingsEnd":9,"specificationStart":10,"specificationEnd":15,"claimsStart":15,"claimsEnd":22,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":4,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":22,"pageCountDisplay":"22","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/RE/043/633","imageFileName":"00000001.tif","cpcCodes":null,"queryId":1,"tags":null,"inventorsShort":"Bookman;
        Marc et al.","familyIdentifierCur":22728288,"familyIdentifierCurStr":"22728288","languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"<span
        term=\"usre43633e\" class=\"highlight18\">US RE43633 E</span>","derwentAccessionNumber":null,"documentSize":197845,"score":0.0,"governmentInterest":null,"kindCode":["E"],"urpn":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"urpnCode":["3872448","4136395","4318184","4384288","4651300","4674065","4689768","4742481","4773009","4817050","4837797","4864501","4868743","4868750","4887212","4893270","4914586","4945476","4958283","4980855","4982344","4994966","5020019","5065315","5088052","5128865","5146439","5146552","5151857","5157606","5204947","5214583","5218697","5222160","5226117","5233513","5241671","5253362","5256067","5267155","5289376","5297249","5303151","5319711","5329446","5331555","5337233","5349368","5351190","5361202","5367621","5375200","5377323","5392386","5404435","5404506","5408655","5416901","5418942","5434974","5438655","5455945","5459860","5491783","5491784","5500859","5506984","5515534","5517409","5530852","5530853","5537132","5537590","5541836","5546447","5546529","5564046","5576955","5581460","5583761","5603025","5606712","5608900","5617488","5629981","5640565","5644740","5646416","5649222","5657259","5659676","5666502","5694523","5708804","5708822","5708825","5724593","5724597","5727129","5729741","5732229","5740252","5745360","5745908","5754847","5754857","5761436","5761656","5761659","5761689","5764906","5778363","5781189","5781900","5781904","5787386","5793972","5794050","5794228","5794259","5799267","5799302","5802559","5805886","5805911","5806079","5815830","5819092","5822539","5822720","5826257","5832496","5835059","5835089","5845238","5855007","5859636","5860073","5862325","5864848","5870702","5870746","5873107","5875443","5875446","5878421","5884247","5884302","5884309","5892919","5893093","5895461","5896321","5896533","5897475","5900004","5905866","5905991","5907838","5913214","5920859","5924090","5926808","5930471","5940843","5946647","5953718","5963205","5963940","5963950","5970505","5974413","5983171","5987403","5987460","5987475","5999938","6006218","6006242","6014677","6021403","6022222","6026088","6026398","6028605","6031537","6038573","6047252","6055531","6061675","6067565","6076088","6085201","6085226","6092074","6094649","6108674","6122647","6126306","6128635","6137911","6151624","6154738","6178434","6182133","6185550","6185576","6233570","6260035","6262730","6272505","6289342","6292768","6308171","6311177","6311194","6323853","6338059","6373502","6438545","6442545","6516321","6519603","6556984","6571241","6601026","6618733","6625581","6629079","6651059","6697824","6732090","6732361","7003522","7032174","7130861","7287218","7496854","RE40731","2002/0035581","2002/0036654","2002/0062353","2002/0065891","2002/0091803","2002/0099730","2002/0184247","2003/0004909","2003/0033290","2003/0154144","2003/0187587","2003/0212527"],"abstractedPatentNumber":null,"assigneeCity":["McLean"],"assigneePostalCode":["N/A"],"assigneeState":["VA"],"assigneeTypeCode":["02"],"curIntlPatentClassificationPrimary":["G06F17/30
        20060101"],"curIntlPatentClassificationPrimaryDateKwicHits":null,"designatedStates":null,"examinerGroup":"2626","issuedUsCrossRefClassification":["345/157","345/160","704/1","704/9","704/10","715/856","715/255","707/999.003","707/758"],"jpoFtermCurrent":null,"languageOfSpecification":null,"chosenDrawingsReference":null,"derwentClass":null,"inventionTitleHighlights":null,"cpcOrigInventiveClassificationHighlights":null,"cpcInventiveDateKwicHits":null,"cpcOrigAdditionalClassification":null,"cpcAdditionalDateKwicHits":null,"curIntlPatentClssficationSecHighlights":null,"fieldOfSearchClassSubclassHighlights":["707/706-708","707/758","707/999.003","704/1","704/9","704/10","715/255","715/856","345/157","345/160"],"cpcCombinationSetsCurHighlights":null,"applicantCountry":null,"applicantCity":null,"applicantState":null,"applicantZipCode":null,"applicantAuthorityType":null,"applicantDescriptiveText":null,"applicationSerialNumber":["480556"],"inventorCity":["Palo
        Alto","Mountain View"],"inventorState":["CA","CA"],"inventorPostalCode":["N/A","N/A"],"standardTitleTermsHighlights":null,"primaryExaminerHighlights":"Hudspeth;
        David","continuityData":["reissue parent-doc US 08676890 19960708 GRANTED
        US 5822720 19981013 child-doc US 12480556\n<br />continuation parent-doc US
        11064519 20050224 US Re. 40731 child-doc US 08676890\n<br />continuation parent-doc
        US 08197157 19940216 ABANDONED child-doc US 11064519\n<br />"],"inventors":null,"uspcFullClassification":null,"uspcCodeFmtFlattened":null,"ipcCode":null,"applicationNumberHighlights":["12/480556"],"dateProduced":"2012-08-15T00:00:00Z","auxFamilyMembersGroupTempPlaceHolder":null,"priorityCountryCode":null,"cpcCurAdditionalClassification":null,"internationalClassificationMain":null,"internationalClassificationSecondary":null,"internationalClassificationInformational":null,"europeanClassification":null,"europeanClassificationMain":null,"europeanClassificationSecondary":null,"lanuageIndicator":null,"intlPubClassificationPrimary":["G06F17/30
        20060101 G06F017/30"],"intlPubClassificationPrimaryDateKwicHits":null,"intlPubClassificationSecondary":["G06F17/28
        20060101 G06F017/28"],"intlPubClassificationSecondaryDateKwicHits":null,"publicationDate":null,"derwentWeekInt":0,"derwentWeek":null,"currentUsOriginalClassification":"704/3","currentUsCrossReferenceClassification":["345/157","704/10","704/9","707/999.003","715/255","707/758","704/1","345/160","715/856"],"locarnoClassification":null,"equivalentAbstractText":null,"hagueIntlRegistrationNumber":null,"hagueIntlFilingDate":null,"hagueIntlFilingDateKwicHits":null,"hagueIntlRegistrationDate":null,"hagueIntlRegistrationDateKwicHits":null,"hagueIntlRegistrationPubDate":null,"hagueIntlRegistrationPubDateKwicHits":null,"curIntlPatentClassificationNoninvention":null,"curIntlPatentClassificationNoninventionDateKwicHits":null,"curIntlPatentClassificationSecondary":["G06F17/28
        20060101"],"curIntlPatentClassificationSecondaryDateKwicHits":null,"abstractHtml":"A
        system for indexing displayed elements that is useful for accessing and understanding
        new or difficult materials, in which a user highlights unknown words or characters
        or other displayed elements encountered while viewing displayed materials.
        In a language learning application, the system displays the meaning of a word
        in context; and the user may include the word in a personal vocabulary to
        build a database of words and phrases. In a Japanese language application,
        one or more Japanese language books are read on an electronic display. Readings
        (`yomi`) for all words are readily viewable for any selected word or phrase,
        as well as an English reference to the selected word or phrase. Extensive
        notes are provided for difficult phrases and words not normally found in a
        dictionary. A unique indexing scheme allows word-by-word access to any of
        several external multi-media references.","descriptionHtml":"(1) BRIEF DESCRIPTION
        OF THE DRAWINGS<br />(2)  FIG. 1 is a block schematic diagram of a language
        learning system according to the invention;<br />(3)  FIG. 2 is a flow diagram
        in which the mechanism for indexing and linking text to external references
        is shown according to the invention;<br />(4)  FIG. 3 is a screen display
        showing a highlighted Japanese word and a pop-up menu, including an English
        reference to the Japanese word, according to the invention;<br />(5)  FIG.
        4 is a screen display showing a highlighted Japanese word and a pop-up menu,
        including Japanese language annotations of the Japanese word, according to
        the invention; and<br />(6)  FIG. 5 is a screen display showing a Japanese
        word listed in a personal dictionary, as well as a word control palette, according
        to the invention.<br />(7) DETAILED DESCRIPTION OF THE INVENTION<br />(8)  The
        invention provides a system that is designed to enhance and improve the way
        one reads or learns to read a difficult text, such as a foreign language,
        especially a language based upon an ideographic alphabet, such as Kanji which
        is used in the Japanese language. The text may be any of actual text based
        material, or audio, video, or graphic based information. In the language learning
        application, the system is modeled on the process by which the foreign language
        is read and addresses the problems most persons face when reading a language
        that is different from their own.<br />(9)  The exemplary embodiment of the
        invention is based upon two powerful functional modules that provide a comprehensive
        approach to reading and learning a foreign language, such as Japanese. The
        first module is an electronic viewer that gives the user access to reference
        information on each word in the electronic text at a word by word level. The
        second module is a relational database that allows a user to create word lists
        with practically no limit in size. The two modules are integrated to provide
        the user with everything needed to read the foreign language quickly and enjoyably,
        as well as to build their own individual vocabulary.<br />(10)  FIG. 1 is
        a block schematic diagram of an exemplary embodiment of the invention that
        implements a language learning system. An electronic book and/or a multi-media
        source material is provided as a teaching resource. A text file 10 and/or
        a multimedia source 14, consisting of an audio/video file 11 and synchronized
        text 13, which may include sound, images, and/or video is edited during construction
        of a linked text database by a visual editor 19 that used to build a wordified
        database 20. The database 20 sources a grammar parser 23 and a link engine
        22 that builds an index 21 which, in turn, locates each textual and audio/video
        reference in the source material. The index provides a location for each reference
        in a database 12 that includes a relational database engine 15, and linkable
        entities, such as text references 16, audio references 17, graphic references
        18, and the like.<br />(11)  The link engine 22 outputs the selected text
        to a word list 28 derived from the input text file 10 and/or audio/video information
        14, and also outputs the reference information 24, consisting of linkable
        entities 25, 26, 27, which are derived from the indexed database 12. The indexor/viewer
        29 creates a multi-media resource 30, such as a file 33 that was processed
        as described above to produce a data resource 34, an offset index 35, and
        linked entities 36 to the data resource for access by the user.<br />(12)  A
        user interface 32 to the system includes an electronic viewer 43 that runs
        along with the system application program 42 and provides the following functional
        elements: index management 37, user display 38, a table of contents 39, a
        pop-up display 40, and a personal dictionary 41.<br />(13)  The electronic
        viewer module is used to view and read the electronic books provided with
        the language learning system. The module includes the following features:
        1. One-click, pop-up information for all words containing foreign language
        words; 2. A word display palette; 3. A contents menu for each book; 4. Search
        functions; 5. Selectable browse and edit modes; and 6. The ability to copy
        words and associated information into personal dictionary.<br />(14)  The
        personal dictionary is a relational database that is optimized to manage and
        study words. Unlike electronic dictionaries, where only the word entries of
        the dictionary are searchable, the personal dictionary of the system herein
        allows one to search on each of eight or more keys associated with a word.<br
        />(15)  The following functions are supported by the personal dictionary:
        1. Display of words in an easy to read, easy to access format; 2. Full relational
        database capabilities for the following: the word, the pronunciation, English
        reference, notes, category, source, priority, and review date; 3. Search capabilities
        for any item; 4. Capabilities to store an unlimited number of words; 5. A
        flash word feature to allow self-testing in sorted or random order; and 6.
        Capabilities to review words sorted by any word key.<br />(16)  The personal
        dictionary also allows the user to enter words from other sources that are
        currently in paper or other electronic formats. For example, a user can copy
        all the words that they have in paper format from study lists and notes. With
        this feature, a student can have all of his study materials in one easy to
        access database. Users can also import and export data in a text format supported
        by standard word processor and spreadsheet programs.<br />(17)  The exemplary
        personal dictionary includes a base 500-word vocabulary list designed for
        the beginning student. A variety of words are included related to such general
        topics as: foods and drink, family, health, the body, commuting and transportation,
        environment, economics, finance, politics, companies, industries, computers,
        sports, and the language itself.<br />(18)  The system includes one or more
        electronic books. The words in each book is fully supported with readings,
        English references, and hypernotes. In the exemplary embodiment of the invention
        there are typically over 10,000 words, as well as over 1,000 notes presented
        in an easy to read, easy to memorize format.<br />(19)  The English reference
        feature of the system provides basic information to help users understand
        the word in its context. For each word, a generalized definition of the word
        is provided. The pop-up fields are used to give the user a quick reference
        to the word and to allow the user to continue reading or reviewing the text.<br
        />(20)  Current electronic book formats provide simple hyperlinks in what
        is termed hypertext or multimedia. Hyperlinks to date have been simple pointers
        that directly link text with other text, graphics, or sound within the text
        file itself. For reference materials, such as electronic encyclopedias, and
        dictionaries, hyperlinks provide a quick and easy way to find related material
        to a topic or subject. However, these links must be hard coded and are therefore
        cumbersome to author. The format of the system herein described provides a
        new means of relating text, pictures, and/or video with information to enrich
        and expand the impact of every element in a text, picture, or video. This
        format differs from current electronic books which only link text with other
        parts of text or content.<br />(21)  In the new format of the present system,
        every word or sound, for example, can be linked to information not contained
        within the text using an indexing method that maps a single word or phrase
        to a table that contains external reference material. This reference can be
        in the form .[.or.]. .Iadd.of .Iaddend.text, graphics, images, movies, and/or
        sound. Thus, the resource materials, such as the text, remains unaltered and
        therefore compact in terms of file size. Thus, the resource materials, for
        example the text, takes up less disk space and runs faster.<br />(22)  FIG.
        2 is a flow diagram in which the mechanism for indexing and linking text to
        external references is shown according to the invention. To find a reference
        to a particular word or other selected entry displayed on the screen, the
        user clicks the text that is viewed with a pointing device, such as a mouse
        (200). The click position is determined and used to calculate an offset value
        within the text (200). In the example shown in FIG. 2, the user clicks at
        a particular location, e.g. horizontal and vertical coordinates 100 and 75,
        respectively, and an offset value of 25 is returned. The offset value is compared
        to the start and end position indices stored in a look-up table (201, 202).
        The link between the selected text and the external reference is resolved
        (203), and the external reference is retrieved and displayed to the user (204).
        In the example of FIG. 2 an offset of 25 is located at the look-up table location
        having a start point of 20 and an end point of 27 and is linked to text located
        at position 200. As can be seen from the look-up table (202), the link may
        be to text, sound, pictures, and video. In the example, the text linkage is
        to the English language word &quot;Japanese economy&quot;.<br />(23)  The
        actual indexing process is completed in several steps, including word cuts,
        linking, and compilation.<br />(24)  Word Cuts<br />(25)  The word cutting
        process is accomplished using a simple visual editor, for example a point
        and click system using a pointing device, such as a mouse. The process divides
        the text into the individual components of text that are linked with the additional
        reference material. The original text is provided by a publisher in electronic
        form in a raw binary text format (e.g. an ASCII text file or other word processor
        file). This text is then divided up into the component word or phrases in
        preparation for the next step.<br />(26)  Linking<br />(27)  The linking process
        takes the text after the word cut process and links it to an external reference.
        The database 20 sources a grammar parser 23 and a link engine 22 that builds
        an index 21 which, in turn, locates each textual and audio/video reference
        in the source material. In the case of language learning, the component words
        and phrases are linked to a foreign language dictionary. In other cases, links
        may be made to other reference materials, such as graphics and/or sound.<br
        />(28)  Compilation<br />(29)  After linking, the text and references are
        compiled. During compilation, the cut text is reassembled to create an image
        of the text that the end user sees. At this point additional formatting may
        be applied to the text for final display. Indices of the component words and
        phrases are built with links to the reference material and duplicate references
        are consolidated to conserve memory and storage requirements.<br />(30)  A
        key feature of the system format is the method by which the original book
        text is indexed and linked with the external references. During the compile
        process an image of the text is created. When the image is created, the cuts
        are indexed based upon the position offset from the beginning of the text.
        The start and end points of the cut text are recorded in a look-up table along
        with the links to external references. The number and type of links for any
        component is dynamic. This means that a single entry could have several different
        references attached to it, each containing different forms of data.<br />(31)  The
        user interacts with the electronic book using a pointing device. When the
        user &quot;clicks&quot; within the text image, the location of the pointer
        is determined. The location is converted into a position offset from the beginning
        of the text and used to determine which component word or phrase was selected.
        The process involves comparing the offset with the start and end values stored
        in the look-up table as discussed above in connection with FIG. 2. When the
        offset value falls between a component''s start and end points, a match is
        made and the external references can be resolved.<br />(32)  English Reference<br
        />(33)  FIG. 3 is a screen display showing a highlighted Japanese word and
        a pop-up menu, including a translation of the Japanese word, according to
        the invention. The following section explains the English reference pop-ups
        associated with each word:<br />(34)  The English reference is intended to
        give the user basic information to help him understand a selected word in
        its context. A majority of the word definitions found in the English reference
        are not the direct translation of the word in that particular context. They
        are mostly generalized definitions of the given word. These pop-up fields
        give the user a quick reference to the word and allow him to continue reading
        or reviewing the text without the need to stop and access a dictionary. In
        applying the invention to other languages, for example Korean or Chinese,
        or to difficult materials, such as highly technical or complex matters, appropriate
        external references should be selected.<br />(35)  In the exemplary embodiment
        of the invention, a priority is placed on making the text readable, rather
        than on creating a detailed grammatical description of it. The English reference
        is not considered a direct translation of the foreign language, but rather
        is preferably a contextual definition based upon the word''s meaning within
        the text.<br />(36)  Definitions<br />(37)  Definitions in dictionaries are
        written for practical use. Accordingly, word and sentence translations are
        preferably written in modern English at a level acceptable to native speakers.
        The types of phrases and words covered by the English reference are preferably
        of great variety. The English translation should therefore be highly readable
        and useful.<br />(38)  Hyper Notes<br />(39)  FIG. 4 is a screen display showing
        a highlighted Japanese word and a pop-up menu, including Japanese language
        annotations of the Japanese word, according to the invention.<br />(40)  Hyper
        notes are provided for a great number of words and phrases included in the
        system. Most of the explanations are grammatical in nature, but others simply
        explain the passage in further depth or rephrase the foreign language word
        or phrase in simpler language. The notes have been written in the foreign
        language because it is believed that this is the best way for students of
        the language to improve their skills. As in the main text, the yomi and meanings
        of the words are given in a pop-up form.<br />(41)  Using the Electronic Viewer
        Module<br />(42)  The electronic viewer module provides the following pull-down
        menus: File, Edit, Words, View.<br />(43)  The File Menu includes: 1. Open
        (opens up a book for reading); 2. Close (closes a book); 3. Personal Dictionary
        (opens the personal dictionary); 4. Import Words (imports a tab delineated
        file into the personal dictionary); 5. Export Words (exports a tab delineated
        file into the personal dictionary); and Quit (quits the applications).<br
        />(44)  The Edit Menu Includes: 1. Undo (undoes a previously deleted entry
        in the personal dictionary fields); 2. Cut (cuts a highlighted block of text
        in the personal dictionary fields); 3. Copy (copies the selected text into
        the clipboard in either the electronic viewer module or the personal dictionary);
        and 4. Paste (pastes the copied text into the target field in the personal
        dictionary).<br />(45)  The Words Menu includes: 1. Find (displays the search
        dialogue box); 2. Find Next (finds the next entry using the previously entered
        search word); 3. Next (goes to the next word in the personal dictionary based
        on the current sort setting); 4. Prey (goes to the previous word in the personal
        dictionary based on the current sort setting); 5. Jump to Text (jumps from
        the personal dictionary to the source of the word in the original text); and
        6. Flash Words (displays the words in the personal dictionary in slide show
        fashion).<br />(46)  The View Menu includes: 1. Browse (sets the program to
        Browse Mode, indicated by the arrow cursor); 2. Edit (sets the program to
        Edit Mode, indicated by the I-beam cursor); 3. Show Note Guides (displays
        the location of the Notes in the text of the viewer); 4. Show Notes (displays
        the Notes field in the personal dictionary); 5. Show Info (displays the Word
        Information and sort control button in the personal dictionary); and 6. Show
        Palette (displays the Word Display Palette with the electronic viewer module).<br
        />(47)  After a study session starts, a Table of Contents for the selected
        book appears. By clicking on any item, the user is able to go to the desired
        section of the book. The selected chapter appears as a normal text file. The
        electronic viewer window has a display region with a button to display the
        Table of Contents. The current chapter name of the selected book is also displayed
        in this area. To select a word or phrase in the book, the user clicks on a
        word that is not understood and a pop-up menu immediately appears (see FIG.
        3). The pop-up information contains the yomi, the English reference, and the
        notes selection. If the pop-up menu does not appear, the selected word is
        not referenced. The yomi also appears in the pop-up menu.<br />(48)  To view
        the English reference information the user selects the English Reference from
        the pop-up menu and the information appears next to the pop-up menu.<br />(49)  To
        see the Note associated with the text, the user selects Notes from the pop-up
        menu and the Note appears in a separate window. If the Notes item is gray
        (for example, as shown in FIG. 3), no Note is available for the word. Notes
        also include a pop-up reference feature. The first word in the text with reference
        information has a black underbar beneath it. This is the Word Pointer, which
        indicates the most recent location for the pop-up menu and defaults to the
        first word. To see where a Note begins and ends, the user selects Show Note
        Guides from the View Menu.<br />(50)  The electronic viewer module also provides
        a Palette. To display the palette, the user selects Show Palette from the
        View Menu. The Word Display Palette displays all the reference information
        for quick viewing. The arrow buttons move the location of the Word Pointer
        and update the reference information. The See Note command displays the Note
        if one exists for the word and is gray if one is not present. The Add to PD
        command automatically copies the word and its associated information to the
        personal dictionary. If a Note is present, it is also copied to the personal
        dictionary.<br />(51)  A limited amount of text can be copied from the book
        by selecting Edit Mode from the View Menu, highlighting the desired text,
        and selecting Copy from the Edit Menu. Words can be searched for in the book
        by selecting Find from the Words Menu.<br />(52)  Using the Personal Dictionary
        Module<br />(53)  FIG. 5 is a screen display showing a Japanese word listed
        in a personal dictionary, as well as a personal dictionary control panel,
        according to the invention. The personal dictionary module in the exemplary
        embodiment of the invention is implemented in a relational database that is
        optimized for managing and studying words. Unlike electronic dictionaries
        where only the word entries of the dictionary are searchable, the personal
        dictionary module allows a user to search on each of the eight or more keys
        associated with a word, as discussed above. To open the personal dictionary,
        the user selects Personal Dictionary from the File menu or double clicks on
        a Personal Dictionary icon.<br />(54)  The words contained in the personal
        dictionary are displayed in large fields with the word on the bottom, the
        yomi above the word, and the English reference on top, as shown in FIG. 5.
        In Browse Mode, clicking on a word alternately hides and shows the word. This
        function is used to enhance review and study of the Main Control Buttons.Iadd..
        .Iaddend.The Main Control Buttons are located just below the Word field. The
        arrow keys display the next or previous words based on the sort key indicated
        by the Sort Button in the bottom left corner. The Show Notes button displays
        the Note information about the Word. This button toggles to Hide Notes when
        the field is displayed and Show Notes when hidden. Additional notes and annotations
        can be entered directly. The Quick Search button displays the word in a pop-window
        for quick search of a single character. After the pop-up is displayed, the
        user can click on the desired character to search. The Flash Words button
        displays the words in the personal dictionary in slide show fashion. Sort
        order or random order are selectable: sort order uses the current sort order.<br
        />(55)  The Find button displays the search dialogue window. Words are searchable
        by the following keys: Word, Yomi, English Reference, Category, Source, Priority,
        or Date. The personal dictionary supports logical &quot;AND&quot; searching
        for each of the above keys. The following features are supported: 1. Jump
        to Text--this button jumps control and display from the personal dictionary
        to the source of the word in the original text; 2. Show Info--this button
        displays the Word Information Buttons, as well as the Date Indicator; this
        button toggles to Hide Info when displayed, and Show Info when hidden; and
        3. Word Information--this button appears on the bottom of the screen and has
        the following functions:  a. Current Sort--sets the sort order for the Dictionary
        to either Category, Source, Priority, or Date; b. Category--provides for a
        set of predefined Categories for words as well as the ability to add new Categories;
        c. Source--indicates the source of the Word: user entered words are indicated
        by the user name or if not available, by the default User; d. Priority--allows
        the user to assign a priority to a word from 1 to 5; and e. Date Display--the
        date is displayed in the bottom right hand corner; the date is automatically
        updated each time the word is displayed. Searching<br />(56)  Both the electronic
        viewer module and the personal dictionary module provide search features accessible
        via the Word Menu. After selecting Find from the menu, the search dialogue
        appears.<br />(57)  The electronic viewer module includes a simple search
        feature that allows the user to search for a string of text anywhere in the
        book. The user enters the desired text and clicks Find to execute the Search.
        Find Next searches for the next occurrence of the word in the text.<br />(58)  In
        the personal dictionary, a slightly more complex search feature is provided.
        The search dialogue allows the user to enter multiple search terms. For example,
        a user can search for a certain term in the `Economics` category or the user
        can look for a Kanji that has a certain reading. More search terms result
        in increased search time. The search term for Word, Yomi, Reference, Note,
        and Source are indexed based on the first seven characters contained in the
        field. Characters appearing after the seventh character in any of these fields
        are not found with the `Starts With` selection. Selecting `Contains` searches
        the entire text in the field.<br />(59)  To search, the user enters the desired
        word or character and then selects `Starts With` or `Contains` from the menu.
        A `Starts With` search is the fastest. The `Category` search terms are based
        on the category list. The integers 1 to 5 can be entered for `Priority.` Date
        searching can be performed as `is`, `is before`, or `is after.` After entering
        the desired search information, the user clicks `Find` to execute the Search.
        Find Next searches for the next occurrence in the personal dictionary.<br
        />(60)  Importing/Exporting Word Lists<br />(61)  Text files can be read into
        the personal dictionary to make data exchange with other programs and colleagues
        feasible. The following format should be followed to allow accurate importing.
        One may use a spreadsheet program to build the word table and export the information
        as a tab delimited file. If a word processor is used, the user must add an
        extra tab for blank fields and follow the format listed below. In the exemplary
        embodiment of the invention, Export and Import uses the following format:<br
        />(62)   TABLE-US-00001 Word\t[TAB]; Pronunciation\t[TAB]; Meaning\t[TAB];
        Notes\t[TAB]; Category\t[TAB]; Source\t[TAB]; Priority\t[TAB]; and Date\t[Hard
        Return].<br />(63)  Setting up the Word field as column A in a spreadsheet
        and then exporting as a text file results in this format. If a word processor
        is used, one should also save as a text file. One should not include any hard
        returns (user entered returns) within the string of text for the given word.
        If given the option, the user should elect to have soft returns (automatically
        entered returns) deleted. To import, the user selects Import Words from the
        File Menu, and then chooses the file for import. To export, the user selects
        Export Words from the File Menu, and then enters a name for the given file.<br
        />(64)  Although the invention is described herein with reference to the preferred
        embodiment, one skilled in the art will readily appreciate that other applications
        may be substituted for those set forth herein without departing from the spirit
        and scope of the present invention. For example, the invention may be used
        to index images such that elements of the image are linked to an external
        reference. Thus, an illustration of the human body may include descriptive
        external resources for each of the body''s internal organs, and would thereby
        aid in the study of anatomy. Likewise, a video or other moving picture display,
        for example animated displays, could be indexed such that the picture could
        be stopped and various elements within a frame of the picture could be examined
        through links to external references. The invention allows such application
        because it does not embed information within the source material as is the
        case with prior art hyperlink technology. Rather, the invention creates a
        physical counterpart to the image in which a selected image position defines
        an offset that locates a desired external reference. Accordingly, the invention
        should only be limited by the claims included below.","claimsHtml":".[.1.
        A system for linking source material to reference material for display comprising:
        a source material image including a plurality of discrete pieces having links
        to external reference materials comprising any of textual, audio, video, and
        picture information, said source material image stored in an electronic database;
        means for determining an address on said electronic database for the beginning
        position of said source material image; means for cutting said source material
        image into said discrete pieces; means for determining an address on said
        electronic database for a start point and an end point of said discrete pieces
        of said image based upon said beginning position of said source material image;
        means for recording said start and said end point addresses in a look-up table;
        means for selecting a discrete portion of said source material image; means
        for determining the address on said electronic database of said selected discrete
        portion; means for converting said address of said selected discrete portion
        to an offset value from said beginning position address of said source material
        image; means for comparing said offset value with said recorded start and
        end point addresses of said discrete pieces in said look-up table; means for
        selecting an external reference that corresponds to said look-up table start
        and end point addresses; and means for reproducing said external reference..].<br
        /> .[.2. The system of claim 1, further comprising: a linking engine for linking
        said source material to said reference information on any of a word-by-word
        and phrase-by-phrase basis..].<br /> .[.3. The system of claim 2, said linking
        engine further comprising: word cut means for dividing said source material
        into discrete pieces; linking means for establishing at least one link between
        each of said discrete pieces and said reference information; compiler means
        for assembling an integrated source image from said discrete pieces; indexing
        means for linking said assembled discrete pieces to said reference information..].<br
        /> .[.4. The system of claim 3, said linking engine further comprising: means
        for building an index to link each of said source material pieces to said
        reference information..].<br /> .[.5. The system of claim 4, wherein said
        index links said source material pieces to said reference information based
        upon the value of the offset of the starting and ending position addresses
        of said source material pieces from the beginning position address of said
        integrated source image..].<br /> .[.6. The system of claim 5, wherein said
        offset locates said reference information to a corresponding source material
        piece based upon offset occurrence within a range defined by the value of
        the offsets of the starting and ending point addresses of said source material
        pieces from said beginning position address of said integrated source image..].<br
        /> .[.7. The system of claim 1, further comprising: means for manipulating
        said stored source material and reference information with at least two user
        keys..].<br /> .[.8. A method for linking source material to reference material
        for display, comprising: determining the beginning position address of a source
        material image stored in an electronic database, said source material image
        including a plurality of discrete pieces having links to external reference
        materials comprising any of textual, audio, video, and picture information;
        cutting said source material image into said discrete pieces; determining
        a starting point address and an ending point address of said discrete pieces
        of said image based upon said beginning position address of said source material
        image; recording said starting and said ending addresses in a look-up table;
        selecting a discrete portion of said source material image; determining the
        address of said selected discrete portion; converting said address of said
        selected discrete portion to an offset value from said beginning position
        address of said source material image; comparing said offset value with said
        recorded start and end point addresses of said discrete pieces in said look-up
        table; selecting an external reference that corresponds to said look-up table
        start and end point addresses; and reproducing said external reference..].<br
        /> .[.9. In a language learning method, a method for linking source material
        to reference material for display, comprising the steps of: reading a foreign
        language source material image including a plurality of discrete pieces having
        links to external reference materials comprising any of textual, audio, video,
        and picture information with an electronic viewer; accessing reference materials
        on selected portions of said source material image; determining the beginning
        position address of said source material image; cutting said source material
        image into said discrete pieces; determining a start point address and an
        end point address of said discrete pieces of said image based upon said beginning
        position address of said source material image; recording said start and said
        end point addresses in a look-up table; selecting a discrete portion of said
        source material image; determining the address of said selected discrete portion;
        converting said address of said selected discrete portion to an offset value
        from said beginning position address of said source material image; comparing
        said offset value with said recorded start and end point addresses of said
        discrete pieces in said look-up table; selecting an external reference that
        corresponds to said look-up table start and end point addresses; and reproducing
        said external reference..].<br /> .[.10. The method of claim 9, further comprising
        the step of: linking said source material to said reference information with
        a linking engine on any of a word-by-word and phrase-by-phrase basis..].<br
        /> .[.11. The method of claim 10, said linking step further comprising the
        steps of: dividing said source material into discrete pieces; establishing
        at least one link between each of said discrete pieces and said reference
        information; assembling an integrated source image from said discrete pieces;
        and linking said assembled discrete pieces to said reference information..].<br
        /> .[.12. The method of claim 11, said linking step further comprising the
        step of: building an index to link each of said source material pieces to
        said reference information..].<br /> .[.13. The method of claim 12, wherein
        said index links said source material pieces to said reference information
        based upon the offset between the starting position address for said source
        material pieces and the beginning position address of said integrated source
        image..].<br /> .[.14. The method of claim 13, wherein said offset locates
        said reference information to a corresponding source material piece based
        upon offset occurrence within a range defined by the value of the offsets
        of the starting and ending position addresses of said source material pieces
        from said beginning position address of said integrated source image..].<br
        /> .[.15. In a language learning system, a system for linking source material
        to reference material for display, comprising: a text image including a plurality
        of discrete pieces having links to external reference materials comprising
        any of textual, audio, video, and picture information; means for determining
        the beginning position address of said text image; means for cutting said
        text image into said discrete pieces; means for determining a starting point
        address and an ending point address of said discrete pieces of said image
        based upon said beginning position address of said source material image;
        means for recording said starting and said ending point addresses in a look-up
        table; means for selecting a discrete portion of said text image; means for
        determining the address of said selected discrete portion; means for converting
        said address of said selected discrete portion to an offset value from said
        beginning position address of said source material image; means for comparing
        said offset value with said recorded start and end point addresses of said
        discrete pieces in said look-up table; means for selecting an external reference
        that corresponds to said look-up table start and end point addresses; and
        means for displaying said external reference..].<br /> .[.16. In a language
        learning method, a method for linking source material to reference material
        for display, comprising the steps of: determining the beginning position address
        of a text image, said text image including a plurality of discrete pieces
        having links to external reference materials comprising any of textual, audio,
        video, and picture information; cutting said source material image into said
        discrete pieces; determining a starting point address and an ending point
        address of said discrete pieces of said image based upon said beginning position
        address of said text image; recording said starting and said ending point
        addresses in a look-up table; selecting a discrete portion of said text image;
        determining the address of said selected discrete portion; converting said
        address of said selected discrete portion to an offset value from said beginning
        position of said text image; comparing said offset value with said recorded
        start and end point addresses of said discrete pieces in said look-up table;
        selecting an external reference that corresponds to said look-up table start
        and end point address; and displaying said external reference..].<br /> .Iadd.17.
        A system for linking textual source material to external reference materials
        for display, the system comprising: means for determining a beginning position
        address of a textual source material stored in an electronic database; means
        for cutting the textual source material into a plurality of discrete pieces;
        means for determining starting point addresses and ending point addresses
        of the plurality of discrete pieces based upon the beginning position address;
        means for recording in a look-up table the starting and ending point addresses;
        means for linking the plurality of discrete pieces to external reference materials
        by recording in the look-up table, along with the starting and ending point
        addresses of the plurality of discrete pieces, links to the external reference
        materials, the external reference materials comprising any of textual, audio,
        video, and picture information; means for selecting a discrete portion of
        an image of the source material; means for determining a display address of
        the selected discrete portion; means for converting the display address of
        the selected discrete portion to an offset value from the beginning position
        address; means for comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces; means for selecting one of the external reference materials
        corresponding to the identified one of the plurality of discrete pieces; and
        means for displaying on a computer the selected one of the external reference
        materials..Iaddend.<br /> .Iadd.18. The system of claim 17, wherein the means
        for linking links the plurality of discrete pieces to external reference materials
        on a word-by-word or phrase-by-phrase basis..Iaddend.<br /> .Iadd.19. The
        system of claim 18, further comprising: means for compiling the source material
        image from at least the plurality of discrete pieces; and means for indexing
        the plurality of discrete pieces and corresponding links to the external reference
        materials..Iaddend.<br /> .Iadd.20. The system of claim 19, further comprising:
        means for building an index for each of the linked external reference materials..Iaddend.<br
        /> .Iadd.21. The system of claim 20, wherein the look-up table links the identified
        one of the plurality of discrete pieces to at least a corresponding one of
        the external reference materials based upon the offset value..Iaddend.<br
        /> .Iadd.22. The system of claim 21, wherein the identified one of the plurality
        of discrete pieces is identified based upon the offset value being within
        a range defined by the starting and ending point addresses of the identified
        one of the plurality of discrete pieces..Iaddend.<br /> .Iadd.23. The system
        of claim 17, further comprising: means for manipulating the source material
        image and the external reference materials with at least two user keys..Iaddend.<br
        /> .Iadd.24. The system of claim 17, wherein cutting the textual source material
        into a plurality of discrete pieces is done manually..Iaddend.<br /> .Iadd.25.
        The system of claim 17, wherein cutting the textual source material into a
        plurality of discrete pieces is done automatically..Iaddend.<br /> .Iadd.26.
        The system of claim 25, wherein automatically cutting the textual source material
        into a plurality of discrete pieces is done using a grammar parser..Iaddend.<br
        /> .Iadd.27. The system of claim 25, wherein automatically cutting the textual
        source material into a plurality of discrete pieces is done without using
        tags..Iaddend.<br /> .Iadd.28. The system of claim 25, wherein automatically
        cutting the textual source material into a plurality of discrete pieces is
        done without reference to any tags which may be located in the textual source
        material..Iaddend.<br /> .Iadd.29. The system of claim 17, wherein the link
        is a hyperlink..Iaddend.<br /> .Iadd.30. The system of claim 17, wherein the
        link is an address of the selected one of the external reference materials..Iaddend.<br
        /> .Iadd.31. The system of claim 17, wherein the link is reference information
        for retrieving the selected one of the external reference materials..Iaddend.<br
        /> .Iadd.32. The system of claim 17, wherein determining a display address
        of the selected discrete portion is done without using tags..Iaddend.<br />
        .Iadd.33. The system of claim 17, wherein determining a display address of
        the selected discrete portion is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.34. The
        system of claim 17, wherein determining a display address of the selected
        discrete portion is done without reference to any hierarchical information
        which may be located in the textual source material..Iaddend.<br /> .Iadd.35.
        The system of claim 17, wherein converting the display address of the selected
        discrete portion to an offset value from the beginning position address is
        done without using tags..Iaddend.<br /> .Iadd.36. The system of claim 17,
        wherein converting the display address of the selected discrete portion to
        an offset value from the beginning position address is done without reference
        to any tags which may be located in the textual source material..Iaddend.<br
        /> .Iadd.37. The system of claim 17, wherein converting the display address
        of the selected discrete portion to an offset value from the beginning position
        address is done without reference to any hierarchical information which may
        be located in the textual source material..Iaddend.<br /> .Iadd.38. The system
        of claim 17, wherein comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces is done without using tags..Iaddend.<br /> .Iadd.39. The
        system of claim 17, wherein comparing the offset value with the starting and
        ending point addresses recorded in the look-up table to identify one of the
        plurality of discrete pieces is done without reference to any tags which may
        be located in the textual source material..Iaddend.<br /> .Iadd.40. The system
        of claim 17, wherein comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces is done without reference to any hierarchical information
        which may be located in the textual source material..Iaddend.<br /> .Iadd.41.
        The system of claim 17, wherein the external reference materials comprise
        a plurality of text based external reference materials..Iaddend.<br /> .Iadd.42.
        The system of claim 17, wherein the external reference materials comprise
        a plurality of image based external reference materials..Iaddend.<br /> .Iadd.43.
        The system of claim 17, wherein the external reference materials comprise
        a plurality of graphic based external reference materials..Iaddend.<br />
        .Iadd.44. The system of claim 17, wherein the external reference materials
        comprise a plurality of audio based external reference materials..Iaddend.<br
        /> .Iadd.45. The system of claim 17, wherein the external reference materials
        comprise a plurality of video based external reference materials..Iaddend.<br
        /> .Iadd.46. The system of claim 17, wherein at least one of the external
        reference materials is a combination of two or more of text based external
        reference material, image based external reference material, graphic based
        external reference material, audio based external reference material, and
        video based external reference material..Iaddend.<br /> .Iadd.47. The system
        of claim 17, wherein linking the plurality of discrete pieces is done manually..Iaddend.<br
        /> .Iadd.48. The system of claim 17, wherein linking the plurality of discrete
        pieces is done automatically..Iaddend.<br /> .Iadd.49. The system of claim
        17, wherein the electronic database is an electronic relational database..Iaddend.<br
        /> .Iadd.50. The system of claim 17, wherein the electronic database is an
        electronic file..Iaddend.<br /> .Iadd.51. The system of claim 17, wherein
        the electronic database is electronic text..Iaddend.<br /> .Iadd.52. The system
        of claim 17, wherein the beginning position address is a beginning location
        of the textual source material in the electronic database..Iaddend.<br />
        .Iadd.53. The system of claim 52, wherein each starting point address is a
        starting location of at least one of the plurality of discrete pieces based
        upon the beginning location of the textual source material..Iaddend.<br />
        .Iadd.54. The system of claim 52, wherein each ending point address is an
        ending location of at least one of the plurality of discrete pieces based
        upon the beginning location of the textual source material..Iaddend.<br />
        .Iadd.55. The system of claim 17, further comprising: means for displaying
        the external reference materials corresponding to the identified one of the
        plurality of discrete pieces in a pop-up menu, prior to selecting one of the
        external reference materials corresponding to the identified one of the plurality
        of discrete pieces..Iaddend.<br /> .Iadd.56. The system of claim 55, wherein
        the pop-up menu is displayed in the textual source material image next to
        the selected discrete portion..Iaddend.<br /> .Iadd.57. The system of claim
        55, wherein the selected one of the external reference materials is selected
        using the pop-up menu..Iaddend.<br /> .Iadd.58. The system of claim 55, wherein
        the pop-up menu displays labels for the external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.59. The system of claim 55, wherein the labels can each be selected
        in the pop-up menu to display the external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.60. The system of claim 17, wherein the selected one of the external
        reference materials is a single word..Iaddend.<br /> .Iadd.61. The system
        of claim 17, wherein each of the external reference materials is a single
        word..Iaddend.<br /> .Iadd.62. A computer-implemented method for linking textual
        source material to external reference materials for display, the method comprising
        the steps of: determining a beginning position address of textual source material
        stored in an electronic database; cutting the textual source material into
        a plurality of discrete pieces; determining starting point addresses and ending
        point addresses of the plurality of discrete pieces based upon the beginning
        position address; recording in a look up table the starting and ending point
        addresses; linking the plurality of discrete pieces to external reference
        materials by recording in the look-up table, along with the starting and ending
        point addresses of the plurality of discrete pieces, links to the external
        reference materials, the external reference materials comprising any of textual,
        audio, video, and picture information; selecting a discrete portion of an
        image of the textual source material; determining a display address of the
        selected discrete portion; converting the display address of the selected
        discrete portion to an offset value from the beginning position address; comparing
        the offset value with the starting and ending point addresses recorded in
        the look-up table to identify one of the plurality of discrete pieces; selecting
        one of the external reference materials corresponding to the identified one
        of the plurality of discrete pieces; and displaying on a computer the selected
        one of the external reference materials..Iaddend.<br /> .Iadd.63. The method
        of claim 62, wherein cutting the textual source material into a plurality
        of discrete pieces is done manually..Iaddend.<br /> .Iadd.64. The method of
        claim 62, wherein cutting the textual source material into a plurality of
        discrete pieces is done automatically..Iaddend.<br /> .Iadd.65. The method
        of claim 64, wherein automatically cutting the textual source material into
        a plurality of discrete pieces is done using a grammar parser..Iaddend.<br
        /> .Iadd.66. The method of claim 64, wherein automatically cutting the textual
        source material into a plurality of discrete pieces is done without using
        tags..Iaddend.<br /> .Iadd.67. The method of claim 64, wherein automatically
        cutting the textual source material into a plurality of discrete pieces is
        done without reference to any tags which may be located in the textual source
        material..Iaddend.<br /> .Iadd.68. The method of claim 62, wherein the link
        is a hyperlink..Iaddend.<br /> .Iadd.69. The method of claim 62, wherein the
        link is an address of the selected one of the external reference materials..Iaddend.<br
        /> .Iadd.70. The method of claim 62, wherein the link is reference information
        for retrieving the selected one of the external reference materials..Iaddend.<br
        /> .Iadd.71. The method of claim 62, wherein determining a display address
        of the selected discrete portion is done without using tags..Iaddend.<br />
        .Iadd.72. The method of claim 62, wherein determining a display address of
        the selected discrete portion is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.73. The
        method of claim 62, wherein determining a display address of the selected
        discrete portion is done without reference to any hierarchical information
        which may be located in the textual source material..Iaddend.<br /> .Iadd.74.
        The method of claim 62, wherein converting the display address of the selected
        discrete portion to an offset value from the beginning position address is
        done without using tags..Iaddend.<br /> .Iadd.75. The method of claim 62,
        wherein converting the display address of the selected discrete portion to
        an offset value from the beginning position address is done without reference
        to any tags which may be located in the textual source material..Iaddend.<br
        /> .Iadd.76. The method of claim 62, wherein converting the display address
        of the selected discrete portion to an offset value from the beginning position
        address is done without reference to any hierarchical information which may
        be located in the textual source material..Iaddend.<br /> .Iadd.77. The method
        of claim 62, wherein comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces is done without using tags..Iaddend.<br /> .Iadd.78. The
        method of claim 62, wherein comparing the offset value with the starting and
        ending point addresses recorded in the look-up table to identify one of the
        plurality of discrete pieces is done without reference to any tags which may
        be located in the textual source material..Iaddend.<br /> .Iadd.79. The method
        of claim 62, wherein comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces is done without reference to any hierarchical information
        which may be located in the textual source material..Iaddend.<br /> .Iadd.80.
        The method of claim 62, wherein the external reference materials comprise
        a plurality of text based external reference materials..Iaddend.<br /> .Iadd.81.
        The method of claim 62, wherein the external reference materials comprise
        a plurality of image based external reference materials..Iaddend.<br /> .Iadd.82.
        The method of claim 62, wherein the external reference materials comprise
        a plurality of graphic based external reference materials..Iaddend.<br />
        .Iadd.83. The method of claim 62, wherein the external reference materials
        comprise a plurality of audio based external reference materials..Iaddend.<br
        /> .Iadd.84. The method of claim 62, wherein the external reference materials
        comprise a plurality of video based external reference materials..Iaddend.<br
        /> .Iadd.85. The method of claim 62, wherein at least one of the external
        reference materials is a combination of two or more of text based external
        reference material, image based external reference material, graphic based
        external reference material, audio based external reference material, and
        video based external reference material..Iaddend.<br /> .Iadd.86. The method
        of claim 62, wherein linking the plurality of discrete pieces is done manually..Iaddend.<br
        /> .Iadd.87. The method of claim 62, wherein linking the plurality of discrete
        pieces is done automatically..Iaddend.<br /> .Iadd.88. The method of claim
        62, wherein the electronic database is an electronic relational database..Iaddend.<br
        /> .Iadd.89. The method of claim 62, wherein the electronic database is an
        electronic file..Iaddend.<br /> .Iadd.90. The method of claim 62, wherein
        the electronic database is electronic text..Iaddend.<br /> .Iadd.91. The method
        of claim 62, wherein the beginning position address is a beginning location
        of the textual source material in the electronic database..Iaddend.<br />
        .Iadd.92. The method of claim 91, wherein each starting point address is a
        starting location of at least one of the plurality of discrete pieces based
        upon the beginning location of the textual source material..Iaddend.<br />
        .Iadd.93. The method of claim 91, wherein each ending point address is an
        ending location of at least one of the plurality of discrete pieces based
        upon the beginning location of the textual source material..Iaddend.<br />
        .Iadd.94. The method of claim 62, further comprising: displaying the external
        reference materials corresponding to the identified one of the plurality of
        discrete pieces in a pop-up menu, prior to selecting one of the external reference
        materials corresponding to the identified one of the plurality of discrete
        pieces..Iaddend.<br /> .Iadd.95. The method of claim 94, wherein the pop-up
        menu is displayed in the textual source material image next to the selected
        discrete portion..Iaddend.<br /> .Iadd.96. The method of claim 94, wherein
        the selected one of the external reference materials is selected using the
        pop-up menu..Iaddend.<br /> .Iadd.97. The method of claim 94, wherein the
        pop-up menu displays labels for the external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.98. The method of claim 94, wherein the labels can each be selected
        in the pop-up menu to display the external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.99. The method of claim 62, wherein the selected one of the external
        reference materials is a single word..Iaddend.<br /> .Iadd.100. The method
        of claim 62, wherein each of the external reference materials is a single
        word..Iaddend.<br /> .Iadd.101. A system for linking textual source material
        to external reference materials for display, the system comprising: means
        for determining a beginning position address of a textual source material
        stored in an electronic database; means for cutting the textual source material
        into a plurality of discrete pieces; means for determining a starting point
        address and an ending point address of at least one of the plurality of discrete
        pieces based upon the beginning position address; means for recording in a
        look-up table the starting and ending point addresses; means for linking at
        least one of the plurality of discrete pieces to at least one of a plurality
        of external reference materials by recording in the look-up table, along with
        the starting and ending point addresses of the at least one of the plurality
        of discrete pieces, a link to the at least one of the plurality of external
        reference materials, the plurality of external reference materials comprising
        any of textual, audio, video, and picture information; means for selecting
        a discrete portion of an image of the source material; means for determining
        a display address of the selected discrete portion; means for converting the
        display address of the selected discrete portion to an offset value from the
        beginning position address; means for comparing the offset value with the
        starting and ending point addresses recorded in the look-up table to identify
        one of the plurality of discrete pieces; means for selecting one of the at
        least one of the plurality of external reference materials corresponding to
        the identified one of the plurality of discrete pieces; and means for displaying
        on a computer the selected one of the plurality of external reference materials..Iaddend.<br
        /> .Iadd.102. The system of claim 101, wherein the means for linking links
        at least one of the plurality of discrete pieces to at least one of a plurality
        of external reference materials on a word-by-word or phrase-by-phrase basis..Iaddend.<br
        /> .Iadd.103. The system of claim 102, further comprising: means for compiling
        the source material image from at least the plurality of discrete pieces;
        and means for indexing at least one of the plurality of discrete pieces and
        corresponding links to the plurality of external reference materials..Iaddend.<br
        /> .Iadd.104. The system of claim 103, further comprising: means for building
        an index for each of the linked plurality of external reference materials..Iaddend.<br
        /> .Iadd.105. The system of claim 104, wherein the look-up table links the
        identified one of the plurality of discrete pieces to at least a corresponding
        one of a plurality of external reference materials based upon the offset value..Iaddend.<br
        /> .Iadd.106. The system of claim 105, wherein the identified one of the plurality
        of discrete pieces is identified based upon the offset value being within
        a range defined by the starting and ending point addresses of the identified
        one of the plurality of discrete pieces..Iaddend.<br /> .Iadd.107. The system
        of claim 101, further comprising: means for manipulating the source material
        image and the plurality of external reference materials with at least two
        user keys..Iaddend.<br /> .Iadd.108. The system of claim 101, wherein cutting
        the textual source material into a plurality of discrete pieces is done manually..Iaddend.<br
        /> .Iadd.109. The system of claim 101, wherein cutting the textual source
        material into a plurality of discrete pieces is done automatically..Iaddend.<br
        /> .Iadd.110. The system of claim 109, wherein automatically cutting the textual
        source material into a plurality of discrete pieces is done using a grammar
        parser..Iaddend.<br /> .Iadd.111. The system of claim 109, wherein automatically
        cutting the textual source material into a plurality of discrete pieces is
        done without using tags..Iaddend.<br /> .Iadd.112. The system of claim 109,
        wherein automatically cutting the textual source material into a plurality
        of discrete pieces is done without reference to any tags which may be located
        in the textual source material..Iaddend.<br /> .Iadd.113. The system of claim
        101, wherein the link is a hyperlink..Iaddend.<br /> .Iadd.114. The system
        of claim 101, wherein the link is an address of the selected one of the plurality
        of external reference materials..Iaddend.<br /> .Iadd.115. The system of claim
        101, wherein the link is reference information for retrieving the selected
        one of the plurality of external reference materials..Iaddend.<br /> .Iadd.116.
        The system of claim 101, wherein determining a display address of the selected
        discrete portion is done without using tags..Iaddend.<br /> .Iadd.117. The
        system of claim 101, wherein determining a display address of the selected
        discrete portion is done without reference to any tags which may be located
        in the textual source material..Iaddend.<br /> .Iadd.118. The system of claim
        101, wherein determining a display address of the selected discrete portion
        is done without reference to any hierarchical information which may be located
        in the textual source material..Iaddend.<br /> .Iadd.119. The system of claim
        101, wherein converting the display address of the selected discrete portion
        to an offset value from the beginning position address is done without using
        tags..Iaddend.<br /> .Iadd.120. The system of claim 101, wherein converting
        the display address of the selected discrete portion to an offset value from
        the beginning position address is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.121. The
        system of claim 101, wherein converting the display address of the selected
        discrete portion to an offset value from the beginning position address is
        done without reference to any hierarchical information which may be located
        in the textual source material..Iaddend.<br /> .Iadd.122. The system of claim
        101, wherein comparing the offset value with the starting and ending point
        addresses recorded in the look-up table to identify one of the plurality of
        discrete pieces is done without using tags..Iaddend.<br /> .Iadd.123. The
        system of claim 101, wherein comparing the offset value with the starting
        and ending point addresses recorded in the look-up table to identify one of
        the plurality of discrete pieces is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.124. The
        system of claim 101, wherein comparing the offset value with the starting
        and ending point addresses recorded in the look-up table to identify one of
        the plurality of discrete pieces is done without reference to any hierarchical
        information which may be located in the textual source material..Iaddend.<br
        /> .Iadd.125. The system of claim 101, wherein the plurality of external reference
        materials comprises a plurality of text based external reference materials..Iaddend.<br
        /> .Iadd.126. The system of claim 101, wherein the plurality of external reference
        materials comprises a plurality of image based external reference materials..Iaddend.<br
        /> .Iadd.127. The system of claim 101, wherein the plurality of external reference
        materials comprises a plurality of graphic based external reference materials..Iaddend.<br
        /> .Iadd.128. The system of claim 101, wherein the plurality of external reference
        materials comprises a plurality of audio based external reference materials..Iaddend.<br
        /> .Iadd.129. The system of claim 101, wherein the plurality of external reference
        materials comprises a plurality of video based external reference materials..Iaddend.<br
        /> .Iadd.130. The system of claim 101, wherein at least one of the plurality
        of external reference materials is a combination of two or more of text based
        external reference material, image based external reference material, graphic
        based external reference material, audio based external reference material,
        and video based external reference material..Iaddend.<br /> .Iadd.131. The
        system of claim 101, wherein linking at least one of the plurality of discrete
        pieces is done manually..Iaddend.<br /> .Iadd.132. The system of claim 101,
        wherein linking at least one of the plurality of discrete pieces is done automatically..Iaddend.<br
        /> .Iadd.133. The system of claim 101, wherein the electronic database is
        an electronic relational database..Iaddend.<br /> .Iadd.134. The system of
        claim 101, wherein the electronic database is an electronic file..Iaddend.<br
        /> .Iadd.135. The system of claim 101, wherein the electronic database is
        electronic text..Iaddend.<br /> .Iadd.136. The system of claim 101, wherein
        the beginning position address is a beginning location of the textual source
        material in the electronic database..Iaddend.<br /> .Iadd.137. The system
        of claim 136, wherein each starting point address is a starting location of
        at least one of the plurality of discrete pieces based upon the beginning
        location of the textual source material..Iaddend.<br /> .Iadd.138. The system
        of claim 136, wherein each ending point address is an ending location of at
        least one of the plurality of discrete pieces based upon the beginning location
        of the textual source material..Iaddend.<br /> .Iadd.139. The system of claim
        101, further comprising: means for displaying the plurality of external reference
        materials corresponding to the identified one of the plurality of discrete
        pieces in a pop-up menu, prior to selecting one of the at least one of the
        plurality of external reference materials corresponding to the identified
        one of the plurality of discrete pieces..Iaddend.<br /> .Iadd.140. The system
        of claim 139, wherein the pop-up menu is displayed in the textual source material
        image next to the selected discrete portion..Iaddend.<br /> .Iadd.141. The
        system of claim 139, wherein the selected one of the plurality of external
        reference materials is selected using the pop-up menu..Iaddend.<br /> .Iadd.142.
        The system of claim 139, wherein the pop-up menu displays labels for the plurality
        of external reference materials corresponding to the identified one of the
        plurality of discrete pieces..Iaddend.<br /> .Iadd.143. The system of claim
        139, wherein the labels can each be selected in the pop-up menu to display
        the at least one of the plurality of external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.144. The system of claim 101, wherein the at least one of the plurality
        of external reference materials is a single word..Iaddend.<br /> .Iadd.145.
        The system of claim 101, wherein each of the plurality of external reference
        materials is a single word..Iaddend.<br /> .Iadd.146. A computer-implemented
        method for linking textual source material to external reference materials
        for display, the method comprising the steps of: determining a beginning position
        address of textual source material stored in an electronic database; cutting
        the textual source material into a plurality of discrete pieces; determining
        a starting point address and an ending point address of at least one of the
        plurality of discrete pieces based upon the beginning position address; recording
        in a look up table the starting and ending point addresses; linking at least
        one of the plurality of discrete pieces to at least one of a plurality of
        external reference materials by recording in the look-up table, along with
        the starting and ending point addresses of the at least one of the plurality
        of discrete pieces, a link to the at least one of the plurality of external
        reference materials, the plurality of external reference materials comprising
        any of textual, audio, video, and picture information; selecting a discrete
        portion of an image of the textual source material; determining a display
        address of the selected discrete portion; converting the display address of
        the selected discrete portion to an offset value from the beginning position
        address; comparing the offset value with the starting and ending point addresses
        recorded in the look-up table to identify one of the plurality of discrete
        pieces; selecting one of the at least one of the plurality of external reference
        materials corresponding to the identified one of the plurality of discrete
        pieces; and displaying on a computer the selected one of the plurality of
        external reference materials..Iaddend.<br /> .Iadd.147. The method of claim
        146, wherein cutting the textual source material into a plurality of discrete
        pieces is done manually..Iaddend.<br /> .Iadd.148. The method of claim 146,
        wherein cutting the textual source material into a plurality of discrete pieces
        is done automatically..Iaddend.<br /> .Iadd.149. The method of claim 148,
        wherein automatically cutting the textual source material into a plurality
        of discrete pieces is done using a grammar parser..Iaddend.<br /> .Iadd.150.
        The method of claim 148, wherein automatically cutting the textual source
        material into a plurality of discrete pieces is done without using tags..Iaddend.<br
        /> .Iadd.151. The method of claim 148, wherein automatically cutting the textual
        source material into a plurality of discrete pieces is done without reference
        to any tags which may be located in the textual source material..Iaddend.<br
        /> .Iadd.152. The method of claim 146, wherein the link is a hyperlink..Iaddend.<br
        /> .Iadd.153. The method of claim 146, wherein the link is an address of the
        selected one of the plurality of external reference materials..Iaddend.<br
        /> .Iadd.154. The method of claim 146, wherein the link is reference information
        for retrieving the selected one of the plurality of external reference materials..Iaddend.<br
        /> .Iadd.155. The method of claim 146, wherein determining a display address
        of the selected discrete portion is done without using tags..Iaddend.<br />
        .Iadd.156. The method of claim 146, wherein determining a display address
        of the selected discrete portion is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.157. The
        method of claim 146, wherein determining a display address of the selected
        discrete portion is done without reference to any hierarchical information
        which may be located in the textual source material..Iaddend.<br /> .Iadd.158.
        The method of claim 146, wherein converting the display address of the selected
        discrete portion to an offset value from the beginning position address is
        done without using tags..Iaddend.<br /> .Iadd.159. The method of claim 146,
        wherein converting the display address of the selected discrete portion to
        an offset value from the beginning position address is done without reference
        to any tags which may be located in the textual source material..Iaddend.<br
        /> .Iadd.160. The method of claim 146, wherein converting the display address
        of the selected discrete portion to an offset value from the beginning position
        address is done without reference to any hierarchical information which may
        be located in the textual source material..Iaddend.<br /> .Iadd.161. The method
        of claim 146, wherein comparing the offset value with the starting and ending
        point addresses recorded in the look-up table to identify one of the plurality
        of discrete pieces is done without using tags..Iaddend.<br /> .Iadd.162. The
        method of claim 146, wherein comparing the offset value with the starting
        and ending point addresses recorded in the look-up table to identify one of
        the plurality of discrete pieces is done without reference to any tags which
        may be located in the textual source material..Iaddend.<br /> .Iadd.163. The
        method of claim 146, wherein comparing the offset value with the starting
        and ending point addresses recorded in the look-up table to identify one of
        the plurality of discrete pieces is done without reference to any hierarchical
        information which may be located in the textual source material..Iaddend.<br
        /> .Iadd.164. The method of claim 146, wherein the plurality of external reference
        materials comprises a plurality of text based external reference materials..Iaddend.<br
        /> .Iadd.165. The method of claim 146, wherein the plurality of external reference
        materials comprises a plurality of image based external reference materials..Iaddend.<br
        /> .Iadd.166. The method of claim 146, wherein the plurality of external reference
        materials comprises a plurality of graphic based external reference materials..Iaddend.<br
        /> .Iadd.167. The method of claim 146, wherein the plurality of external reference
        materials comprises a plurality of audio based external reference materials..Iaddend.<br
        /> .Iadd.168. The method of claim 146, wherein the plurality of external reference
        materials comprises a plurality of video based external reference materials..Iaddend.<br
        /> .Iadd.169. The method of claim 146, wherein at least one of the plurality
        of external reference materials is a combination of two or more of text based
        external reference material, image based external reference material, graphic
        based external reference material, audio based external reference material,
        and video based external reference material..Iaddend.<br /> .Iadd.170. The
        method of claim 146, wherein linking at least one of the plurality of discrete
        pieces is done manually..Iaddend.<br /> .Iadd.171. The method of claim 146,
        wherein linking at least one of the plurality of discrete pieces is done automatically..Iaddend.<br
        /> .Iadd.172. The method of claim 146, wherein the electronic database is
        an electronic relational database..Iaddend.<br /> .Iadd.173. The method of
        claim 146, wherein the electronic database is an electronic file..Iaddend.<br
        /> .Iadd.174. The method of claim 146, wherein the electronic database is
        electronic text..Iaddend.<br /> .Iadd.175. The method of claim 146, wherein
        the beginning position address is a beginning location of the textual source
        material in the electronic database..Iaddend.<br /> .Iadd.176. The method
        of claim 175, wherein each starting point address is a starting location of
        at least one of the plurality of discrete pieces based upon the beginning
        location of the textual source material..Iaddend.<br /> .Iadd.177. The method
        of claim 175, wherein each ending point address is an ending location of at
        least one of the plurality of discrete pieces based upon the beginning location
        of the textual source material..Iaddend.<br /> .Iadd.178. The method of claim
        146, further comprising: displaying the plurality of external reference materials
        corresponding to the identified one of the plurality of discrete pieces in
        a pop-up menu, prior to selecting one of the at least one of the plurality
        of external reference materials corresponding to the identified one of the
        plurality of discrete pieces..Iaddend.<br /> .Iadd.179. The method of claim
        178, wherein the pop-up menu is displayed in the textual source material image
        next to the selected discrete portion..Iaddend.<br /> .Iadd.180. The method
        of claim 178, wherein the selected one of the plurality of external reference
        materials is selected using the pop-up menu..Iaddend.<br /> .Iadd.181. The
        method of claim 178, wherein the pop-up menu displays labels for the plurality
        of external reference materials corresponding to the identified one of the
        plurality of discrete pieces..Iaddend.<br /> .Iadd.182. The method of claim
        178, wherein the labels can each be selected in the pop-up menu to display
        the at least one of the plurality of external reference materials corresponding
        to the identified one of the plurality of discrete pieces..Iaddend.<br />
        .Iadd.183. The method of claim 146, wherein the at least one of the plurality
        of external reference materials is a single word..Iaddend.<br /> .Iadd.184.
        The method of claim 146, wherein each of the plurality of external reference
        materials is a single word..Iaddend.","briefHtml":"(1) BACKGROUND OF THE INVENTION<br
        />(2)  1. Technical Field<br />(3)  The present invention relates to indexing
        displayed elements. More particularly, the present invention relates to a
        novel indexing scheme that is useful in such applications as learning a foreign
        language, for example a language based upon an ideographic alphabet, such
        as Japanese.<br />(4)  2. Description of the Prior Art<br />(5)  As the global
        economy turns the world''s many nations into what media visionary Marshall
        McLuhan referred to as a global village, the need to learn and use new or
        specialized information, such as a language other than one''s native language,
        becomes increasingly important. For example, there is a tremendous international
        demand for information related to Japan. Inside Japan, there is an abundance
        of information available in the Japanese language in numerous media forms.
        Japan has five national newspapers, seven major broadcasting networks, and
        several hundred book and magazine publishers. Japanese television focuses
        on the most obscure topics; and there are special interest magazines covering
        the full spectrum of Japanese society. Speakers of the Japanese language can
        find information on just about any topic imaginable. Unfortunately, outside
        of Japan this information is in short supply and the information that is available
        is primarily in English.<br />(6)  Individuals trying to learn about Japan
        are faced with the dilemma of either relying on English language sources or
        going through the pains of learning Japanese. English language information
        on Japan must go through the translation process. This results in time delays
        in obtaining necessary information, as well as in distortions in meaning.
        Furthermore, economics itself places restrictions on what information makes
        it''s way into English and what information does not. For general and introductory
        information on Japan, the English-based media is providing a valuable service.
        But for people who want to do more than scratch the surface, such information
        is far from sufficient.<br />(7)  A large number of non-native speakers have
        sought to study Japanese in universities or in professional language schools.
        In recent years, the interest level in Japanese among first year level college
        students has soared, such that it is rated second only to Spanish in some
        surveys. The number of people studying Japanese in the mid-1980''s in the
        United States was 50,000. This number has recently grown to 400,000 persons.
        But the study of Japanese language is plagued by the burdens of learning Kanji,
        the ideographic alphabet in which Japanese is written. Thus, the standing
        room only first-year Japanese language class in many universities soon becomes
        the almost private lesson-like third year class due to student attrition resulting
        from the difficulty of mastering Kanji.<br />(8)  The situation in Japan for
        foreigners is not much more encouraging. The cost of living in Japan poses
        a major barrier for both business people and students. There are currently
        over 300,000 United States citizens working or studying in Japan. But in recent
        years, foreign companies have been cutting their foreign staff. This, in part,
        has been in response to the enormous expense associated with maintaining them
        in Japan; but it is also a statement about the effectiveness of a large percentage
        of these people, who typically possess no Japanese language skills or background.
        Nevertheless, the necessity to do business in Japan is clear to most major
        United States companies, and access to Japan''s inside information is critical
        to the success of such companies.<br />(9)  The situation in Japanese universities
        is also discouraging. There are currently about 30,000 foreign students in
        Japanese universities, compared to a total of over 300,000 foreign students
        studying in the United States. Ninety percent of the foreign students in Japan
        are from Asia, while there are less than 1,000 students in Japan from the
        United States. The cost of living and housing again contribute greatly to
        this disparity, but the language barrier must be seen as the prime hurdle
        that causes students to abandon the attempt to explore Japan. In the future,
        the desirability for students and researchers to work in Japan should increase
        due to the growth of &quot;science cities&quot; and the increase in the hiring
        of foreign researchers by Japanese corporations. The burden of studying Japanese,
        however, remains.<br />(10)  In total there are over 60,000 people enrolled
        in Japanese language programs in Japan; and according to the Japan Foundation,
        there are approximately 1,000,000 Japanese language students worldwide, with
        a total of over 8,200 Japanese language instructors in 4,000 institutes. However,
        without a more effective and productive methodology for reading Japanese and
        for building Japanese language vocabulary, the level and breadth of the information
        making its way to non-natives should not be expected to improve.<br />(11)  The
        foregoing is but one example of the many difficulties one is faced with when
        acquiring or using difficult or unfamiliar material. The first challenge anyone
        reading a difficult text, is faced with is the issue of character recognition
        and pronunciation. For example, a student of the Japanese language spends
        many frustrating hours counting character strokes and looking up characters
        in a dictionary. Challenges such as this are the primary reason so many people
        give up on Japanese after a short trial period. It is also the reason that
        people who continue to pursue the language are unable to build an effective
        vocabulary.<br />(12)  Knowing the &quot;yomi&quot; or pronunciation or reading
        of a word is essential to memorize and assimilate the word into one''s vocabulary.
        This allows the student to read a word in context and often times deduce its
        meaning. But in many cases, the word may be entirely new to the reader, or
        it may be a usage that the reader has never seen. Looking up the word in the
        dictionary or asking a native speaker are the only options available to a
        student. Once the yomi for the word is known, i.e. the meaning and understanding
        of the word in context, the final challenge is to memorize the word and make
        it a part of a usable vocabulary.<br />(13)  The sheer number of characters
        in ideographic alphabets, such as Kanji, presents unique challenges for specifying
        and identifying individual characters.<br />(14)  Various schemes have been
        proposed and descriptions can be found in the literature for the entry of
        Kanji characters into computers and the like.<br />(15)  See, for example,
        Y. Chu, Chinese/Kanji Text and Data Processing, IEEE Computer (January 1985);
        J. Becker, Typing Chinese, Japanese, and Korean, IEEE Computer (January 1985);
        R. Matsuda, Processing Information in Japanese, IEEE Computer (January 1985);
        R. Walters, Design of a Bitmapped Multilingual Workstation, IEEE Computer
        (February 1990); and J. Huang, The Input and Output of Chinese and Japanese
        Characters, IEEE Computer (January 1985).<br />(16)  And, see J. Monroe, S.
        Roberts, T. Knoche, Method and Apparatus for Processing Ideographic Characters,
        U.S. Pat. No. 4,829,583 (9 May 1989), in which a specific sequence of strokes
        is entered into a 9.times.9 matrix, referred to as a training square. This
        sequence is matched to a set of possible corresponding ideographs. Because
        the matrix senses stroke starting point and stroke sequences based on the
        correct writing of the ideograph to be identified, this system cannot be used
        effectively until one has mastered the writing of the ideographic script.
        See, also G. Kostopoulos, Composite Character Generator, U.S. Pat. No. 4,670,841
        (2 Jun. 1987); A. Carmon, Method and Apparatus For Selecting, Storing and
        Displaying Chinese Script Characters, U.S. Pat. No. 4,937,745 (26 Jun. 1990);
        and R. Thomas, H. Stohr, Symbol Definition Apparatus, U.S. Pat. No. 5,187,480
        (16 Feb. 1993).<br />(17)  A text revision system is disclosed in R. Sakai,
        N, Kitajima, C. Oshima, Document Revising System For Use With Document Reading
        and Translation System, U.S. Pat. No. 5,222,160 (22 Jun. 1993), in which a
        foreigner having little knowledge of Japanese can revise misrecognized imaged
        characters during translation of the document from Japanese to another language.
        However, the system is provided for commercial translation services and not
        intended to educate a user in the understanding or meaning of the text.<br
        />(18)  Thus, although much attention has been paid, for example, to the writing,
        identification, and manipulation of ideographic characters, none of these
        approaches are concerned with providing a language learning system. The state
        of the art for ideographic languages, such as Japanese, does not provide an
        approach to learning the language that meets the four primary challenges discussed
        above, i.e. reading the language (for example, where an ideographic alphabet
        is used), comprehending the meaning of a particular word encountered while
        reading the language, understanding the true meaning of the word within the
        context that the word is used, and including the word in a personal dictionary
        to promote long term retention of the meaning of the word. A system that applies
        this approach to learning a language would be a significant advance in bridging
        the gap between the world''s diverse cultures because of the increased understanding
        that would result from an improved ability to communicate with one another.
        Such system would only be truly useful if it were based upon an indexing scheme
        that allowed meaningful manipulation and display of the various elements of
        the language.<br />(19) SUMMARY OF THE INVENTION<br />(20)  The invention
        provides a unique system for indexing displayed elements and finds ready application,
        for example in a language learning system that enhances and improves the way
        non-natives read foreign languages, for example the way a native English speaker
        reads Japanese. The language learning system provides a more effective way
        for people to read and improve their command of the foreign language, while
        at the same time communicating insightful and relevant cultural, social, and
        economic information about the country.<br />(21)  The learning model used
        by the language learning system is straightforward and is based upon methods
        that are familiar to most learners of foreign languages. The system addresses
        the four challenges of reading a foreign language, such as Japanese: i.e.
        reading the foreign word or character, such as Kanji in the case of a language
        having an ideographic alphabet, such as Japanese; comprehending the meaning
        of the word; understanding the word in context; and including the word in
        a personal vocabulary.<br />(22)  The exemplary embodiment of the invention
        includes one or more foreign language books that are read on an electronic
        display of a personal computer. English word references are available for
        each word in such books. The definitions of such words are derived from well
        known foreign language dictionaries. With regard to the Japanese language,
        the system saves significant amounts of time and effort by eliminating the
        need for the user to look up Japanese characters in a Kanji dictionary.<br
        />(23)  When one uses the system, the pronunciations or readings (`yomi`)
        for all words are immediately viewable in a pop-up window without accessing
        a disk based database, for example by clicking a mouse on a selected word
        or phrase. In the same pop-up window, the system provides an English reference
        to any word that is also selected by clicking on the selected word or phrase.
        The system provides extensive notes for difficult phrases and words not normally
        found in a dictionary, and includes a relational database designed for managing
        and studying words. This allows a user to build a personal database of words
        that he would like to master. Words may also be entered from other sources
        that are currently in paper or other electronic formats. A unique indexing
        scheme allows word-by-word access to any of several external multi-media references.","backgroundTextHtml":".Iadd.Note:
        More than one reissue patent application has been filed forthe reissue of
        U.S. Pat. No. 5,822,720. The reissue patent applications are U.S. Reissue
        patent application Ser. No. 11/064,519, filed Feb. 24, 2005, and issued as
        U.S. Pat. No. Re. 40,731, and the present U.S. Reissue patent application
        Ser. No. 12/480,556, filed Jun. 8, 2009, which is a continuation reissue application
        of U.S. Reissue patent application Ser. No. 11/064,519 filed Feb. 24, 2005
        now U.S. Pat. No. Re 40,731..Iaddend. \n\n This is a continuation of application
        Ser. No. 08/197,157 filed Feb. 16,\n1994 now abandoned.","subHeadingM0Html":null,"subHeadingM1Html":null,"subHeadingM2Html":null,"subHeadingM3Html":null,"subHeadingM4Html":null,"subHeadingM5Html":null,"subHeadingM6Html":null,"usClassIssued":"704/3","issuedUsDigestRefClassifi":null,"datePublYear":"2012","applicationYear":"2009","pfDerwentWeekYear":null,"pfApplicationYear":null,"pfPublYear":null,"reissueApplNumber":["12480556"],"abstractHeader":null,"abstractedPublicationDerwent":null,"affidavit130BFlag":null,"affidavit130BText":null,"applicantGroup":null,"applicantHeader":null,"applicationFilingDateInt":20090608,"applicationFilingDateIntKwicHits":null,"applicationRefFilingType":null,"applicationReferenceGroup":null,"applicationSeriesAndNumber":"12480556","applicationSeriesCode":"12","assignee1":null,"assigneeDescriptiveText":["N/A"],"patentAssigneeTerms":null,"associateAttorneyName":null,"attorneyName":null,"biologicalDepositInformation":null,"applicationType":null,"unlinkedDerwentRegistryNumber":null,"unlinkedRingIndexNumbersRarerFragments":null,"claimStatement":"We
        claim:","claimsTextAmended":null,"continuedProsecutionAppl":null,"cpcAdditionalLong":null,"cpcCisClassificationOrig":null,"cpcCombinationClassificationOrig":null,"cpcInventive":["G09B19/06
        20130101","G09B5/065 20130101"],"cpcInventiveCurrentDateKwicHits":null,"cpcAdditional":null,"cpcAdditionalCurrentDateKwicHits":null,"cpcOrigClassificationGroup":null,"curIntlPatentClassificationGroup":null,"curUsClassificationUsPrimaryClass":null,"curUsClassificationUsSecondaryClass":null,"customerNumber":null,"depositAccessionNumber":null,"depositDescription":null,"derwentClassAlpha":null,"designatedstatesRouteGroup":null,"docAccessionNumber":null,"drawingDescription":null,"editionField":null,"exchangeWeek":null,"exemplaryClaimNumber":["17"],"familyIdentifierOrig":null,"fieldOfSearchCpcClassification":null,"fieldOfSearchCpcMainClass":null,"fieldOfSearchIpcMainClass":null,"fieldOfSearchIpcMainClassSubclass":null,"fieldOfSearchSubclasses":["706-708;758;999.003","1;9;10","255;856","157;160"],"foreignRefGroup":["EP
        0 093 250 19831100 cited by other","EP 0 725 353 19960700 cited by other","EP
        0 840 240 19980500 cited by other","JP 03-174653 19910700 cited by other","JP
        04-220768 19920800 cited by other","JP 04-288674 19921000 cited by other","JP
        04-320530 19921100 cited by other","JP 04-320551 19921100 cited by other","JP
        05-012096 19930100 cited by other","JP 05-128157 19930500 cited by other","WO
        95/04974 19950200 cited by other"],"foreignRefPubDate":["19831100","19960700","19980500","19910700","19920800","19921000","19921100","19921100","19930100","19930500","19950200"],"foreignRefPubDateKwicHits":["19831100","19960700","19980500","19910700","19920800","19921000","19921100","19921100","19930100","19930500","19950200"],"foreignRefCitationClassification":["N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A"],"foreignRefPatentNumber":["0
        093 250","0 725 353","0 840 240","03-174653","04-220768","04-288674","04-320530","04-320551","05-012096","05-128157","95/04974"],"foreignRefCitationCpc":null,"foreignRefCountryCode":["EP","EP","EP","JP","JP","JP","JP","JP","JP","JP","WO"],"iceXmlIndicator":"Y","internationalClassificationHeader":null,"internationalClassificationInformationalGroup":null,"intlPubClassificationGroup":["20060101
        A G06F G06F17/30 F I B US H 20120904","20060101 A G06F G06F17/28 L I B US
        H 20120904"],"intlPubClassificationNonInvention":null,"inventorCitizenship":null,"inventorCorrection":null,"inventorDeceased":null,"inventorStreetAddress":["N/A","N/A"],"inventorText":["N/A","N/A"],"jpoFiClassification":null,"legalRepresentativeCity":null,"legalRepresentativeCountry":"[]","legalRepresentativeName":null,"legalRepresentativePostcode":null,"legalRepresentativeState":null,"legalRepresentativeStreetAddress":null,"legalRepresentativeText":null,"messengerDocsFlag":null,"newRecordPatentDerwent":null,"numberOfClaims":"168","numberOfDrawingSheets":"5","numberOfFigures":"5","numberOfPagesInSpecification":null,"numberOfPagesOfSpecification":null,"objectContents":null,"objectDescription":null,"parentDocCountry":null,"parentGrantDocCountry":null,"patentBibliographicHeader":null,"pctOrRegionalPublishingSerial":null,"pfDerwentWeekNum":null,"principalAttorneyName":null,"priorityApplicationCountry":null,"priorityClaimsCountry":null,"priorityNumberDerived":null,"publicationIssueNumber":null,"refCitedPatentDocNumber":null,"refCitedPatentDocDate":null,"refCitedPatentDocKindCode":null,"referenceCitedCode":null,"referenceCitedGroup":null,"referenceCitedSearchPhase":null,"referenceCitedText":null,"registrationNumber":null,"reissueApplCountry":["US"],"reissueParentKind":null,"reissueParentNumber":["5822720"],"reissueParentPubCountry":["US"],"reissuePatentGroup":["US
        08676890 19960708 GRANTED US 5822720 19981013 US 12480556"],"reissuePatentParentStatus":["GRANTED"],"reissuedPatentApplCountry":["US"],"reissuedPatentApplKind":null,"reissuedPatentApplNumber":["08676890"],"relatedApplChildPatentCountry":["US","US"],"relatedApplChildPatentName":null,"relatedApplChildPatentNumber":["08676890","11064519"],"relatedApplCountryCode":null,"relatedApplParentGrantPatentKind":null,"relatedApplParentGrantPatentName":null,"relatedApplParentPatentKind":null,"relatedApplParentPatentName":null,"relatedApplParentPctDoc":null,"relatedApplParentStatusCode":["ABANDONED"],"relatedApplPatentNumber":["Re.
        40731"],"relatedApplRelatedPub":null,"relatedApplTypeOfCorrection":null,"rule47Flag":null,"selectedDrawingCharacter":null,"selectedDrawingFigure":null,"statutoryInventionText":null,"termOfExtension":null,"termOfPatentGrant":null,"titleTermsData":null,"additionalIndexingTerm":null,"applicationYearSearch":"2009","pfApplicationYearSearch":null,"assigneeCountry":["US"],"certOfCorrectionFlag":null,"citedPatentLiteratureAddressInformation":null,"citedPatentLiteratureClassificationIpc":null,"citedPatentLiteratureOrganizationName":null,"citedPatentLiteratureRefNumber":null,"crossReferenceNumber":null,"country":"US","cpiManualCodes":null,"cpiSecondaryAccessionNumber":null,"curIntlPatentAllClassificationLong":null,"currentUsOriginalClassificationLong":null,"datePublSearch":"2012-09-04T00:00:00.000+00:00","datePublYearSearch":"2012","epiManualCodes":null,"fieldOfSearchMainClassNational":["707","704","715","345"],"inventorCountry":["US","US"],"ipcAllMainClassification":["G06F"],"issuedUsClassificationFull":["345/157","715/255","704/10","707/758","704/9","345/160","704/1","707/999.003","715/856","704/3"],"issuedUsDigestRefClassification":["345/157","345/160","704/1","704/9","704/10","715/856","715/255","707/999.003","707/758"],"jpoFiCurrentAdditionalClassification":null,"jpoFiCurrentInventiveClassification":null,"legalFirmName":["Wilmer
        Cutler Pickering Hale and Dorr LLP"],"locarnoMainClassification":null,"nonCpiSecondaryAccessionNumber":null,"objectId":null,"otherRefPub":["Allen,
        &quot;Introduction to Natural Language Understanding&quot;, Natural Language
        Understanding, Chapter 1, p. 1-19, The Benjamin/Cummings Publishing Company,
        Inc., 1988. cited by other\n<br />Almasi, et al., Highly Parallel Computing
        2.sup.nd Edition, The Benjamin/Cummings Publishing Company, Inc.,Chapter 2,
        p. 38-51 and 87-95, 1994. cited by other\n<br />Anonymous, &quot;Hypertext
        Method&quot;, IBM Technical Bulletin, 1 page, Oct. 1989. cited by other\n<br
        />Anonymous, Microsoft Corporation, Microsoft Word 97, screen printouts, pp.
        1-4,1997. cited by other\n<br />Baez, et al., &quot;Portable Translator&quot;,
        IBM Technical Bulletin, vol. 37, No. 11, p. 185-188, Nov. 1994. cited by other\n<br
        />Brookshear, Introduction to &quot;Computer Science, An Overview&quot;, Benjamin/Cummings
        Publishing, p. 17, 1988. cited by other\n<br />Cohen, et al., &quot;Method
        for Automatic Analysis of Meter in (both) Poetry and Prose&quot;, IBM Technical
        Bulletin, vol. 32, No. 9B, p. 224-226, Feb. 1990. cited by other\n<br />Dunnington,
        et al., &quot;Methodology and Apparatus for Translation of Text to Sign Language
        Images&quot;, vol. 37, No. 04A, p. 229-230, Apr. 1994. cited by other\n<br
        />Eisen, et al., &quot;Multilingual Multimedia Hyperlink Network Design&quot;,
        IBM Technical Bulletin, vol. 36, No. 09B, p. 471-472, Sep. 1993. cited by
        other\n<br />Eisen, et al., &quot;OS/2 Presentation Manager Controls Enabled
        for Hypermedia Link Markers&quot;, IBM Technical Bulletin, vol. 34, No. 10B,
        p. 433-434, Mar. 1992. cited by other\n<br />Elliott, &quot;Tuning up HyperCard''s
        Database Engine&quot;, Supplement to Dr. Dobb''s Journal, p. 39s-41s, Apr.
        1993. cited by other\n<br />Foley, et al., Introduction to &quot;Computer
        Graphics--Principles and Practice 2.sup.nd Ed. in C&quot;, Addison-Wesley
        Publishing Company, Inc., p. 1-9, 1996. cited by other\n<br />Fuger et al.,
        Proceedings of The First IEEE Conference on Evolutionary Computation, IEEE
        World Congress on Computational Intelligence, Walt Disney World Dolphin Hotel,
        Orlando, FL, vol. I, p. 229-234, Jun. 27-Jun. 29, 1994. cited by other\n<br
        />Germain, et al., &quot;Hypertext Document Update&quot;, IBM Technical Bulletin,
        vol. 34, No. 8, p. 22-23, Jan. 1992. cited by other\n<br />Glinert, A Pumped-Up
        Publishing Pro, Computer Shopper, Computer Select, p. 462, Apr. 1997. cited
        by other\n<br />Goodman, Web Documents Without HTML, Computer Select, Computer
        Shopper, Apr. 1997. cited by other\n<br />Goose, et al., &quot;Unifying Distributed
        Processing and Open Hypermedia through a Heterogeneous Communication Model&quot;,
        University of Southampton, Technical Report No. 95-6, p. 1-12, Nov. 1995.
        cited by other\n<br />Marshall, Acrobat, Common Ground Extend Reach Beyond
        Document Viewing, InfoWorld, Computer Select, Apr. 21, 1997, p. 105. cited
        by other\n<br />Montana, &quot;Automated Parameter Tuning for Interpretation
        of Synthetic Images&quot;, Handbook of Genetic Algorithms, edited by Lawrence
        Davis, Chapter 19, p. 282-311, 1991. cited by other\n<br />Nelson, FrenchNow
        3.0 Language-Learning Tool, Macworld Reviews, p. 83, Dec. 1995. cited by other\n<br
        />Takehi, &quot;Implementing Memory Efficient Hypertext in Online Manual Tool&quot;,
        IBM Technical Bulletin, vol. 33, No. 11, p. 259-263, Apr. 1991. cited by other\n<br
        />Weibel, Publish to Paper and the Web, Computer Select, Dec. 1996, PC/Computing,
        p. 130. cited by other\n<br />Winston, et al., Finding Patterns in Images,
        LISP 3.sup.rd Edition, Chapter 31, p. 456-483, Addison-Wesley Publishing Company,
        1989. cited by other\n<br />Yankelovich, et al., &quot;Reading and Writing
        the Electronic Book&quot;, Computer, vol. 18, No. 10, p. 15-30, Oct. 1985.
        cited by other\n<br />Addressmate Automatic Envelope Addressing Program, User''s
        Manual (1991). cited by other\n<br />User Manual for AddressMate and AddressMate
        Plus by AddressMate Software (1994-1995). cited by other\n<br />Apple Internet
        Address Detector User''s Manual, Aug. 28, 1997. cited by other\n<br />Microsoft
        Word 97 Help File entitled Automatically check spelling and grammar as you
        type (1997). cited by other\n<br />Developer''s Guide to Apple Data Detectors,
        Dec. 1997. cited by other\n<br />Novell GroupWise User''s Guide for Windows
        16-Bit, Version 5.2, 1993, MS 125993, Novell, Inc., Orem, Utah. cited by other
        (1993). cited by other\n<br />Haak, Personal Computing--WordPerfect News:
        WordPerfect for Windows 7.0; A Sneak Preview, Colorado State University, Vector
        Academic Computing &amp; Network Services, vol. 13, No. 3, Jan./Feb. 1996.
        cited by other\n<br />Thistlewaite, &quot;Automatic Construction and Management
        of Large Open Webs&quot;, Information Processing and Management, vol. 33,
        pp. 161-173 (published Mar. 1997). cited by other\n<br />Delivery and Retrieval
        Technology, Electronic Delivery, Document Management, Catalog Publishing,
        Page Layout, Hardware, Seybold Seminars Boston--Special Report, vol. 2, No.
        8, Apr. 1994. cited by other\n<br />Seybold Seminars and Imprinta ''92, part
        1: RIPs and Recorders, Seybold Report on Publishing Systems, vol. 21, No.
        12, p. 10, 1992. cited by other\n<br />Order re: Construction of Claim 8 of
        United States Patent No. 5,822,720 in the matter of Sentius Corporation v.
        Flyswat, Inc. in the United States District Court for the Northern District
        of California dated Apr. 4, 2002. cited by other\n<br />Order in the matter
        of Sentius Corporation v. Flyswat, Inc. in the United States District Court
        for the Northern District of California dated Aug. 5, 2002. cited by other\n<br
        />A sales brochure from Transparent Language of Hollis, NH about the Transparent
        Language System software, no date. cited by other\n<br />A sample screen from
        the software of Transparent Language of Hollis, NH, no date. cited by other\n<br
        />"],"pageNumber":null,"patentAssigneeCode":null,"patentAssigneeNameTotal":null,"patentFamilyDate":null,"patentFamilyDocNumber":null,"patentFamilyKind":null,"patentFamilyKindCode":null,"patentFamilyLanguage":null,"patentFamilyName":null,"patentNumberOfLocalApplication":null,"pct102eDate":null,"pct371c124Date":null,"pct371c124DateKwicHits":null,"pctFilingDate":null,"pctFilingDateKwicHits":null,"pctFilingDocCountryCode":null,"pctFilingKind":null,"pctFilingNumber":null,"pctName":null,"pctOrRegionalPublishingCountry":null,"pctOrRegionalPublishingKind":null,"pctOrRegionalPublishingName":null,"pctOrRegionalPublishingText":null,"pctPubDate":null,"pctPubDateKwicHits":null,"pctPubDocIdentifier":null,"pctPubNumber":null,"pfApplicationDateSearch":null,"pfApplicationType":null,"pfDerwentWeekDate":null,"pfPublDateSearch":null,"pfPublDateSearchKwicHits":null,"pfPublYearSearch":null,"polymerIndexingCodes":null,"polymerMultipunchCodeRecordNumber":null,"polymerMultipunchCodes":null,"priorPublishedDocCountryCode":null,"priorPublishedDocDate":null,"priorPublishedDocDateKwicHits":null,"priorPublishedDocIdentifier":null,"priorPublishedDocKindCode":null,"priorPublishedDocNumber":null,"priorityApplYear":null,"priorityApplicationDate":null,"priorityClaimsDateSearch":null,"priorityClaimsDocNumber":null,"priorityPatentDid":null,"priorityPatentNumber":null,"ptabCertFlag":null,"pubRefCountryCode":null,"pubRefDocNumber":"RE43633","pubRefDocNumber1":"RE043633","publicationData":null,"recordPatentNumber":null,"reexaminationFlag":null,"refCitedOthers":null,"refCitedPatentDocCountryCode":null,"refCitedPatentDocName":null,"refCitedPatentRelevantPassage":null,"reissueParentIssueDate":["1998-10-13T00:00:00.000+00:00"],"reissuedPatentApplFilingDate":["1996-07-08T00:00:00.000+00:00"],"relatedAccessionNumbers":null,"relatedApplChildPatentDate":null,"relatedApplFilingDateKwicHits":null,"relatedApplNumber":["11064519","08197157"],"relatedApplPatentIssueDate":null,"relatedApplPatentIssueDateKwicHits":null,"relatedDocumentKindCode":null,"securityLegend":null,"sequenceCwu":null,"sequenceListNewRules":null,"sequenceListOldRules":null,"sequencesListText":null,"standardTitleTerms":null,"supplementalExaminationFlag":null,"usBotanicLatinName":null,"usBotanicVariety":null,"usRefClassification":["N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A","N/A"],"usRefCpcClassification":null,"usRefGroup":["US
        3872448 A 19750300 Mitchell, Jr. et al. cited by other","US 4136395 A 19790100
        Kolpek et al. cited by other","US 4318184 A 19820300 Millett et al. cited
        by other","US 4384288 A 19830500 Walton cited by other","US 4651300 A 19870300
        Suzuki et al. cited by other","US 4674065 A 19870600 Lange et al. cited by
        other","US 4689768 A 19870800 Heard et al. cited by other","US 4742481 A 19880500
        Yoshimura cited by other","US 4773009 A 19880900 Kucera et al. cited by other","US
        4817050 A 19890300 Komatsu et al. cited by other","US 4837797 A 19890600 Freeny,
        Jr. cited by other","US 4864501 A 19890900 Kucera et al. cited by other","US
        4868743 A 19890900 Nishio cited by other","US 4868750 A 19890900 Kucera et
        al. cited by other","US 4887212 A 19891200 Zamora et al. cited by other","US
        4893270 A 19900100 Beck et al. cited by other","US 4914586 A 19900400 Swinehart
        et al. cited by other","US 4945476 A 19900700 Bodick et al. cited by other","US
        4958283 A 19900900 Tawara et al. cited by other","US 4980855 A 19901200 Kojima
        cited by other","US 4982344 A 19910100 Jordan cited by other","US 4994966
        A 19910200 Hutchins cited by other","US 5020019 A 19910500 Ogawa cited by
        other","US 5065315 A 19911100 Garcia cited by other","US 5088052 A 19920200
        Spielman et al. cited by other","US 5128865 A 19920700 Sadler cited by other","US
        5146439 A 19920900 Jachmann et al. cited by other","US 5146552 A 19920900
        Cassorla et al. cited by other","US 5151857 A 19920900 Matsui cited by other","US
        5157606 A 19921000 Nagashima cited by other","US 5204947 A 19930400 Bernstein
        et al. cited by other","US 5214583 A 19930500 Miike et al. cited by other","US
        5218697 A 19930600 Chung cited by other","US 5222160 A 19930600 Sakai et al.
        cited by other","US 5226117 A 19930700 Miklos cited by other","US 5233513
        A 19930800 Doyle cited by other","US 5241671 A 19930800 Reed et al. cited
        by other","US 5253362 A 19931000 Nolan et al. cited by other","US 5256067
        A 19931000 Gildea et al. cited by other","US 5267155 A 19931100 Buchanan et
        al. cited by other","US 5289376 A 19940200 Yokogawa cited by other","US 5297249
        A 19940300 Bernstein et al. cited by other","US 5303151 A 19940400 Neumann
        cited by other","US 5319711 A 19940600 Servi cited by other","US 5329446 A
        19940700 Kugimiya et al. cited by other","US 5331555 A 19940700 Hashimoto
        et al. cited by other","US 5337233 A 19940800 Hofert et al. cited by other","US
        5349368 A 19940900 Takeda et al. cited by other","US 5351190 A 19940900 Kondo
        cited by other","US 5361202 A 19941100 Doue cited by other","US 5367621 A
        19941100 Cohen et al. cited by other","US 5375200 A 19941200 Dugan et al.
        cited by other","US 5377323 A 19941200 Vasudevan cited by other","US 5392386
        A 19950200 Chalas cited by other","US 5404435 A 19950400 Rosenbaum cited by
        other","US 5404506 A 19950400 Fujisawa et al. cited by other","US 5408655
        A 19950400 Oren et al. cited by other","US 5416901 A 19950500 Torres cited
        by other","US 5418942 A 19950500 Krawchuk et al. cited by other","US 5434974
        A 19950700 Loucks et al. cited by other","US 5438655 A 19950800 Richichi et
        al. cited by other","US 5455945 A 19951000 Vanderdrift cited by other","US
        5459860 A 19951000 Brunett et al. cited by other","US 5491783 A 19960200 Douglas
        et al. cited by other","US 5491784 A 19960200 Douglas et al. cited by other","US
        5500859 A 19960300 Sharma et al. cited by other","US 5506984 A 19960400 Miller
        cited by other","US 5515534 A 19960500 Chuah et al. cited by other","US 5517409
        A 19960500 Ozawa et al. cited by other","US 5530852 A 19960600 Meske, Jr.
        et al. cited by other","US 5530853 A 19960600 Schell et al. cited by other","US
        5537132 A 19960700 Teraoka et al. cited by other","US 5537590 A 19960700 Amado
        cited by other","US 5541836 A 19960700 Church et al. cited by other","US 5546447
        A 19960800 Skarbo et al. cited by other","US 5546529 A 19960800 Bowers et
        al. cited by other","US 5564046 A 19961000 Nemoto et al. cited by other","US
        5576955 A 19961100 Newbold et al. cited by other","US 5581460 A 19961200 Kotake
        et al. cited by other","US 5583761 A 19961200 Chou cited by other","US 5603025
        A 19970200 Tabb et al. cited by other","US 5606712 A 19970200 Hidaka cited
        by other","US 5608900 A 19970300 Dockter et al. cited by other","US 5617488
        A 19970400 Hong et al. cited by other","US 5629981 A 19970500 Nerlikar cited
        by other","US 5640565 A 19970600 Dickinson cited by other","US 5644740 A 19970700
        Kiuchi cited by other","US 5646416 A 19970700 Van De Velde cited by other","US
        5649222 A 19970700 Mogilevsky cited by other","US 5657259 A 19970800 Davis
        et al. cited by other","US 5659676 A 19970800 Redpath cited by other","US
        5666502 A 19970900 Capps cited by other","US 5694523 A 19971200 Wical cited
        by other","US 5708804 A 19980100 Goodwin et al. cited by other","US 5708822
        A 19980100 Wical cited by other","US 5708825 A 19980100 Sotomayor cited by
        other","US 5724593 A 19980300 Hargrave, III et al. cited by other","US 5724597
        A 19980300 Cuthbertson et al. cited by other","US 5727129 A 19980300 Barrett
        et al. cited by other","US 5729741 A 19980300 Liaguno et al. cited by other","US
        5732229 A 19980300 Dickinson cited by other","US 5740252 A 19980400 Minor
        et al. cited by other","US 5745360 A 19980400 Leone cited by other","US 5745908
        A 19980400 Anderson et al. cited by other","US 5754847 A 19980500 Kaplan et
        al. cited by other","US 5754857 A 19980500 Gadol cited by other","US 5761436
        A 19980600 Nielsen cited by other","US 5761656 A 19980600 Ben-Shachar cited
        by other","US 5761659 A 19980600 Bertoni cited by other","US 5761689 A 19980600
        Rayson cited by other","US 5764906 A 19980600 Edelstein et al. cited by other","US
        5778363 A 19980700 Light cited by other","US 5781189 A 19980700 Holleran et
        al. cited by other","US 5781900 A 19980700 Shoji et al. cited by other","US
        5781904 A 19980700 Oren et al. cited by other","US 5787386 A 19980700 Kaplan
        et al. cited by other","US 5793972 A 19980800 Shane cited by other","US 5794050
        A 19980800 Dahlgren et al. cited by other","US 5794228 A 19980800 French et
        al. cited by other","US 5794259 A 19980800 Kikinis cited by other","US 5799267
        A 19980800 Siegel cited by other","US 5799302 A 19980800 Johnson et al. cited
        by other","US 5802559 A 19980900 Bailey cited by other","US 5805886 A 19980900
        Skarbo et al. cited by other","US 5805911 A 19980900 Miller cited by other","US
        5806079 A 19980900 Rivette et al. cited by other","US 5815830 A 19980900 Anthony
        cited by other","US 5819092 A 19981000 Ferguson et al. cited by other","US
        5822539 A 19981000 Van Hoff cited by other","US 5822720 A 19981000 Bookman
        et al. cited by other","US 5826257 A 19981000 Snelling, Jr. cited by other","US
        5832496 A 19981100 Anand et al. cited by other","US 5835059 A 19981100 Nadel
        et al. cited by other","US 5835089 A 19981100 Skarbo et al. cited by other","US
        5845238 A 19981200 Fredenburg cited by other","US 5855007 A 19981200 Jovicic
        et al. cited by other","US 5859636 A 19990100 Pandit cited by other","US 5860073
        A 19990100 Ferrel et al. cited by other","US 5862325 A 19990100 Reed et al.
        cited by other","US 5864848 A 19990100 Horvitz et al. cited by other","US
        5870702 A 19990200 Yamabana cited by other","US 5870746 A 19990200 Knutson
        cited by other","US 5873107 A 19990200 Borovoy et al. cited by other","US
        5875443 A 19990200 Nielsen cited by other","US 5875446 A 19990200 Brown et
        al. cited by other","US 5878421 A 19990300 Ferrel et al. cited by other","US
        5884247 A 19990300 Christy cited by other","US 5884302 A 19990300 Ho cited
        by other","US 5884309 A 19990300 Vanechanos, Jr. cited by other","US 5892919
        A 19990400 Nielsen cited by other","US 5893093 A 19990400 Wills cited by other","US
        5895461 A 19990400 De La Huerga et al. cited by other","US 5896321 A 19990400
        Miller et al. cited by other","US 5896533 A 19990400 Ramos et al. cited by
        other","US 5897475 A 19990400 Pace et al. cited by other","US 5900004 A 19990500
        Gipson cited by other","US 5905866 A 19990500 Nakabayashi et al. cited by
        other","US 5905991 A 19990500 Reynolds cited by other","US 5907838 A 19990500
        Miyasaka et al. cited by other","US 5913214 A 19990600 Madnnick et al. cited
        by other","US 5920859 A 19990700 Li cited by other","US 5924090 A 19990700
        Krellenstein cited by other","US 5926808 A 19990700 Evans et al. cited by
        other","US 5930471 A 19990700 Milewski et al. cited by other","US 5940843
        A 19990800 Zucknovich et al. cited by other","US 5946647 A 19990800 Miller
        et al. cited by other","US 5953718 A 19990900 Wical cited by other","US 5963205
        A 19991000 Sotomayor cited by other","US 5963940 A 19991000 Liddy et al. cited
        by other","US 5963950 A 19991000 Nielsen et al. cited by other","US 5970505
        A 19991000 Ebrahim cited by other","US 5974413 A 19991000 Beauregard et al.
        cited by other","US 5983171 A 19991100 Yokoyama et al. cited by other","US
        5987403 A 19991100 Sugimura cited by other","US 5987460 A 19991100 Niwa et
        al. cited by other","US 5987475 A 19991100 Murai cited by other","US 5999938
        A 19991200 Bliss et al. cited by other","US 6006218 A 19991200 Breese et al.
        cited by other","US 6006242 A 19991200 Poole et al. cited by other","US 6014677
        A 20000100 Hayashi et al. cited by other","US 6021403 A 20000200 Horvitz et
        al. cited by other","US 6022222 A 20000200 Guinan cited by other","US 6026088
        A 20000200 Rostoker et al. cited by other","US 6026398 A 20000200 Brown et
        al. cited by other","US 6028605 A 20000200 Conrad et al. cited by other","US
        6031537 A 20000200 Hugh cited by other","US 6038573 A 20000300 Parks cited
        by other","US 6047252 A 20000400 Kumano et al. cited by other","US 6055531
        A 20000400 Bennett et al. cited by other","US 6061675 A 20000500 Wical cited
        by other","US 6067565 A 20000500 Horvitz cited by other","US 6076088 A 20000600
        Paik et al. cited by other","US 6085201 A 20000700 Tso cited by other","US
        6085226 A 20000700 Horvitz cited by other","US 6092074 A 20000700 Rodkin et
        al. cited by other","US 6094649 A 20000700 Bowen et al. cited by other","US
        6108674 A 20000800 Murakami et al. cited by other","US 6122647 A 20000900
        Horowitz et al. cited by other","US 6126306 A 20001000 Ando cited by other","US
        6128635 A 20001000 Ikeno cited by other","US 6137911 A 20001000 Zhilyaev cited
        by other","US 6151624 A 20001100 Teare et al. cited by other","US 6154738
        A 20001100 Call cited by other","US 6178434 B1 20010100 Saitoh cited by other","US
        6182133 B1 20010100 Horvitz cited by other","US 6185550 B1 20010200 Snow et
        al. cited by other","US 6185576 B1 20010200 McIntosh cited by other","US 6233570
        B1 20010500 Horvits et al. cited by other","US 6260035 B1 20010700 Horvitz
        et al. cited by other","US 6262730 B1 20010700 Horvitz et al. cited by other","US
        6272505 B1 20010800 De La Huerga cited by other","US 6289342 B1 20010900 Lawrence
        et al. cited by other","US 6292768 B1 20010900 Chan cited by other","US 6308171
        B1 20011000 De La Huerga cited by other","US 6311177 B1 20011000 Dauerer et
        al. cited by other","US 6311194 B1 20011000 Sheth et al. cited by other","US
        6323853 B1 20011100 Hedloy cited by other","US 6338059 B1 20020100 Fields
        et al. cited by other","US 6373502 B1 20020400 Nielsen cited by other","US
        6438545 B1 20020800 Beauregard et al. cited by other","US 6442545 B1 20020800
        Feldman et al. cited by other","US 6516321 B1 20030200 De La Huerga cited
        by other","US 6519603 B1 20030200 Bays et al. cited by other","US 6556984
        B1 20030400 Zien cited by other","US 6571241 B1 20030500 Nosohara cited by
        other","US 6601026 B2 20030700 Appelt et al. cited by other","US 6618733 B1
        20030900 White et al. cited by other","US 6625581 B1 20030900 Perkowski cited
        by other","US 6629079 B1 20030900 Spiegel et al. cited by other","US 6651059
        B1 20031100 Sundaresan et al. cited by other","US 6697824 B1 20040200 Bowman-Amuah
        cited by other","US 6732090 B2 20040500 Shanahan et al. cited by other","US
        6732361 B1 20040500 Andreoli et al. cited by other","US 7003522 B1 20060200
        Reynar et al. cited by other","US 7032174 B2 20060400 Montero et al. cited
        by other","US 7130861 B2 20061000 Bookman et al. cited by other","US 7287218
        B1 20071000 Knotz et al. cited by other","US 7496854 B2 20090200 Hedloy cited
        by other","US RE40731 E 20090600 Bookman et al. cited by other","US 2002/0035581
        A1 20020300 Reynar et al. cited by other","US 2002/0036654 A1 20020300 Evans
        et al. cited by other","US 2002/0062353 A1 20020500 Konno et al. cited by
        other","US 2002/0065891 A1 20020500 Malik cited by other","US 2002/0091803
        A1 20020700 Imamura et al. cited by other","US 2002/0099730 A1 20020700 Brown
        cited by other","US 2002/0184247 A1 20021200 Jokela et al. cited by other","US
        2003/0004909 A1 20030100 Chauhan et al. cited by other","US 2003/0033290 A1
        20030200 Garner et al. cited by other","US 2003/0154144 A1 20030800 Pokorny
        et al. cited by other","US 2003/0187587 A1 20031000 Swindells et al. cited
        by other","US 2003/0212527 A1 20031100 Moore et al. cited by other"],"usRefIssueDate":["19750300","19790100","19820300","19830500","19870300","19870600","19870800","19880500","19880900","19890300","19890600","19890900","19890900","19890900","19891200","19900100","19900400","19900700","19900900","19901200","19910100","19910200","19910500","19911100","19920200","19920700","19920900","19920900","19920900","19921000","19930400","19930500","19930600","19930600","19930700","19930800","19930800","19931000","19931000","19931100","19940200","19940300","19940400","19940600","19940700","19940700","19940800","19940900","19940900","19941100","19941100","19941200","19941200","19950200","19950400","19950400","19950400","19950500","19950500","19950700","19950800","19951000","19951000","19960200","19960200","19960300","19960400","19960500","19960500","19960600","19960600","19960700","19960700","19960700","19960800","19960800","19961000","19961100","19961200","19961200","19970200","19970200","19970300","19970400","19970500","19970600","19970700","19970700","19970700","19970800","19970800","19970900","19971200","19980100","19980100","19980100","19980300","19980300","19980300","19980300","19980300","19980400","19980400","19980400","19980500","19980500","19980600","19980600","19980600","19980600","19980600","19980700","19980700","19980700","19980700","19980700","19980800","19980800","19980800","19980800","19980800","19980800","19980900","19980900","19980900","19980900","19980900","19981000","19981000","19981000","19981000","19981100","19981100","19981100","19981200","19981200","19990100","19990100","19990100","19990100","19990200","19990200","19990200","19990200","19990200","19990300","19990300","19990300","19990300","19990400","19990400","19990400","19990400","19990400","19990400","19990500","19990500","19990500","19990500","19990600","19990700","19990700","19990700","19990700","19990800","19990800","19990900","19991000","19991000","19991000","19991000","19991000","19991100","19991100","19991100","19991100","19991200","19991200","19991200","20000100","20000200","20000200","20000200","20000200","20000200","20000200","20000300","20000400","20000400","20000500","20000500","20000600","20000700","20000700","20000700","20000700","20000800","20000900","20001000","20001000","20001000","20001100","20001100","20010100","20010100","20010200","20010200","20010500","20010700","20010700","20010800","20010900","20010900","20011000","20011000","20011000","20011100","20020100","20020400","20020800","20020800","20030200","20030200","20030400","20030500","20030700","20030900","20030900","20030900","20031100","20040200","20040500","20040500","20060200","20060400","20061000","20071000","20090200","20090600","20020300","20020300","20020500","20020500","20020700","20020700","20021200","20030100","20030200","20030800","20031000","20031100"],"usRefIssueDateKwicHits":["19750300","19790100","19820300","19830500","19870300","19870600","19870800","19880500","19880900","19890300","19890600","19890900","19890900","19890900","19891200","19900100","19900400","19900700","19900900","19901200","19910100","19910200","19910500","19911100","19920200","19920700","19920900","19920900","19920900","19921000","19930400","19930500","19930600","19930600","19930700","19930800","19930800","19931000","19931000","19931100","19940200","19940300","19940400","19940600","19940700","19940700","19940800","19940900","19940900","19941100","19941100","19941200","19941200","19950200","19950400","19950400","19950400","19950500","19950500","19950700","19950800","19951000","19951000","19960200","19960200","19960300","19960400","19960500","19960500","19960600","19960600","19960700","19960700","19960700","19960800","19960800","19961000","19961100","19961200","19961200","19970200","19970200","19970300","19970400","19970500","19970600","19970700","19970700","19970700","19970800","19970800","19970900","19971200","19980100","19980100","19980100","19980300","19980300","19980300","19980300","19980300","19980400","19980400","19980400","19980500","19980500","19980600","19980600","19980600","19980600","19980600","19980700","19980700","19980700","19980700","19980700","19980800","19980800","19980800","19980800","19980800","19980800","19980900","19980900","19980900","19980900","19980900","19981000","19981000","19981000","19981000","19981100","19981100","19981100","19981200","19981200","19990100","19990100","19990100","19990100","19990200","19990200","19990200","19990200","19990200","19990300","19990300","19990300","19990300","19990400","19990400","19990400","19990400","19990400","19990400","19990500","19990500","19990500","19990500","19990600","19990700","19990700","19990700","19990700","19990800","19990800","19990900","19991000","19991000","19991000","19991000","19991000","19991100","19991100","19991100","19991100","19991200","19991200","19991200","20000100","20000200","20000200","20000200","20000200","20000200","20000200","20000300","20000400","20000400","20000500","20000500","20000600","20000700","20000700","20000700","20000700","20000800","20000900","20001000","20001000","20001000","20001100","20001100","20010100","20010100","20010200","20010200","20010500","20010700","20010700","20010800","20010900","20010900","20011000","20011000","20011000","20011100","20020100","20020400","20020800","20020800","20030200","20030200","20030400","20030500","20030700","20030900","20030900","20030900","20031100","20040200","20040500","20040500","20060200","20060400","20061000","20071000","20090200","20090600","20020300","20020300","20020500","20020500","20020700","20020700","20021200","20030100","20030200","20030800","20031000","20031100"],"usRefPatenteeName":["Mitchell,
        Jr. et al.","Kolpek et al.","Millett et al.","Walton","Suzuki et al.","Lange
        et al.","Heard et al.","Yoshimura","Kucera et al.","Komatsu et al.","Freeny,
        Jr.","Kucera et al.","Nishio","Kucera et al.","Zamora et al.","Beck et al.","Swinehart
        et al.","Bodick et al.","Tawara et al.","Kojima","Jordan","Hutchins","Ogawa","Garcia","Spielman
        et al.","Sadler","Jachmann et al.","Cassorla et al.","Matsui","Nagashima","Bernstein
        et al.","Miike et al.","Chung","Sakai et al.","Miklos","Doyle","Reed et al.","Nolan
        et al.","Gildea et al.","Buchanan et al.","Yokogawa","Bernstein et al.","Neumann","Servi","Kugimiya
        et al.","Hashimoto et al.","Hofert et al.","Takeda et al.","Kondo","Doue","Cohen
        et al.","Dugan et al.","Vasudevan","Chalas","Rosenbaum","Fujisawa et al.","Oren
        et al.","Torres","Krawchuk et al.","Loucks et al.","Richichi et al.","Vanderdrift","Brunett
        et al.","Douglas et al.","Douglas et al.","Sharma et al.","Miller","Chuah
        et al.","Ozawa et al.","Meske, Jr. et al.","Schell et al.","Teraoka et al.","Amado","Church
        et al.","Skarbo et al.","Bowers et al.","Nemoto et al.","Newbold et al.","Kotake
        et al.","Chou","Tabb et al.","Hidaka","Dockter et al.","Hong et al.","Nerlikar","Dickinson","Kiuchi","Van
        De Velde","Mogilevsky","Davis et al.","Redpath","Capps","Wical","Goodwin et
        al.","Wical","Sotomayor","Hargrave, III et al.","Cuthbertson et al.","Barrett
        et al.","Liaguno et al.","Dickinson","Minor et al.","Leone","Anderson et al.","Kaplan
        et al.","Gadol","Nielsen","Ben-Shachar","Bertoni","Rayson","Edelstein et al.","Light","Holleran
        et al.","Shoji et al.","Oren et al.","Kaplan et al.","Shane","Dahlgren et
        al.","French et al.","Kikinis","Siegel","Johnson et al.","Bailey","Skarbo
        et al.","Miller","Rivette et al.","Anthony","Ferguson et al.","Van Hoff","Bookman
        et al.","Snelling, Jr.","Anand et al.","Nadel et al.","Skarbo et al.","Fredenburg","Jovicic
        et al.","Pandit","Ferrel et al.","Reed et al.","Horvitz et al.","Yamabana","Knutson","Borovoy
        et al.","Nielsen","Brown et al.","Ferrel et al.","Christy","Ho","Vanechanos,
        Jr.","Nielsen","Wills","De La Huerga et al.","Miller et al.","Ramos et al.","Pace
        et al.","Gipson","Nakabayashi et al.","Reynolds","Miyasaka et al.","Madnnick
        et al.","Li","Krellenstein","Evans et al.","Milewski et al.","Zucknovich et
        al.","Miller et al.","Wical","Sotomayor","Liddy et al.","Nielsen et al.","Ebrahim","Beauregard
        et al.","Yokoyama et al.","Sugimura","Niwa et al.","Murai","Bliss et al.","Breese
        et al.","Poole et al.","Hayashi et al.","Horvitz et al.","Guinan","Rostoker
        et al.","Brown et al.","Conrad et al.","Hugh","Parks","Kumano et al.","Bennett
        et al.","Wical","Horvitz","Paik et al.","Tso","Horvitz","Rodkin et al.","Bowen
        et al.","Murakami et al.","Horowitz et al.","Ando","Ikeno","Zhilyaev","Teare
        et al.","Call","Saitoh","Horvitz","Snow et al.","McIntosh","Horvits et al.","Horvitz
        et al.","Horvitz et al.","De La Huerga","Lawrence et al.","Chan","De La Huerga","Dauerer
        et al.","Sheth et al.","Hedloy","Fields et al.","Nielsen","Beauregard et al.","Feldman
        et al.","De La Huerga","Bays et al.","Zien","Nosohara","Appelt et al.","White
        et al.","Perkowski","Spiegel et al.","Sundaresan et al.","Bowman-Amuah","Shanahan
        et al.","Andreoli et al.","Reynar et al.","Montero et al.","Bookman et al.","Knotz
        et al.","Hedloy","Bookman et al.","Reynar et al.","Evans et al.","Konno et
        al.","Malik","Imamura et al.","Brown","Jokela et al.","Chauhan et al.","Garner
        et al.","Pokorny et al.","Swindells et al.","Moore et al."],"volumeNumber":null,"correspondenceNameAddress":null,"correspondenceAddressCustomerNumber":null,"ibmtdbAccessionNumber":null,"inventorsName":["Bookman;
        Marc","Yamanaka; Brian"],"applicationKindCode":"E","inventorNameDerived":null,"intlPubClassificationClass":["G06F","G06F"],"issuedUsOrigClassification":"704/3","curCpcSubclassFull":["G09B"],"cpcCurAdditionalClass":null,"cpcCurInventiveClass":["G09B","G09B"],"cpcCurClassificationGroup":["G
        G09B G09B19/06 20130101 L I B H 20191011 US","G G09B G09B5/065 20130101 F
        I B H 20190822 US"],"curCpcClassificationFull":["G09B5/065 20130101","G09B19/06
        20130101"],"cpcCombinationClassificationCur":null,"cpcCombinationTallyCur":null,"intlFurtherClassification":null,"currentUsPatentClass":["345","704","707","715"],"idWithoutSolrPartition":"US-US-RE043633","curIntlPatentClassifictionPrimaryDateKwicHits":null,"curIntlPatentClssifSecHlights":null,"internationalClassificationInfom":null,"cpcOrigInventvClssifHlghts":null,"descriptionStart":10,"descriptionEnd":15,"publicationReferenceDocumentNumberOne":"RE043633"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 18 Jan 2023 17:22:46 GMT
      Server-Timing:
      - intid;desc=9f6a1bb42955e0f7
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: ''
version: 1
